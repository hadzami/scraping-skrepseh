penId,penNoDDC,penEntriUtama,penJudul,penDeskripsiFisik,penCatatanUmum,penKataKunci,penEntriTambahan,penBhsId,penJnspenId,penKota,penTahun,penPenerbit,penAbstraksi,penAbstract,tglEdit,userEdit,penTerimaId,penDigitasiId,penIjinPublikasiId,penPathCover,penCatatanBibliografi,penEntriTambahanKonprensi,penEntriTambahanOrang,penHalPendahuluan,penProdi,penProdiMigrasi,penAllowFulltext,penTglTambah,penUserIdTambah,penTglUbah,penUserIdUbah,penNoKlasifikasi,penTglTambah_etd2017,penUserIdTambah_etd2017,penTglUbah_etd2017,penUserIdUbah_etd2017,pbtId,pbtName,pbtKota,jnspenId,jnspenJenisPenelitian,prodiId,prodiFakKode,prodiNama
234694,,M AMRIDHAN MAHDI,Recyclable Waste Image Classification in Indonesia using Convolutional Neural Network,,,"Recyclable Waste, Image Classification, Convolutional Neural Network, VGG16","Afiahayati, S. kom., M.Cs., Ph.D",2,3,0,2024,1,"<p>Sampah merupakan permasalahan yang dihadapi oleh setiap negara. Jenis sampah dapat dibedakan berdasarkan bahan, tingkat bahaya, dan tempat atau luas sampah yang dihasilkan seperti sampah industri atau sampah rumah tangga dan juga sampah dapat dibedakan menjadi sampah organik dan anorganik. Untuk mengatasi permasalahan sampah di Indonesia, beberapa cara telah dilakukan, seperti pengelolaan sampah yang efektif, daur ulang sampah, dan pengurangan penggunaan sampah. Daur ulang sampah merupakan cara yang paling efektif dalam mengelola sampah. Oleh karena itu, kebutuhan akan klasifikasi sampah menjadi sangat penting terutama bagi rumah tangga dan sebagian besar perusahaan di Indonesia.</p><p>Klasifikasi sampah daur ulang Indonesia berbasis gambar berhasil dibuat dengan menggunakan Convolutional Neural Network (CNN). Dataset dibuat dengan mengumpulkan foto-foto sampah daur ulang Indonesia seperti plastik, logam, kaca, kertas, dan karton. Dataset yang diperoleh sekitar 180 - 220 gambar untuk setiap kelas, dengan total sekitar 975 gambar. Metode klasifikasi akan menggunakan VGG16 sebagai model dasar, yang telah dilatih sebelumnya pada kumpulan data gambar berskala besar yang disebut ImageNet.</p><p>Berdasarkan beberapa percobaan yang dilakukan seperti augmentasi dan menggunanakan beberapa layers, pelatihan dengan augmentasi dan beberapa layers berjalan dengan sangat baik dengan dataset yang dibuat yang dibuktikan oleh model, memperoleh akurasi 89%. Hasil yang diperoleh menunjukkan bahwa metode tersebut mampu mengklasifikasikan gambar sampah daur ulang yang ada di Indonesia.</p>","<p>Waste is a problem faced by every country. Types of waste can be divided based on material, level of hazard, and place or area of waste generated such as industrial waste or household waste and also waste can be divided into organic and inorganic waste. In order to overcome the waste problem in Indonesia, several methods have been carried out, such as effective waste management, waste recycling, and waste use reduction. Waste recycle is the most effective one in managing the waste. Therefore, the need of waste classification is really important especially for households and most companies in Indonesia.</p><p>Image-based classification of Indonesian recyclable waste successfully created by using Convolutional Neural Network (CNN). The dataset created by gathering photos of Indonesian recyclable waste such as plastic, metal, glass, paper, and cardboard. The dataset obtained around 180 - 220 images for each class, with the total about 975 images. The classification method will use VGG16 as a base model, pre-trained on large scale image dataset called ImageNet.</p><p>Based on the several experiments done such as augmentation and several layers, the training with augmentation and several layers executed very well with the proposed dataset that is proven by the model, scored on 89?curacy. The result obtained showed that the method is able to classify recyclable waste images in Indonesia.</p>",,,187912,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219648,,GENTHA WIBHI MIASA,IMAGE SEGMENTATION USING K-MEANS CLUSTERING ON BUCCAL MUCOSA CELL IMAGES,,,"Buccal Mucosa, K-Means Clustering, Segmentation, RMSE, PSNR  ","Afiahayati, S.Kom., M.Kom., Ph.D; Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2023,1,"DNA merupakan salah satu bagian penting dari tubuh manusia yang dapat mengalami kerusakan sehingga diperlukan deteksi dini. Kerusakan DNA dapat diukur dengan menggunakan Comet Assay sebagai salah satu cara yang paling sederhana yang sering dikenal dengan single cell gel electrophoresis (SCGE). Penelitian ini menggunakan citra Mukosa Buccal pada lapisan dalam bibir dan pipi, bagian dalam mulut yang bersentuhan dengan gigi yang berpotensi menjadi kanker. Dikarenakan data penelitian berupa gambar digital, maka penggunaan algoritma pada Comet Assay sangat diperlukan guna mendapatkan hasil yang bervariasi. 
Penelitian ini menggunakan 275 citra Mukosa Bukal. Citra tersebut akan disegmentasi menggunakan Algoritma K-Means Clustering, kemudian dievaluasi menggunakan RMSE dan PSNR untuk menentukan nilai K yang terbaik untuk mensegmentasi citra.
Hasil penelitian menunjukkan bahwa nilai RSME dan PSNR masing-masing memiliki rata-rata 1,95 dan 42,48. Nilai K terbaik untuk melakukan segmentasi citra adalah K sama dengan 5.
","DNA is one of essential part of human body that can be damaged thus an early detection is required. DNA damage can be measured using Comet Assay to measure a damaged DNA in simple way, also known as single cell gel electrophoresis (SCGE). This research used Buccal Mucosa images, the inner lining of the lips and cheeks, inside the mouth where they touch the teeth where a potential cancer is located. Comet assay is one standard method for detecting DNA damage. Since it comes in the form of digital image, thus the use of computer is important. Some algorithms can be used as method for detecting comet assay, and the result may vary.
This research uses 275 Buccal Mucosa images. Those images would be segmented using K-Means Clustering Algorithm, and then evaluated using RMSE and PSNR to decide which value of K is the best for segmenting images. 
The result shows that RSME and PSNR value has average 1.95 and 42.48 respectively. The best values of K for segmenting the images are K equal to 5.
",,,170818,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221460,,ADIKA CAFAIZI P,Remote Electronic Health Record Concept Testing with Webcam-Based Eye Tracking and Heatmap Algorithms,,,"electronic health record, concept testing, WebGazer.js, system usability scale, personas","Dr. Mardhani Riasetiawan, SE Ak, MT",2,3,0,2023,1,"Pengujian konsep (concept testing) merupakah sebuah praktik untuk memasukkan prinsip-prinsip desain suatu produk, terutama untuk menilai kekokohan ide. Objek yang diuji harus inklusif, termasuk Electronic Health Record (EHR). Literatur sebelumnya telah menunjukkan bahwa tanpa keterlibatan pengguna dalam tahap pengembangan atau bahkan tanpa pengujian sebelum operasi EHR setiap hari untuk tenaga kesehatan, efek buruk dari kelelahan (burnouts) pada dokter, risiko keselamatan pasien, dan peningkatan kematian telah diidentifikasi. Pengujian konsep pada objek yang diminati ini perlu dilakukan, karena dapat mengurangi risiko tersebut. Tes pengujian konsep sendiri terdiri dari dua pendekatan (1) kuantitatif dan (2) kualitatif. Kedua pendekatan ini melibatkan System Usability Scale (SUS), pendekatan pelacakan mata (eye tracking), Metode Think-Aloud, dan heatmaps berdasarkan data pelacakan mata. Pengujian konsep menggunakan pelacak mata library JavaScript, yang disebut WebGazer.js, yang berbasis webcam. Tujuannya di sini adalah untuk mencapai skor SUS &quot;rata-rata&quot;, yaitu enam puluh delapan. Ada 8 tenaga kesehatan yang bergabung sebagai partisipan dalam pengujian konsep jarak jauh ini: menilai tata letak dan memberikan pendapat profesional dan pribadi mereka. Saat pengujian dan SUS dilakukan, disimpulkan bahwa ada beberapa fitur di EHR yang diuji tidak berfungsi sebagaimana mestinya. Ini mendorong untuk mendesain ulang tata letak fitur-fitur tersebut. Bukan hanya deskripsi mereka, heatmaps juga mendukung analisis masalah apa yang dialami para partisipan. Karya kolaboratif data kualitatif dan kuantitatif ini memperkuat gagasan desain ulang berbasis data. Ilmu yang diterima juga mendukung terciptanya persona yang berguna untuk memahami kebutuhan calon pengguna. Pada akhirnya, skor SUS rata-rata untuk komponen yang diuji adalah 81,5625 atau dapat dikatakan &quot;sangat baik&quot;.","Concept testing is a practice to include the design principles of a product, especially to assess the soundness of the idea. The objects that are being tested should be inclusive, including electronic health records. Previous literature have shown that without any involvement of users in the development stage or even without testing before the EHR operation on a daily basis for healthcare professionals, adverse effects of burnouts to the physicians, patient safety is at risk, and increased mortality have been identified. Concept testing in this object of interest should be conducted, as it might reduce those risks. The concept testing test alone consists of two approaches (1) quantitative and (2) qualitative. These two approaches involve the System Usability Scale (SUS), eye tracking approach, the Think-Aloud Method, and heatmap based on the eye tracking data.  The concept testing uses an eye tracker via a library in JavaScript, called WebGazer.js, which is webcam-based. The aim here is to achieve the &quot;average&quot; SUS score, which is sixty-eight. There were 8 healthcare professionals joined as participants in this remote concept testing: assessing the layouts and giving their professional and personal opinion. As the testing and the SUS was conducted, it was concluded that there are some features in the tested EHR that do not work as intended. This pushes for a redesign of the layout. Not just their descriptions, but the heatmap also supports the analysis of what problems are the participants having. These collaborative works of qualitative and quantitative data solidify the idea of a data-driven redesign. The knowledge received also supports the creation of personas, which is useful to understand the needs of potential users.  In the end, the average SUS score for the tested component is 81.5625 or can be perceived as &quot;excellent&quot;.",,,172777,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223516,,ADAM IBNU FIADI,THE USE OF INTERACTIVE IPAD OS INTERFACE TO ASSIST 3-5 YEAR OLD INDONESIAN CHILDREN IN GAINING BASIC FUNDAMENTALS OF LITERACY AND NUMERACY ,,,"Education, Basic Fundamentals, Indonesian Children, Interactive, Interface, iPadOS, Literacy, Numeracy, Read, Count","Drs. Medi, M.Kom.",2,3,0,2023,1,"Saat ini, alat pembelajaran digital digunakan secara efektif di kelas untuk meningkatkan keterlibatan siswa, membantu guru membuat rencana pelajaran yang lebih baik, dan mempromosikan pengajaran individual. Sayangnya di Indonesia, sebaran teknologi yang dimanfaatkan untuk pendidikan tidak merata dan kurang interaktif. Hal ini juga berdampak signifikan pada pendidikan Indonesia, khususnya di bidang literasi dan numerasi yang menempati peringkat terbawah dari ECDI (BPS, 2018). Di sisi lain, anak Indonesia seharusnya sudah bisa membaca dan berhitung sebelum menginjak usia enam tahun atau masuk sekolah dasar. Hal ini menjadi masalah karena anak Indonesia diharapkan mampu membaca dan berhitung pada usia yang lebih muda dari usia sebenarnya.

Hasilnya, Tesis Solusi Inovatif ini bertujuan untuk membuat antarmuka iPadOS interaktif menggunakan animasi berbasis bingkai yang akan membantu memotivasi anak-anak Indonesia berusia 3 hingga 5 tahun untuk mempelajari dasar-dasar literasi dan numerasi. Untuk itu, penulis memulai dari skala kecil dengan menguji 10 anak beserta orang tuanya yang berdomisili di dekat tempat tinggal penulis di Tangerang Selatan. Solusi ini akan diselesaikan menggunakan metode CBL untuk meneliti masalah anak dan membuat aplikasi game iPadOS menggunakan arsitektur ECS sebagai algoritma utama.","Nowadays, digital learning tools are used effectively in the classroom to boost student engagement, assist teachers in creating better lesson plans, and promote individualized instruction. Unfortunately in Indonesia, the distribution of technology utilized for education is uneven and not interactive enough. This has a significant impact on Indonesian education as well, particularly in the areas of literacy and numeracy where it is ranked the lowest indicated of ECDI (BPS, 2018). On the other hand, Indonesian children ought to be expected to be able to read and count before they turn six or enter primary school. This is a problem due to the fact that Indonesian children are expected to be able to read and count at a younger age than they actually are.

As a result, the goal of this Innovative Solution Thesis is to create an interactive iPadOS interface using frame-based animation that will help motivate Indonesian children aged 3 to 5 learn the essentials of literacy and numeracy. In order to do so, the author started at a small scale by testing 10 children and their parents domiciled near the author&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s residence in Tangerang Selatan. This solution will be solved using the CBL method for researching children&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s problems and create an iPadOS game application using the ECS architecture as the main algorithm.
",,,174551,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223785,,MUHAMMAD GILAND M,Comparison of Incremental Machine Learning Algorithms for Human Detection,,,"Incremental Machine Learning, Human Detection, Incremental Support Vector Machines, Unified Classifier via Rebalancing, Dynamically Expandable Representation","Mhd. Reza M.I Pulungan, M.Sc., Dr.-Ing., Prof.",2,3,0,2023,1,"Karena tindakan mendeteksi manusia, baik untuk keamanan, identifikasi,
analisis kemacetan, dll., mulai dilakukan oleh mesin melalui penggunaan
pembelajaran mesin dan visi komputer, menerapkan algoritme pembelajaran yang
sesuai adalah langkah yang diperlukan untuk memastikan kelancaran sistem. Agar
sistem dapat mendeteksi manusia secara akurat, sejumlah besar data diperlukan
untuk diproses yang mungkin tidak dapat dilakukan dalam situasi tertentu seperti
tidak cukupnya daya pemrosesan dalam batas waktu atau tidak cukupnya data yang
tersedia untuk dilatih oleh sistem pada awalnya.
Untuk mengatasi kebutuhan untuk memproses data dalam jumlah besar
sekaligus, metode pembelajaran mesin inkremental dapat diimplementasikan di
mana sistem dapat dilatih secara bertahap sesuai dengan kebutuhan sistem, di mana
jika membutuhkan data tambahan, sistem dapat belajar tanpa harus melatihnya dari
awal, hal ini juga dapat diterapkan pada situasi di mana tidak ada data awal yang
mencukupi untuk dilatih. Metode pembelajaran inkremental yang diteliti dalam
penelitian ini yaitu: Incremental Support Vector Machines, Unified Classifier via
Rebalancing, dan Dynamically Expandable Representation dimana ketiganya akan
dibandingkan berdasarkan keakuratan dalam pendeteksian terhadap manusia,
running time, dan konsumsi memori.
Hasil dari penelitian ini menunjukkan bahwa DER memiliki performa
terbaik dalam hal akurasi dan penggunaan memori yang paling rendah, ISVM
memiliki waktu berjalan yang paling singkat namun menggunakan lebih banyak
memori, UCIR memiliki performa terburuk dengan konsumsi memori paling tinggi
dan akurasi yang paling rendah.
","As the act of detecting humans, whether for security, identification,
congestion analysis, etc., is starting to be done by machines through the usage of
machine learning and computer vision implementing suitable learning algorithms
is a necessary step to ensure a smooth system. For a system to be able to detect
humans accurately large amounts of data are needed to be processed which may not
be viable in certain situation such as not enough processing power within a time
constraint or not enough data is available for the system to initially train from.
To counteract the need to process large amounts of data at once incremental
machine learning methods may be implemented where the system can be trained in
batches according to the needs of the system, where if it needs additional data, it
can learn without having to train it all over again, this can also be applied to
situations where there is insufficient initial data to train from. The incremental
learning methods examined in this study are namely: Incremental Support Vector
Machines, Unified Classifier via Rebalancing, and Dynamically Expandable
Representation where they will be compared based on their accuracy on their
detection of humans, running time, and memory consumption.
The results of this study show that DER performed the best in terms of
accuracy and peak memory usage, ISVM had the shortest running time but used
more memory, UCIR performed the worst having the highest memory consumption
and lowest accuracy.
",,,174779,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219183,,FAIZ KHANSA ADRIKA,Two-Stage Steel Surface Defect Detection Using Convolutional Neural Network (CNN) Classification and U-net Segmentation,,,"steel defect detection, classification, segmentation, CNN, U-net","Agus Sihabuddin S.Si., M.Kom., Dr.",2,3,0,2023,1,"Deteksi kecacatan merupakan hal yang penting dilakukan dalam mengontrol kualitas produksi baja. Hal ini merupakan langkah untuk mencegah dan mengoreksi anomali pada baja agar dapat diteruskan ke tahap produksi selanjutnya. Salah satu solusi deteksi cacat yang paling banyak saat ini adalah dengan menggunakan deteksi objek atau segmentasi citra, namun mereka rawan dalam mendeteksi cacat pada citra yang tidak cacat. Penyaringan data adalah salah satu cara untuk mencegah kesalahan deteksi sehingga data yang tidak cacat tidak akan dikenali sebagai cacat.
Klasifikasi citra memiliki kemampuan untuk mengklasifikasi pada tingkat citra, yang dapat berfungsi sebagai mekanisme penyaringan untuk membuang data yang tidak cacat. Penelitian ini mengusulkan metode deteksi cacat baja dua tahap dengan memanfaatkan kombinasi klasifikasi biner dan model segmentasi. Tujuannya adalah untuk mengamati pengaruh pengklasifikasi gambar cacat tambahan terhadap kinerja deteksi cacat yang biasa digunakan menggunakan segmentasi gambar. Pengklasifikasi biner dirancang dengan arsitektur berbasis CNN MobileNetV2 dan tugas segmentasi dilakukan dengan model U-net. Performa akan diukur dengan membandingkan dice coefficient performa model segmentasi sendiri dengan performa pipeline 2 tahap
Pada penelitian ini, model MobileNet dengan akurasi validasi 85,41% dapat meningkatkan hasil prediksi cacat sebesar ~2%, dengan menggunakan 2-stage pipeline, dibandingkan dengan hasil yang hanya menggunakan model segmentasi saja. Salah satu perbandingan model menunjukkan peningkatan dice coefficient dari 82.253 menjadi 84.321.","Defect detection is one of the main task in steel production quality control. It is a step to prevent and correct anomalies in steel to be passed on to the next stage of production. The deterioration of steel is costly, as it could result in high risk of steel manufacturing failure, the price of defective steel is also significantly lower. One of the most solution in defects detection currently is by using object detection or image segmentation, but they are prone to detecting defects in non-defective images. Filtering the data is one way to prevent false-positives in detection so the non-defective data would not be recognized as defective. But it is a doubtful method, whether an additional filtering could improve the defect detection accuracy.
Image classification has the ability to classify in image-level, thus could serve as a filtering mechanism to discard the non-defective data. The research proposes a two-stage steel defect detection method by utilizing a combination of independent binary classification and segmentation model. The goal would be to observe the effect of an additional defect image classifier to the performance of commonly used defect detection using image segmentation. The binary classifier is designed with a CNN-based architecture MobileNetV2 and the segmentation task is done with U-net model. The performance will be measured by comparing the dice coefficient of a standalone model performance to the performance of the 2-stage pipeline
In this research, the MobileNet model with validation accuracy of 85,41% could improve the defect prediction result by ~2%, by using the 2-stage pipeline, compared to the result only using standalone segmentation model. One of the comparison shows the model increase the dice coefficient from 82.253 up to 84.321.",,,170426,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219446,,WAHYU WIJIYANTO,ANALISIS DAN OPTIMASI PENGALAMAN PENGGUNA PADA APLIKASI MOBILE SIMASTER UNIVERSITAS GADJAH MADA DENGAN METODE DESIGN THINKING,,,"Design Thinking, SIMASTER, System Usability Scale (SUS), User Experience (UX), User Interface (UI), Usability Testing","Agus Sihabuddin, S.Si., M.Kom., Dr.; Dzikri Rahadian Fudholi, S.Kom., M.Comp.",2,3,0,2023,1,"Era globalisasi yang saat ini terus berkembang memberikan dampak yang nyata pada berbagai aspek kehidupan manusia, salah satunya adalah semakin meningkatnya arus informasi sebagai akibat dari perkembangan sistem dan teknologi informasi yang sangat pesat. Universitas Gadjah Mada juga turut andil dalam pengembangan sistem dan teknologi informasi dengan meluncurkan aplikasi mobile dengan nama SIMASTER pada tahun 2017 lalu. Peluncuran aplikasi  mobile SIMASTER bertujuan untuk mengakomodasi peningkatan jumlah pengguna perangkat mobile di Indonesia serta untuk memenuhi kebutuhan pengguna yang mengedepankan kepraktisan dan mobilitas.

Namun, berdasarkan studi literatur yang telah penulis lakukan, peningkatan jumlah pengguna perangkat mobile ternyata tidak berbanding lurus dengan peningkatan jumlah pengguna aplikasi mobile SIMASTER. Selain itu, berdasarkan penelitian awal yang telah penulis lakukan terhadap 10 orang mahasiswa program sarjana Universitas Gadjah Mada, aplikasi mobile SIMASTER memiliki karakteristik pengalaman pengguna yang kurang baik pada aspek usable serta masih dapat ditingkatkan kualitasnya pada aspek useful dan enjoyable. Penulis juga menyimpulkan bahwa terdapat setidaknya 13 poin permasalahan dan kebutuhan pengguna aplikasi mobile SIMASTER yang terdiri dari 3 kategori, yaitu fitur, interaksi pengguna, dan desain visual. Hal ini tentu mengimplikasikan bahwa aplikasi mobile SIMASTER masih memiliki peluang pengembangan agar dapat memberikan pengalaman pengguna yang lebih memuaskan.

Dengan menggunakan metode design thinking, penulis melakukan analisis dan perancangan ulang terhadap desain antarmuka aplikasi mobile SIMASTER untuk kemudian diuji dengan metode usability testing, baik secara kuantitatif maupun kualitatif. Hasilnya, prototipe aplikasi mobile SIMASTER yang dirancang oleh penulis berhasil memberikan pengalaman pengguna yang lebih memuaskan dengan menghasilkan nilai completion rate sebesar 95,45%, nilai time based efficiency sebesar 0,19 goals/sec, nilai single ease question sebesar 6,73 dari 7, dan nilai system usability scale sebesar 90,75 dari 100.","The era of globalization which is currently growing has a big impact on various aspects of human life, one of them is the increasing flow of information as a result of the rapid development of information systems and technology. Universitas Gadjah Mada is also took part in the development of information systems and technology by launching a mobile application called SIMASTER in 2017. The SIMASTER mobile application aims to accommodate the increasing number of mobile device users in Indonesia, as well as to meet the users needs who prioritize practicality and mobility aspect.

However, based on the literature study that researcher have done, the increasing number of mobile device users is not directly proportional to the increasing number of SIMASTER mobile application users. In addition, based on the preliminary research that researcher have conducted on 10 undergraduate students of Universitas Gadjah Mada, the SIMASTER mobile application has a bad user experience characteristic on its usable aspect and still can improved on its useful and enjoyable aspect. It's also concludes that there are at least 13 items of problems and needs of  SIMASTER mobile application users which consists of 3 categories, that is feature, user interaction, and visual design. It's implies that the current SIMASTER mobile application still has a number of development opportunities in order to provide a better user experience.

Using the design thinking method, the authors conducted a deep analysis and redesigned the interface of SIMASTER mobile application. The prototype was tested using the usability testing method, both quantitatively and qualitatively. As a result, the prototype of SIMASTER mobile application succeeded in providing a better user experience by producing a completion rate score of 95,45%, a time-based efficiency score of 0,19 goals/sec, a single ease question score of 6,73 out of 7, and a system usability scale score of 90.75 out of 100.",,,170642,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223035,,DON RUI TORNADO ROSA,WEB BASED POINT OF SALES APPLICATION TO ASSIST DIGITALIZATION FOR SMALL TO MEDIUM SIZED BUSINESSES,,,"Key Words: Web Based Application, Point of Sales, Flask, MongoDB","Medi, Drs., M.Kom",2,3,0,2023,1,"Yogyakarta merupakan salah satu tujuan wisata populer di Indonesia yang membutuhkan beragam infrastruktur untuk mempertahankan populasinya dimana bisnis penyedia makanan dan minuman menyediakan pasokan pangan yang siap dikonsumsi pendatang dan lokal. Transaksi yang dilakukan bisnis-bisnis ini beragam dalam hal frekuensi dan jumlah dimana penggunaan kertas dan buku sebagai metode pencatatan. Metode ini beresiko kehilangan data dan potensi untuk berkembang, seperti yang telah Penulis hadapi sebelumnya di kesempatan pekerjaan sampingan dalam bidang ini.

Aplikasi Point of Sales (POS) merupakan kumpulan alat yang dapat digunakan untuk menyelesaikan masalah ini tetapi ada kemungkinan biaya yang tinggi. Di penelitian ini, beberapa bisnis akan diberikan proposal untuk dibuatkan POS. Fungsi manajemen inventori, aplikasi kasir, dan dashboard performa akan dibuat untuk memenuhi kebutuhan sesuai dengan kebutuhan yang dibutuhkan bisnis yang menerima. POS ini dibuat menggunakan metode linear waterfall dengan Flask dan MongoDB sebagai alat utamanya.","Yogyakarta is one of the popular tourist destinations in Indonesia where it requires infrastructures to maintain said tourists where food and beverage businesses provide supply of ready food to feed the tourists as well as locals. These businesses handle a varied number of transactions in terms of frequency and value utilizing paper and books as traditional means of record keeping. This method has high risk of losing data and eventually growth potential, as the author&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s experience in conducting part-time work in this sector.
Point of Sales (POS) application is a collection of tools that could be used in solving this issue that are already available today but could be relatively costly. In this research, multiple businesses would be given a proposal to be created a POS. Inventory management, cashiering application, and performance dashboard functionalities are created to fulfil the requirements as a discussion with the establishment that accepted. The POS is web-based and developed using linear waterfall methodology along with Flask and MongoDB as its tools.",,,174040,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221773,,PHILIPUS HANS C,A CNN-LSTM HYBRID APPROACH FOR GOLD PRICE FORECASTING,,,"Analytical Study, Algorithm Implementation, Error Measurement Tools, Gold Price Forecasting, Prediction Model","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2023,1,"Gold has been present since the beginning of time. Throughout history the use of gold has many forms of variations. In the current era, gold has been used as a sort of investment which has been deemed reliable. To obtain the most amount of profit in an investment, we need to be able to predict the possible values that might become a reality. In this research, we will explore the possibilities of algorithms that can be used to build a predictive model. 
CNN and LSTM are algorithms that can be used for creating predictions. CNN can be used on huge amounts of data however it can result in overfitting and imbalance of results while LSTM can produce a semi-accurate prediction but it&acirc;€™s processing time is very slow when used. By implementing a hybrid approach, we can try to reduce the negative effect of both algorithms as well as limit the overfitting and imbalance while keeping a decent processing speed. The hybrid model used is a series type where the two algorithms will be used simultaneously using the result of the first as an input for the second. In this case, CNN will be used first to process the large dataset and the result will then be inputted as input data for the LSTM. The evaluation criteria for this hybrid involve the use of, RMSE, MAPE, and RMAE which allows us to see the extent of the current model&acirc;€™s results.
The result of the training showed that the hybrid model showed the best outcome. With an average error of 3.6% for USD currency and 4.6% for IDR currency. The outcome presented by the prediction dataset further proves the versatility of the created model which are then used to test the validation data which gave an error value of 10.4%. With a certain condition applied, it was considered to be validated and can then be used for comparison purposes. Using the same model to be used for comparison dataset, with the parameters based off of previous research, we obtain an error value of 11.75% which is was proven to have been better by 0.61% compared to the original result which was 12.66%. This showed slight improvement by implementing the hybrid model.","Gold has been present since the beginning of time. Throughout history the use of gold has many forms of variations. In the current era, gold has been used as a sort of investment which has been deemed reliable. To obtain the most amount of profit in an investment, we need to be able to predict the possible values that might become a reality. In this research, we will explore the possibilities of algorithms that can be used to build a predictive model. 
CNN and LSTM are algorithms that can be used for creating predictions. CNN can be used on huge amounts of data however it can result in overfitting and imbalance of results while LSTM can produce a semi-accurate prediction but it&acirc;€™s processing time is very slow when used. By implementing a hybrid approach, we can try to reduce the negative effect of both algorithms as well as limit the overfitting and imbalance while keeping a decent processing speed. The hybrid model used is a series type where the two algorithms will be used simultaneously using the result of the first as an input for the second. In this case, CNN will be used first to process the large dataset and the result will then be inputted as input data for the LSTM. The evaluation criteria for this hybrid involve the use of, RMSE, MAPE, and RMAE which allows us to see the extent of the current model&acirc;€™s results.
The result of the training showed that the hybrid model showed the best outcome. With an average error of 3.6% for USD currency and 4.6% for IDR currency. The outcome presented by the prediction dataset further proves the versatility of the created model which are then used to test the validation data which gave an error value of 10.4%. With a certain condition applied, it was considered to be validated and can then be used for comparison purposes. Using the same model to be used for comparison dataset, with the parameters based off of previous research, we obtain an error value of 11.75% which is was proven to have been better by 0.61% compared to the original result which was 12.66%. This showed slight improvement by implementing the hybrid model.
",,,172825,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219731,,MUHAMMAD FAQIH AMRU,Komparasi Algoritma Apriori dan FP-Growth untuk Mencari Kombinasi Kartu pada Gim Clash Royale,,,"Clash Royale,data mining,association rule mining,Apriori,FP-Growth","Medi, Drs., M.Kom",2,3,0,2023,1,"Clash Royale merupakan salah satu gim yang populer bahkan sudah memiliki e-sport. Clash Royale sendiri merupakan sebuah gim kartu multiplayer di mana setiap pemain menyusun sebuah dek berisi 8 kartu. Saat ini sudah terdapat lebih dari 100 kartu yang bisa dikombinasikan, sehingga akan ada banyak dek yang bisa disusun. Kombinasi kartu ini bisa dianggap sebagai sebuah association rule yang bisa dicari dengan menggunakan data mining.
Dua algortima untuk mencari association rule di antaranya yaitu Apriori dan FP-Growth. Kedua algortima ini memiliki metode yang berbeda di dalam mencari association rule. Peneliti melakukan analisa untuk mengetahui algoritma mana yang memiliki performa yang lebih baik dalam mencari association rule pada dataset dek Clash Royale. Performa yang dicari yaitu waktu eksekusi dan penggunaan memori ketika program untuk algoritma dijalankan. Penelitian juga dilakukan untuk membandingkan jumlah association rule yang dihasilkan oleh kedua algoritma.
Hasil analisa dari penelitian ini menunjukkan bahwa algoritma FP-Growth secara umum memiliki performa yang lebih baik dibandingkan algoritma Apriori. Algoritma FP-Growth memiliki waktu eksekusi yang lebih cepat daripada algoritma Apriori. Secara rata-rata, algoritma FP-Growth 10518,3% atau sekitar 10 kali lebih cepat dibandingkan dengan algoritma Apriori. Hasil analisa juga menunjukkan bahwa tidak ada perbedaan yang jauh dalam penggunaan memori kedua algoritma. Algoritma FP-Growth menggunakan sedikit lebih banyak memori dibandingkan Apriori. Secara rata-rata, algoritma FP-Growth memakai memori 54,94% lebih banyak dibandingkan dengan algoritma Apiori. Selain itu dengan metode pencarian association rule yang berbeda, baik algoritma Apriori dan FP-Growth menghasilkan data association rule yang sama.","Clash Royale is one of the popular games which already has an e-sport. Clash Royale itself is a multiplayer card game where each player set a deck consisting of 8 cards. Currently, there are already  100 cards that can be combined so there are a lot of decks that can be constructed. This card combination can be called an association rule that can be obtained using data mining.
The two algorithms for finding association rules are Apriori and FP-Growth. These algorithms have different methods for finding association rules. The researcher analysed to find out which algorithm has better performance in finding association rules in the Clash Royale deck dataset. The performance sought is the execution time and memory usage when the program for the algorithm is executed. The research was also conducted to compare the number of association rules generated by the two algorithms.
The results of the analysis of this study indicate that the FP-Growth algorithm in general has better performance than the Apriori algorithm. The FP-Growth algorithm has a faster execution time than the Apriori algorithm. On average, the FP-Growth algorithm is 10518.3% or about 10 times faster than the Apriori algorithm. The results of the analysis also show that there is not much difference in the memory usage of the two algorithms. The FP-Growth algorithm uses slightly more memory that Apriori. On average, the FP-Growth algorithm uses 54.94% more memory compared to the Apiori algorithm. In addition, with different association rule search methods, both of the Apriori and FP-Growth algorithms generate the same association rule data.",,,170923,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219735,,SANGALABROR PUJIANTO,The Development of Face Recognition System in Computer Vision Using Deep Learning: Deep Siamese Networks Method,,,"Facial Recognition, Deep Learning, Siamese Neural Network, Computer Vision. ","Suprapto, Drs, M.Kom., Dr.",2,3,0,2023,1,"Penggunaan pengenalan biometrik telah sangat menonjol dan semakin menjadi teknologi yang signifikan dalam beberapa tahun terakhir. Secara khusus, penggunaan sistem pengenalan wajah telah diakui sebagai penggunaan keamanan modern, sistem bisnis, dan media sosial yang paling nyaman dan andal. Penggunaan populer dari teknologi ini adalah teknologi FaceID Apple untuk membuka kunci ponsel cerdas mereka, iPhone dimulai dengan iPhone X pada tahun 2018.
Pada penelitian ini akan dikembangkan model sistem pengenalan wajah menggunakan deep learning, model dibangun dengan jaringan syaraf tiruan siam. Model akan dikembangkan sebagai aplikasi prototipe dimana pengguna dapat menguji model dan menggunakan wajah mereka untuk dikenali oleh sistem. Model dilatih dengan gambar pengguna dan membandingkannya dengan gambar sampel negatif dari kumpulan data online (Faces in the Wild). Penelitian kemudian akan menggunakan model untuk mendapatkan hasil akurasi dengan membandingkan bagaimana gambar pengguna diverifikasi (mirip) dengan ambang batas 80% agar model dapat mengenali dan memverifikasi wajah pengguna. Hasil akurasi tersebut kemudian akan dibandingkan dengan penelitian lain dengan metode pengenalan wajah yang berbeda.
Penelitian ini telah berhasil mengembangkan sistem pengenalan wajah menggunakan jaringan saraf siam dan memperoleh hasil akurasi sebesar 98,7%, lebih tinggi dibandingkan dengan hasil peneliti lain dengan metode lain. Hasil akurasi ditentukan untuk mengukur efektivitas dan efisiensi sistem dalam mengenali wajah pengguna. Dengan hasil ini penelitian telah membangun sebuah model yang terbukti lebih akurat dibandingkan dengan metode lain seperti WEKA library, Eigen Faces, Fisher Faces, dan metode Pearson Correlation untuk pengenalan wajah.","The use of biometric recognition has been very prominent and increasingly a significant technology in the recent years. Specifically, the use of facial recognition system has been recognized to be the most convenient and reliable use of modern security, business systems and social media. A popular use of this technology would be Apple's FaceID technology to unlock their smartphone the iPhone started with iPhone X in 2018. 
In this research, a model of facial recognition system will be developed using deep learning, the model is built with the Siamese neural networks. The model will be developed as a prototype application to which users can test the model and use their face to be recognized by the system. The model is trained with images of the users and comparing it to the images of negative samples from an online dataset (Faces in the Wild). The research will then use the model to obtain accuracy results by comparing how images of the users are verified (similar) with a threshold of 80% for the model to recognize and verify the face of the user. The accuracy results will then be compared to other research with different methods of facial recognition. 
This research has successfully developed a facial recognition system using the Siamese neural network and has obtained an accuracy result of 98.7%, which is higher compared to other researchers results with other methods. The accuracy results determined to measure the system's effectiveness and efficiency to recognize the face of the user. With this result the research has built a model that is proven to be more accurate than other methods such as WEKA library, Eigen Faces, Fisher Faces, and Pearson Correlation method for facial recognition. 
",,,171473,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219484,,KHAIRUL HASHFI M,PERBANDINGAN UNJUK KERJA ANTARA NOMAD DAN KUBERNETES DALAM PENERAPAN INTERPLANETARY FILE SYSTEM SEBAGAI CONTAINER,,,"IPFS, Kubernetes, Nomad, latency, throughput.","Khabib Mustofa, S.Si., M.Kom., Dr. techn.",2,3,0,2023,1,"Cloud computing memunculkan container yang mampu mengemas aplikasi menjadi berukuran lebih kecil. Sistem File Terdistribusi (DFS) juga terpengaruh karena berfungsi sebagai penyimpanan data aplikasi berbasis container, salah satunya adalah Interplanetary File System (IPFS).
IPFS menggunakan metode Peer-to-Peer yang mempartisi data dan mendistribusikan data kepada tiap IPFS Node. Namun, IPFS dapat mengalami penurunan unjuk kerja secara drastis karena masalah skalabilitas.
Demi mengatasi masalah tersebut, IPFS dapat dibuat sebagai container dan ditempatkan dalam container orchestration seperti Kubernetes yang berfungsi mengatur IPFS berbasis container. Selain Kubernetes, terdapat Nomad yang memiliki sumber daya kecil dan lebih cepat dari sisi penyediaan service. Namun, belum ada penjelasan unjuk kerja IPFS di dalam Kubernetes dan Nomad, padahal pengembang atau peneliti membutuhkan penjelasan unjuk kerja IPFS sebagai bahan pertimbangan sebelum menggunakan IPFS di dalam container orchestration.
Penelitian ini bertujuan untuk mengimplementasikan IPFS ke dalam Kubernetes dan Nomad, serta melakukan perbandingan unjuk kerja latency dan throughput menggunakan file 1MB, 16MB, 64MB dan operasi ADD/GET.
Hasil analisa statistik menunjukkan nilai p-value &lt; 0.05 yang berarti terdapat perbedaan signifikan antara Kubernetes dan Nomad. Nomad terbukti memiliki latency lebih rendah dan throughput lebih tinggi dibandingkan Kubernetes.","Cloud computing gives rise to containers that are able to package applications into smaller sizes. Distributed File Systems (DFS) are also affected because they function as data storage for container-based applications, one of which is the Interplanetary File System (IPFS).
IPFS uses a Peer-to-Peer method that partitions data and distributes data to each IPFS Node. However, IPFS can experience drastic performance degradation due to scalability issues.
In order to overcome this problem, IPFS can be containerized and placed in a container orchestration such as Kubernetes that manages container-based IPFS. Besides Kubernetes, there is Nomad which has small resources and is faster in terms of service provisioning. However, there is no explanation of IPFS performance in Kubernetes and Nomad, whereas developers or researchers need an explanation of IPFS performance as a consideration before using IPFS in container orchestration.
This research aims to implement IPFS into Kubernetes and Nomad, and compare latency and throughput performance using 1MB, 16MB, 64MB files and ADD/GET operations.
The statistical analysis results show a p-value &lt; 0.05 which means there is a significant difference between Kubernetes and Nomad. Nomad is proven to have lower latency and higher throughput than Kubernetes.",,,170680,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221288,,ALEXIUS ADHITYA K,Analisis Performa Swap zram dan Swap Disk Terhadap Beban Kerja Container Image Build,,,"container image build, kontainer, memori swap disk, memori swap zram, pengujian performa","Mardhani Riasetiawan, SE., Akt., MT., Dr.",2,3,0,2023,1,"	Container image build adalah proses membuat container image yang berisi salinan sistem operasi dan aplikasi yang disusun dalam layered filesystem. Peningkatan kompleksitas dan frekuensi pengembangan container image dapat menyebabkan container image build melambat. Optimasi sumber daya menjadi solusi untuk mempercepat container image build, salah satunya memori swap. Memori swap berfungsi sebagai memori tambahan dengan menggunakan disk yang lambat atau memori terkompresi (zram) yang secara teori lebih cepat.

	Penelitian ini menganalisis performa memori swap zram dan swap disk dengan pengujian performa. Pengujian performa menjalankan container image build sebanyak 3 kali di 4 kapasitas memori dan 4 parameter jumlah build paralel. Setiap container image build diatur untuk membuat 40 container images. Pengujian performa mencatat durasi container image build, jumlah build sukses, dan metrics performa sistem.

	Hasil penelitian menunjukkan bahwa container image build berjalan lebih cepat sekitar 26 detik di memori swap zram daripada di memori swap disk. Keluaran container image build di memori swap disk lebih banyak sekitar 2,1 images daripada di memori swap zram. Kedua memori swap tidak memiliki perbedaan signifikan dalam utilisasi sumber daya sistem.","	Container image build is the process to create a container image that consists of operating system and application copies in a layered file system. The increase in complexity and development frequency of container images can cause the container image build to slow down. Resource optimization becomes a solution to speed up the container image build, one of the targets is swap memory. Swap memory is used as an extra memory by using a disk which is slow or a compressed memory (zram) which is faster on paper.

	This research is analyzing the performance of zram swap memory and disk swap memory with performance testing. The performance testing runs container image build 3 times in 4 different number of memory capacity and 4 parameters of number of parallel build. Each container image build is set to build 40 container images. The performance testing collects the container image build duration, number of successful build, and system performance metrics.

	The result concludes that container image build runs 26 second faster on average in zram swap memory than in disk swap memory. Container image build in disk swap memory builds around 2,1 images more than in zram swap memory. Both swap memory are not different significantly in system resource utilization.",,,172431,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221050,,ALFIKRI ZEIN MUBARAK,Predicting Soybeans Yield In Central Java Using Climatic Properties And Average Precipitation Through Neural Network.,,,"Agriculture, Machine Learning, Prediction, Multi-Layered Perceptron, Artificial Neural Network, Soybeans","Azhari, Drs., M.T., Dr",2,3,0,2023,1,"Soybeans is one of the most consumable commodities behind rice and maize in Indonesia. The Indonesian government has tried multiple times to be self-sufficient in their soybeans production but to no avail and to make up for the deficit in supplies, they rely on imports. There have been several research on predicting the crops yield production using machine learning and as such this research will try to emulate it on the soybeans yield production in Indonesia, particularly in Central Java, as it is where one of the most influential soybean productions is in Grobogan, Central Java.
The research will use the climatic properties such as the temperature, humidity, and the solar radiation exposure alongside the production of the previous year and the monthly precipitation as the input for the machine learning model. Datasets are obtained from NASA's LaRC POWER project for climatic dataset, and various Indonesian government agencies for the soybean's dataset.
The machine learning methods that are used in this research are Multi Layered Perceptron (MLP) and Multiple Linear Regression (MLR), with MLP being able to outperform MLR with a testing RMSE value of 0.239, whereas MLR testing RMSE value of 0.385.
","Soybeans is one of the most consumable commodities behind rice and maize in Indonesia. The Indonesian government has tried multiple times to be self-sufficient in their soybeans production but to no avail and to make up for the deficit in supplies, they rely on imports. There have been several research on predicting the crops yield production using machine learning and as such this research will try to emulate it on the soybeans yield production in Indonesia, particularly in Central Java, as it is where one of the most influential soybean productions is in Grobogan, Central Java.
The research will use the climatic properties such as the temperature, humidity, and the solar radiation exposure alongside the production of the previous year and the monthly precipitation as the input for the machine learning model. Datasets are obtained from NASA's LaRC POWER project for climatic dataset, and various Indonesian government agencies for the soybean's dataset.
The machine learning methods that are used in this research are Multi Layered Perceptron (MLP) and Multiple Linear Regression (MLR), with MLP being able to outperform MLR with a testing RMSE value of 0.239, whereas MLR testing RMSE value of 0.385.",,,172286,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
222085,,SARAH FATRIA MUMTAZ,Perbandingan Performa Beberapa Metode Rekomendasi Lokasi,,,"perbandingan metode, rekomendasi lokasi, rantai Markov aditif, gravity model, teknik sekuensial, teknik sosial, teknik geografi","Mhd. Reza M.I Pulungan, M.Sc., Dr.-Ing., Prof.",2,3,0,2023,1,"Rekomendasi lokasi adalah salah satu fitur dalam Jaringan Geososial untuk memberikan rekomendasi-rekomendasi lokasi kepada penggunanya. Rekomendasi lokasi menjadi penting karena keingintahuan pengguna akan lokasi baru untuk dikunjungi sama besarnya dengan sikap konservatif pengguna untuk mengunjungi lokasi yang sudah pernah dikunjungi. Rekomendasi lokasi terus mengalami perkembangan dan juga bermunculan metode baru. Banyaknya metode yang tersedia memberi banyak pilihan saat membangun suatu rekomendasi lokasi. Berkaitan dengan hal tersebut, diperlukan suatu perbandingan untuk menentukan metode rekomendasi lokasi yang paling tepat.
Penelitian ini membandingkan akurasi dan rata-rata lama waktu yang dibutuhkan untuk memberikan rekomendasi lokasi kepada seorang pengguna dari metode yang memanfaatkan teknik sekuensial saja; metode yang memanfaatkan gabungan teknik sekuensial, sosial, dan geografi; dan juga metode yang memanfaatkan gabungan teknik sekuensial dan sosial. Selain itu, ketiga metode tersebut sama-sama memanfaatkan rantai Markov aditif, namun dalam salah satu penelitian, rantai Markov aditif tersebut dimodelkan menjadi gravity model. Perbandingan akurasi dilakukan dengan mambandingkan nilai precision, recall, dan F1-score.
Hasil penelitian ini menunjukkan bahwa metode yang hanya memanfaatkan teknik sekuensial menghasilkan nilai precision, recall, dan F1-score terbaik dan run-time tersingkat di kedua dataset ketika jumlah lokasi yang direkomendasikan adalah di bawah 10. Metode yang memanfaatkan gabungan teknik sekuensial, sosial, dan geografi menempati peringkat kedua untuk nilai precision, recall, dan F1-score namun memiliki run-time terlama di dataset Gowalla. Kemudian, untuk metode yang memanfaatkan teknik sekuensial dan sosial memiliki nilai precision, recall, dan F1-score terrendah dan run-time tertinggi.","Location recommendation is one of the features in the Geosocial Network to provide location recommendations to its users. Location recommendation is important because the user's curiosity about visiting new locations is as great as the user's conservative attitude towards visiting visited locations. Location recommendations continue to develop and new methods emerge. As many methods being available, that provides many choices when building a recommendation system. In this regard, a comparison is needed to determine the most appropriate location recommendation method.
This study compares the accuracy and average length of run-time needed to provide a location recommendation to a user from a method that only utilizes sequential techniques; a method that utilizes a combination of sequential, social, and geographic techniques; and also a method that utilizes a combination of sequential and social techniques. In addition, the three methods utilize additive Markov chains, but in one of the studies, the additive Markov chain was modeled into a gravity model. Accuracy comparisons are made by comparing the values of precision, recall, and F1-score.
The results of this study indicate that the method that only utilizes sequential techniques produces the best precision, recall, and F1-score values and the shortest run-time in both datasets when the number of recommended locations is below 10. The method that utilizes a combination of sequential, social, and geography ranks second for precision, recall, and F1-score but has the longest run-time in the Gowalla dataset. Then, the method that utilizes sequential and social techniques has the worst precision, recall, and F1-score and the longest run-time.",,,173116,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
220557,,DEWA DWI AL-MATIN,Decision Support System for True Wireless Stereo Earbuds Selection using Analytical Hierarchy Process and TOPSIS,,,"Sistem Pendukung Keputusan, True Wireless Stereo Earbuds, Analytical Hierarchy Process, Technique for Order Preference by Similarity to Ideal Solution","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.",2,3,0,2023,1,"Earbuds nirkabel menjadi semakin populer karena fitur-fitur praktis yang dimilikinya. Dengan berbagai kemajuan, memilih earbuds nirkabel yang tepat dapat menjadi keputusan yang sulit, oleh karena itu, Sistem Pendukung Keputusan (SPK) dapat berguna. SPK adalah sistem interaktif yang membantu pengguna untuk membuat keputusan dengan menggunakan data dan model keputusan untuk menyelesaikan masalah tidak terstruktur atau semi-terstruktur. Dalam penelitian ini, SPK menggunakan kombinasi dari Analytical Hierarchy Process (AHP) dan Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) akan dikembangkan. Data untuk spesifikasi earbuds akan dikumpulkan dari situs web resmi mereka dan menjadi alternatif dalam sistem ini. Penelitian ini berhasil membangun sistem pendukung keputusan berbasis web untuk True Wireless Stereo Earbuds dalam bentuk prototipe yang berfungsi menggunakan kombinasi HTML dan PHP untuk sistem front-end dan back-end. Sistem ini mengeksekusi semua persyaratan tanpa masalah, memiliki tingkat konsistensi 100%, dan mencetak 84.75 pada System Usability Scale.","Wireless earbuds are becoming increasingly popular due to their practical features. With a wide range of advancements, choosing the right wireless earbuds can be a difficult decision, which is why a Decision Support System (DSS) can be useful. DSS is an interactive system that helps users make decisions by using data and decision models to solve unstructured or semi-structured problems. In this study, a DSS using the combination of Analytical Hierarchy Process (AHP) and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) method will be developed. The data for the earbuds specifications will be gathered from their official websites and be the alternatives for this system. This research successfully built a web- based decision support system for True Wireless Stereo Earbuds in the form of a working prototype using a combination of HTML and PHP for both front-end and back-end systems. The system executed all of the requirements without a problem, had a consistency rate of 100%, and scored 84.75 on the System Usability Scale.
",,,171778,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221071,,FELIX OGUSTINO,PERINGKASAN BERITA BERBAHASA INDONESIA SECARA ABSTRAKTIF DENGAN REPRESENTASI INPUT GABUNGAN WORD EMBEDDING DAN POS TAG MENGGUNAKAN ENCODER-DECODER GATED RECURRENT UNIT,,,"Peringkasan Teks Otomatis, Abstraktif, GRU","Agus Sihabuddin, S.Si., M.Kom., Dr; Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2023,1,"Data teks merupakan salah satu data yang mudah ditemukan karena ketersediaannya secara publik dan jumlahnya yang banyak. Salah satu contoh data teks yang tersedia secara publik dan jumlahnya banyak adalah data dalam bentuk artikel berita daring. Ketersediaan artikel berita yang banyak membuat pembaca bisa membaca berita dengan berbagai macam kategori dan sumber. Akan tetapi,
artikel berita yang diterbitkan sangat banyak setiap harinya, sehingga dibutuhkan waktu yang lama jika ingin mengekstrak informasi dari banyak artikel berita tersebut.
Peringkasan teks otomatis merupakan salah satu alternatif cara yang dapat membantu dalam menghadapi permasalahan tersebut.

Penelitian ini membuat suatu peringkasan berita berbahasa indonesia secara abstraktif dengan representasi input gabungan word embedding dan POS Tag menggunakan Encoder-Decoder Gated Recurrent Unit pada dataset artikel berbahasa Indonesia. Untuk mengevaluasi performa dari metode tersebut, akan digunakan metode ROUGE untuk membandingkan kualitas ringkasan yang dibuat secara
otomatis dengan ringkasan yang dibuat manual oleh manusia. Berdasarkan pengujian yang telah dilakukan, model memperoleh skor fmeasure ROUGE-1 sebesar 9,63%, ROUGE-2 sebesar 0,10%, dan ROUGE-L sebesar 8,46% pada skenario pengujian dengan representasi input hanya word embedding saja. Untuk skenario pengujian dengan tambahan representasi input POS Tag, model memperoleh skor f-measure ROUGE-1 sebesar 10,72%, ROUGE-2 sebesar 0,16%, dan ROUGE-L sebesar 9,47%.","Text is one of the most common types of data that is easy to find because of its public availability and high volume. Online news articles are one example of text data that is widely accessible and available in large quantities. The availability of news articles allows readers to read news from various categories and sources. However, a large number of news articles are published every day, making the process of extracting information from numerous news articles time-consuming. The use of automatic text summarization is an alternative approach that can be used to addres these issues.

This study uses a combined word embedding and POS Tag input representation with Encoder-Decoder Gated Recurrent Unit to construct an Abstractive Summarization of Indonesian News to process a dataset that consists
of Indonesian news articles. The ROUGE approach will be used to compare the quality of the summary produced automatically with the summary produced manually
in order to assess the effectiveness of the method.
Based on the result of the evaluation, the model get f-measure score 9.63% for ROUGE-1, 0.10% for ROUGE-2, and 8.46% for ROUGE-L in the testing scenario using only word embeddings as input representation. When POS Tag input representation was added in the testing scenario, the model get f-measure score 10.72% for ROUGE-1, 0.16% for ROUGE-2, and 9.47% for ROUGE-L.",,,172618,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
220817,,FARHAN ELMANSYAH S,Smoke Test Automation User Interface On Simaster Universitas Gadjah Mada ,,,"Katalon Studio, Automation User Interface, Smoke Test","Erwin Eko Wahyudi, S.Kom., M.Cs.",2,3,0,2023,1,"Saat ini banyak software tester dan quality assurance yang banyak menggunakan software atau modul untuk menjalankan sistemnya. Biasanya, sistem itu sendiri memiliki banyak langkah untuk dijalankan hingga mendapatkan hasil akhir. Dari sistem tersebut siswa dapat mengerjakan tugasnya (misalnya mengecek jadwal pelajaran, membuat kartu rencana studi atau bahkan menyerahkan pekerjaan rumahnya). Penulis perlu melakukan penelitian ini karena dengan melakukan penelitian ini dapat membantu tim pengembangan dan penjaminan mutu untuk menguji sistem sebelum diluncurkan ke publik. Penelitian ini akan menggunakan simaster sebagai sistem dasar karena ini adalah sistem terdekat yang peneliti gunakan untuk kuliah.
Dengan semua yang disebutkan, dalam penelitian ini, antarmuka pengguna otomasi akan dibangun yang akan membantu penguji untuk menjalankan sistem tanpa menambah beban kerja mereka. Penelitian ini akan membandingkan pengujian otomatis dengan pengujian manual. Perbandingan pada penelitian ini akan difokuskan pada seberapa cepat tes otomatis dibandingkan dengan tes manual. Sistem akan dibangun menggunakan Katalon Studio sebagai aplikasi yang dapat membangun dan menjalankan otomatisasi user interface. Sistem akan menggunakan metode uji asap untuk melakukan pengujian.
Hasil dari penelitian ini adalah pengujian otomatis terbukti lebih cepat dibandingkan pengujian manual dengan rata-rata 9,88% lebih cepat pada pengujian otomatis dibandingkan pengujian manual.","Nowadays, many software tester and quality assurance use a lot of software or module to run their system. Usually, the system itself have a lot of steps to run until they got the final result. From the system the students can do their work (e.g. check the class schedule, making their study plan cards or even submit their homework). The author needs to do this research because by doing this research, it can help the development and quality assurance team to test the system before rollout to the public. This research will use simaster as the base system because this is the closest system that the researcher used for college.
	With all that being mentioned, in this research, an automation user interface will be build which going to help the tester to run the system without adding their workload. This research will be comparing the automated testing with manual testing. The comparison on this research will be focus on how much faster the automated test compare to manual test. The system will be build using Katalon Studio as the application who can build and run automation user interface. The system will use smoke test method to do the testing.
The result of this research is the automated testing proof to be faster than using manual testing with an average of 9.88% faster on automated than manual test.
",,,172054,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
222099,,MUHAMMAD AZRIEL W,Analisis Sentimen Aplikasi Berbasis Aspek pada aplikasi Code Atma Menggunakan LDA dan SVM,,,"Topik, Sentimen, Aplikasi Permainan, Ulasan, SVM, LDA","Medi, Drs., M.Kom",2,3,0,2023,1,"Aplikasi permainan merupakan salah satu bisnis pada bidang teknologi yang menjanjikan. Untuk dapat terus mengembangkan suatu aplikasi permainan, dapat dilakukan analisa pada ulasan pengguna aplikasi tersebut. Dalam ulasan terdapat topik tertentu dan nilai polaritas sentimennya.
Pada penelitian ini dilakukan analisis sentimen berbasis aspek untuk mengetahui sentimen dan aspek yang terdapat pada ulasan pengguna aplikasi permainan Code Atma. Pemodelan topik dilakukan menggunakan metode LDA dengan pembobotan kata untuk pembuatan nilai vektor ulasan menggunakan TF-IDF. Pengklasifikasian sentimen menggunakan metode SVM dengan penerapan hyperparameter tunning menggunakan GridSearchCV untuk mendapatkan hasil yang lebih baik. Data penelitian ini diambil dari Google Play Store.
Dari hasil penelitian, berhasil dilakukan pemodelan topik dari metode LDA dengan skor koherensi Umass -11,34 dan perplexity 320.25 yang menghasilkan 5 aspek. Penelitian juga berhasil membuat model klasifikasi dari metode SVM yang mampu mendeteksi sentimen ulasan di setiap aspek aplikasi, dengan performa yang cukup baik, yakni dengan rata-rata skor dari seluruh model klasifikasi dari hasil pengujian yang memiliki skor f1-score 90,31% dan akurasi 83,25%. Sementara itu, hasil analisis menunjukkan bahwa mayoritas sentimen bernilai positif di seluruh aspek aplikasi terutama pada aspek bahasa atau cerita dan karya bangsa yang menandakan pengguna menyukai pembawaan tema dan budaya Indonesia dalam aplikasi permainan.","Game application is one of the most promising businesses in the technology sector. To be able to continue to develop a game application, an analysis of user reviews of the application can be carried out. In the review there is a certain topic and the value of the polarity of the sentiment.
In this study, an aspect-based sentiment analysis was carried out to find out the sentiments and aspects contained in user reviews of the Code Atma game application. Topic modelling was carried out using the LDA method with word weighting using TF-IDF. Sentiment classification uses the SVM method with the implementation of hyperparameter tunning using GridSearchCV to get better results. This research data is taken from the Google Play Store.
From the research, topic modeling was successfully carried out using the LDA method with a Umass coherence score of -11.34 and a perplexity of 320.25 which resulted in 5 aspects. The research also succeeded in making a classification model of the SVM method which is able to detect review sentiment in every aspect of the application, with good performance, namely with the average score of all classification models from the test results which have an f1-score of 90.31% and 83.25% accuracy. Meanwhile, the results of the analysis show that the majority of sentiments are positive in all aspects of the application, especially in aspects of language or stories and national creations which indicate that users like the presentation of Indonesian themes and culture in game applications.",,,173068,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219285,,RAPHAEL DISCKY ZUNDRIA PUTRA,Pembangkitan Deskripsi Gambar dalam Bahasa Indonesia Menggunakan Pendekatan Berbasis Transformer,,,"Pembangkitan Deskripsi Gambar, Bahasa Indonesia, Transformer, EfficientNet, ResNet, Computer Vision, Natural Language Processing","Azhari, Drs., M.T., Dr; Diyah Utami Kusumaning Putri, S.KOM., M.Sc., M.Cs.",2,3,0,2023,1,"Image captioning merupakan sistem yang menggabungkan bidang computer vision dengan Natural Language Processing (NLP) yang bertujuan untuk menghasilkan kalimat yang mendeskripsikan sebuah gambar. Sistem ini bermanfaat dalam membantu kegiatan manusia memahami konten visual pada gambar dan membantu mendeskripsikan gambar kepada tunanetra. Penelitian image captioning yang dihasilkan dalam bahasa Indonesia sebagian besar menggunakan gabungan Convolutional Neural Network (CNN) dan Recurrent Neural Network (RNN), sedangkan pada penelitian terkini menunjukkan bahwa menggunakan arsitektur transformer dapat menghasilkan performa model yang lebih baik.
Penelitian ini mengembangkan model image captioning yang mempunyai performa yang lebih baik dalam menghasilkan deskripsi gambar dalam bahasa Indonesia dibandingkan penelitian sebelumnya dengan menggunakan arsitektur berbasis transformer. Model yang dikembangkan adalah transformer dengan penambahan pretrained Convolutional Neural Network (CNN) yaitu EfficientNet dan Residual Neural Network (ResNet) pada lapisan encoder untuk mengekstraksi fitur gambar dan lapisan decoder untuk menghasilkan kalimat dalam bahasa Indonesia. Data diperoleh dari COCO dataset dengan pembagian menurut penelitian oleh Karpathy.
Hasil eksperimen menunjukkan bahwa model yang dikembangkan menggunakan penambahan EfficientNet mempunyai hasil yang lebih baik dibandingkan jika menggunakan ResNet dengan kenaikan sebesar 4.9% dari hasil rata-rata nilai metrik BLEU-1 hingga BLEU-4, METEOR, ROUGE-L, dan CIDEr.
","Image captioning is a system that combines the field of computer vision with Natural Language Processing (NLP) which aims to produce sentences that describe an image. This system is useful in helping human activities understand the visual content of images and helping describe images to the visually impaired. Research on image captioning in Indonesian mostly uses a combination of Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), while recent research shows that using a transformer architecture can produce better model performance.
This research develops an image captioning model based on transformer architecture which has good performance in producing image descriptions in Indonesian. The model developed is a transformer with the addition of pretrained Convolutional Neural Network (CNN) namely EfficientNet and Residual Neural Network (ResNet) at the encoder layer to extract image features and decoder layer to produce sentences in Indonesian. Data obtained from the COCO dataset with division according to research by Karpathy.
The experimental results show that the model developed using the addition of EfficientNet has better results than using ResNet with an increase of 4.9% from the average yield of BLEU-1 to BLEU-4, METEOR, ROUGE-L, and CIDEr metric values.",,,170506,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223382,,FITRA RAHMANI K,MANAJEMEN DAYA PADA SISTEM HPC MENGGUNAKAN METODE DEEP REINFORCEMENT LEARNING BERBASIS ADVANTAGE ACTOR-CRITIC,,,"HPC, Manajemen Daya, Penggunaan Energi, Deep Reinforcement Learning, Advantage Actor-Critic"," Lukman Heryawan, S.T., M.T., Ph.D.; Muhammad Alfian Amrizal, B.Eng., M.I.S., Ph.D.",2,3,0,2023,1,"Tingginya penggunaan energi merupakan salah satu permasalahan utama dalam menjalankan sistem High-Performance Computing (HPC). Pada umumnya, sistem
HPC terdiri dari ratusan hingga ribuan node komputasi yang menggunakan banyak daya listrik meskipun sedang dalam keadaan idle. Salah satu cara untuk meningkatkan
efisiensi manajemen daya adalah dengan menerapkan metode Backfilling pada job scheduler First Come First Serve (FCFS) (FCFS + Backfilling). Metode Backfilling memperbolehkan job yang datang setelah job pertama di antrian untuk dieksekusi terlebih dahulu jika waktu mulai job pertama tidak tertunda sehingga meningkatkan throughput dan efisiensi energi sistem. Node yang telah idle selama periode waktu tertentu juga dapat ditidurkan untuk meningkatkan efisiensi energi lebih lanjut. Namun, menidurkan node berdasarkan waktu idle dapat mengurangi efisiensi energi dan throughput sistem alih-alih meningkatkannya. Sebagai contoh, job baru dapat langsung tiba setelah node ditidurkan sehingga menghilangkan peluang untuk langsung mengeksekusi job tersebut melalui Backfilling.

Dalam penelitian ini, diusulkan metode berbasis Deep Reinforcement Learning (DRL) untuk memprediksi timing yang tepat untuk menidurkan atau menyalakan node. Agen DRL dilatih menggunakan algoritma Advantage Actor-Critic (A2C) untuk menentukan node mana yang harus ditidurkan atau dinyalakan pada timestep tertentu. Reward yang digunakan untuk melatih agen A2C-DRL terdiri dari pemborosan energi dan rata-rata job waiting time. Untuk evaluasi, agen A2C-DRL dibandingkan dengan metode timeout policy (node dimatikan setelah idle selama periode tertentu) dan baseline (node tidak dimatikan sama sekali) menggunakan simulator Batsim-py.

Hasil simulasi pada dataset job HPC DIKE UGM menunjukkan bahwa agen A2C-DRL mengungguli semua konfigurasi dan menghemat 58,2% energi dibanding timeout policy optimal (1 menit). Pada dataset job NASA iPSC/860, agen A2C-DRL
mengungguli 11 dari 12 timeout policy, menghemat hingga 21,34% energi dan hanya kalah 2% dibanding timeout policy optimal (15 menit). Agen A2C-DRL cenderung mematikan node lebih sering sehingga memiliki rata-rata job waiting time yang tinggi, namun agen menghemat lebih banyak energi dibandingkan metode timeout policy yang sama agresifnya. Hasil ini membuktikan bahwa agen A2C-DRL mampu mematikan dan menyalakan node pada waktu yang tepat.","The high energy consumption is one of the major issues in running High-Performance Computing (HPC) systems. Typically, HPC systems consist of hundreds to thousands of computing nodes that consume a significant amount of electricity even when idle. One way to increase power management efficiency is by implementing the Backfilling method on the First Come First Serve (FCFS) job scheduler (FCFS + Backfilling). The Backfilling method allows jobs arriving after the first job in the queue to be executed first if the starting time of the first job is not delayed, thus increasing
the throughput and energy efficiency of the system. Nodes that have been idle for a certain period of time can also be put to sleep to further increase energy efficiency. However, putting nodes to sleep based solely on idle time can actually decrease
energy efficiency and system throughput instead of increasing it. For example, a new job can arrive immediately after a node is put to sleep, thereby eliminating the opportunity to immediately execute the job via Backfilling.

In this study, we propose a Deep Reinforcement Learning (DRL)-based method to predict the proper timing for putting nodes to sleep or turning them on. The DRL agent is trained using the Advantage Actor-Critic (A2C) algorithm to determine
which nodes should be put to sleep or turned on at a particular timestep. The reward used to train the A2C-DRL agent consists of energy waste and average job waiting time. For evaluation, the A2C-DRL agent is compared with the timeout policy method (nodes are turned off after idle for a certain period) and baseline method (nodes are never turned off) using the Batsim-py simulator.

The simulation results on the DIKE UGM HPC job dataset show that the A2CDRL agent outperforms all configurations and saves 58.2% of energy compared to the optimal timeout policy (1 minute). On the NASA iPSC/860 job dataset, the A2CDRL agent outperforms 11 of the 12 timeout policies, saving up to 21.34% of energy and only losing 2% compared to the optimal timeout policy (15 minutes). The A2CDRL agent tends to turn off nodes more often, thus having a higher average job waiting time, but the agent saves more energy than the same aggressive timeout policy method. These results demonstrate that the A2C-DRL agent can put nodes to sleep and turn them on at the right time.",,,174425,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219801,,MUHAMMAD KEVIN ROZAL,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN JENIS TABUNGAN PERBANKAN MENGGUNAKAN METODE PROFILE MATCHING (PM) DAN SIMPLE ADDITIVE WEIGHTING (SAW) (STUDI KASUS: BANK TABUNGAN NEGARA),,,"Tabungan, SPK, Profile Matching, Interpolasi Linear, SAW","Retantyo Wardoyo, Drs., M.Sc., Ph.D.",2,3,0,2023,1,"Menabung merupakan suatu tindakan yang dilakukan oleh masyarakat untuk menyisihkan sebagian dari pendapatannya untuk memenuhi kebutuhan di masa yang akan datang. Terdapat beberapa cara yang dapat dilakukan untuk menabung, salah satunya adalah dengan memanfaatkan jasa yang ditawarkan oleh bank. Banyaknya pilihan jenis tabungan yang ditawarkan oleh bank serta tidak adanya informasi kuantitatif yang memadai menyulitkan masyarakat dalam memilih jenis tabungan yang tepat. Oleh karena itu, diperlukan Sistem Pendukung Keputusan (SPK) untuk membantu calon nasabah dalam memilih jenis tabungan yang sesuai dengan kebutuhannya.
Penelitian ini bertujuan untuk membangun dan mengembangkan SPK pemilihan jenis tabungan perbankan di Bank Tabungan Negara (BTN) berbasis web dengan menggunakan metode Profile Matching (PM) dengan interpolasi linear dan Simple Additive Weighting (SAW). Metode PM digunakan untuk menghitung tingkat perbedaan nilai data suatu alternatif dengan nilai yang diharapkan, metode interpolasi linear digunakan untuk proses normalisasi nilai gap dari kriteria alternatif. Metode SAW digunakan untuk menentukan besarnya bobot kriteria yang akan dinilai dan melakukan penjumlahan terbobot. 
Hasil penelitian berupa pengujian menunjukkan bahwa sistem yang telah dibangun dan dikembangkan dapat berjalan dengan baik sesuai dengan yang diharapkan dan menghasilkan informasi yang akurat dan dapat diandalkan. Saran untuk penelitian selanjutnya adalah penambahan kriteria penilaian, pengaplikasian metode pengambilan keputusan lain, dan proses pengambilan data alternatif yang lebih akurat dengan berkonsultasi langsung dengan pihak yang terkait.","Saving is an action taken by the people to set aside a portion of their income to meet future needs. There are several ways to save, one of which is by using the services offered by banks. The many options for types of savings offered by banks and the lack of adequate quantitative information make it difficult for people to choose the right type of savings. Therefore, a Decision Support System (DSS) is needed to help potential customers choose the type of savings that suits their needs.
This research aims to build and develop a DSS for selecting types of savings at the Bank Tabungan Negara (BTN) based on the web using the Profile Matching (PM) method with linear interpolation and Simple Additive Weighting (SAW). The PM method is used to calculate the level of difference in value of data of an alternative with the expected value, the linear interpolation method is used for the normalization process of the gap value of the alternative criteria. The SAW method is used to determine the size of the criteria weight to be evaluated and to perform weighted summation.
The results of the research in the form of testing show that the system that has been built and developed can run well as expected and produce accurate and reliable information. Suggestions for further research include the addition of evaluation criteria, the application of other decision-making methods, and the process of obtaining alternative data that is more accurate by consulting directly with the relevant parties.",,,171043,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
222628,,ALLISSYA AYU DEFANIA,IOS Mobile Application Development for Store Keepers at Toko Kelontong to Manage Their Price List in Indonesia,,,"Indonesia, MSMEs, Toko Kelontong, Storekeeper, Barcode Scanner, Mobile Application, iOS","Medi, Drs., M.Kom",2,3,0,2023,1,"Toko kelontong merupakan toko yang bergerak di bidang penjualan produk makanan dan minuman. Toko ini bergerak dalam bidang penjualan aneka makanan, minuman, dan kebutuhan lainnya seperti nasi, roti, indomie, air mineral, dan lain-lain. Pertumbuhan Usaha Mikro, Kecil, dan Menengah (UMKM) merupakan salah satu faktor penggerak perekonomian di Indonesia. Toko kelontong berperan besar dalam UMKM dengan jumlah toko sekitar 3,6 juta yang tersebar di 34 provinsi Indonesia. Toko kelontong masih menggunakan cara manual dalam penjualan dan pencatatan laporan barang. Indonesia adalah salah satu pasar aplikasi seluler dengan pertumbuhan tercepat di dunia, untuk meningkatkan pengembangan toko kelontong itulah alasan aplikasi seluler iOS dirancang. Berdasarkan perancangan aplikasi mobile iOS di toko kelontong, dapat disimpulkan bahwa aplikasi yang dibuat bertujuan untuk memberikan proses tampilan aplikasi mobile bagi pemilik toko toko kelontong yang ingin mengetahui informasi harga produk terupdate dengan memberikan informasi harga jual untuk toko kelontong mereka. produk yang memiliki pemindai kode batang dan sinkronisasi data.","Toko kelontong is a store engaged in the sale of food and beverage products. This shop is engaged in selling various foods, drinks, and other needs such as rice, bread, indomie, mineral water, and others. The growth of Micro, Small, and Medium Enterprises (MSMEs) is one of the factors that drive the economy in Indonesia. Toko kelontong plays a big role in MSMEs with the number of stores around 3.6 million spread across 34 Indonesian provinces. Toko kelontong still uses the manual way of selling and recording goods reports. Indonesia is one of the world's fastest growing mobile app markets, to enhance toko kelontong development that is the reason an iOS mobile app is designed. Based on the design of the iOS mobile application at the toko kelontong, it can be concluded that the application created aim to provide a mobile application display process for toko kelontong shopkeepers who want to know updated product price information by providing their list price information for their products that have a barcode scanner and data synchronization.",,,173853,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221613,,NURRIZKY IMANI," CLICKBAIT TEXT STYLE TRANSFER BASED ON ENCODER DECODER MODEL FOR INDONESIAN NEWS HEADLINES USING DELETE, RETRIEVE, GENERATE APPROACH",,,"Clickbait, Text Style Transfer, Encoder Decoder Model, News datasets","Anny Kartika Sari, S.Si., M.Sc., Ph.D; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2023,1,"Perkembangan media digital telah menghasilkan peningkatan konten online di internet, yang mendorong industri berita untuk menarik perhatian pembaca dengan membuat konten yang menarik. Salah satu pendekatan untuk mencapainya adalah dengan memasukkan elemen clickbait ke dalam judul headline. Saat ini, menghasilkan judul yang menarik dilakukan dengan menggunakan teknik yang mengandalkan keterampilan manual manusia, yang dapat menjadi tidak efisien dan memakan waktu. Sementara itu, kemajuan dalam pemrosesan bahasa alami telah mengarah pada peluang untuk mengotomatisasi teknik pembuatan teks yang dapat mentransfer gaya teks.
Dalam penelitian ini, kami menyajikan implementasi untuk mentransfer kalimat non-clickbait ke kalimat clickbait untuk headline berita Indonesia. Penelitian ini menggunakan metode prototype editing, yang mendemonstrasikan penerapan pendekatan Delete, Retrieve, Generate berdasarkan jaringan syaraf tiruan untuk bahasa Indonesia. Dengan menggunakan kumpulan data headline berita Indonesia dengan 15.000 headline beranotasi yang diberi label sebagai clickbait dan non-clickbait, penelitian ini menerapkan BLEU dan metrik kerugian untuk menilai hasil akhir transfer gaya teks.
Hasil penelitian ini menunjukkan potensi pendekatan Delete, Retrieve, Generate dalam transfer gaya judul artikel clickbait berdasarkan model DeleteOnly dengan hasil terbaik yang dicapai dalam validasi kedua metrik BLEU yaitu 0,1628 dan loss 4,5428. Namun, penelitian ini juga menunjukkan kemampuan terbatas saat menggunakan pendekatan DeleteRetrieve dalam hal mengambil atribut clickbait, dengan hasil yang hanya mencapai metrik BLEU 0,0022 dan loss 7,976 pada validasi. Kedua model menunjukkan hasil kinerja yang berbeda antara data latih dan validasi, yang menyoroti tantangan yang ditimbulkan oleh jumlah data yang terbatas.","The growth of digital media has led to an increase in online content on the internet, which has challenged the news industry to grab readers&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; attention by creating engaging content. One approach to achieving this is to incorporate clickbait elements within headline titles. Currently, generating engaging titles is done using techniques that rely on manual human skills, which can be inefficient and time-consuming. Meanwhile, advancements in natural language processing have led to the opportunity to automate the text generation techniques that can transfer text style.
In this research, we present an implementation for transferring non-clickbait sentences to clickbait sentences for Indonesian news headlines. The research uses a prototype editing method, which demonstrates the application of the Delete, Retrieve, Generate approach based on encoder decoder models for the Indonesian language. Using an Indonesian headline news datasets with 15,000 annotated headlines labeled as clickbait and non-clickbait, the research applies BLEU and loss metrics to assess the end results of text style transfers.
The results of this research show the potential of the Delete, Retrieve, Generate approach in clickbait headline style transfer based on the DeleteOnly model with the best result achieved in validation both BLEU metrics 0.1628 and 4.5428 loss. However, it also shows limited ability when using the DeleteRetrieve approach in terms of retrieving clickbait attributes with result only achieve BLEU metrics 0.0022 and 7.976 loss both validation. Both models show contrasting performance results in training and validation, highlighting the challenges posed by the limited amount of data available.",,,172724,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223661,,ILHAM AKBAR T K W,Development of E-commerce Mobile Application Using Reactive Programming (Case Study Music Gear),,,"E-commerce, Music Gear, Mobile Application, iOS, Reactive programming","Nur Rokhman, S.Si., M.Kom., Dr.",2,3,0,2023,1,"Selama dekade terakhir karena pesatnya pertumbuhan internet dan teknologi seluler, e-commerce telah tumbuh secara eksponensial. Salah satu pasar yang paling menjanjikan dalam e-commerce saat ini adalah pasar perlengkapan musik. Di luar negeri seperti Amerika serikat dan Eropa terdapat platform e-commerce C2C yang khusus menjual perlengkapan musik. Di Indonesia saat ini belum ada platform e-commerce C2C yang khusus menjual perlengkapan musik, hal ini dapat mengakibatkan penjual perseorangan mengalami kesulitan untuk menjual perlengkapan musiknya. Saat ini banyak e-commerce dibangun dan tersedia di aplikasi seluler. Karena aplikasi seluler modern lebih rumit dan kompleks, pemrograman reaktif telah menjadi pendekatan populer untuk mengembangkan aplikasi seluler modern. Saat mengembangkan aplikasi e-commerce modern sangat kompleks, tanpa pemrograman reaktif, membangun aplikasi e-commerce modern akan sulit. Karena tanpa pemrograman reaktif semua tugas asinkron, penanganan kesalahan dan antarmuka pengguna yang rumit harus ditangani secara manual. Pada penelitian ini, dengan menggunakan paradigma pemrograman reaktif, sebuah aplikasi mobile iOS dengan nama Arpegio dibangun untuk membuat platform C2C e-commerce yang fokus menjual peralatan musik untuk membantu penggemar peralatan musik untuk membeli dan menjual peralatan musik dengan lebih mudah. Aplikasi mobile iOS Arpegio yang dibuat pada penelitian ini dibangun menggunakan iOS native development, database online Firebase dan RxSwift untuk pemrograman reaktif. Setelah pengembangan aplikasi Arpegio selesai, aplikasi tersebut disebarkan melalui Apple TestFlight. Terakhir, setelah aplikasi diterapkan, peserta yang telah menguji aplikasi arpegio mengisi survei yang mengukur kegunaan, fungsionalitas, pengalaman pengguna aplikasi, dan opini keseluruhan pengguna tentang aplikasi tersebut. Dari keseluruhan proses penelitian ini disimpulkan bahwa pengguna puas dengan aplikasi arpegio, aplikasi arpegio berfungsi dengan baik meskipun ada beberapa masalah kecil, dan fitur pemrograman reaktif pada aplikasi arpegio bekerja dengan cukup baik.","During the past decade because of the rapid growth of internet and mobile technology, e-commerce has grown exponentially. One of the most promising markets in today's e-commerce is the music gears market. In foreign country such as USA and Europe there is C2C e-commerce platform that specifically sells music gear. In Indonesia currently, there is no C2C e-commerce platform that specifically sells music gear, which can be resulted in the individual seller having difficulty selling their music gear. Right now a lot of e-commerce are built and available on the mobile app. Because modern mobile applications are more complicated and complex, reactive programming has become a popular approach to developing modern mobile application. While developing a modern e-commerce app is highly complex, without reactive programming building a modern e-commerce app would be difficult. Since without reactive programming all the asynchronous tasks, error handling and complex user interfaces have to be handled manually. On this research , with the use of reactive programming paradigm, an iOS mobile application with the name Arpegio are built to make a C2C music gear- focused e-commerce platform to help music gears enthusiast to buy and sell music gear more easily. The Arpegio iOS mobile application that is created on this research is built using iOS native development, Firebase online database and RxSwift for reactive programming. After the development of the Arpegio app is finished, the app is deployed through Apple TestFlight. Finally after the app is deployed, participant that have tested the arpegio app fill in a survey that measures the usability, functionality, user experience of the app, and overall opinion of users about the app. From the whole process on this research it is concluded that the users satisfied with the arpegio app, the arpegio app functions properly despite a few minor issues, and the reactive programming feature on the arpegio app works pretty well.",,,174607,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
225201,,TRI WAHYU PUTRA,Optimasi Pemilihan Posisi dan Role Terbaik Pemain Muda pada Gim Football Manager 2022 dengan Multilabel Naive Bayes – Label Dependence,,,"pemilihan posisi dan role, football manager, perbandingan algoritma, multilabel naive bayes-label dependence","Prof. Dr.-Ing. Mhd. Reza M. I. Pulungan, S.Si., M.Sc.;Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2023,1,"<p><i>Football Manager</i> 2022 adalah salah satu gim simulasi manajemen olahraga yang populer, di mana pemain berperan sebagai manajer sekaligus pelatih klub sepak bola yang bertanggung jawab dalam membuat keputusan strategis yang salah satunya adalah memilih posisi dan <i>role</i> para pemain. Memilih posisi dan <i>role</i> yang tepat bagi pemain terutama untuk pemain muda perlu memperhitungkan atribut dari pemain sehingga menjadi tugas yang cukup sulit jika dilakukan tanpa metode tertentu.</p><p>Penelitian ini membandingkan kinerja algoritma MLNB konvensional dan MLNB yang mempertimbangkan <i>label dependence </i>dalam konteks pemilihan posisi dan <i>role </i>terbaik pemain muda di <i>Football Manager</i> 2022. Perbandingan kinerja algoritma dilakukan dengan membandingkan akurasi model dan seberapa besar kinerja pemain saat pertandingan. Kontribusi pemain saat pertandingan juga dibandingkan agar diketahui bagaimana performa tim ketika pemain berada di lapangan.</p><p>Hasil analisa dari penelitian ini menunjukkan bahwa algoritma MLNB yang mempertimbangkan <i>label dependence</i> (MLNB-LD) secara umum memiliki performa yang lebih baik dibandingkan dengan algoritma MLNB konvensional. Algoritma MLNB-LD memiliki nilai akurasi yang lebih baik pada 80% <i>dataset</i> dengan peningkatan nilai rata-rata akurasi sebesar 0,445?ri algoritma MLNB. Selain itu, algoritma MLNB-LD memiliki nilai kinerja pemain yang lebih baik pada 100% pemain dengan peningkatan nilai rata-rata kinerja pemain sebesar 2,855?ri algoritma MLNB. Hasil analisa juga menunjukkan bahwa algoritma MLNB-LD tidak terlalu mempengaruhi nilai kontribusi pemain. Algoritma MLNB-LD hanya memiliki nilai kontribusi pemain yang lebih baik pada 60% pemain dengan peningkatan nilai rata-rata kontribusi pemain sebesar 0,545?ri algoritma MLNB.</p>","<p>Football Manager 2022 is one of the popular sports management simulation games, in which players act as managers and coaches of football clubs who are responsible for making strategic decisions, one of which is choosing the positions and roles of the players. Choosing the right position and role for players, especially for youth players, needs to consider the attributes of the players so it becomes a fairly difficult task if done without a certain method.</p><p>This study compares the performance of conventional MLNB algorithm and MLNB that consider label dependence algorithm in the context of youth player's best position and role selection in Football Manager 2022. Algorithm performance comparisons are made by comparing the accuracy of model and player’s performance during the match. The player's contribution during the match is also compared so that it is known how the team performs when the player is on the field.</p><p>The results of the analysis of this study indicate that the MLNB that considers label dependence (MLNB-LD) algorithm in general has better performance than the conventional MLNB algorithm. The MLNB-LD algorithm has a better accuracy value at 80% of the dataset with an increase in the average accuracy value of 0,445% than the MLNB algorithm. In addition, the MLNB-LD algorithm has a better player performance value at 100% of players with an increase in the average player performance value of 2,855% than the MLNB algorithm. The results of the analysis also show that the MLNB-LD algorithm does not significantly affect the player's contribution value. The MLNB-LD algorithm only has a better player contribution value at 60% of players with an increase in the average player contribution value of 0,545% than the MLNB algorithm.</p>",,,176386,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219826,,SHAQINA YASMIN,A Hybrid Approach for Clickbait Detection,,,"clickbait detection, machine learning, deep learning, hybrid approach, convolutional neural network, support vector machine, logistic regression","Afiahayati, S.Kom., M.Cs, Ph.D; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2023,1,"Media berita online menggunakan berbagai macam strategi yang menarik perhatian untuk menghasilkan pendapatan. Salah satu strategi yang digunakan adalah clickbait. Clickbait merupakan tajuk menarik yang dengan sengaja dibuat untuk menarik perhatian pembaca. Sering kali tajuk tersebut menggunakan bahasa yang menarik dan berlebihan. Namun, konten dari tajuk clickbait umumnya tidak sesuai apa yang dituliskan dalam tajuk tersebut. Keberadaan clickbait mengganggu bagi para pembaca.
Terdapat banyak cara untuk mendeteksi clickbait, salah satunya menggunakan traditional machine learning. Metode traditional machine learning sering digunakan untuk mendeteksi keberadaan clickbait. Namun, traditional machine learning bergantung erat kepada rekayasa fitur. Riset ini menggunakan metode yang menggabungkan antara traditional machine learning dengan deep learning untuk menangani berita clickbait. Terdapat delapan model yang dibandingkan satu sama lain, mulai dari logistic regression, SVM, CNN1D, CNN2D, CNN1D dengan logistic regression, CNN1D dengan SVM, CNN2D dengan logistic regression dan CNN2D dengan SVM.
Menggabungkan Convolutional Neural Network (CNN) dengan model traditional machine learning menunjukan bahwa metode gabungan atau metode hibrida dapat mengungguli model yang menggunakan model traditional machine learning dan deep learning tunggal untuk berita berbahasa Indonesia dari 12 media berita online. Kombinasi CNN dengan Support Vector Machine (SVM) meraih performa tertinggi di bidang akurasi sebesar 91,2%.","To generate revenue, online news media employ a variety of attention getting strategies. Clickbait is one of the techniques used. It is an enticing headline whose purpose is to attract readers&acirc;€™ clicks. The headlines of clickbait articles frequently use catchy or exaggerated language. Clickbait articles mostly contain poor content. Due to the poor quality of the content, the promises made in the headlines cannot be met. The presence of clickbait is disruptive to readers.
A variety of approaches, such as traditional machine learning, have been used to overcome clickbait issues. Traditional machine learning methods, however, rely significantly on feature engineering. This research presents a hybrid technique that combines traditional machine learning with deep learning to overcome this issue. There are eight models compared against each other, from logistic regression, SVM, CNN1D, CNN2D, CNN1D with logistic regression, CNN1D with SVM, CNN2D with logistic regression, CNN2D with SVM.
Combining Convolutional Neural Network (CNN) with traditional machine learning models such as logistic regression or Support Vector Machine (SVM) demonstrates that the hybrid approach outperformed the stand-alone traditional machine learning and deep learning approaches in the Indonesian news headlines from twelve online news sources. The CNN and SVM combination achieves the highest performance in terms of accuracy of 91.2% compared to other models

",,,171007,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219831,,ADELLA SUKANTI I,sentiment analysis and multi-label classification for determining shopee customer satisfaction,,,"Multi-label classification, Text-Classification, TF-IDF, Customer satisfaction, Logistic regression","Mhd.Reza M.I Pulungan, M.Sc., Dr.-Ing., Prof.",2,3,0,2023,1,"Perusahaan menggunakan analisis sentimen dan pembelajaran mesin untuk menghitung kinerja dan kebahagiaan konsumen mereka. Salah satu platform e-commerce online terpopuler di Indonesia adalah Shopee. Pada awal Shopee terbentuk terbatas pada layanan e-commerce Shopee yang memungkinkan transaksi 
bisnis online. Setelah beberapa lama, Shopee mulai mengenalkan lebih banyak layanan termasuk Shopee Food dan Shopee Pay. Menjadi lebih sulit untuk secara 
otomatis menganalisis polaritas sentimen untuk setiap kategori layanan karena Shopee mengembangkan kategori tambahan. Kumpulan sampel pembelajaran yang 
ditautkan ke klasifikasi label tunggal adalah fokus dari algoritma klasifikasi standar atau klasifikasi label tunggal. Pada penelitian ini akan difokuskan untuk mengklasifikasikan layanan Shopee berdasarkan dua kelas target, yaitu kategori layanan Shopee dan klasifikasi sentimen polaritas. Penelitian ini juga menggunakan 
Twitter sebagai dataset yang merupakan platform untuk mengekspresikan pendapat dan emosi mereka. Melalui metodologi penelitian ini berisi persiapan Dataset, Ekstraksi fitur, Proses penambangan teks dasar, dataset pelatihan dan pemisahan dan mengimplementasikan metode klasifikasi multi label menggunakan multi 
output classifier dan regresi logistik. Berdasarkan dataset, layanan yang paling banyak disebut adalah Shopee diikuti oleh Shopee Food dan Shopee Pay. Untuk polaritas sentimen kami mendapatkan ulasan paling positif dibandingkan empat polaritas sentimen lainnya yaitu sangat positif,netral,negatif,sangat negatif. Nilai
akurasi tertinggi sebesar 97,91% untuk kategori layanan Shopee dan 81,45% untuk klasifikasi sentimen polaritas sentimen","Companies utilize sentiment analysis and machine learning to calculate their performance and consumer satisfaction. One of the most popular online e-commerce 
platforms in Indonesia is Shopee. Shopee's first offering was limited to e-commerce services that allowed for online business transactions. After a while, Shopee started 
to create more services including Shopee Food and Shopee Pay. It becomes more difficult to automatically analyze the sentiment polarity for each category of 
services as Shopee develops additional categories. Learning set of samples linked to a single label classification is the focus of a standard classification algorithm or single-label classification. With the limited research on tackling multi-label classification. By this research will be focused on classify Shopee services based 
on two-class target, which are Shopee service categories and polarity sentiment classification. This research also use Twitter as the dataset where it is a platform to 
express their opinion and emotion. Through that this research methodology contains Dataset preparation, Feature extraction, Basic text mining process, training and split dataset and implemented multi label classification method using multi outputclassifier and logistic regression. Based on the dataset, the most mentioned service is Shopee followed by Shopee Food and Shopee Pay. For the sentiment polarity we get most positive reviews compared to other four sentiment polarity which are strongly positive, neutral, negative, strongly negative. The highest accuracy score of 97.91% for Shopee services category and 81.45% for sentiment polarity 
sentiment classification.",,,171059,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
221113,,AHMAD AZKA KUSUMA E,FIRST AID MOBILE APP DEVELOPMENT FOR PET OWNER IN iOS PLATFORM,,,"First aid, First treatment, Pet owner, Vet, Proper first care, Decision support system, Expert system, iOS","Drs. Medi, M.Kom.",2,3,0,2023,1,"Information technology advancements have led to a rise in digitization across a variety of fields, including veterinary medicine. The COVID-19 outbreak has increased pet ownership and need for pet care in Indonesia. By offering instructions for first aid and emergency care through an iOS-based application, the study intended to save pet owners money on unnecessary appointments to the veterinarian. This program uses expert systems and decision support systems (DSS) to give pet owners reliable information and suggestions. The program was created in Swift using xcode as the integrated development environment, and it works with iOS 13 and later. Three veterinary professionals tested the expert system, and the results showed that it was 62% accurate when compared to actual veterinary expert advice.
The program promises to enhance the standard of living for both pets and their owners in Indonesia by giving pet owners access to information and suggestions whenever and wherever they need it. In conclusion, the creation of this application represents a significant advancement in the fields of veterinary medicine and animal welfare. It makes use of specialized expertise and cutting-edge technology to give pet owners precise and trustworthy information so they can make decisions regarding the health and wellbeing of their animals. The application can help pet owners avoid unnecessary trips and expenses, while providing their pets with the care they need in an emergency.","Information technology advancements have led to a rise in digitization across a variety of fields, including veterinary medicine. The COVID-19 outbreak has increased pet ownership and need for pet care in Indonesia. By offering instructions for first aid and emergency care through an iOS-based application, the study intended to save pet owners money on unnecessary appointments to the veterinarian. This program uses expert systems and decision support systems (DSS) to give pet owners reliable information and suggestions. The program was created in Swift using xcode as the integrated development environment, and it works with iOS 13 and later. Three veterinary professionals tested the expert system, and the results showed that it was 62% accurate when compared to actual veterinary expert advice.
The program promises to enhance the standard of living for both pets and their owners in Indonesia by giving pet owners access to information and suggestions whenever and wherever they need it. In conclusion, the creation of this application represents a significant advancement in the fields of veterinary medicine and animal welfare. It makes use of specialized expertise and cutting-edge technology to give pet owners precise and trustworthy information so they can make decisions regarding the health and wellbeing of their animals. The application can help pet owners avoid unnecessary trips and expenses, while providing their pets with the care they need in an emergency.",,,172318,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219838,,FINANNISA ZHAFIRA,OPTIMIZING LONG SHORT-TERM MEMORY NETWORKS WITH MULTILAYER PERCEPTRON FOR TIME SERIES PREDICTION,,," Property, Scraping, LSTM, MLP","Mardhani Riasetiawan, M.T., Dr  ; Dzikri Rahadian Fudholi, S.Kom., M.Comp.",2,3,0,2023,1,"Due to the Covid-19 pandemics, occupancy rates in most major cities
range from 8.6% - 22.6% with commercial properties in Bali being the lowest.
Property business actors struggle to predict the dynamics of hospitality industry
demand. Beside that, LSTM is designed to give better performance in capturing
long-term dependencies in sequential data such as time series data. However,
there are several drawbacks in utilizing LSTM, such as computationally expensive
to train than simpler models like feedforward neural networks. With that stated,
we find an opportunity to minimize the drawbacks by combining LSTM with one
of the feedforward neural networks, MLP. Through this research, the author would
like to: (1) Compare the performance of commercial property availability rate
forecast using Vanilla LSTM, Stacked LSTM and LSTM Window Method and (2)
Optimize the model performance based on the best modeling among Vanilla
LSTM, Stacked LSTM and LSTM Window Method with MLP.
The main stages of the execution of this study are summarized in the
following mechanism: data collection using web scraping techniques, data
cleaning &amp; preprocessing, data modeling and data evaluation. This mechanism is
used to execute the process of comparing the Vanilla LSTM, Stacked LSTM and
LSTM Window Method models to get the best model, as well as to execute the
best model optimization.
Based on experiments conducted using the three models, the LSTM
Window Method model was obtained with the smallest error performance of 6.03
RMSE. Therefore, we develop an advanced model for the LSTM Window
Method. The development of an advanced model for the LSTM Window Method
is carried out by combining it with the other best neural network models, namely
the Multilayer Perceptron (MLP). From this experiment, the best model was
obtained using 5 windows with the smallest error performance of 4.53 RMSE","Due to the Covid-19 pandemics, occupancy rates in most major cities
range from 8.6% - 22.6% with commercial properties in Bali being the lowest.
Property business actors struggle to predict the dynamics of hospitality industry
demand. Beside that, LSTM is designed to give better performance in capturing
long-term dependencies in sequential data such as time series data. However,
there are several drawbacks in utilizing LSTM, such as computationally expensive
to train than simpler models like feedforward neural networks. With that stated,
we find an opportunity to minimize the drawbacks by combining LSTM with one
of the feedforward neural networks, MLP. Through this research, the author would
like to: (1) Compare the performance of commercial property availability rate
forecast using Vanilla LSTM, Stacked LSTM and LSTM Window Method and (2)
Optimize the model performance based on the best modeling among Vanilla
LSTM, Stacked LSTM and LSTM Window Method with MLP.
The main stages of the execution of this study are summarized in the
following mechanism: data collection using web scraping techniques, data
cleaning &amp; preprocessing, data modeling and data evaluation. This mechanism is
used to execute the process of comparing the Vanilla LSTM, Stacked LSTM and
LSTM Window Method models to get the best model, as well as to execute the
best model optimization.
Based on experiments conducted using the three models, the LSTM
Window Method model was obtained with the smallest error performance of 6.03
RMSE. Therefore, we develop an advanced model for the LSTM Window
Method. The development of an advanced model for the LSTM Window Method
is carried out by combining it with the other best neural network models, namely
the Multilayer Perceptron (MLP). From this experiment, the best model was
obtained using 5 windows with the smallest error performance of 4.53 RMSE",,,171102,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
222688,,NAMIRA SALMA FAKHIRA,PERBANDINGAN STEGANOGRAFI CITRA MENGGUNAKAN METODE DCT-OTP PADA CITRA ABU-ABU SEBAGAI CITRA RAHASIA DAN CITRA SAMPUL,,,"Steganografi Citra, Kriptografi Citra, Discrete Cosine Transform (DCT), One-Time Pad (OTP)","Janoe Hendarto, Drs., M.Kom",2,3,0,2023,1,"Steganografi Citra merupakan metode untuk menyembunyikan pesan rahasia ke dalam citra digital sebagai citra sampul. Salah satu metode steganografi citra adalah Discrete Cosine Transformation (DCT). Telah banyak penelitian yang dilakukan untuk mengembangkan metode DCT. Metode DCT-OTP 16 adalah salah satunya dengan melakukan kriptografi One-Time Pad (OTP) terhadap citra rahasia sebelum disembunyikan ke dalam citra sampul. Metode tersebut menggunakan subblok DCT berukuran 16x16 dan koefisien DC pada citra sampul sebagai tempat penyembunyian. Oleh karena itu, metode tersebut dapat dikembangkan dengan memodifikasi ukuran subblok dan tempat penyembunyian untuk meningkatkan kapasitas penyembunyian maksimum dan nilai PSNR citra rahasia yang dipulihkan.
Penelitian ini melakukan modifikasi terhadap metode DCT-OTP 16 dengan memperkecil ukuran subblok pada metode DCT menjadi 8x8 dan mengganti tempat penyembunyian pada citra sampul menjadi koefisien AC(7,7). Hasil berupa nilai MSE, PSNR dan NCC dari citra stego dan citra rahasia hasil ekstraksi-dekripsi; nilai UACI dan NPCR dari citra rahasia terenkripsi; dan kapasitas penyembunyian maksimum citra sampul akan dibandingkan antara metode DCT-OTP 16 (sebelum modifikasi) dan DCT-OTP 8 (sesudah modifikasi).
Metode DCT-OTP 16 dan DCT-OTP 8 menghasilkan nilai UACI dan NPCR yang sama dan citra stego dengan nilai PSNR yang baik, namun metode DCT-OTP 8 memiliki kapasitas penyembunyian maksimum lebih besar dan citra rahasia yang diekstraksi menggunakan metode tersebut memiliki nilai MSE yang lebih kecil daripada metode DCT-OTP 16 walaupun citra hasil ekstraksi dari kedua metode memiliki nilai NCC mendekati 1 (satu).","Image steganography is a method for hiding secret messages into a digital image as the cover image. One of the image steganography method is Discrete Cosine Transformation (DCT). There have been many studies to develop the DCT method. DCT-OTP 16 method is one of them by performing One-Time Pad (OTP) cryptography on secret image before being hidden into the cover image. This method uses a 16x16 DCT subblock and a DC coefficient on cover image ad a hiding place. Therefore, the method can be developed by modifying the DCT subblock size and the hiding place to increase the maximum hiding capacity and to increase the PSNR value of secret image from the extraction process.
This research will modify DCT_OTP 16 method on its DCT subblock size to 8xx8 and used AC(7,7) coefficient as the hiding place. The result are in the form of MSE, PSNR and NCC values from stego image and extracted secret image; UACI and NPCR values from encrypted secret image; and cover image's maximum hiding capacity are to be compared between DCT-OTP 16 method (before modification) and DCT-OTP 8 (after modification).
The DCT-OTP 16 method and DCT-OTP 8 method obtained the same values UACI and NPCR evaluation and stego images with good PSNR values, but the DCT-OTP 8 method has a larger maximum hiding capacity and the extracted secret images from these method have MSE values which is smaller than the DCT-OTP 17 method obtained even though the extracted secret images from both methods has an NCC values close to 1 (one).",,,173672,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223458,,THEOFILUS ARKHI S,	PERBANDINGAN KLASIFIKASI CITRA BERBASIS HIERARCHICAL NEURAL NETWORK DENGAN MODEL CONVOLUTIONAL NEURAL NETWORK TUNGGAL UNTUK DATA BERJENJANG,,,"Pembelajaran mesin, klasifikasi citra, data berjenjang, hierarchical neural network","Dzikri Rahadian Fudholi, S.Kom., M.Comp.; Erwin Eko Wahyudi, S.Kom., M.Cs.",2,3,0,2023,1,"Belakangan ini, permasalahan pembelajaran mesin berkembang menjadi
semakin kompleks. Salah satunya permasalahan pembelajaran mesin yang
kompleks adalah permasalahan klasifikasi citra pada data berjenjang dengan label
bertingkat dari tingkat yang umum (coarse) hingga tingkat yang lebih spesifik
(fine). Pada penelitian ini, diusulkan penggunaan arsitektur Hierarchical Neural
Network (HNN) yang terdiri dari beberapa model CNN untuk melakukan klasifikasi
citra pada data berjenjang.
Pada penelitian ini, dilakukan perbandingan antara arsitektur HNN dan
CNN tunggal pada tingkat prediksi fine berdasarkan nilai akurasi sebagai metrik
utama dan nilai F1-score sebagai metrik tambahan. Dataset yang digunakan pada
penelitian ini adalah subset dari CIFAR-100. Diperoleh hasil nilai akurasi pada
tingkat prediksi fine dari arsitektur HNN dan CNN tunggal masing-masing sebesar
52,08% dan 52,25%. Sedangkan diperoleh nilai F1-score untuk arsitektur HNN dan
CNN tunggal masing-masing sebesar 0,5173 dan 0,5185. Pada hasil prediksi data
validasi, terdapat kasus di mana CNN menghasilkan prediksi yang benar tetapi
terdapat kesalahan prediksi HNN di tingkat coarse dan/atau fine. Akan tetapi,
terdapat juga kasus di mana CNN menghasilkan prediksi yang salah sementara
HNN mampu menghasilkan prediksi yang benar untuk tingkat coarse dan/atau fine.
Meskipun arsitektur HNN tidak menghasilkan hasil prediksi yang lebih baik
dibandingkan dengan CNN tunggal pada percobaan ini, arsitektur HNN mampu
menghasilkan prediksi secara bertahap dari tingkat kelas yang umum hingga tingkat
kelas yang spesifik.","Nowadays, machine learning tasks have been increasing in complexity. One
of the complex problems is image classification on hierarchical dataset with
hierarchical labels from coarse to fine levels. Therefore, we propose to implement
Hierarchical Neural Network (HNN) architecture based on multiple CNN models
to classify images from hierarchical dataset.
In this research, we compare our HNN architecture with a single CNN
model based on accuracy and F1-score metrics on fine prediction level. We use a
subset of CIFAR-100 dataset in this experiment. On fine prediction, our HNN and
CNN models reached accuracy score of 52.08% and 52.25% respectively. On the
other hand, our HNN and CNN models reached F1-score of 0.5173 and 0.5185
respectively. For each image in our validation split, there were cases where CNN
could give the correct prediction while our HNN architecture gives the wrong label
in either coarse or fine level. Conversely, there were cases where CNN gave the
wrong label while our HNN architecture could give the correct coarse and/or fine
label. Although our HNN architecture could not achieve higher scores in either
metric in this experiment, our HNN architecture could give step-by-step prediction
from coarse to fine levels.
",,,174492,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
223465,,NUR ALFI LAILY,Komparasi Performa Deteksi Depresi pada Tweet di Twitter Menggunakan Bi-LSTM dan BERT,,,"Bidirectional Encoder Representations from Transformers, Deep Learning, depresi, Natural Language Processing, Transformer","Bidirectional Encoder Representations from Transformers, Deep Learning, depression, Natural Language Processing, Transformer",2,3,0,2023,1,"Depresi sudah menjadi permasalahan kesehatan mental yang serius di masa sekarang. Perawatan untuk depresi sudah ada. Namun, orang yang mengalami gangguan jiwa di negara-negara berkembang tidak mendapatkan pengobatan. Di sisi lain, lebih dari separuh penduduk di dunia sudah menggunakan media sosial. Salah satu media sosial yang sering digunakan adalah Twitter. Twitter juga merupakan media sosial yang bisa digunakan oleh penggunanya untuk mengungkapkan bagaimana perasaan dan kondisi mereka. Oleh karena itu, Twitter dapat dimanfaatkan untuk melakukan analisis atau deteksi terkait depresi pada seseorang melalui cuitan publik yang ada di Twitter.
Pada penelitian ini, digunakan BERTweet dan Bi-LSTM sebagai metode untuk melakukan deteksi depresi pada sosial media Twitter. Pada tahap preprocessing, tindakan yang dilakukan adalah dengan melakukan case folding, data cleaning, tokenisasi, stopword removal, dan lematisasi. Selanjutnya, pada dataset dilakukan proses tokenisasi sesuai dengan BERTweet Tokenizer. 
Dari hasil dari proses training menggunakan BERTweet, model tersebut mampu meraih akurasi sebesar 0,806; 0,812; dan 0,823 pada setiap percobaan. Sedangkan untuk proses training menggunakan Bi-LSTM, model tersebut meraih nilai akurasi sebesar 0,627; 0,635; dan 0,639. Dari seluruh hasil pengujian menggunakan kedua jenis model, diperoleh kesimpulan bahwa BERTweet dapat melakukan task klasifikasi deteksi depresi data Twitter dengan lebih baik dibandingkan dengan Bi-LSTM.","Depression has become a serious mental health problem nowadays. Treatments for depression have already existed. However, people with mental disorders in developing countries do not get proper treatment. On the other hand, more than half of the world's population already uses social media. One of the social media that is often used is Twitter. Twitter is also a social media that users can use to express their feelings and conditions. Therefore, Twitter can be used to carry out analysis or detection related to depression in a person through public tweets on Twitter.
In this study, BERTweet and Bi-LSTM were used as a method for detecting depression on social media Twitter. At the preprocessing stage, the actions taken are case folding, data cleaning, tokenization, stopword removal, and lemmatization. Furthermore, the dataset is tokenized according to the BERTweet Tokenizer.
From the results of the training process using BERTweet, the model was able to achieve an accuracy of 0,806; 0,812; and 0,823 in each trial. As for the training process using Bi-LSTM, the model achieves 0,627; 0,635; and 0,639 accuracy. From all the test results using both types of models, it can be concluded that BERTweet can perform the task of classification of depression detection on Twitter data better than Bi-LSTM.",,,174527,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
220911,,TEUKU MUHAMMAD RIFQI,A VISION-BASED APPROACH ON FALL DETECTION ,,,"Keyword: Computer Vision, Recurrent Neural Network, Convolution Neural Network, Jatuh.","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2023,1,"Jatuh adalah sumber cedera yang sering, namun sering diabaikan. Satu dari setiap tiga orang di atas usia 65 tahun, dan setengah dari mereka yang berusia di atas 80 tahun, akan jatuh setidaknya setahun sekali. Dengan orang lanjut usia berada dalam kondisi yang lebih rentan terhadap jatuh yang dapat menopang cedera yang lebih kuat atau bahkan fatal.
Sebuah metode diperlukan untuk mendeteksi jatuh ketika orang lanjut usia jatuh dan tanpa pengawasan sehingga bantuan dapat dikirim ke arah mereka dan meminimalkan cedera mereka. Pendekatan berbasis video merupakan salah satu cara agar kejatuhan dapat dideteksi, pada tugas akhir ini dibangun aktivitas pengenalan manusia menggunakan metode RNN-CNN dan dilatih pada dataset kejatuhan untuk mendeteksi kejatuhan. Tesis ini kemudian akan melakukan penelitian tentang apakah metode berbasis penglihatan adalah pilihan yang lebih layak untuk mendeteksi jatuh dibandingkan dengan metode deteksi jatuh lainnya.
Dengan pendekatan ini menggunakan set data jatuh URfall untuk pelatihan mencapai akurasi 42,86% dan menggunakan set data jatuh beberapa kamera mencapai skor akurasi 91,67%. 

","Falls are a frequent, yet often neglected, source of injuries. One in every three persons over the age of 65, and half of those over the age of 80, will fall at least once a year. With elderly people being in the more vulnerable state against a fall that can sustain a stronger or even a fatal injury. 
A method is needed to detect a fall for when an elderly people fell and in an unattended manner so that help could be sent their way and minimize their injury. A video-based approach is one way that a fall can be detected, on this thesis a human recognition activity was build using RNN-CNN method and trained on fall datasets to detect falls. This thesis will then conduct research on if a vision-based method is a more viable option of detecting falls compare to other methods of fall detection.
With this approach using the URfall dataset for training it achieved 42.86% accuracy and using the multiple camera fall dataset achieved an accuracy score 91.67%.   
",,,172103,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
210944,,YOHANKRISTIAN P. T.,Perbandingan Akurasi Model Machine Learning dalam Pendeteksian Down Syndrome Menggunakan Gambar Wajah,,,"identifikasi gambar, down syndrome, machine learning, deep learning, perbandingan model","Lukman Heryawan, S.T., M.T. Ph.D",2,3,0,2022,1,"Down syndrome merupakan sebuah kelainan genetik yang kemunculannya termasuk dalam kategori langka, dan dapat menyebabkan berbagai kecacatan selama perkembangan. Identifikasi yang lebih awal dan cepat dapat membantu para pengidap down syndrome. Tetapi dengan semakin berkembangnya teknologi yang cepat, terutama dalam bidang kecerdasan buatan, menyebabkan identifikasi dengan menggunakan gambar wajah memiliki banyak model yang memiliki kelebihan dan kekurangannya masing-masing. Salah satu cabang dari Artificial Intelligence atau AI yaitu machine learning memiliki berbagai banyak model, dan adanya sub bidang dari machine learning yaitu deep learning yang dikatakan memiliki kemampuan lebih dalam memproses data yang diberikan dan membuat suatu kesimpulan sehingga memberikan hasil yang lebih memuaskan.
Pada penelitian kali ini bertujuan untuk melakukan perbandingan terhadap keempat model machine learning yaitu logistic regression, support vector machine, convolutional neural network, dan sebuah model hybrid yaitu long short-term memory + convolutional neural network. Perbandingan dilakukan untuk mengetahui masing-masing kelebihan dan kekurangan serta performa dari keempat model tersebut dengan menggunakan dataset berupa gambar wajah dari pengidap down syndrome dan gambar wajah dari orang normal yang dicampur dengan tujuan untuk melakukan identifikasi down syndrome dari gambar wajah tersebut.
Hasil penelitian yang telah diperoleh menunjukkan bahwa performa model CNN dan model hybrid (LSTM + CNN) lebih baik daripada performa model machine learning lainnya. Sebagai contoh, model hybrid memiliki akurasi sebesar 96,97% dengan waktu training maksimal 27 menit diiringi waktu testing maksimal 3 detik, sedangkan untuk efisiensi waktu dan resource yang dimiliki, model logistic regression memiliki performa yang lebih baik dan training yang cepat dengan maksimal 3 menit diiringi dengan waktu testing maksimal 0,11 detik dan memiliki akurasi sebesar 92%.","Down syndrome is a genetic disorder whose appearance is included in the rare category, and can cause various defects during development. Early and rapid identification can help people with down syndrome. With the advanced development of technology, especially in the field of Artificial Intelligence, identification using facial images has many models that have their respective advantages and disadvantages.
One of the branches of Artificial Intelligence or AI, namely machine learning has various models, and there is a sub-field of machine learning, namely deep learning which is said to have better ability to process the given data and make a conclusion so as to give more satisfactory results.
This study aims to compare the four machine learning models, namely logistic regression, support vector machine, convolutional neural network, and a hybrid model, namely long short-term memory + convolutional neural network. Comparisons were made to find out the respective advantages and disadvantages as well as the performance of the four models using a dataset in the form of facial images of people with Down syndrome and facial images of normal people mixed with the aim of identifying Down syndrome from these facial images.
The results obtained show that the performance of the CNN model and the hybrid model (LSTM + CNN) is better than the performance of other machine learning models. For example, the hybrid model has an accuracy of 96.97% with a maximum training time of 27 minutes and a maximum testing time of 3 seconds, while for time and resource efficiency, the logistic regression model has better performance and fast training with a maximum of 3 minutes and maximum testing time of 0.11 seconds with an accuracy of 92%.",,,162074,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214277,,MUHARROMAN ATTORIQ Z,PEMBANGUNAN MODEL CNN UNTUK KLASIFIKASI JENIS BUAH-BUAHAN DENGAN PREPROCESSING GRABCUT DAN MOBILENETS UNTUK IMPLEMENTASI ANDROID,,,"Klasifikasi gambar, CNN, GrabCut, MobileNets","Dr. Agus Sihabuddin, S.Si., M.Kom.",2,3,0,2022,1,"Convolutional Neural Networks (CNN) merupakan kelas deep learning yang paling umum digunakan untuk klasifikasi gambar. CNN dilakukan dengan melatih neural network pada dataset pelatihan tertentu. Agar tidak ada bias dalam pelatihan model, dataset pelatihan cenderung bersifat bersih. Hal ini berakibat buruk pada implementasi dunia nyata terutama untuk klasifikasi gambar, karena sifat gambar nyata yang memiliki noise dan fitur yang tidak diinginkan seperti latar gambar. Maka, dibutuhkan metode yang dapat membersihkan noise dan latar pada gambar. GrabCut merupakan algoritma graph-cut iteratif yang dirancang untuk memisahkan objek dari latar gambar dengan interaksi pengguna minimal. Interaksi yang dibutuhkan berupa menggambar kotak sekeliling objek gambar. Area di luar kotak diidentifikasi sebagai area latar, dan informasi tersebut digunakan untuk mengidentifikasi area objek dan latar di dalam kotak. Pada penelitian ini, GrabCut akan digunakan untuk membersihkan latar pada gambar agar gambar dapat diidentifikasi oleh model CNN. Kelas buah dibatasi menjadi apel, pisang, lemon dan jeruk. Untuk mengetahui efek GrabCut pada hasil klasifikasi model, dibuat dataset gambar nyata hasil pemotretan. Lalu, digunakan GrabCut untuk membuat dataset serupa dengan latar yang bersih. Saat model diuji pada dataset tes bersih, gambar nyata sebelum dan setelah GrabCut didapatkan hasil akurasi secara berurutan 0.9970, 0.8891 dan 0.9238. Saat sistem diimplementasikan pada perangkat android, ditemukan perbedaan waktu eksekusi dimana prediksi gambar nyata secara langsung membutuhkan waktu 0,1 hingga 0,5 detik, sementara dengan GrabCut rata-rata waktunya di atas 1 detik.","Convolutional Neural Networks (CNN) is a deep learning class most commonly used for image classification. CNN is used by training neural networks on a training dataset. In order to avoid bias in model-training, training datasets tend to be clean. This affects negatively on real world implementation especially on image classification, because real pictures tend to have noises and unwanted features such as background. Thus, a method to clean noise and image background is needed. GrabCut is an algorithm of iterative graph-cut designed to segment object from its background with minimum user interaction. The system requires user to draw a rectangle around the object. Area outside the rectangle will be classified as background, and this information is used to separate object and background within the rectangle. In this research, GrabCut will be used to clean image background in order for the image to be identified with the CNN model. Fruit classes are limited to apple, orange, banana and lemon. To observe the effect of GrabCut on the CNN model, real-life pictures of fruits are taken as testing dataset. Then, GrabCut is used to create a to create similar dataset with clean background. The CNN model is tested on the testing dataset from Kaggle, real-life pictures before and after GrabCut. The accuracy on each dataset is  0.9970, 0.8891 dan 0.9238. When the system is implemented on android device, there is a difference in execution time for prediction with and without GrabCut. Without GrabCut, prediction takes 0.1-0.5 seconds, while with GrabCut it takes more than 1 second on average.",,,165628,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
216071,,JUANDITO BATARA K,Pengembangan Model Prakiraan Harga Saham Menggunakan Long Short-Term Memory dengan Technical Indicator dan Categorical Embedding,,,"prakiraan, saham, long short-term memory, data historikal, indikator teknikal, categorical embedding","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2022,1,"Saham dan instrumen investasi lainnya menjadi salah satu topik yang ramai diperbincangkan oleh publik terutama di tengah kondisi pandemi, sehingga terjadi peningkatan minat masyarakat untuk berinvestasi di pasar modal. Saham dikenal sebagai suatu pilihan instrumen investasi yang tidak mudah diprakirakan, sehingga dalam menentukan keputusan untuk membeli/menjual saham digunakan beberapa data seperti indikator teknikal dan keterangan Seasonality sebagai data acuan tambahan. Penelitian terkait sistem prakiraan harga saham masih sering dilakukan hingga saat ini dan masih diperlukannya penelitian terkait prediksi harga saham yang membandingkan pengaruh penggunaan data acuan, terutama data olahan berdasarkan data historikal harga.
Penelitian ini mengembangkan model prakiraan harga saham dan membandingkan pengaruh penggunaan kombinasi data masukan harga beserta hasil olahannya, seperti indikator teknikal dan seasonality yang direpresentasikan dalam bentuk categorical embedding. Adapun penelitian ini menggunakan arsitektur Long Short-Term Memory untuk pembuatan model, dikarenakan penggunaan cell state yang dapat membawa informasi jangka panjang yang ditentukan menggunakan struktur gate untuk penambahan/pengurangan informasi. Penelitian ini menggunakan beberapa data saham yang termasuk dalam indeks IDX30, yaitu PT Vale Indonesia Tbk dan PT Indofood Sukses Makmur Tbk.
Hasil penelitian menunjukkan bahwa penggunaan hasil olahan data harga terutama data indikator teknikal sebagai data masukan tambahan pada model Long Short-Term Memory untuk prakiraan harga dapat memberikan peningkatan pada performa prediksi model berdasarkan perhitungan metrik evaluasi Root Mean Squared Error. Selain itu, model dengan data masukan harga dan indikator teknikal mempunyai waktu rata-rata pelatihan model yang lebih cepat dibandingkan dengan jenis model lainnya.","Stocks and other investment instruments have become one of the hot topics discussed by the public, particularly during pandemic conditions, resulting in public interest increases in investing in the capital market. Stocks are known as an investment option that is tricky to predict, so to determine the buy/sell stocks decisions, several data such as technical indicators and seasonality information are used as additional reference. Research related to stock price forecasting systems is still often carried out to date and there is still a need for research related to stock price prediction that compares the effect of using reference data, particularly processed data based on historical price data.
This research developed a stock price forecasting model and compared the effect of using a combination of price input data and its processed results, such as technical indicators and seasonality represented in the form of categorical embedding. This research used the Long Short-Term Memory as the model architecture, due to the use of cell states that can carry long-term information that is determined using a gate structure for adding/removing information. This research used stocks data included in the IDX30 index, namely PT Vale Indonesia Tbk and PT Indofood Sukses Makmur Tbk.
The results showed that the use of processed price data, particularly technical indicators as additional input data in the Long Short-Term Memory model for price forecasting can provide an increase in model prediction performance based on the calculation of the Root Mean Squared Error evaluation metric. In addition, models with price and technical indicator input data have a faster average model training time compared to other types of models.",,,167108,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217614,,M SYAMIL SUMANJAYA,Handling Imbalance Fraud Data Using Sampling Method,,,"Fraud Detection, Supervised Learning, Resampling Method","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2022,1,"Di era baru di mana transformasi digital telah menjadi hal umum di sektor industri. Transformasi digital memberikan industri dengan peluang yang yang tak tertandingi. Transformasi digital telah melanda seluruh sektor industri. Saat industri mengadopsi teknologi baru yang imersif, mereka juga menghadapi risiko penipuan baru. Fraud adalah tindakan kriminal yang dimaksudkan untuk menghasilkan keuntungan finansial secara pribadi. Dengan bantuan teknologi canggih, penjahat mengembangkan mekanisme baru untuk melakukan penipuan.
Penelitian ini bertujuan untuk menemukan metode terbaik untuk memecahkan masalah fraud, terutama dalam penggunaan kartu kredit sehari-hari. Karena berdasarkan data di dunia nyata, jumlah transaksi fraud sangat kecil dibandingkan dengan yang sah, karena itu resampling method terbaik perlu diteliti untuk memberikan data training yang baik untuk model klasifikasi. Penelitian ini menggunakan tiga metode pengambilan sampel yaitu Random Under Sampling, Random Over Sampling, dan Synthetic Minority Oversampling Technique  untuk menyeimbangkan data fraud. Kemudian tiga metode klasifikasi, Support Vector Machine, Decision Tree, dan Gaussian Naive Bayes, digunakan untuk mengklasifikasikan setiap data training yang telah dijadikan sampel. 
Hasil penelitian ini menunjukkan bahwa penggunaan metode sampling memang mempengaruhi kinerja classifier dalam mengklasifikasikan data. Hal ini ditunjukkan bahwa
menggabungkan SMOTE untuk menyeimbangkan data dan menggunakan algoritma Decision Tree memberikan hasil terbaik dibandingkan kombinasi lainnya dengan skor recall 0,99745, precision 0.99695, F-measure 0.9972, dan skor AUC 0.993.","In the new era where digital transformation has become common in the industrial sector. Digital transformation provides the industry with unparalleled
opportunities for value creation. Digital transformation has swept across the industrial sector. As the industry adopts immersive new technologies, they also encounter new fraud risks. Fraud is a wrongful or criminal deception intended to result in financial or personal gain. With the help of advanced technology, criminals develop a new mechanism to commit fraud.
This research aims to find the best method for solving fraud problems, especially in the daily usage of credit cards. Since, in real world data, the number of fraudulent transactions is very small compared to legitimate ones, the best balancing method needs to be researched to provide good training data for the classification model.This research uses three sampling methods, Random Under Sampling, Random Over Sampling, and Synthetic Minority Oversampling Technique to balance the fraud data. Then three classification methods, Support Vector Machine, Decision Tree, and Gaussian Naive Bayes, are used to classify each training data that has been sampled.
The results of this study indicate that the use of sampling methods indeed influences the performance of the classifier in classifying the data. It is shown that combining SMOTE to balance data and using the Decision Tree algorithm is the best result compared to other combinations with the scores of recall 0.99745,
precision 0.99695, F-measure 0.99720, and the AUC scores 0.993.",,,168943,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
216079,,ARIF FARHAN BUKHORI,KONTEKSTUAL DAN NON-KONTEKSTUAL WORD EMBEDDING DALAM MENDETEKSI TWEET MULTICLASS BERNUANSA DEPRESIF DI TWITTER MENGGUNAKAN LONG SHORT-TERM MEMORY&Atilde;‚&Acirc;&cedil;,,,"Klasifikasi Teks, Tweet Depressive, BERT, Kontekstual Embedding, Long Short-Term Memory. ","Lukman Heryawan, S.T., M.T., Ph.D.; Drs. Edi Winarko, M.Sc.,Ph.D.",2,3,0,2022,1,"Salah satu bentuk emosi yang diekspresikan melalui media sosial adalah perasaan yang bernuansa depresif. Salah satu situs jejaring sosial yang paling banyak digunakan adalah Twitter, dengan 300 juta lebih pengguna aktif, twitter telah menjadi platform besar untuk mengekspresikan pandangan seseorang, sehingga banyak peneliti yang kini mulai menggunakannya sebagai sumber yang baik untuk memperoleh data yang berkaitan dengan Kesehatan mental. Analisis sentimen merupakan cara untuk mengidentifikasi perilaku depresif di media sosial. Beberapa jenis word embedding dalam melakukan analisis sentimen yang terkenal adalah Word2Vec, FastText, dan juga BERT. Penelitian ini bertujuan untuk melakukan perbandingan kontekstual dan non kontektsual word embedding Word2Vec, FastText, dan BERT dalam mendeteksi tweet bernuansa depresif di media sosial twitter menggunakan model LSTM. Berdasarkan hasil pengujian dan perbandingan, kombinasi model word embedding kontekstual BERT dan Long Short-Term Memory terbukti dapat memberikan nilai  yang lebih baik dalam melakukan klasifikasi pada tweet bernuansa depresif, dengan skor F1-Score mencapai diatas 95% terhadap dataset bersifat multiclass.","Feelings of depression are one type of emotion expressed on social media. With over 300 million active users, one of the most popular social networking sites, Twitter, has become a huge platform for people to express their feelings. So many researchers are now using it as a good source for obtaining data related to mental health. Sentiment analysis is a way to identify depressive behavior on social media. Word2Vec, FastText, and BERT are three well-known types of word embedding in sentiment analysis. This study aims to compare contextual and non-contextual word embedding Word2Vec, FastText, and BERT in detecting depressive tweets on Twitter social media using the LSTM model. Based on tests and comparisons, the combination of the BERT contextual word embedding model and Long Short-Term Memory proved to provide a better value in classifying tweets with depression nuances, with an F1-Score reaching 95% for a multiclass dataset.",,,167118,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219663,,ARYASAKTI WIRASENA,Analisis Perbandingan Metode Hierarchical Clustering Untuk Multiple Sequence Alignment,,,"Multiple sequence alignment, Clustal, Hierarchical clustering","Afiahayati, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Multiple sequence alignment atau MSA adalah sebuah proses penting dalam bidang bioinformatika yang melakukan pengolahan data sekuens protein asam amino atau basa DNA. Salah satu implementasi MSA adalah Clustal, yang membagi proses MSA menjadi tiga tahap. Salah satu tahap Clustal adalah pembuatan guide tree menggunakan algoritma hierarchical clustering, yang menentukan urutan pemrosesan sekuens dalam tahap progressive alignment. Karena terdapat beberapa metode hierarchical clustering, diperlukan perbandingan performa beberapa metode hierarchical alignment dalam membentuk guide tree untuk proses MSA.
Penelitian ini membandingkan metode single linkage, complete linkage, dan average linkage dalam membuat guide tree untuk melakukan MSA pada tiga buah data sekuens dari BaliBASE 4. Guide tree yang dihasilkan dari masing-masing metode dijadikan input untuk program ClustalX untuk menghasilkan alignment. Metrik penilaian column scoring digunakan untuk menilai kualitas sebuah alignment.
Hasil penelitian menunjukkan bahwa metode single linkage cenderung menghasilkan alignment dengan quality score yang lebih tinggi dari kedua metode lainnya. Akan tetapi pada sebagian besar kasus keunggulan single linkage tidak begitu signifikan dan dapat dikalahkan oleh metode lain pada beberapa kasus. Metode single linkage dinilai paling efektif jika data yang digunakan mengandung sejumlah outlier.","Multiple sequence alignment or MSA is an important process in the field of bioinformatics that performs processing on a sequence of amino acid protein or sequence of DNA bases. One implementation of MSA is the Clustal program, that divides the MSA process into three stages. One of those stages is guide tree generation using a hierarchical clustering algorithm which determines in what order the sequences will be processed in the progressive alignment stage. Since there are many hierarchical clustering methods, there is a need to compare the performace of hierarchical clustering methods in generating a guide tree for MSA.
This study compares the single linkage, complete linkage, and average linkage methods in generating a guide tree for MSA on three sequence data taken from BaliBASE 4. The guide trees generated by each method is used as input to the ClustalX program to aid in creating an alignment. The built-in column scoring metric is used to determine the quality of an alignment.
Results show that the single linkage method tends to generate an alignment with higher quality score in comparison to the other two methods. However, on most cases the difference of score between single linkage and the other methods are insignificant and sometimes outdone by the other methods on other cases. Single linkage may be more effective if the data used contains a number of outliers.",,,171040,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213264,,RIEZA FERDIANSYAH,EVALUASI KRITERIA PENILAIAN KUALITAS BATIK MENGGUNAKAN METODE FUZZY PIPRECIA (STUDI KASUS INDUSTRI BATIK DI KOTA PEKALONGAN),,," PIPRECIA, Fuzzy Logic, Batik","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.; Muhammad Alfian Amrizal, B.Eng., M.I.S., Ph.D.",2,3,0,2022,1,"14 kriteria kualitas batik dari BBKB yang berpotensi digunakan sebagai pedoman bagi produsen batik. Namun, karena ranking kepentingannya tidak diketahui, 14 kriteria ini belum bisa dijadikan pedoman untuk menghasilkan batik-batik yang berkualitas. Metode Fuzzy PIPRECIA yang memiliki perhitungan bobot kriteria secara dua arah digunakan dalam meranking 14 kriteria kualitas penilaian batik dari BBKB. Kemudian, dilakukan evaluasi hasil kualitas perankingan yang dihasilkan Fuzzy PIPRECIA dengan membandingkannya dengan Fuzzy SWARA terhadap 10 responden utama dan lima responden evaluasi akhir. Selain itu, telah dihitung nilai Spearman and Pearson Rank Correlation Coefficients pada Fuzzy PIPRECIA, root mean square error (RMSE), dan perbandingan waktu lamanya pengisian. Hasilnya adalah kualitas ranking top 1 pada Fuzzy PIPRECIA adalah yang terbaik dibandingkan keseluruhan top n. Kualitas ranking top 1 pada Fuzzy PIPRECIA dan Fuzzy SWARA adalah yang terbaik dibandingkan keseluruhan top n. Fuzzy PIPRECIA memiliki nilai spearman and pearson correlation coefficients yang searah dan positif. Fuzzy PIPRECIA cenderung lebih baik dibandingkan Fuzzy SWARA seiring bertambahnya jumlah kriteria yang diranking. Waktu pengisian Fuzzy PIPRECIA memakan waktu hampir tiga kali lebih lama dibandingkan Fuzzy SWARA. Penelitian ini diharapkan dapat memberikan arahan bagi industri batik mengenai kriteria apa yang harus diprioritaskan serta penggunaan metode dalam penelitian lainnya.","14 batik quality criterias from BBKB have the potential to be used as a guide for batik producers. However, since the importance ranking is unknown, these criterias cannot be used as guidelines to produce best quality batik. Fuzzy PIPRECIA method, which has a two-way calculation of criterion weights, is used to rank these criterias. Then, evaluation of resulting ranking quality is carried out Fuzzy PIPRECIA by comparing it with Fuzzy SWARA against 10 primary respondents and five respondents for the final evaluation. Then, Spearman and Pearson Rank Correlation Coefficients values, root mean square error (RMSE), and the comparison of charging time have been calculated on Fuzzy PIPRECIA. The result is quality of top 1 rank in Fuzzy PIPRECIA and Fuzzy SWARA is the best compared to overall top-n. Fuzzy PIPRECIA has a positive and unidirectional Spearman and Pearson correlation coefficient. Fuzzy PIPRECIA tends to be better than Fuzzy SWARA as the number of criteria ranked is increased. Fuzzy PIPRECIA charging time takes almost three times longer than Fuzzy SWARA. This research is expected to be able to provide direction for the batik industry regarding what criteria should be prioritized as well as the use of methods in other research.",,,164185,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207633,,ABEL ATHALLAH C,MODERN BANKING SOLUTION: A CASE STUDY OF CROSS BORDER MONEY TRANSACTION USING BLOCKCHAIN TECHNOLOGY,,,"blockchain, ledger, smart contract, cross-border money transaction, remittance","Bambang Nurcahyo Prastowo, Drs., M.Sc.",2,3,0,2022,1,"Sejak dibuatnya Bitcoin pada tahun 2009 oleh Satoshi Nakamoto, metode yang mendasari keberlangsungannya, blockchain, telah menunjukkan banyak prospek aplikasi yang menjanjikan serta menarik perhatian dari banyak profesional pada berbagai bidang di seluruh dunia terutama industri perbankan. Kemampuan teknologi blockchain untuk mencatat transaksi pada buku besar yang didistribusikan juga menawarkan peluang alternatif baru bagi lembaga keuangan untuk meningkatkan transparansi, pencegahan penipuan, serta menciptakan kepercayaan di antara pelanggan mereka, dan berdasarkan itulah, blockchain mengubah sistem keuangan konvensional yang sudah ada. Sistem konvensional dihadapi teknologi baru seperti jaringan Ripple yang didasarkan pada teknologi buku besar terdistribusi blockchain dengan token atau koin cryptocurrency mereka tersendiri.
Dengan demikian, tujuan daripada penelitian ini adalah untuk memberikan ringkasan pada penggunaan jaringan teknologi terdistribusi berbasis blockchain pada industri perbankan untuk pengiriman uang &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12; pengiriman uang yang ditransfer ke pihak lain, seperti transaksi uang lintas negara/batas. Penelitian ini juga akan mencakup penjelasan yang mendalam mengenai aspek dan prinsip utama di teknologi blockchain seperti smart contract, big book ledger pada perbankan otomatis, dan penjelasan singkat pada fasilitas pengiriman uang global seperti yang disebutkan sebelumnya. Oleh karena itu, kita akan membahas SWIFT, yaitu sebagai jaringan konvensional untuk pengiriman pesan yang luas yang digunakan oleh bank dan lembaga keuangan lainnya untuk instruksi pengiriman uang. Studi ini juga akan melanjutkan pembahasan ke teknologi berbasis blockchain sebagai solusi alternatif, yang kemudian dilanjutkan dengan membahas keuntungan dan masalah utama yang harus dipertimbangkan ketika menerapkan teknologi berbasis  ledger tersebut.
","Since the creation of Bitcoin back in 2009 by Satoshi Nakamoto, its technique which is underlying the continuity, blockchain, has shown many promising application prospects and attracts attention from many professional fields around the world especially the banking industry. The ability of the blockchain technology to record transactions on distributed ledgers offers new alternative opportunity for financial institutions to improve their transparency, fraud prevention and create trust among their customers, and just like that the blockchain transforms the traditional financial systems. The traditional system is facing new technology such as Ripple network which is based on the blockchain distributed ledger technology with its own cryptocurrency tokens or coins.
Thus, the goal of this research study is to provide a summary of the blockchain based distributed technology network&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;s use case in the banking industry for remittance &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12; payment of money that is transferred to another party, such as cross-border money transaction. This research will also cover an insightful description of blockchain main technological aspects and principles such as smart contracts, automated banking ledgers and brief description through facilitating global money remittance as mentioned above. From there, we will discuss the SWIFT, as a vast messaging network that is used by banks and other financial institutions for money transfer instructions. This study will proceed to the emerging blockchain based technology, then proceeds to discuss the advantages and key issues that must be considered when applying such ledger-based technology.
",,,158850,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217362,,L TARANGGA ARIEF G,PERBANDINGAN NAIVE BAYES TERMODIFIKASI UNTUK DETEKSI ARTIKEL CLICKBAIT BERBAHASA INDONESIA ,,," naive bayes, clickbait, judul, penerbit, klasifikasi teks","Dzikri Rahadian Fudholi, S.Kom., M.Comp.",2,3,0,2022,1,"Judul clickbait telah memberikan dampak negatif ke masyarakat. Masyarakat merasa ditipu dan termanipulasi karena berita tersebut. Di lain pihak sudah banyak penelitian yang menggunakan pembelajaran mesin untuk mendeteksi judul clickbait. Penelitian-penelitian tersebut sayangnya hanya berfokus kepada kata-kata yang digunakan tanpa mempertimbangkan penerbit dari artikel tersebut. 	Pada penelitian ini  dataset yang digunaan adalah CLICK-ID dataset yang terdiri dari 15000 data dengan 8710 data non-clickbait dan 6290 artikel clickbait yang memiliki 12 penerbit yang berbeda. Dataset pada penelitian ini akan dipreprocessing dengan menggunakan lowercase conversion, stemming, dan stopword removal. Dataset yang telah dibersihkan dimasukkan kedalam 2 algoritma Naive Bayes yang telah dilakukkan oleh penelitian sebelumnya yaitu Multinomial Naive Bayes (MNB) dan Tree Augmented Naive Bayes (TANB) serta 2 algoritma Naive Bayes yang dimodifikasi yaitu Publisher Multinomial Naive Bayes (PMNB) dan Title Publisher Multinomial Naive Bayes (TPMNB). Hasil dari penelitian ini didapati bahwa TPMNB memilki akurasi, presisi, dan recall tertinggi dengan nilai  berturut-turut 0.909826, 0.936554, dan 0.886057 untuk pelatihan dan 0.774, 0.766667, dan 0.720528 untuk pengujian sedangkan TANB memiliki nilai recall tertinggi dengan nilai 0.876067 untuk pelatihan dan 0.7173 untuk pengujian. Selain itu juga diperoleh bahwa MNB, PMNB, dan TPMNB memiliki kompleksitas pelatihan dan prediksi yang sama yaitu O(nt) dan O(t) sedangkan TANNB memiliki kompleksitas yang lebih besar dibandingkan algoritma lain dalam penelitian ini.","Clickbait titles have harmed society. People feel cheated and manipulated because of the news. On the other hand, many studies have used machine learning to detect clickbait titles. These studies unfortunately only focus on the words used without considering the publisher of the article. In this study, the dataset used is the CLICK-ID dataset which consists of 15000 data with 8710 non-clickbait data and 6290 clickbait articles with 12 different publishers. The dataset in this study will be preprocessed using lowercase conversion, stemming, and stopword removal. The cleaned dataset is entered into 2 Naive Bayes algorithms that have been carried out by previous research, namely Multinomial Naive Bayes (MNB) and Tree Augmented Naive Bayes (TANB) as well as 2 modified Naive Bayes algorithms namely Publisher Multinomial Naive Bayes (PMNB) and Title Publisher Multinomial Naive Bayes (TPMNB). The results of this study found that TPMNB had the highest accuracy, precision, and recall with values &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;of 0.909826, 0.936554, and 0.886057 respectively for training and 0.774, 0.766667, and 0.720528 for testing. In contrast, TANB had the highest recall values &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;with values &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;of 0.876067 for movement and 0.7173 for testing. In addition, it was also found that MNB, PMNB, and TPMNB had the same training and prediction complexity, namely O(nt) and O(t) while TANNB had greater complexity than other algorithms in this study.",,,170493,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212245,,FEDORA R W P,FINE-GRAINED SENTIMENT CLASSIFICATION ON COVID-19 VACCINATION IN INDONESIA USING BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS (BERT) AND CONVOLUTIONAL NEURAL NETWORK (CNN) ,,,"klasifikasi sentimen, klasifikasi sentimen secara fine-grained, BERT, Word2Vec, convolutional neural network, deep learning, tweets","Azhari, Drs., M.T., Dr",2,3,0,2022,1,"       Untuk meningkatkan kualitas, pendapat merupakan hal penting sebagai masukan. Analisis sentimen adalah salah satu bidang pemrosesan bahasa alami yang mempelajari tentang opini, sentimen, dan emosi yang tersedia pada teks. Terkadang, opini tidak bisa digolongkan menjadi hanya baik atau buruk, bisa jadi di antara keduanya. 
        Klasifikasi sentiment secara fine-grained dapat digunakan untuk mengklasifikasikan opini ke dalam banyak kelas dengan tujuan untuk memahami opini lebih baik. Tidak banyak penelitian klasifikasi sentimen yang menggunakan kelas lebih dari tiga, tetapi beberapa telah menerapkannya dalam bahasa Inggris dengan kombinasi model deep learning, namun untuk menemukan topik ini dalam Bahasa Indonesia tidaklah mudah.
	Penelitian ini menggunakan model kombinasi BERT sebagai contextualized word embedding dan CNN, yang dibandingkan dengan conventional word embedding Word2Vec dan CNN. Hasil klasifikasi sentimen pada kedua model memberikan hasil terbaik dari kombinasi model BERT-CNN dengan skor macro-average precision sebesar 61.08, macro-averaged recall sebesar 59.49, macro-averaged f1-score sebesar 59.57, dan akurasi sebesar 61.32.","        In order to improve qualities, opinions are essential as feedback. Sentiment analysis is one of natural language processing field that studies about opinions, sentiments, and emotions that are available on texts. Sometimes, opinion can only not be good or bad, it might be somewhere in between. 
        Fine-grained sentiment classification can be used to classify opinion into many classes to understand the opinions better. There are not a lot of sentiment classification research using classes more than three, but some have implemented it in English with the combination of deep learning model, however, to find about this topic in Bahasa is not easy.
	This research uses the combination model of BERT as the contextualized word embedding and CNN, which is compared to the conventional word embedding Word2Vec and CNN. The result of fine-grained sentiment classification on both model gives the best result from the combination of BERT-CNN model with macro-average precision score of 61.08, macro-averaged recall score of 59.49, macro-averaged f1-score of 59.57, and accuracy of 61.32",,,163303,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217109,,FARIS SATYA WIBISANA,A Comparison Between Linear Regression and Neural Network To Predict Tea Yield Prediction,,,"Long Short Term Memory, Multi-Layer Perceptron, Linear Regression, Time Series, Tea Yield Prediction","Muhammad Alfian Amrizal, B.Eng., M.I.S., Ph.D.;I Gede Mujiyatna, S.Kom., M.Kom.;",2,3,0,2022,1,"Prediksi hasil teh adalah teknik untuk memprediksi produksi teh, yang penting untuk mendukung proses pengambilan keputusan tingkat makro dan tingkat mikro di suatu negara. Dengan integrasi algoritma machine learning pada prediksi hasil teh, diharapkan dapat mencapai prediksi yang lebih akurat dan bermanfaat bagi semua pihak. Banyak model machine learning yang dapat digunakan untuk membuat model prediksi hasil panen, dengan kompleksitas yang berbeda untuk setiap model. Model yang dipilih untuk penelitian ini adalah Linear Regression yang mewakili model statistik klasik, Multi-Layer Perceptron, yang mewakili model jaringan saraf tiruan sederhana, dan Long Short-Term Memory, yang mewakili jaringan saraf yang lebih kompleks. Hasil penelitian menunjukkan bahwa Multi-Layer Perceptron memiliki kinerja terbaik dibandingkan dengan yang lain, diikuti oleh Regresi Linier. Multi-layer Perceptron mengungguli Linear Regression sebesar ~18% dan ~40% dibandingkan dengan Long Short-Term Memory dalam Mean Squared Error. Namun, model Linear Regression masih yang tercepat dibandingkan dua lainnya. Memori Jangka Pendek Panjang, meskipun berkinerja buruk dibandingkan dengan dua lainnya, adalah yang paling stabil ketika berbicara tentang lonjakan data yang tiba-tiba. Ini menekankan potensi yang dimiliki jaringan saraf dalam prediksi hasil panen, yang, jika disetel dengan benar, akan mengungguli model statistik klasik.","Tea yield prediction is a technique to predict tea production, essential to support macro-level and micro-level decision-making processes in a country. With the integration of machine learning algorithms to tea yield prediction, it is expected that a more accurate prediction can be achieved and benefit all parties. Many machine learning models may be utilized to create a crop yield prediction model, with different complexity for each model.  The models that are selected for this research are Linear Regression which represents the classical statistical model, Multi-Layer Perceptron, which represents a simple artificial neural network model, and Long Short-Term Memory, which represents a more complex neural network. The result shows that Multi-Layer Perceptron has the best performance compared to the other, followed by Linear Regression. Multi-Layer Perceptron outperforms Linear Regression by ~18% and ~40% compared to Long Short-Term Memory in Mean Squared Error. However, the Linear Regression model is still the fastest compared to the other two. Long Short-Term Memory, even though it underperformed compared to the other two, is the most stable when talking about the sudden spikes in the data. This emphasizes the potential that neural networks have in crop yield prediction, which, if tuned correctly, will outperform the classical statistical model.",,,168225,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
209184,,IRVIN ANDERSEN,Deep Learning-based Jeans Fabric Defect Detection Using Modified Network Yolov5,,,"computer vision, deep learning, fabric defect, object detection, yolo","Faizal Makhrus, S.Kom., M.Sc., Ph.D. ; Suprapto, Drs., M.I.Kom., Dr.",2,3,0,2022,1,"Teknologi otomasi seperti pembelajaran mesin memainkan peran yang semakin besar dalam kehidupan sehari-hari. Pentingnya otomasi computer vision sangat penting dalam deteksi cacat kain, karena biaya tenaga kerja inspeksi cacat manual tinggi dan juga kurangnya akurasi inspeksi. Inspeksi cacat manual dibatasi oleh kemampuan manusia karena banyaknya jumlah kain dan varietas cacat. Namun, ada tantangan dalam mengembangkan deteksi cacat kain, seperti memaksimalkan nilai recall dan presisi dengan jumlah dataset yang terbatas.

 Pada penelitian ini mengusulkan model jaringan YOLOv5 yang dimodifikasi untuk menentukan model terbaik untuk deteksi cacat kain jeans. Untuk mengembangkan model yang dapat melakukan tugas deteksi cacat dengan baik, dibutuhkan penentuan posisi cacat yang akurat, dan presisi. Implementasi sistem menggunakan 367 gambar cacat kain dengan total 2 kelas cacat kain jeans. Dataset digunakan augmentasi data untuk membuat lebih banyak dataset. 

Hasil evaluasi sistem menunjukkan model terbaik adalah Improved YOLOv5x dengan akurasi sempurna dan F1-Score. Improved YOLOv5x mengidentifikasi dan mengklasifikasikan semua objek cacat kain pada dataset pengujian. Selain itu, Improved YOLOv5x berhasil memenuhi kecepatan komputasi Realtime dengan kecepatan komputasi 31,15 FPS.","Automation technologies such as machine learning play an important role in everyday life. The importance of computer vision automation is crucial in detecting fabric defects, because of the high labor costs of manual defect inspection and also the lack of inspection. Manual defect inspection is limited by human capabilities due to a large number of fabrics and varieties of defects. However, there are several challenges in developing fabric defect detection, such as maximizing recall and precision with a limited number of data sets.

This research proposes a modified network YOLOv5 model to determine the best model for jeans fabric defect detection. To develop a model that can perform the defect detection task well, it is necessary to find an accurate, precise defect position. The implementation of the program is using 367 fabric defect images with a total of 2 classes of jeans fabric defect and image augmentation to create more datasets.

The results of the system evaluation show the best model is Improved YOLOv5x with perfect accuracy and F1-Score. Improved YOLOv5x successfully identifies and classifies all fabric defect objects in the test dataset. Moreover, the Improved YOLOv5x model successfully fulfills real-time computing speed with a computational speed of 31.15 FPS.",,,160659,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211233,,YUSUF YUDHISTIRA P,Sistem Rekomendasi Pemilihan Technology Stack untuk Pengembangan Software Menggunakan Algoritma Genetika,,,"Technology stack, Software, Algoritma Genetika, Sistem Rekomendasi.","Drs. Retantyo Wardoyo, M.Sc., Ph.D.",2,3,0,2022,1,"Technology stack merupakan salah satu bagian penting dalam arsitektur software, yaitu kombinasi teknologi yang akan digunakan pada pengembangan software. Pemilihan technology stack yang tepat dapat memberikan pengaruh positif terhadap kinerja software. Saat ini pemilihan technology stack itu sendiri masih dilakukan secara manual, didasarkan pada pengalaman dan penilaian dari pengembang software itu sendiri. Hal ini akan memakan waktu yang lama untuk menganalisa masing-masing variasi technology stack yang tersedia.
	Maka dari itu pada penelitian ini akan merancang sistem rekomendasi dengan metode content-based filtering yang secara otomatis dapat menghasilkan rekomendasi untuk technology stack yang optimal dengan menggunakan algoritma genetika. Algoritma genetika digunakan untuk melakukan pencarian technology stack yang optimal, berdasarkan perhitungan dari nilai setiap atribut yang dimiliki teknologi tersebut dan prioritas atribut yang ditentukan oleh pengguna sistem.
	Pengujian pertama dilakukan dengan melakukan percobaan 24 kombinasi parameter algoritma genetika, hasil dari pengujian ini mendapatan score maksimal sebesar 319. Pengujian kedua melakukan survei kepada 12 orang responden yang memiliki pengalaman dalam mengembangkan software. hasil dari pengujian ini mendapatakan nilai rata-rata 4 untuk relevance, 3.83 untuk novelty, 3.91 untuk serendipity dan 3.83 untuk diversity.

","Technology stack is one of the important things in software architecture, it is a technology combination that will be used in software development. The right choice of technology stack can make a positive effect on software development. Now, Technology stack selection is still done manually. It will take a long time to analyze each of the available technology stack variations.
	Therefore, this research will design the recommender system with a content-based filtering method that can automatically generate a recommendation for the optimal technology stack by using genetic algorithm.  Genetic algorithm is used to search the optimal technology stack, based on the calculation from each attribute's value in that technology stack and the priority attributes that are determined by the system user.
	The first test was conducted by experimenting with 24 combinations of genetic algorithm parameters, the results of this test obtained a maximum score of 319. The second test conducted a survey of 12 respondents who had experience developing software. the results of this test get an average value of 4 for relevance, 3.83 for novelty, 3.91 for serendipity, and 3.83 for diversity.

",,,162363,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
210466,,ROFIIF ROBBAANII A,SMART COMMUNITY DEVELOPMENT USING SMART SERVICE MOBILE APPLICATION &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;PUBLIK PINTAR&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;,,,"Smart City, Jakarta, Publik Pintar, Digital Government, Smart Community, Mobile Application.","Dr. Mardhani Riasetiawan, M.T.",2,3,0,2022,1,"Smart City adalah sebuah topik yang terus meningkat sejak 2014, dengan kemajuan Smart City Technology (SCT) semakin cepat dari pendahulunya, Indonesia harus mengikuti kemajuan teknologinya agar dapat berkompetisi di dunia internasional. Namun, pengetahuan dan keakraban warga juga harus mengikuti kemajuan teknologinya. Dengan demikian implementasi dari SCT yang dapat membaur dengan kehidupan sehari-hari dan pada waktu yang bersamaan menjadi simpel dan fungsional untuk menjamin penggunaan oleh warga sangat dibutuhkan. Masuklah &quot;Publik Pintar&quot;, sebuah aplikasi yang menyediakan fungsi yang simpel dan mudah dimengerti untuk komunitas kecil yang memiliki beberapa fitur yang berguna. Penggunaan aplikasi ini akan membantu pemerintah lokal untuk membuat sebuah Digital Government dan Smart Community yang akan memperkenalkan SCT kepada warga dan pada waktu yang bersamaan mendorong kemajuan SCT indonesia. Desain dan implementasi dari aplikasi akan menggunakan sarana bernama android studio dan SQLite sebagai database aplikasi. Aplikasinya akan di tes oleh partisipan yang lalu akan mengisi sebuah survey untuk mengukur performa dari aplikasi debandingkan dengan ekspektasi awal mereka. Aplikasinya memfiturkan sebuah fungsi forum untuk pengguna untuk membahas masalah yang ada di komunitas kecilnya, sebuah fungsi News untuk pengguna untuk melihat berita terbaru di komunitasnya, terkahir, aplikasinya juga memfitur sebuah fungsi dokumen untuk pengguna menyimpan dokumen pentingnya dalam bentuk file digital untuk menghindari kerusakan atau kehilangan dari dokumen tersebut. Kesimpulannya, aplikasinya bekerja seperti ekspektasi menurut partisipan survey dengan para partisipan memberi aplikasinya nilai 8.3 secara rata-rata. Dengan beberapa kritik dan kekurangan para partisipan paham dan cukup puas dengan performa aplikasinya.","Smart City is a growing topic worldwide since 2014, with the advancement of Smart City Technology (SCT) being quicker than ever, Indonesia must keep up with the growth of the technology so as to be competitive on the international board. However, the knowledge and familiarity of the citizen must be able to keep up with the technology. As such, the usage and implementation of an SCT that could seamlessly blend in with daily life while at the same time being simple and functional enough to warrant usage by the normal citizen are highly required. Enter &quot;Publik Pintar&quot;, an application that provides simple and easy-to-use functions for small communities that has various useful features. The usage of this application will help the local government set up a simple Digital Government and Smart Community that will introduce SCT to the normal citizen while at the same time pushing forward the SCT of Indonesia. The design and implementation of the application will use a tool called android studio, and SQLite as the database of the application. The application will then be tested by participants which will later fill out a survey to measure the performance of the application compared to the initial expectations. The application features a forum function for the users to discuss the issues of their small communities, it also features a news function for the users to browse any significant event or problem that appears in their community, lastly, it features a document function for the users to store a softcopy file of their essential document to prevent damage of loss of the document. In conclusion, the application worked as intended according to the participants of the final survey with the participants giving the application an overall score of 8.3. With only minor inconvenience and critics, overall the participants of the application testing understood and were satisfied by the application.",,,161794,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212514,,M RIZQI AMIR P G,Decision Support System for 250CC Sport Motorcycle Selection using Analytical Hierarchy Process and TOPSIS,,,"Sport Motorcycle, Decision Support System, Analytical Hierarchy Process (AHP), TOPSIS","Retantyo Wardoyo, Drs., M.Sc., Ph.d.",2,3,0,2022,1,"Sepeda motor merupakan salah satu kendaraan yang paling banyak dimiliki oleh masyarakat Indonesia. Ada banyak jenis sepeda motor yang dijual di Indonesia salah satunya adalah sepeda motor sport. Sepeda Motor Sport adalah jenis sepeda motor yang memiliki tampilan seperti sepeda motor balap dan memiliki performa mesin yang lebih baik dibandingkan dengan sepeda motor lainnya. Di Indonesia, motor sport dari pabrikan yang berbeda hadir dengan spesifikasi yang berbeda-beda sehingga membuat orang yang ingin membeli motor sport bingung untuk memilih motor sport yang paling cocok untuk mereka.
Masalah seperti ini dapat diselesaikan dengan menggunakan bantuan sistem komputer yaitu sistem pendukung keputusan. Di dalam penelitian ini, Sistem Pendukung Keputusan akan menggunakan dua metode yang berbeda yaitu metode Analytical Hierarchy Process (AHP) untuk proses pembobotan dan metode TOPSIS untuk proses perankingan, dengan beberapa kriteria perbandingan yaitu: Harga, Kapasitas Kubik, Fitur, Horsepower, Berat, Torsi, Konsumsi Bahan Bakar dan Kapasitas tangki bensin untuk memilih motor sport 250cc.
Hasil dari penelitian ini menunjukkan bahwa sistem pendukung keputusan pemilihan sepeda motor sport 250cc dengan metode Analytical Hierarchy Process (AHP) dan TOPSIS dapat memberikan daftar peringkat sepeda motor sport yang dapat digunakan oleh masyarakat sebagai acuan untuk memilih sepeda motor sport 250cc yang paling tepat untuk mereka.","Motorcycles are one of the most owned vehicles by the Indonesia citizen. There are many types of motorcycles sold in Indonesia and one of them is sport motorcycle. Sport Motorcycle is a type of motorcycle that has an appearance just like racing motorcycle and it has a better engine performance compared to the other motorcycle. In Indonesia, sport motorcycle from different manufacturer comes with different specifications and it can make people who wants to buy sport motorcycle confuse to choose the most appropriate sport motorcycle for them.
Problems like this can be solved using the help of computer systems, namely decision support systems. In this research, the Decision Support System will use two different methods, namely Analytical Hierarchy Process (AHP) method for weighting process and TOPSIS method for ranking process, with several comparison criteria namely: Price, Cubic capacity, Features, Horsepower, Weight, Torque, Fuel consumption and Gasoline tank capacity to choose a 250cc sports motorcycle. 
The result of this research show that decision support systems for 250cc sport motorcycle selection using Analytical Hierarchy Process (AHP) and TOPSIS methods can provide a ranking list of sport motorcycle that can be use by people as a reference to choose the most appropriate 250cc sports motorcycle for them.",,,163583,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211245,,DZAKY LIGARWALY R, CLASSIFICATION AND ANALYSIS OF BETA THALASSEMIA AND IRON DEFICIENCY ANEMIA ON BLOOD DISORDER USING MULTI-LAYER PERCEPTRON TEST,,,"Blood Disorder, Screening classification, Machine Learning, Artificial Neural Network, Support Vector Machine ","Afiahayati, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Gangguan darah adalah suatu kondisi yang mempengaruhi kemampuan darah untuk
berfungsi dengan benar. Alasan adanya kelainan darah berkisar dari:
cacat yang menyebabkan penurunan komponen darah atau mempengaruhi
berfungsi untuk mentransfer oksigen seperti anemia (gangguan yang melibatkan
sel darah), leukopenia (gangguan yang mempengaruhi sel darah putih),
trombositopenia (gangguan menyangkut trombosit). Kategori darah
kelainan yang meningkatkan komponen darah adalah eritrositosis (kelainan
yang melibatkan sel darah merah), leukositosis (kelainan yang mempengaruhi
sel darah), trombositemia atau trombositosis (gangguan yang
trombosit).
Penggunaan pembelajaran mesin adalah untuk mengklasifikasikan gangguan. Itu
Gangguan tersebut adalah anemia defisiensi besi, defisiensi besi, hemoglobin
Thalassemia dan Beta-thalassemia disebutkan dalam makalah ini sebagai IDA, DB,
HbE dan BTT. Menurut tim medis dari kesehatan UGM
departemen, kedua gangguan tersebut dianggap sebagai kejadian umum di
Indonesia. Pentingnya penelitian ini adalah untuk memberikan pemahaman awal yang esensial
algoritma penyaringan yang dapat membantu dokter dengan cepat menilai yang diperlukan
langkah-langkah untuk mengeluarkan tindakan yang benar.","A blood disorder is a condition that impacts the blood's ability to
function correctly. The reasons for blood disorders exist range from
defects that cause the decrease in blood components or affect their
function to transfer oxygen such as anemia (a disorder that involves red
blood cells), leukopenia (a disorder that affects white blood cells),
thrombocytopenia (a disorder concerns platelets). Categories of blood
disorders that increase blood components are erythrocytosis (a disorder
that involves red blood cells), leukocytosis (a disorder that affects white
blood cells), thrombocythemia or thrombocytosis (a disorder that concerns
platelets).
The use of machine learning is to classify disorders. Those
disorders are Iron deficiency anemia, Iron Deficiency, Hemoglobin
Thalassemia and Beta-thalassemia are mentioned in this paper as IDA, DB,
HbE and BTT. According to the medical team from the UGM health
department, those two disorders are considered common occurrences in
Indonesia. The importance of this research is to provide an essential early
screening algorithm that may help doctors quickly assess the necessary
steps to issue the correct course of action. ",,,162525,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218925,,M TAUFAN OKKA M,Penerapan Jaringan Saraf Tiruan Untuk Prediksi Hasil Pertandingan Regular Season Liga NBA Menggunakan Four Factors,,,"Jaringan Saraf Tiruan, NBA, Regular Season, Four Factors","I Gede Mujiyatna, S.Kom., M.Kom. ",2,3,0,2022,1,"Dean Oliver (2004) menyatakan bahwa terdapat beberapa faktor yang bisa menentukan kemenangan sebuah tim dalam pertandingan bola basket, yang disebutnya sebagai Four Factors. Four Factors mampu menggambarkan seberapa besar peluang sebuah tim untuk memenangkan pertandingan dengan menggunakan setiap komponen Four Factors pada sebuah tim, namun hal tersebut tidak bisa dijadikan sebagai acuan dalam menentukan pemenang dalam sebuah pertandingan yang mempertemukan dua tim. 
Penelitian ini bertujuan untuk merancang sebuah sistem yang dapat memprediksi hasil pertandingan liga NBA. Komponen Four Factors dari kedua tim yang bertanding digunakan sebagai fitur yang akan diterapkan dalam model jaringan saraf tiruan. Data pertandingan dari lima musim liga pada periode 2014- 2019 dikumpulkan, di mana satu musim liga digunakan sebagai data uji dan sisanya sebagai data latih. Pemilihan model dilakukan dengan mengimplementasikan K-Fold Cross Validation terhadap seluruh data latih. Model terbaik didapatkan pada konfigurasi model 32 neuron hidden layer, tanpa dropout, 0,0005 learning rate, serta 100 epochs dengan akurasi pada tahap validasi senilai 64,95%. Prediksi terhadap data uji dilakukan dengan menggunakan model terpilih dan menghasilkan nilai akurasi 63,41%. Nilai akurasi tersebut masih lebih rendah dibandingkan penelitian sebelumnya dengan pengujian serupa.","Dean Oliver (2004) states that there are several factors that can determine a team's victory in a basketball match, which he calls the Four Factors. Four Factors are able to describe how much chance a team has to win a match by using each component of the four Factors on a team, but it cannot be used as a reference in determining the winner in a match that brings two teams together. 
This research aims to design a system that can predict the outcome of NBA league matches. The Four Factors components of the two competing teams are used as feature to be applied in the artificial neural network model. Match data from five league seasons in the period 2014-2019 were collected, where one league season was used as test data and the rest as training data. Model selection was done by implementing K-Fold Cross Validation on all training data. The best model was obtained in the model configuration of 32 hidden layer neurons, no dropout, 0.0005 learning rate, and 100 epochs with an accuracy at the validation of 64.95%. Prediction of test data was carried out using the selected model and resulted in an accuracy value of 63.41%. The accuracy value is still lower than previous research with similar tests",,,170078,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211504,,ANDRE,Protokol Pengiriman File Berbasis User Datagram Protocol dan Message Broker,,,"protokol pengiriman file,UDP,message broker,user datagram protocol,efisien,reliable","Suprapto, Drs., M.I.Kom., Dr.;Faizal Makhrus, S.Kom., M.Sc.",2,3,0,2022,1,"Pengiriman file atau data telah menjadi salah satu hal yang penting dalam 
era informasi saat ini. Kegiatan chatting secara daring merupakan salah satu 
kegiatan yang membutuhkan pengiriman file atau data. Teks, gambar, video dan 
audio menjadi data atau file yang dikirim pada platform chatting online. Efisiensi 
dalam pengiriman file pun perlu ditingkatkan mengingat ukuran file yang dikirim 
semakin lama akan semakin besar. Tanpa adanya teknologi yang memadai untuk 
tetap mengikuti kebutuhan ini, maka waktu yang diperlukan untuk mengirimkan 
file akan semakin lama. Protokol pengiriman file yang sering digunakan dalam 
mengirimkan file adalah file transfer protocol (FTP). Namun, FTP sekarang masih 
tergolong lambat. Diperlukan protokol pengiriman file lain yang lebih efisien dan 
reliable dalam mengirimkan file.
Dalam penelitian ini dikembangkan sistem pengiriman file memanfaatkan
user datagram protocol (UDP) dan message broker. Diharapkan sistem yang 
dikembangkan bisa lebih cepat dibandingkan dengan protokol FTP dan tetap 
menjamin reliabilitas. Evaluasi protokol pengiriman file yang dirancang akan 
dibandingkan dengan metriks waktu yang dibutuhkan untuk mengirimkan file 
sepenuhnya.
Pengujian terhadap protokol pengiriman file baik menggunakan UFTPNSQ maupun FileZilla dilakukan pada jaringan lokal dengan delay 0.1 detik dan 
ukuran chunk file 60 KB, file 203026 KB. Berdasarkan pengujian diperoleh bahwa 
waktu pengiriman adalah 1.0265836 detik dan 1.3 detik masing-masing dengan 
menggunakan UFTP-NSQ dan FileZilla. Jadi UFTP-NSQ lebih cepat dibanding 
FileZilla.","Nowadays, transferring files is one of the most important basic things in the 
information era. For example, in online chatting, sharing text, images, videos, and 
sounds have been common things to do. All these files will always grow bigger and 
bigger so a faster and more efficient system for transferring files is required. File 
transfer protocol has been a go-to system to transfer files but FTP right now is 
considered slow.
This study developed a new file transfer system based on user datagram 
protocol and message broker. User Datagram Protocol will be used as a protocol 
for transferring files and message broker are used to ensure reliability in the system. 
This study evaluates this new system by the time needed to transfer the files and 
compare it with vastly used FTP application.
Transferring file experiment conducted in local network environments using 
UFTP-NSQ and Filezilla. The file size transferred is 203026 KB with 60 KB chunk 
size and 1 second delay. Based on the result, UFTP-NSQ needs 1.0265836 and 
Filezilla needs 1.3 seconds to transfer the file. It shows that UFTP-NSQ is better 
than Filezilla.",,,162529,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214323,,MUHAMMAD NAUFAL A R,ANALISIS SENTIMEN PENGGUNA TWITTER TERHADAP LAYANAN INDIHOME MENGGUNAKAN FASTTEXT EMBEDDING DAN LONG SHORT TERM MEMORY (LSTM),,,"Analisis sentimen, Indihome, Twitter, LSTM, fastText","Dr. Sri Mulyana, M.Kom.",2,3,0,2022,1,"Di Twitter, pengguna dapat mengunggah pesan status bernama twit. Ada kemungkinan twit yang diunggah mengandung opini terhadap suatu produk atau layanan, baik itu opini positif, netral, atau negatif. Opini tersebut dapat dimanfaatkan sebagai bahan evaluasi bagi perusahaan. Salah satu contoh perusahaan tersebut adalah PT Telkom Indonesia, dengan salah satu produknya adalah Indihome. Indihome merupakan salah satu layanan penyedia internet yang banyak digunakan di Indonesia. Berbagai opini pengguna di Twitter mengenai Indihome dapat dianalisis dan digunakan sebagai bahan evaluasi bagi PT Telkom Indonesia.
Salah satu metode yang dapat digunakan untuk melakukan analisis tersebut adalah analisis sentimen yang bertujuan untuk menentukan nilai sentimen pengguna terhadap layanan Indihome. Penelitian ini menggunakan algoritma long short term memory yang dikombinasikan dengan fastText embedding untuk merealisasikannya. Penelitian ini menggunakan dua kelas sentimen, yaitu positif dan negatif.
Penelitian ini menghasilkan model terbaik penggabungan LSTM dengan fastText embedding dengan akurasi sebesar 74,44%. Parameter yang digunakan adalah epoch sebanyak 60, jumlah unit LSTM sebanyak 32, dan learning rate sebesar 0,0001. Model LSTM memiliki performa yang baik dalam mengklasifikasikan data yang digunakan karena berhasil melampaui performa dari model baseline, namun model tersebut belum dapat melampaui performa dari penelitian terdahulu dalam menyelesaikan kasus yang serupa.","On Twitter, users can upload a status message called tweet. There are possibilities that the uploaded tweet contains an opinion towards a product or service, either positive, neutral, or negative opinion. That opinion can be used as evaluation material by the company. One example of the company is PT Telkom Indonesia, with one of their products is Indihome. Indihome is one of the widely used internet service providers in Indonesia. A lot of different opinions in Twitter towards Indihome can be analyzed and used as evaluation material by PT Telkom Indonesia.
One of the methods that can be used to do that analysis is sentiment analysis for determining the value of user sentiment towards Indihome. This research uses long short term memory combined with fastText embedding to carry out the sentiment analysis. This research uses two sentiment classes, which are positive and negative.
This research results in the best model from the combination of LSTM and fastText embedding with an accuracy of 74,44%. The parameters used are 60 epochs, 32 LSTM units, and a learning rate of 0,0001. LSTM Model has a good performance in classifying the data used because it surpasses the performance of the baselines, but the model has not been able to surpass the performance of previous studies in solving similar cases.",,,165293,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207156,,NADIRA AMELIA,A Web-Based Social Network for Pet Sitters and Pet Owners,,,"Software Engineering, Human-Computer Interaction, Pets","Anny Kartika Sari, S.Si., M. Sc., Ph. D",2,3,0,2022,1,"Untuk kebanyakan orang, hewan peliharaan merupakan salah satu bagian penting dalam kehidupan mereka. Namun, banyak pemilik hewan yang kesulitan dalam mencari pengasuh hewan peliharaannya karena biaya yang terlalu mahal dan dengan tidak ada acara bagi para pengasuh hewan untuk membangun jaringan dengan para pemilik hewan dengan sistem-sistem daring pemesanan pengasuh hewan peliharaan. Hal ini menyebabkan kesulitan pada pemilik hewan untuk menentukan apakah pengasuh tersebut dapat dipercaya atau tidak. Dengan begitu, pemasaran daring dapat membantu pengasuh untuk mempromosikan dirinya sendiri.
Berdasarkan permasalahan tersebut, pada riset ini sebuat situs telah dibuat dengan menggunakan Firebase, ReactJS, dan TailwindCSS yang berfokus pada bagaimana pengguna berinteraksi dengan system dan membuat pengalaman berguna yang lebih baik yang dapat membiarkan pemilik dan pengasuh hewan peliharaan untuk saling mengenal satu sama lain.
Setelah implementasi dan pengujian,walaupun tidak semua fitur dapat diimplementasi karena waktu yang tidak banyak, dapat disimpulkan setelah pengujian terhadap pengguna bahwa system mudah digunakan. Sistem telah mencapai hasil yang baik dan bekerja sebagamana mestinya.","For most people, pets play an important role in their lives. However, it has always been difficult for people (or pet owners) to find the perfect pet sitter as it can be very expensive and since there is no possibility for pet sitters in the current available pet sitter finder websites to create a social engagement with the pet owners, it is hard to tell if the pet sitter is trustworthy or experienced enough just based on the profile and ratings. Marketing can really help solve this issue because pet sitters can promote themselves and engage with the pet owners.
So based on this issue, in this research a website is created using Firebase, ReactJS, and TailwindCSS by focusing on how users would interact with the system and create a better user experience which allows both pet sitters and pet owners to get to know one another. The aim if the project is solving the issue that has been explained above and testing has been done to ensure that the website is easy and efficient to use.
After building the system and tests being done, it is concluded that even though there are some features that are not implemented due to limited time, the user test concludes that the system is easy to use and navigate. Finally, the system has achieved good results, works as intended with a few glitches and improvements can be done in the future.",,,158532,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212021,,LUIS TANOTO,PENGEMBANGAN MODEL PERINGKAS ABSTRAKTIF DOKUMEN INDONESIA MENGGUNAKAN STACKED EMBEDDING DAN TRANSFORMER DECODER,,,"peringkas teks, ringkasan, abstraktif, Liputan6.com, encoder, decoder, stacked embedding, BERT, BPE, FastText, Transformer, data latih, lapisan BERT, precision, recall, F-1, ROUGE","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2022,1,"Informasi merupakan bagian penting dalam kehidupan manusia sebagai sumber pengetahuan dan pengalaman, terlebih di era media digital saat ini, informasi menjadi sangat mudah dan cepat diperoleh. Namun, artikel berita yang ada sekarang cenderung sangat panjang dan sulit dicerna sehingga dibutuhkan suatu ringkasan. Meringkas secara manual dirasa kurang efektif dari berbagai sisi sehingga dibutuhkan suatu peringkas teks otomatis. Terdapat dua jenis ringkasan yaitu ringkasan ekstraktif dan ringkasan abstraktif. Dari segi jumlah penelitian, ringkasan abstraktif masih terhitung sedikit khususnya untuk dokumen berbahasa Indonesia padahal ringkasan abstraktif memiliki struktur dan koherensi yang lebih baik.
Penelitian ini membangun sebuah model peringkas abstraktif untuk dokumen berbahasa Indonesia menggunakan stacked embedding sebagai encoder dan decoder berbasis Transformer. Ini disebabkan karena hasil yang diperoleh dari penelitian Koto et al. (2020) masih terdapat beberapa kekurangan yang ditunjukkan dari performanya sehingga perlu dilakukan perbaikan model, terutama di bagian encoder. Adapun stacked embedding yang digunakan berfokus pada BERT, BPE, dan FastText. Data yang digunakan diambil dari penelitian Koto et al. (2020) yang berisi artikel berita Liputan6.com dan hasil ringkasannya. Penelitian ini juga melakukan percobaan terhadap pengaruh pemilihan lapisan BERT dan pengaruh jumlah data latih yang digunakan (50.000 data dan 75.000 data).
Hasil penelitian menunjukkan bahwa penggunaan seluruh lapisan BERT dalam model menghasilkan performa terbaik. Selain itu, untuk model yang dilatih menggunakan 50.000 data latih menunjukkan bahwa adanya peningkatan tidak signifikan stacked embedding terhadap performa model. Skenario ini mencapai performa terbaik ketika menggunakan kombinasi BERT + CE + BPE dengan nilai F-1 sebesar 34,17% (R-1), 13,98% (R-2), dan 31,51% (R-L). Sebaliknya, untuk model yang dilatih menggunakan 75.000 data latih tidak menunjukkan adanya pengaruh stacked embedding terhadap performa model. Performa terbaik untuk skenario ini dicapai ketika menggunakan model BERT dengan nilai F-1 sebesar 37,18% (R-1), 18,19% (R-2), dan 34,28% (R-L). Salah satu penyebabnya adalah nilai precision yang terlalu rendah.","Information is a crucial part of human life as a source of knowledge and experience, particularly in the current digital media era in which all information can be easily and rapidly obtained. However, the existing news articles now tend to be extremely lengthy and ununderstandable, so summaries are needed. Manual summarizing seems ineffective from any side, so automatic text summarization is needed. There are two kinds of summary which are extractive summary and abstractive summary. In terms of the number of researches done, abstractive summarization is still a few, especially for Indonesian documents, whereas it gives better grammar and word coherence.
This research developed an abstractive summarizer model for Indonesian documents using stacked embedding as the encoder and a Transformer-based decoder. The background was the result obtained still had some mistakes reflected in the model performance, so there was an urgency to develop or fix the model, especially the encoder part. As for the pre-trained embeddings, the combination only focused on BERT, BPE, and FastText. The data used were taken from Koto et al. (2020), containing the news articles from Liputan6.com and their summaries. This research also conducted experiments to show the effect of BERT layers selection and the effect of a number of training data used (50,000 data and 75,000 data).
The results showed that using all layers of BERT in the training model gave the best performance. Besides that, models trained with 50,000 training data showed an insignificant increase in using the stacked embedding towards the model performance. The best performance for this scenario was achieved by the combination of BERT+CE+BPE with F-1 scores of 34.17% (R-1), 13.98% (R-2), and 31.51% (R-L). Meanwhile, models trained with 75,000 training data did not show any effect in using the stacked embedding towards the model performance. The best performance was achieved by the BERT model with F-1 scores of 37.18% (R-1), 18.19% (R-2), and 34.28% (R-L). One of the causes might be the low precision value for each model.",,,163066,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
216631,,TEGAR TAUFIK RAHMAN,BOOK HYBRID RECOMMENDATION SYSTEM BASED ON MATRIX FACTORIZATION &amp; METADATA EMBEDDING,,,"Hybrid Recommendation Systems, Content Based Filtering, Collaborative Filtering,Metadata Embedding,Matrix Factorizations","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2022,1,"Pada era digital ini, terdapat banyak sekali informasi yang bisa kita akses dengan mudah ini bisa berakibat baik dan juga buruk. Sisi baiknya adalah informasi menjadi lebih mudah didapat namun itu juga dapat megakibatkan kesulitan untuk mendapatkan informasi yang diinginkan jika tidak dilakukan menggunakan kata kunci yang benar. Disinilah peran system rekomendasi dirasakan. Sistem rekomendasi adalah sebuah perangkat lunak yang dapat memberikan rekomendasi kepada pengguna ketika pengguna tersebut dihadapkan pada ketersediannya informasi yang berlimpah. Content based filtering dan collaborative filtering adalah 2 metode yang paling sering digunakan dalam pembuatan sistem rekomendasi, namun 2 metode ini mempunyai kelemahannya masing masing. Untuk mengatasi permasalahan ini metode hybrid yang menggunakan gabungan dari 2 metode sebelumnya dapat dijadikan solusi.
Sistem rekomendasi hybrid menjadi basis model sistem rekomendasi yang dilaksanakan oleh penelitian ini. Penelitian ini bertujuan untuk membangun sistem rekomendasi buku berbasis hybrid, membandingkan performa sistem rekomendasi berbasis hybrid dengan sistem rekomendasi collaborative filtering, dan juga untuk membuktikan bahwa model hybrid yang diajukan dapat mengatasi permasalahan cold start yang selama ini dialami oleh sistem rekomendasi collaborative filtering. Metode yang digunakan dalam sistem rekomendasi berbasis hybrid ini adalah metadata embedding dan matrix faktorisasi hybrid dan menggunakan gradient descent sebagai algoritma pelatihan. Sistem rekomendasi ini akan memberikan prediksi kepada pengguna yang diketahui dengan menggunakan user item interactions. Sedangkan untuk pengguna baru sistem ini akan memberikan prediksi sesuai dengan informasi tambahan yang ada pada user tersebut, oleh karena itu sistem rekomendasi ini disebut sistem rekomendasi berbasis hybrid.
Tiga loss function digunakan untuk mengetahui loss function yang terbaik bagi model ini. loss function itu adalah WARP, BPR, dan Logistic, selain itu Area Under Curve dan juga Reciprocal Rank digunakan untuk menguji akurasi dari sistem rekomendasi ini. Hasil penilitian ini secara kesuluruhan menunjukkan bahwa sistem rekomendasi berbasis hybrid menggunakan loss function WARP &amp; BPR mendapatkan skor AUC yang lebih tinggi dibandingkan collaborative filtering. Sistem rekomendasi ini juga dapat memberikan rekomendasi kepada pengguna baru, yang menyatakan bahwa model ini dapat mengatasi masalah cold start.","In this digital era, much information is available for us to access. This situation could affect in a good way or bad way. The good side is the information is become easier to obtain, while it could also cause the information to become harder to find if the user is not using the right keywords; this is where the recommendation system comes into play. Recommendation system is software that could recommend to the user when this user is faced with a vast amount of information, Content Based Filtering and Collaborative Filtering are the methods that are primarily being used as a recommendation system; however, these 2 methods have their liability, to solve this issue hybrid recommendation system in which it&acirc;€™s method is using the combination between the two previous methods could be used as a solution.
The hybrid recommendation system is the primary model used in this research. This research aims to build a hybrid-based book recommendation system, compare its performance against the collaborative filtering model, and prove that the proposed model can solve the cold start problem that the collaborative filtering model has faced. The algorithms used for this model are metadata embedding and hybrid matrix factorizations while using gradient descent as the learning algorithm. This model will predict the known users using the user-item interaction while giving predictions to the unknown users using additional user information. Hence this recommendation model is considered a hybrid recommendation system.
Three loss functions are being used to find the best suitable for the model. Those loss functions are WARP, BPR, and Logistics. Area Under Curve and Reciprocal Rank is being used as model evaluation matrices. The results show that using the WARP &amp; BPR loss functions and the hybrid recommendation system can generate a better AUC score than the Collaborative Filtering model. Moreover, this model could also generate a new recommendation for the new user, showing that the model could solve the cold start problem.",,,167795,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219193,,ARIF FADILLAH R,Evaluation on Prediction Accuracy and Computational Time for Detecting Attacks with PySpark,,,"akurasi prediksi, waktu komputasi, serangan, deteksi, data, pembelajaran mesin, anomali, PySpark","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2022,1,"Dengan meningkatnya jumlah data, serangan, dan jaringan di internet, sistem deteksi intrusi telah menjadi strategi yang populer dan berguna untuk mendeteksi data anomali dan serangan. Latar belakang penelitian ini adalah untuk melihat kemampuan PySpark dalam meningkatkan waktu komputasi ketika mengklasifikasikan serangan pada dataset yang berlabel. Data membantu meningkatkan produk dan layanan, meningkatkan kepuasan pelanggan, memaksimalkan profitabilitas, dan beroperasi lebih efektif. Ini juga memungkinkan komputasi dan pembelajaran mesin untuk mengekstrak nilai secara optimal dari data. Dengan penelitian ini, pihak-pihak tersebut akan dapat menyarankan layanan data yang lebih baik kepada penggunanya.
Dijelaskan dalam penelitian ini adalah investigasi kelayakan penerapan salah satu teknologi big data, Apache Spark, untuk mengklasifikasikan berbagai serangan dan mendeteksi anomali serta meningkatkan waktu komputasi. Penelitian ini menggunakan algoritma pembelajaran mesin pada dataset berlabel bernama KDD-CUP-99. Proses ini disatukan dalam program python yang dibuat menggunakan implementasi PySpark dan metode cross-validasi. Penelitian ini juga melakukan proses pembersihan dan membuat aplikasi python yang serupa dengan tidak menggunakan PySpark untuk perbandingan.
Hasilnya menunjukkan bahwa penggunaan teknologi big data menambahkan beberapa manfaat pada kecepatan deteksi anomali data daripada pendekatan pembelajaran mesin tradisional dalam hal meningkatkan waktu komputasi.","With the growing amount of data, attacks and network traffic across the internet, intrusion detection systems have become a popular and useful strategy to detect anomalies and attacks. The background behind this research was to see the ability of PySpark in improving computational time when classifying attacks in a labelled dataset. Data helps to improve products and services, increase customer satisfaction, maximise profitability, and operate more effectively. It also enables computing and machine learning to extract value in an optimal manner from data. With this research, these parties will be able to suggest better services to their users. 
Explained in this research is the investigation of the feasibility of applying one of the big data technologies, Apache Spark, to classify different attacks and detect anomalies as well as improving the computational time. This research employs machine learning algorithms on a labelled dataset of the KDD-CUP-99. This process is all put together in a created python program using the implementation of PySpark and performing the cross-validation technique. The research also performs dataset preprocessing as well as creating a similar application without PySpark for comparison. 
The results demonstrate that employing big data technologies adds several benefits to data anomaly detection than traditional machine learning environments in terms of  improving computational time.",,,170654,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211770,,DHANY LAUDZA A P,PREDIKSI HASIL PERTANDINGAN E-SPORTS MENGGUNAKAN JARINGAN SYARAF TIRUAN,,,"Backpropagation,E-sport,Valorant","Aina Musdholifah, S.Kom., M.Kom. Ph.D",2,3,0,2022,1,"Perkembangan dalam bidang video game berkembang dengan pesat. Mulai munculnya video game online yang kompetitif membuat adanya tim yang bermain sebagai profesional mulai berpikir tentang strategi yang akan para profesional gunakan untuk memenangkan permainan. Strategi dapat dibuat dengan mudah jika ada prediksi bagaimana hasil akhir dari permainan itu sendiri. Beberapa metode telah dikembangkan untuk membuat prediksi dari hasil pertandingan khususnya menggunakan jaringan syaraf tiruan. Penelitian ini dilakukan sebagai usaha untuk mendapatkan prediksi kemenangan dari pertandingan Valorant dengan 4 fitur yang dipilih menggunakan jaringan syaraf tiruan backpropagation.
Proses pelatihan dan pengujian menggunakan data set dari pertandingan player public valorant yang terdiri dari 58 neuron input dan 1 neuron output, terdiri dari 4138 data dengan 3310 data pada tahap pelatihan dan 828 data pada tahap pengujian.
Hasil pengujian yang menunjukkan akurasi terbaik berada pada kombinasi fungsi aktivasi hidden layer sigmoid dan output layer sigmoid dengan learning rate 0,01 dengan 1 hidden layer yang berisi 40 neuron dengan akurasi sebesar 91,79%.
","Developments in the field of video games are growing rapidly. With the emergence of competitive online video games, teams that play as professionals begin to think about the strategies they will use to win the game. Strategies can be made easily if there is a prediction of how the final outcome of the game itself will be. Several methods have been developed to make predictions from the results of matches, especially using artificial neural networks. This research was conducted as an attempt to get a winning prediction from a Valorant match with 4 selected features using a backpropagation neural network.
The training and testing process uses a dataset from a public valorant player match consisting of 58 input neurons and 1 output neuron, consisting of 4138 data with 3310 data at the training stage and 828 data at the testing stage.
The test results that show the best accuracy are in the combination of the activation function of the sigmoid hidden layer and the output layer of the sigmoid with a learning rate of 0.01 with 1 hidden layer containing 40 neurons with an accuracy of 91.79%. 
",,,162777,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212282,,FERI SETIAWAN,AUGMENTED REALITY IN DIESEL ENGINE PRACTICUM FOR MECHANICAL ENGINEERING DEPARTMENT,,,"augmented reality, immersive, education, vocational education and training","Moh, Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2022,1,"Covid-19 berdampak pada banyak orang di Indonesia tidak hanya dalam masalah kesehatan tetapi juga dalam kondisi ekonomi, sosial dan pendidikan mereka. Dalam dunia pendidikan, banyak siswa yang mengalami kesulitan belajar dalam lingkungan online. Salah satu Sekolah Vokasi di Universitas Gadjah Mada menunda jadwal praktikum mereka dan melaksanakannya pada semester akhir. Ini adalah keputusan yang bagus di saat seperti ini, tetapi mungkin mempengaruhi kemajuan belajar siswa.
					
Penelitian ini mengusulkan penggunaan teknologi Augmented Reality sebagai metode pengajaran lain untuk pembelajaran praktikum. Teknologi ini memungkinkan siswa untuk berinteraksi dengan objek 3D di dunia nyata melalui kamera smartphone mereka. Teknologi ini dapat membantu siswa dalam proses belajar mereka melalui metode pembelajaran interaktif daripada membaca buku modul. Dalam penelitian ini, Plane Tracking digunakan sebagai metode Augmented Reality untuk aplikasi tersebut.
					
Penelitian ini menemukan bahwa penggunaan aplikasi Augmented Reality dapat meningkatkan motivasi belajar siswa didukung oleh analisis statistik Mann-Whitney U-test dimana nilai Ustat sebesar 17,5 lebih rendah dari Ucrit yaitu 36 untuk n sama dengan 12. penelitian juga menemukan bahwa dengan menggunakan best practice proses pengembangan aplikasi mobile dapat menghasilkan aplikasi yang hebat didukung oleh skor System Usability Scale dengan skor 76,2. ","Covid-19 have affected many people in Indonesia not just in health issues but also in their economic condition, social and education. In education, many students having a hard time learning in an online environment. One of the vocational schools in Universitas Gadjah Mada postpone their practicum schedule into their last semester. This is a great decision in a time like this, but it might affect the learning progress of the students.
This research propose the use of Augmented Reality technology as another teaching method for practical learning session. This technology allow students to interact with 3D object in the real world through the camera of their smartphones. This technology might help students in their learning process through interactive learning method rather than reading a module book. In this research, Plane Tracking was used as the Augmented Reality method for the application.
This research founds that using Augmented Reality application can raise students learning motivation supported by statistical analysis of Mann-Whitney U- test where the score of Ustat is equals to 17.5 which is lower than Ucrit which is 36 for n equals to 12. This research also founds that using best practice of mobile application development process can produce great application supported by scoring of System Usability Scale with the score of 76.2.",,,163496,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218427,,ALILAH BALQIS,Twitter Hate Speech Analysis by Using Support Vector Machines ,,,"SVM, Twitter, Hate-speech","Mardhani Riasetiawan, M.T., Dr",2,3,0,2022,1,"Di era kemajuan media sosial, ujaran kebencian dan kata-kata kasar semakin meningkat. Penelitian ini bertujuan untuk mendeteksi ujaran kebencian dan bahasa kasar di Twitter. Meskipun banyak penelitian sebelumnya berdasarkan mekanisme multilabel telah muncul dalam literatur, keuntungan dari pendekatan yang digunakan dalam penelitian ini telah meningkat. Oleh karena itu, tujuan dari penelitian ini adalah untuk membuat model klasifikasi multi-label, mengecualikan ujaran kebencian dan bahasa yang menyinggung dari kumpulan data, dan mengevaluasi model yang diusulkan sesuai dengan parameter kinerja. Kaggle digunakan untuk mendapatkan teks berbasis tweet untuk penelitian ini. Pendekatan model deteksi sentimen kebencian dilatih oleh model mesin pendukung vektor berbasis powerset dan chainer classifier. Tahap pra-pemrosesan dilakukan setelah meninjau data yang diunduh, dataset pelatihan dan pengujian dikumpulkan. Hasilnya disajikan analisis komparatif untuk kedua model. Terlihat dari hasil bahwa model SVM berdasarkan classifier Chainer mengungguli model SVM berdasarkan classifier Powerset. Hasil ini, dievaluasi terhadap parameter evaluasi, menyoroti keunggulan model SVM berbasis pengklasifikasi Chainer, serta efektivitas dan ketahanannya.","In the era of social media advancement, hate speech and offensive language are increasing tremendously. This research aims to detect hate speech and offensive language on Twitter. Although many previous studies based on multi label mechanisms have appeared in literatures, the advantages of the approach used in this research have increased. Therefore, the aim of this research is to create a multi-label classification model, exclude hate speech and offensive language from the dataset, and evaluating the proposed model according to the performance parameters. Kaggle is used to gain the tweet-based text for this research. The hate sentiment detection model approach was trained by powerset and chainer classifier-based support vector machine models. Pre-processing phase was done after reviewing the downloaded data, training and test datasets were collected. The results presented a comparative analysis for both models. It was observed from the results that the SVM model based on the Chainer classifier outperforms the SVM model based on the Powerset classifier. These results, evaluated against the evaluation parameters, highlight the superiority of the Chainer classifier-based SVM model, as well as its effectiveness and robustness.",,,169632,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211521,,SHERINE DEVI SUTOMO,EVALUASI PENGARUH MEKANISME ATTENTION TERHADAP MODEL MESIN PENERJEMAH BAHASA JAWA-INDONESIA BERBASIS NEURAL MACHINE TRANSLATION,,,"mesin penerjemah berbasis neural network, seq2seq, BiLSTM, mekanisme attention","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Sebagai makhluk sosial, manusia bergantung pada bahasa sebagai sarana interaksi dan komunikasi antar sesama kelompoknya. Setiap kelompok dapat menggunakan bahasa yang berbeda-beda. Di Indonesia sendiri terdapat banyak  sekali bahasa daerah yang tercatat, dengan bahasa Jawa menempati urutan pertama dalam jumlah penutur terbanyak. Mesin penerjemah dapat menjadi salah satu sarana untuk menjembatani komunikasi antar bahasa yang berbeda, dalam hal ini bahasa Jawa-Indonesia.
Saat ini, mesin penerjemah berbasis neural machine translation (NMT) mendominasi hasil state-of-art, namun pemanfaatannya pada bahasa daerah terutama bahasa Jawa masih terbatas. Maka dari itu, penelitian ini mengembangkan model mesin penerjemah bahasa Jawa ke bahasa Indonesia berbasis NMT. Penelitian ini memanfaatkan arsitektur seq2seq, dengan Bidirectional Long Short Term Memory (BiLSTM) dan juga mekanisme attention. Hasil terjemahan model mesin penerjemah dievaluasi dengan metrik skor BLEU.
Hasil dari penelitian ini menunjukkan bahwa penggunaan mekanisme attention dalam model mesin penerjemah bahasa Jawa-Indonesia dapat meningkatkan kualitas hasil translasi dengan signifikan. Dengan mekanisme attention, meskipun terjadi penambahan waktu inferensi sebesar 19,9%, BLEU score yang diperoleh dapat mencapai 176% dari hasil tanpa attention. BLEU score terjemahan model mesin penerjemah dengan attention juga dapat kembali meningkat setelah model dilatih kembali dengan data latih yang lebih banyak. ","Humans as social beings depend on language as a means of interaction and communication between their fellow groups. Each group usually uses a different language; In Indonesia alone there are many recorded regional languages, with Javanese being the first in the number of speakers. Machine translation can be a means to bridge communication between different languages, in this case Javanese-Indonesian.
Currently, neural machine translation (NMT) dominates state-of-art results, but their use in regional languages, especially Javanese, is still limited. Thus, this study developed a neural machine translation model for translating Javanese to Indonesian. This research utilizes the seq2seq architecture which consists of an encoder and a decoder, with Bidirectional Long Short Term Memory (BiLSTM) and also attention mechanism. The translation results were evaluated with the BLEU score metric.
The result of this study shows that the use of attention mechanisms in the Javanese-Indonesian machine translation model can significantly improve the quality of translation results. With the attention mechanism, although there is an increase in model inference time of 19.9%, the BLEU score obtained can reach up to 176%. The BLEU score of the translation from the model with attention can also further increase after model was retrained with more training data. ",,,162550,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218946,,MUHAMMAD RIZKI D,AUTHOR OBFUSCATION TERHADAP ARTIKEL BERITA DAN TWEET  MENGGUNAKAN METODE BERBASIS ATURAN,,," authorship obfuscation, stylometri, metode berbasis aturan","Edi Winarko, Drs., M.Sc. Ph.D.  ; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2022,1,"      Perkembangan model authorship attribution saat ini menjadi ancaman bagi para penulis yang ingin mempublikasikan tulisannya secara anonim. Salah satu penangkalnya adalah penelitian mengenai authorship obfuscation. Metode 
obfuskasi merupakan salah satu teknik untuk memparafrase teks untuk menjaga anonimitas penulis berdasarkan stylometri dari teks. Stylometri merupakan informasi linguistik yang menjadi ciri khas penulis. Fitur stylometri biasanya digunakan sebagai metriks dalam pendeteksian author. 
    Pada penelitian ini, dilakukan authorship obfuscation dengan metode berbasis aturan dengan mengkalkulasi fitur-fitur penting pada stylometri. Dari hasil rata-rata tiap metriks akan dilakukan transformasi teks menggunakan metode seperti POS tagging, Word2Vec, spelling correction dan lain sebagainya. Beberapa aturan transformasi teks tersebut bertujuan untuk mengubah nilai rata-rata dari 
metriks yang ada sekaligus memparafrase teks.
     Dalam evaluasi ada beberapa metriks yang diperhatikan yaitu safety, soundness, dan sensibility. Untuk penilaian safety, akan dibuat dua model sebagai benchmark dan memiliki rata-rata penurunan akurasi serta F1 score 0,16 dan 0,164 pada data berita serta 0,385 dan 0,41 pada data tweet. Untuk soundness, digunakan perhitungan cosine similariy untuk menilai kesamaan dengan teks asli dan juga 
dinilai oleh beberapa responden mengenai konteks dari teksnya. Model memiliki soundness cukup baik dengan nilai 0,6 pada data berita namun kurang baik pada data tweet pada skenario tertentu. Lalu untuk sensibility, juga dilakukan penilaian secara langsung oleh beberapa orang mengenai grammar dan tata bahasanya.Dari hasil penilaian sensibleness bisa dibilang cukup baik dimana beberapa artikel bisa dibaca dengan baik, namun masih banyak juga yang memiliki tata bahasa yang berantakan.

Kata-kata kunci : authorship obfuscation, stylometri, metode berbasis aturan
","    The development of the authorship attribution model is currently a threat for writers who want to publish their writings anonymously. One of the antidotes is 
research on authorship obfuscation. Obfuscation method is a technique for paraphrasing text to maintain the anonymity of the author based on the stylometry of the text. Stylometry is linguistic information that characterizes the author. The 
stylometric feature is usually used as a metric in author detection.
    In this study, authorship obfuscation was carried out using a rule-based method by calculating the important features of stylometry. From the average results of each metric, text transformation will be carried out using methods such as POS tagging, Word2Vec, spelling correction and so on. Some of these text transformation rules aim to change the average value of an existing metric as well as paraphrase the text.
    In the evaluation there are several metrics to consider, namely safety, soundness, and sensibility. For safety assessment, two models will be made as benchmarks and have an average decrease in accuracy and F1 scores of 0.16 and 0.164 on news data and 0.385 and 0.41 on tweet data. For soundness, the calculation of cosine similarity is used to assess the similarity with the original text and also assessed by several respondents regarding the context of the text. The model has a fairly good soundness with a value of 0.6 on news data but not good on tweet data in certain scenarios. Then for sensibility, several people directly assessed the grammar and grammar. From the results of the sensibleness assessment, it could be said that it was quite good where some already had articles that were acceptable, 
but there were still many whose grammar looked untidy.

Keyword : authorship obfuscation, stylometri, rules based method
",,,170237,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213316,,MARIO ADRIEL,A Neural Network-based Method for Classifying Depression Severity Levels on Indonesian Twitter Users,,,"depression,ordinal classification,convolutional neural network,long short term memory,consistent rank logits","Drs. Retantyo Wardoyo, M.Sc., Ph.D.;Muhammad Alfian Amrizal, B.Eng., M.I.S., Ph.D.",2,3,0,2022,1,"Mengklasifikasikan tingkat keparahan depresi pengguna Twitter dapat membantu profesional medis untuk mendeteksi pengguna pada tahap awal depresi. Karena banyaknya data yang dihasilkan oleh pengguna Twitter, tidak praktis untuk menganalisisnya secara manual. Oleh karena itu, pembelajaran mesin telah dipelajari secara ekstensif dalam literatur. BDI adalah salah satu metrik yang banyak digunakan untuk mengevaluasi tingkat keparahan depresi seseorang. BDI membagi tingkat keparahan depresi seseorang menjadi enam tingkatan, dari depresi normal hingga ekstrim. Oleh karena itu, memprediksi tingkat keparahan pengguna Twitter dapat dianggap sebagai masalah klasifikasi ordinal, yang dapat diselesaikan dengan teknik pembelajaran mendalam.

Secara khusus, Convolutional Neural networks (CNN) dan Long Short Term Memory (LSTM) adalah salah satu teknik paling canggih untuk mengekstrak fitur tersembunyi dalam tweet. Penelitian ini membandingkan kinerja kedua arsitektur tersebut untuk mengklasifikasikan tingkat keparahan depresi berdasarkan BDI. Sebuah survei yang melibatkan lebih dari 1.000 pengguna Twitter dilakukan untuk mendapatkan tingkat BDI mereka, kemudian tweet mereka dalam tiga bulan terakhir dikumpulkan dan digunakan sebagai dataset untuk melatih model. Terakhir, CNN dan LSTM dibandingkan menggunakan Mean Absolute Error (MAE).

Rata-rata, LSTM sedikit lebih baik dari CNN, dengan rata-rata MAE 1,313 +- 0,09 dan 1,382 +- 0,04, masing-masing. Namun, LSTM mengungguli CNN ketika mengkategorikan depresi tingkat batas (Level 3) hingga depresi tingkat ekstrem (Level 6), di mana data pada level ini secara signifikan lebih kecil dibandingkan dengan level yang lebih rendah (Level 1 dan 2). Ini menekankan potensi LSTM untuk mendeteksi depresi dini pada pengguna Twitter.","
Classifying the depression severity levels of Twitter users could help medical professionals to detect users in the early stage of depression. Due to the vast amount of data generated by Twitter users, it is impractical to analyze them manually. Hence, machine learning has been extensively studied in the literature. BDI is one of the widely used metrics to evaluate the depression severity level of a person. BDI divides a person&acirc;€™s depression severity level into six levels, from normal to extreme depression. Hence, predicting the severity level of Twitter users can be considered an ordinal classification problem, which deep learning techniques can solve.

In particular, Convolutional Neural networks (CNN) and Long Short Term Memory (LSTM) are among the most advanced techniques to extract hidden features in tweets. This research compared the performance of these two architectures for classifying depression severity levels based on BDI. A survey involving more than 1,000 Twitter users was conducted to obtain their BDI levels, then their tweets in the past three months were collected and used as the dataset to train the models. Finally, CNN and LSTM were compared using the Mean Absolute Error (MAE).

On average, LSTM was slightly better than CNN, with an average MAE of 1.313 +- 0.09 and 1.382 +- 0.04, respectively. However, LSTM outperformed CNN when categorizing borderline level depression (Level 3) to extreme level depression (Level 6), where the data on these levels are significantly smaller than those of the lower levels (Levels 1 and 2). This emphasizes the potential of LSTM for detecting early depression in Twitter users.",,,164364,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
216902,,ZHAFIRA ELHAM FAWNIA,Google Play Store Rating Prediction Utilizing Random Forest and K-Nearest Neighbors Regression with Feature Sensitivity Analysis and K-Means Clustering Feature Analysis,,,"Regression, Random Forest, K-Nearest Neighbors, Clustering, K-Means, Continuous Variable Prediction, Market Feature Analysis","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.; Faizal Makhrus, S.Si., M.Sc., Ph.D",2,3,0,2022,1,"Di era digitalisasi ini, persaingan untuk memprediksi atribut keberhasilan suatu aplikasi semakin beragam. Google Play Store adalah salah satu sumber terbaik bagi konsumen untuk mendapatkan aplikasi dan game seluler secara khusus. Oleh karena itu, menimbulkan pertanyaan apakah mungkin untuk memprediksi peringkat game berdasarkan fitur pasar mereka, jenis kluster apa yang ada dalam data dan apa yang dapat berkontribusi pada kesuksesan prediksi.
Prediksi dan pengelompokan variabel berkelanjutan berada di bawah istilah umum analitik prediktif. Metode regresi biasanya digunakan untuk prediksi variabel kontinu. Oleh karena itu, model pembelajaran mesin yang sering digunakan untuk masalah prediksi dan klasifikasi yang cocok dengan variabel diskrit dan kontinu adalah Random Forest; karena ia membangun serangkaian pohon keputusan dengan nilai variasi yang ditentukan, dengan kombinasi bagging dan pemilihan random forest. Untuk membandingkan hasil regresi Random Forest, model regresi K-Nearest Neighbors diterapkan pada masalah yang sama. Hasilnya menunjukkan Random Forest (95,19%) berkinerja lebih baik daripada K-Nearest Neighbors (94,3%) saat memprediksi variabel kontinu untuk peringkat game seluler.
Analisis pengelompokan setiap fitur dilakukan untuk memahami tingkat pentingnya setiap fitur, dan faktor keberhasilan aplikasi dalam Google Play Store. Pendekatan clustering menggunakan Unsupervised K-Means Clustering; sebuah algoritma yang mengelompokkan data menjadi beberapa kelompok untuk melihat kemungkinannya satu sama lain. Hasilnya menunjukkan bahwa ada tiga kluster berbeda dalam dataset Google Play Store.","Within this age of digitalization, the competition to predict successful attributes of an application is getting more and more versatile. The Google Play Store is amongst the top sources for consumers to obtain mobile applications and games specifically. Therefore, it raises the question of whether it is possible to predict the rating of games based on their market features, what kind of clusters exist within the data and what could contribute to their success.
Continuous variable prediction and clustering are under the umbrella term of predictive analytics. A regression method is commonly used for continuous variable prediction. Hence, a machine learning model often used for both prediction and classification problems that fits both discrete and continuous variables is Random Forest; as it builds a series of decision trees with a decided variation value, with a combination of bagging and random forest selections. To compare the results of the Random Forest regression, a K-Nearest Neighbors regression models applied to the same problem. The results show Random Forest (95.19%) performs better than K-Nearest Neighbors (94.3%) when predicting continuous variables for mobile games rating.
Clustering analysis of each feature is performed to understand the importances, and success factors of the applications within the Google Play Store. The clustering approach is using the unsupervised K-Means Clustering; an algorithm that clusters data into multiple clusters to see its likelihood of one another. The results show that there are three distinct clusters within the Google Play Store dataset.",,,168036,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214087,,MUHAMMAD FAWWAZ MAYDA,Verifikasi Tanda Tangan Menggunakan Siamese Network dan Metode Self Supervised Learning,,,"pembelajaran mesin, siamese network, self supervised learning, verifikasi  tanda tangan","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Keberadaan dan penggunaan tanda tangan yang sangat lazim menandakan proses ini sangatlah penting dan kritikal. Meskipun begitu semua proses tentu memiliki kelebihan dan kekurangan yang akan selalu ada tak terkecuali tanda tangan itu sendiri. Dalam praktiknya ada saja oknum yang memalsukan tanda tangan dengan berbagai motif yang tentu saja tindakan ini dikategorikan sebagai illegal dan mungkin melanggar hukum. Oleh karenanya dikembangkanlah suatu sistem untuk membantu memverifikasi keabsahan suatu tanda tangan yang ada.
Dalam penelitian ini dikembangkan sistem verifikasi tanda tangan yang berbasis citra digital serta bisa digunakan untuk tanda tangan berbagai orang secara umum. Penelitian ini menggunakan beberapa element dari penelitian terbaru dalam ranah pembelajaran mesin meliputi teknik self supervised learning dan siamese network. Hasil akhir dari penelitian ini didapatkan bahwa teknik self supervised learning dapat meningkatkan akurasi sistem verifikasi tanda tangan dengan nilai rata-rata tertinggi adalah 76% serta lebih tinggi daripada rata-rata tertinggi teknik yang tidak menggunakan yaitu sebesar 69%. Hal ini membawa manfaat besar dikarenakan bisa mengatasi masalah keterbatasan jumlah data tanda tangan yang bisa berpengaruh terhadap akurasi yang dicapai.
","The existence and use of very prevalent signatures indicates that this process is very important and critical. However, all processes certainly have advantages and disadvantages that will always exist, including the signature itself. In practice, there are individuals who falsify signatures with various motives where this action is categorized as illegal and may be unlawful. Therefore, a system was developed to help verify the validity of an existing signature.					In this study, a signature verification system was developed based on digital imagery and can be used for the signatures of various people in general. This research uses several elements of the latest research in the field of machine learning, including self-supervised learning techniques  and Siamese network. The final result of this study found that self-supervised learning methods can improve the accuracy of signature verification systems with highest average of 76% which is better than the highest average of 69% when this method is not used. This has great benefits because it can overcome the problem of a limited amount of signature data that can affect the accuracy achieved.",,,165137,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214095,,FAJAR KENICHI K P,Mendeteksi Bahasa Isyarat SIBI dengan Transfer Learning Berbasis EfficientNet,,,"Sign Language, SIBI, Transfer Learning, EfficientNet","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2022,1,"Tunarungu didefinisikan sebagai istilah terhadap suatu individu yang memiliki gangguan pendengaran (Anugerah et al., 2020). Menurut WHO (2013), jumlah penderita tunarungu di dunia mencapai lebih dari 360 juta jiwa. Dengan jumlah yang cukup banyak tersebut, maka interaksi antara orang biasa dan tunarunggu akan mengalami kesusahan. Dibutuhkan cara untuk berkomunikasi antara kedua belah pihak. Salah satunya dengan bahasa isyarat. Sebelum bisa berkomunikasi dengan bahasa isyarat, orang harus mengenal bahasa isyarat itu sendiri. Dengan perkembangan teknologi, muncul beberapa cara yang menggunakan teknologi untuk mendeteksi bahasa isyarat. Dalam penelitian ini, akan dibangun model yang dapat mendeteksi bahasa isyarat SIBI dengan bantuan model transfer learning berbasis EfficientNet. Model akan dilatih agar bisa mendeteksi 24 huruf abjad SIBI. Model akan mengalami 2 kali tahapan evaluasi. Tahapan pertama berupa evaluasi hyperparameter tuning dimana dalam hal ini adalah penggunaan droput. Setelah mendapatkan nilai dropout terbaik, model akan dievaluasi lagi dengan pemberian dataset baru. Hasil dari penelitian inidapat disimpulkan menjadi 2 kesimpulan. Hal pertama adalah model dengan penerapan nilai dropout sebesar 0.1 mejadi model dengan hasil akurasi tertinggi, yaitu sekitar 98%. Hal terakhir yang bisa disimpulkan adalah model berhasil mendeteksi 24 jenis huruf bahasa isyarat SIBI dengan tingkat akurasi sebesar 99% setelah diberikan dataset baru untuk dideteksi","Deaf is defined as a condition where someone is having a hearing problem or condition (Anugerah et al., 2020). Based on WHO (2013), it is said that there are more than 360 million people around the world that is deaf. With that amount huge amount of people, it will be hard for both deaf and normal people to interact with each other. There needs to be a solution that can help both community to communicate with one another. One of the solution that can help is by using sign language. Before being able to communicate with sign language, there is a need to know first about the sign language itself. By using technology, there are many answers to that problem. This paper tries to answer that challenge by building a deep learning model that can detect SIBI sign language. The model will be made using the transfer learning method that is based on the EfficientNet model. The model will be trained so that it can detect 24 images as output. The model will undergone 2 evaluation step. The first evaluation starts by comparing the dropout rate. The model will be given a certain amount of dropout rate and will be measured its accuracy based on the dropout rate that is given. The final evaluation is conducted by allowing the model to detect a new dataset that it never seen before. There are 2 conclusions that can be noted from this research. The first conclusion is the dropout rate. A value of 0.1 in the dropout rate is concluded as the best value that can create a model with the best result. By using this metrics, the model is able to achieve an accuracy around 98%. For the final test, the model that has been given a new dataset as input is able to detect 24 classes of SIBI language letters by gaining an accuracy around 99%",,,165048,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217167,,MUHAMMAD FARHAN F M,Perbandingan Kinerja Beberapa Metode Word Embedding pada Bahasa Indonesia,,,"perbandingan algoritma, word embedding, Word2Vec, GloVe, fastText, BERT, analisis sentimen, named entity recognition","Dr-Ing, MHD. Reza M.I. Pulungan, M. Sc.",2,3,0,2022,1,"Word embedding adalah representasi vektor bernilai riil dari kata-kata dengan menyematkan makna semantik dan sintaksis yang diperoleh dari korpus besar yang tidak berlabel. Word embedding sangat umum digunakan untuk menyelesaikan permasalahan natural language processing. Word embedding terus mengalami perkembangan dan juga bermunculan algoritma terbaru dengan model arsitektur yang berbeda dari sebelumnya. Banyaknya word embedding yang tersedia memberi banyak pilihan saat membangun suatu arsitektur. Berkaitan dengan hal tersebut diperlukan suatu perbandingan untuk menentukan model word embedding yang paling tepat digunakan sebagai bagian dari arsitektur natural language processing.
Penelitian ini membandingkan kinerja dan lama waktu training dari word embedding Word2Vec, GloVe, fastText, dan BERT. Perbandingan dilakukan dengan menggunakan dua topik yang diselesaikan yaitu analisis sentimen dan named entity recognition.
Hasil penelitian ini menunjukkan bahwa model BERT menghasilkan solusi terbaik namun dengan waktu training terlama. Model Word2Vec, GloVe, dan fastText memiliki kemiripan pada waktu training dengan selisih rata-rata 70 milidetik. Model Word2Vec menghasilkan kinerja terbaik kedua dan tercepat ketiga. Model GloVe menghasilkan kinerja terbaik ketiga dan tercepat kedua. Model fastText menghasilkan kinerja terburuk namun menjadi yang paling cepat di antara semua model.","Word embedding is a real-valued vector representation of words by embedding the semantic and syntactic meanings derived from a large unlabeled corpus. word embedding continues to develop and new algorithms emerge with different architectural models than before. The large number of word embedding available gives us many choices when building an architecture. In this regard, a comparison is needed to see which word embedding model is most appropriate to use as part of the natural language processing architecture.
This study compares the performance and training time from Word2Vec, GloVe, fastText, and BERT. Comparisons were made using two such as sentiment analysis and named entity recognition.
The result of this research shows that BERT model produces the best solution but with the longest training time. Word2Vec, GloVe, and fastText models are similar in training time with an average difference of 70 milliseconds. The Word2Vec has the second best performance with the third fastest time. The GloVe model has the third best performance with the second fastest time. The fastText model has the worst performance with the fastest time in training.",,,168303,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217168,,FADHLAN PASYAH A F,Authorship Verification pada Teks Berbahasa Indonesia Menggunakan Siamese Network,,,"Authorship Verification, Siamese Neural Network, Natural Language Processing ","Anny Kartika Sari, S.Si., M.Sc., Ph.D.; Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2022,1,"Authorhsip Verification merupakan metode untuk memverifikasi penulis dua buah teks, apakah ditulis oleh orang yang sama atau berbeda. Metode tersebut bermanfaat dalam mencegah dan menginvestigasi terjadinya kejahatan yang melibatkan penulisan atau dokumen (text forensic) yang sangat mungkin terjadi seiring majunya teknologi dan persebaran informasi, salah satunya pemalsuan dokumen. Authorship verification dilakukan dengan membandingkan perbedaan informasi penulisan antara dua teks, sehingga dibutuhkan dua input dalam proses pembelajaran model yang dikembangkan. Siamese Neural Network merupakan pengembangan arsitektur jaringan syaraf tiruan yang mampu memproses dua input dengan parameter yang sama, sehingga sesuai untuk digunakan pada masalah authorship verification sebagai masalah perbandingan.
Penelitian ini bertujuan membangun model pembelajaran mesin berbasis Siamese Neural Network untuk masalah Authorship Verification. Data yang digunakan merupakan teks berita dan tweet berbahasa Indonesia tanpa batasan topik untuk menghasilkan model independent terhadap topik teks. Teks yang digunakan direpresentasikan menggunakan pre-trained word embedding BERT dan Fasttext untuk menemukan word embedding dan konfigurasi paling sesuai untuk kedua dataset.
Model yang dikembangkan mampu menghasilkan nilai akurasi training 78.89% pada data teks berita dan 63.99% pada data tweet. Sedangkan untuk hasil pengujian, model dapat menghasilkan akurasi sebesar 76.55% pada data teks berita dan 57.84% pada data teks tweet. Berdasarkan hasil pengujian dapat disimpulkan bahwa jaringan syaraf tiruan Siamese dapat digunakan untuk masalah Authorship Verification pada teks berbahasa Indonesia. ","Authorship Verification is a method to verify the writer of two texts, classifying whether they were written by the same or different author. The method is very useful to prevent and investigate text-based crime action which has considerable possibility along with technology development and information broadcasting, for example document forgery. Authorship verification is conducted by comparing the difference of writing information between two texts; therefore two parallel inputs would be required on the developed model learning step. Siamese Neural Network is a development of artificial neural network which capable to process two inputs with the same parameter, thus making it suitable for authorship verification task which considered to be a comparison task.
This research is conducted by developing machine learning model for authorship verification task. The used text data are Indonesian news and tweet text without any limitation on their topic to produce a topic-independent model. The text is then represented using pre-trained BERT and Fasttext word embeddings to find the most suitable configurations and word embedding for each of the two datasets.
The developed model can produce the training accuracy of 78.89% on news text and 63.99% on tweet text. As for the testing result, the model can achieve 76.55% accuracy and 57.84% on tweets. Based on the result produced by the model, it can be concluded that Siamese Neural Network can be utilized to develop a model for Authorship Verification task on Indonesian text.",,,168308,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211027,,AULIA RASYID,KLASIFIKASI PENYAKIT TUBERCULOSIS (TB) ORGAN PARU MANUSIA BERDASARKAN CITRA RONTGEN THORAX MENGGUNAKAN  METODE CONVOLUTIONAL NEURAL NETWORK (CNN),,,"Tuberculosis, Convolutional Neural Network (CNN) , X-ray , MobileNet.","Lukman Heryawan, S.T., M.T. Ph.D",2,3,0,2022,1,"Paru-paru merupakan bagian dari sistem pernapasan yang berfungsi sebagai tempat terjadinya pertukaran karbon dioksida dan oksigen dalam darah. Gangguan pada paru-paru merupakan gangguan yang cukup serius dimana dapat menyerang sistem pernapasan manusia dan bisa berakibat fatal jika tidak ditangani dengan serius. Salah satu penyakit yang menyerang paru-paru adalah tuberculosis. Tuberculosis (TB) merupakan salah satu penyakit berbahaya yang dapat menular lewat udara dan sering menyebabkan kematian apabila tidak cepat ditangani. Penyakit TB bisa disembuhkan dengan deteksi dini sehingga penderita dapat segera mendapatkan pengobatan yang tepat. Salah satu cara dokter ahli mendiagnosis penyakit TB adalah dengan melihat citra X-ray paru-paru. 
Pada penelitian ini akan merancang sistem otomatis untuk mengklasifikasi penyakit tuberculosis berdasarkan citra x-ray paru-paru berbasis Convolutional Neural Network (CNN) menggunakan arsitektur MobileNet.  Dataset yang diambil berupa citra X-ray paru yang akan digunakan sebagai masukan untuk proses pengolahan citra atau image processing. Tahapan pertama yaitu input berupa citra X-ray, selanjutnya dilakukan proses preprocessing citra (resizing, grayscale, contrast), dilanjutkan dengan proses segmentation (thresholding), dilanjutkan dengan proses augmentasi data, terakhir citra akan diklasifikasikan menjadi tiga kelas yaitu normal, TBR (tuberculosis bagian kanan), TBRL (tuberculosis bagian kanan dan kiri). Tuberculosis bagian kiri tidak masuk kedalam klasifikasi dikarenakan tuberculosis lebih cenderung terkena pada bagian kanan atau di kedua bagian. 
Dalam penelitian ini dilakukan pengujian terhadap pengaruh citra ( tanpa image processing , preprocessing dan segmentation) , pengaruh batch size ,pengaruh variasi epoch, pengaruh augmentasi dan perbandingan terhadap performa klasifikasi. Hasil akhir dari penelitian ini menunjukan optimizer terbaik yaitu Adam menggunakan preprocessing CLAHE pada epoch 50 batch size 32 dan menghasilkan nilai rerata akurasi data uji sebesar accuracy 93.64466667%.","The lungs are part of the respiratory system that functions as a place for the exchange of carbon dioxide and oxygen in the blood. Lung disorders are quite serious disorders which can attack the human respiratory system and can be fatal if not treated seriously. One of the diseases that attack the lungs is tuberculosis. Tuberculosis (TB) is a dangerous disease that can be transmitted through the air and often causes death if not treated quickly. TB disease can be cured with early detection so that patients can immediately get the right treatment. One way specialist doctors diagnose TB disease is by looking at X-ray images of the lungs.
In this study, we will design an automatic system for classifying tuberculosis based on x-ray images of the lungs based on Convolutional Neural Network (CNN) using the MobileNet architecture. The dataset taken is an X-ray image of the lungs which will be used as input for image processing. The first stage is input in the form of an X-ray image, then the image preprocessing process (resizing, grayscale, contrast), followed by a segmentation process (thresholding), followed by a data augmentation process, finally the image will be classified into three classes, namely normal, TBR (tuberculosis). right side), TBRL (right and left tuberculosis). Tuberculosis left side is not included in the classification because tuberculosis is more likely to be affected on the right or in both parts.
In this study, we tested the effect of image (without image processing, preprocessing and segmentation), the effect of batch size, the effect of epoch variations, the effect of augmentation and comparison on classification performance. The final result of this study shows that the best optimizer is Adam using CLAHE preprocessing on epoch 50 batch size 32 and resulted in the average value of the accuracy of the test data of 93.64466667% accuracy.
",,,162202,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217172,,RAFQI MUHAMMAD A,MODIFIED SELF-TRAINING USING K-MEANS CLUSTERING FOR CLICKBAIT DETECTION IN BAHASA INDONESIA,,,"Clickbait Detection, Natural Language Processing, Fasttext, TF-IDF, Semi-supervised Learning, K-Means, Support Vector Machine.","Azhari, Dr., MT.; Yunita Sari, S.Kom., M.Sc., Ph.D. ",2,3,0,2022,1,"Di zaman modern ini, penggunakan clickbait sering digunakan oleh individu atau kelompok untuk memberikan keunggulan dalam jumlah pengunjung situs atau video untuk konten yang mereka buat dengan memberikan pengunjung judul berita atau konten yang menyesatkan dan sangat menarik sedangkan isi dari konten tersebut tidak berkorelasi dengan judul dari kontennya tersebut.
Ada banyak dataset untuk melakukan klasifikasi pada clickbait tetapi mayoritas adalah data yang belum memiliki label. Pada penelitian ini, penulis mengusulkan sebuah metode menggunakan semi supervised-learning untuk membantu me-label data secara otomatis dan membuat korpus data yang sudah dilabel yang dapat digunakan untuk kepentingan klasifikasi.
Penelitian ini mengusulkan sebuah modifikasi pada metode semi supervised-learning menggunakan K-means. Penggunaan &Acirc;&not;metode clustering K-means pada penelitian ini bertujuan untuk memilih data yang memiliki nilai kebenaran tinggi sebelum dimasukkan kedalam metode klasifikasi untuk dilabel. Penelitian ini membandingkan antara penggunaan Fasttext dan TF-IDF sebagai representasi kata menggunakan metode self-training yang sudah dimodifikasi dengan K-means clustering. Hasil yang didapatkan dari komparasi kedua model menyatakan bahwa penggunaan kombinasi representasi kata Fasttext dan modifikasi self-training adalah hasil yang terbaik dengan accuracy score mencapai 85.20%, precision score mencapai 90.80%, recall score mencapai 81.60%, dan F1-score mencapai 85.90%. 
","Nowadays, the term clickbait is used by individuals or organizations to gain viewer counts advantage on their content by giving viewers or readers misleading and very interesting headlines while the content itself is unrelated to the headlines. To overcome this problem, it can be solved by using Clickbait Detection. Clickbait Detection is a task to classify text or headlines between two classes which are clickbaits and non-clickbaits. 
There are thousands of clickbait datasets but most of them are unlabelled. In this research, author proposed a method of using semi supervised-learning method to help label the data and create a corpus of labelled data that can be used for classification purposes. 
This research proposed a modified version of semi-supervised learning method using K-means to select high value data before putting them into the classifier to be labelled. A comparison between Fasttext and TF-IDF as the word representation combining with the modified self-training method using K-means clustering method is conducted in this research. The result of clickbait detection on both models gives the best result from the combination of Fasttext and modified version of self-training with 85.20% on accuracy, precision score of 90.80%, recall score of 81.60%, and F1-score of 85.90%
",,,168291,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213596,,DONI TAN HERO,Pendeteksian Autisme Berbasis Citra Digital Menggunakan Metode  CNN dan LBP,,,"Autisme, Convolutional Neural Network (CNN), Local  Binary Pattern (LBP), waktu eksekusi, feature map, stabilitas akurasi","Afiahayati, S.Kom., M.Cs., Ph.D",2,3,0,2022,1,"Autisme (Autism Spectrum Disorder) menjadi salah satu gangguan yang 
menyerang perkembangan anak dan ada di setiap negara dengan jumlah yang tidak 
sedikit. Pendeteksian autisme umumnya membutuhkan bantuan dokter untuk 
diagnosis. Namun, metode ini membutuhkan waktu serta biaya yang tidak sedikit 
karena perlu melihat gejala-gejala pada anak secara langsung. Di sisi lain, teknologi 
terus berkembang terutama dalam Deep Learning. Para ahli telah memanfaatkan 
berbagai metode Deep Learning dengan berbagai dataset yang dan yang paling 
efektif adalah citra digital. Citra digital terutama gambar wajah dapat digunakan 
sebagai dataset karena pada wajah penderita autisme terdapat biomarker yakni 
peningkatan orbital hyptertelorism dan penurunan facial midline. CNN merupakan 
salah satu metode yang dapat digunakan untuk mengolah dataset citra digital. 
Namun, CNN memiliki beberapa kekurangan yaitu kebutuhan dataset yang tinggi, 
masalah overfitting, dan waktu eksekusi yang lama.
Penelitian ini mencoba untuk mengoptimalkan CNN dari sisi waktu 
eksekusi. Hal yang dilakukan adalah dengan menerapkan LBP pada dataset
sehingga menghasilkan suatu feature map yang menjadi dataset baru. Dataset ini 
kemudian digunakan untuk melatih CNN. Dengan menggunakan feature map CNN 
akan lebih mudah mengekstraksi fitur-fitur yang ada di dalamnya. Walaupun waktu 
eksekusi berkurang, peneliti tetap menjaga stabilitas akurasi dari hasil pembelajaran 
dengan dataset LBP agar tidak terlalu berbeda jauh dengan akurasi pembelajaran 
dengan dataset normal.
Hasil yang didapatkan adalah dataset LBP membutuhkan waktu training
yang lebih baik dibandingkan dataset normal di mana dataset LBP hanya 
membutuhkan waktu 1643 detik sedangkan dataset normal membutuhkan waktu 
1823 detik dalam 60 epoch. Akurasi yang didapatkan oleh dataset LBP adalah 
sebesar 81.67% dan dataset normal sebesar 80.33%. Namun hal ini tidak 
mengindikasikan bahwa dataset LBP dapat memiliki akurasi yang lebih baik secara 
pasti karena dalam beberapa percobaan dataset normal juga dapat memiliki akurasi 
yang lebih baik tetapi akurasi yang dihasilkan masih tetap stabil. Dari penelitian ini 
dapat diketahui bahwa penerapan LBP pada dataset untuk pembelajaran CNN 
terbukti dapat mengurangi waktu training dengan tetap menghasilkan akurasi yang 
stabil","ASD( Autism Spectrum Disorder) is a mental disorder that interfere with 
child development and its exist in every country with a large number. Autism 
detection generally need a doctor for diagnosis. But, this method need a lot of time 
and expensive because they need to look up the symptoms on the child directly. On 
the other hand, the technology continues to evolve especially in Deep Learning. 
Some experts have utilized various Deep Learning method with various dataset, the 
most effective is digital image. Digital images, especially facial images can be used 
as a dataset because the faces of people with autism have biomarkers, an increase
in orbital hypertelorism and a decrease in facial midline. CNN is one of the method 
that can be used to process digital image datasets. However, CNN have several 
disadvantages such as high dataset requirements, overfitting problem, and long 
execution time.
This research tries to optimize CNN in terms of execution time. What is 
done is to applying LBP to the dataset to produce a feature map which becomes a 
new dataset. This dataset is then used to train the CNN. By using the CNN feature 
map, it will be easier to extract the features in it. Even though the execution time is 
reduced, the researcher still maintain the stability of the accuracy of the learning 
outcomes with the LBP dataset so that it is not too different from the learning 
accuracy with the normal dataset
The results obtained are the LBP dataset have a better training time than the 
normal dataset where the LBP dataset only takes 1643 seconds while the normal 
dataset takes 1823 seconds in 60 epochs. The accuracy obtained by the LBP dataset 
is 81.67% and the normal dataset is 80.33%. However, this does not indicate that 
the LBP dataset can have better accuracy for sure because in some experiments the 
normal dataset can also have better accuracy but the resulting accuracy remains 
stable. From this study, it can be seen that the application of LBP to datasets for 
CNN learning has been shown to reduce training time while still producing stable 
accuracy.",,,164589,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218975,,MUHAMMAD HAIDER A,Classifying customer reviews based on product category using recurrent neural networks (rnns) on amazon product review dataset,,,"Text classification, LSTM, GRU, CNN, product category detection, word2vec.","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2022,1,"Banyak situs web e-niaga berisi hampir semua jenis produk yang dicari pelanggan. Kategori produk yang tersedia untuk pelanggan sangat luas dan terus berkembang, banyak kategori produk berisi sub-kategori mereka sendiri. Dari kategori tersebut banyak wawasan yang dapat dikumpulkan dari tinjauan konsumen. Dengan banyaknya ulasan yang dihasilkan tentang produk, ulasan tersebut perlu dikelompokkan ke dalam kategori produk tetapi untuk mengklasifikasikan kategori produk dari ulasan pelanggan adalah sebuah tantangan.
Dengan semua itu, pada penelitian ini akan dibangun model klasifikasi teks yang secara otomatis akan mengkategorikan ulasan pelanggan berdasarkan kategori produk. Model akan dilatih menggunakan kumpulan data ulasan produk amazon. Model ini akan menggunakan penyematan kata word2vec untuk mengekstraksi konteks kalimat dengan lebih baik, dan menggunakan arsitektur LSTM untuk melatih model, selain LSTM, jaringan saraf lain seperti CNN dan GRU juga akan diuji.
Setelah penelitian ini selesai, saat menguji berbagai jaringan saraf, model LSTM memiliki kinerja terbaik dalam set pengujian. Akurasi yang dicapai model LSTM adalah 86,70%, CNN memiliki akurasi 86,28% dan GRU memiliki akurasi 86,44% pada test set. Pada set validasi, model LSTM juga memiliki akurasi terbaik. Akurasi Model LSTM sebesar 86,84%, CNN memiliki akurasi sebesar 85,96% dan GRU memiliki akurasi sebesar 86,60%.","Many ecommerce websites contain almost any type of product that customers are looking for. The categories of products available for the customer are vast and it's ever-growing, many categories of products contain their very own sub-categories. From those categories many insights can be gathered from consumer review. With the vast amount of reviews generated about products, those reviews need to be grouped into the categories of the product but to classify the product category from customer review is a challenge. 
With all that being mentioned, in this research, a text classification model will be built which will automatically categorize customer reviews based on the product category. The model will be trained using the amazon product review dataset. The model will use word2vec word embedding to better extract the context of sentences, and use LSTM architecture to train the model, besides LSTM other neural networks such  CNN and GRU will also be tested.   
Upon the completion of this research, while testing out various neural networks, the LSTM model had the best performance in the test set. The accuracy achieved by the LSTM model was 86.70%, CNN had accuracy of 86.28% and GRU  had accuracy of 86.44% in the test set. In the validation set, the LSTM model also had the best accuracy. The accuracy Of LSTM model was 86.84%, CNN  had accuracy of 85.96% and  GRU had accuracy of 86.60%.
",,,170425,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213344,,AMARILIS KHAIRINA M,PEMBANGKITAN OTOMATIS SAMPIRAN PANTUN MENGGUNAKAN ALGORITMA LONG SHORT TERM MEMORY,,,"Pembangkitan otomatis teks, Long Short Term Memory, Word2Vec, Pantun","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Pantun merupakan warisan budaya nusantara yang termasuk dalam puisi lama. Bagi masyarakat umum, membuat pantun dapat memakan waktu yang lama karena memerlukan keahlian mengenai perbendaharaan kata yang luas agar pantun yang dibuat dapat berirama yang baik dan logis atau mudah dimengerti untuk membuat sampiran pantun. 
Penelitian mengenai pembangkitan otomatis teks umumnya menggunakan algoritma Long Short Term Memory (LSTM), sehingga penelitian ini akan membuat sistem untuk membuat sampiran pantun dengan menggunakan algoritma LSTM dan ekstraksi fitur Word2Vec untuk membantu masyarakat umum membuat pantun dan membantu melestarikannya. 
Sistem dievaluasi menggunakan kuesioner yang dibagi menjadi dua bagian, yaitu Tes Turing dan analisis sampiran pantun menggunakan skala Likert. Akurasi responden dalam menjawab pertanyaan Tes Turing secara benar adalah 68%. Hasil dari analisis sampiran pantun yang telah dibuat menunjukkan kriteria rima yang dihasilkan sistem adalah &quot;baik&quot;, kriteria logis dan keterbacaannya adalah &quot;cukup&quot;, sedangkan berdasarkan responden yang berasal dari rumpun ilmu budaya menilai rima dan keterbacaan secara &quot;baik&quot; dan menilai logis secara &quot;cukup&quot;.
","Pantun is a cultural heritage of the Indonesian archipelago which is included in old poetry. For the general public, making rhymes can take a long time because it requires expertise in extensive vocabulary so that the rhymes that are made can be rhymed well and logically or easily understood to make sampiran rhymes.
Research on automatic text generation usually uses Long Short Term Memory (LSTM) algorithm, so this research will create a system for making sampiran pantun using the LSTM algorithm and Word2Vec feature extraction to help the society make pantun and help preserve of it. 
The system was evaluated using a questionnaire which was divided into two parts, namely Turing Test and sampiran pantun analysis using a Likert Scale. The accuracy of respondents in answering Turing Test questions correctly is 68%. The results of the sampiran pantun analysis that have been made show that the rhyme criteria produced by the system are 'good', the logical and readability criteria are 'enough', while based on respondents from the cultural sciences group, they rated rhyme and readability as 'good' and logically assessed 'enough'.",,,164366,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211812,,CHRYSTIAN,SP-BatikGAN: A Generative Adversarial Network for Seamless Pattern Batik Creation,,,"Generative Adversarial Network, GAN, Batik, Image Generation, Generative Modeling","Wahyono, Ph.D",2,3,0,2022,1,"Mengikuti kemajuan GAN dalam data dan komputasi yang terbatas, makalah ini berfokus pada aplikasi arus utama GAN pada generasi batik yang semakin populer, ikon populer, dan mode di Asia Tenggara. Karena kesulitan dan kerumitan dalam menghasilkan tugas-tugas batik, dalam penelitian ini, kami fokus pada salah satu karakteristik umum batik, yaitu sifat mulus dan simetris. Namun, tugas pembuatan pola yang mulus dan simetris tidak terbatas pada dan dapat digunakan pada aplikasi lain seperti ubin Portugis juga. Pertama, kami mengumpulkan dan memberikan kepada publik 1.216 gambar patch batik berkualitas tinggi pertama langsung dari file desain. Selanjutnya, kami membuat penerapan jahitan dan simetri, metode pengawasan mandiri yang meningkatkan dan mempercepat pelatihan pada konfigurasi GAN apa pun untuk tugas pembuatan tambalan batik. Kemudian, kami menyelidiki mekanisme perhatian yang lebih murah untuk mengadaptasi GAN berbasis perhatian ke pengaturan terbatas kami. Dengan kedua teknik tersebut, kami membuat SNS-BatikGAN dan membandingkannya dengan FastGAN, GAN terbaik untuk pengaturan terbatas, SNS-BatikGAN meningkatkan skor FID dari 110,11 menjadi 90,76, penurunan 18%, dan skor recall dari 0,047 menjadi 0,204, peningkatan 334% . Kami percaya penelitian ini akan menjadi landasan penelitian masa depan di batik atau aplikasi pola mulus dan simetris pada umumnya.","Following GAN's advancement in limited data and computation, this paper focuses on the mainstream applications of GAN on the increasingly popular batik generation, a popular icon, and fashion in Southeast Asia. Due to the difficulties and complexity of generating batik tasks, in this research, we focused on one of the common batik characteristics, which is seamless and symmetric properties. The seamless and symmetric pattern generation tasks, however, are not limited to and can be used on other applications such as Portuguese tiles as well. First, we collect and provide publicly the first-ever high-quality 1,216 batik patch images straight from the design files. Next, we create seam and symmetry enforcement, a self-supervision method that improves and accelerate training on any GAN configuration for batik patch generation tasks. Then, we investigate a cheaper attention mechanism to adapt attention-based GAN to our limited settings. With both techniques, we create SNS-BatikGAN and compared it to FastGAN, the best GAN for limited settings, SNS-BatikGAN improve FID score from 110.11 to 90.76, an 18% decrease, and recall score from 0.047 to 0.204, a 334% increase. We believe this research will be the foundation of future research in batik or seamless and symmetric pattern applications in general.",,,162869,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217190,,RAYYAN FATHURRAHMAN,Sentiment and Emotion Analysis of Amazon's Book Review using Long Short-Term Memory and Word-Emotion Association Lexicon,,,"sentiment analysis, emotion analysis, RNNs, word-emotion lexicon, word embedding.","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Sejak diperkenalkannya Web 2.0, yang telah digunakan dalam pembuatan blog, forum, dan jejaring sosial online, pengguna telah dapat membangun percakapan dan mengekspresikan ide-ide mereka tentang berbagai topik. Analisis sentimen, juga dikenal sebagai penggalian opini, adalah studi tentang perasaan, sentimen, evaluasi, penilaian, sikap, dan emosi orang terhadap berbagai objek, seperti produk, layanan, organisasi, orang, situasi, peristiwa, dan subjek, serta fitur mereka. Selain itu, analisis emosi juga merupakan teknik untuk mengenali jenis emosi manusia yang berbeda, seperti kemarahan, kebahagiaan, dan depresi.
Sebagaimana dikemukakan di atas, dalam penelitian ini akan dilakukan analisis sentimen dan emosi. Long Short-Term Memory, subset dari RNNs, dan NRC Word-emotion Association akan digunakan masing-masing untuk melatih model penelitian ini. Pedagang (dalam contoh ini, penulis) dapat memanfaatkan ulasan buku untuk mengidentifikasi kualitas buku dan meningkatkan kualitas toko mereka, sementara pelanggan dapat menganalisis isi buku berdasarkan ulasan, berkat penggunaan analisis sentimen dan analisis emosi. Selain itu, pemilik platform, seperti Amazon, mendapatkan keuntungan dari ulasan karena mereka meningkatkan lalu lintas ke situs mereka.
Penelitian ini di LSTM berkinerja baik secara makro atau rata-rata tertimbang. Meskipun dataset berisi sangat sedikit review negatif, model dapat menghasilkan 81,66%, 96,92%, 97,12%, dan 9,63% di rata-rata makro F1-Score, rata-rata tertimbang F1-Score, akurasi, dan kerugian masing-masing. Analisis lebih lanjut dilakukan untuk mendeteksi emosi yang dirasakan pengulas. Ketiga buku di dalam dataset tersebut memiliki emosi dominan yang sama yaitu kepercayaan. Hal ini menunjukkan bahwa ketiga buku tersebut layak untuk direkomendasikan kepada pembaca lain karena resensinya merasa percaya pada buku-buku tersebut. The Hobbit and Divergent memiliki antisipasi sebagai emosi paling dominan kedua dalam distribusi yang dapat disimpulkan bahwa review memiliki beberapa kesamaan dalam genre fiksi. Sedangkan The All The Light We Can't See memiliki kegembiraan sebagai emosi paling dominan kedua. Hal ini memberikan indikasi bahwa para reviewer menyukai cerita perang fiktif sebagai genrenya.","Since the introduction of Web 2.0, which has resulted in the creation of blogs, forums, and online social networks, users have been able to establish a conversation and express their ideas on a variety of topics. Sentiment analysis, also known as opinion mining, is the study of people's feelings, sentiments, evaluations, appraisals, attitudes, and emotions toward various objects, such as products, services, organizations, people, situations, events, and subjects, as well as their features. Furthermore, emotion analysis is also a technique for recognizing distinct types of human emotions, such as fury, happiness, and depression.
As stated above, in this research, a sentiment and emotion analysis will be conducted. Long Short-Term Memory, a subset of RNNs, and NRC Word-emotion Association will be used respectively to train the model for this research. Merchants (in this example, writers) may utilize reviews on books to identify the quality of the books and improve the quality of their stores, while customers can analyze the contents of the books based on the reviews, thanks to the use of sentiment analysis and emotion analysis. Additionally, platform owners, such as Amazon, gain from reviews since they enhance traffic to their sites.
This research resulted in LSTM performing well in terms of macro- or weighted-average. Although the dataset contains very little on the negative review, the model can result 81.66%, 96.92%, 97.12%, and 9.63% in macro average F1-Score, weighted average F1-Score, accuracy, and loss respectively. Further analysis was performed to detect the emotion that the reviewers felt. The three books inside the dataset have the same dominant emotion which is trust. This indicates that the three books are worth recommending to other readers since the reviewer felt trust in those books. The Hobbit and Divergent has anticipation as the second most dominant emotion in the distribution which can be concluded that the review has some similarity in the fiction genre. While The All The Light We Cannot See has joy as the second most dominant emotion. This gave an indication that the reviewers like fictional war stories as the genre.",,,168326,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217450,,ABYAN BURHANUDDIN A,PERBANDINGAN EKSTRAKSI FITUR TF-IDF DAN BAG OF WORDS UNTUK KLASIFIKASI EMOSI PADA DATA TWITTER MENGGUNAKAN SVM DAN KNN,,,"Bag-of-Words, klasifikasi emosi, KNN, SVM, TF-IDF, Twitter","Aina Musdholifah, S.Kom., M.Kom., Ph.D; Diyah Utami K. P., S.Kom., M.Sc., M.Cs",2,3,0,2022,1,"Sosial media telah berkembang pesat di seluruh dunia selama satu dekade terakhir ini. Berdasarkan data statistik Hootsuite di tahun 2020, mayoritas pengguna internet juga merupakan pengguna sosial media. Salah satu media sosial paling populer di Indonesia adalah twitter. Twitter sering digunakan penggunanya untuk melakukan diskusi dan percakapan dalam bentuk tweet. Percakapan dan diskusi dalam bentuk tweet tersebut mengandung berbagai data yang dapat dianalisa seperti emosi untuk mengetahui respon, perasaan, atau pendapat orang-orang mengenai topik tertentu.
Penelitian ini melakukan perbandingan performa ekstraksi fitur TF-IDF dan Bag-of-Words pada permasalahan emosi klasifikasi menggunakan dataset twitter berbahasa Indonesia. Penelitian ini juga menggunakan K-Nearest Neighbor (K-NN) dan Support Vector Machine (SVM) sebagai metode klasifikasi. 
Setelah dilakukannya penelitian ini, diperoleh kesimpulan bahwa secara keseluruhan, pada model dengan parameter terbaik, ekstraksi fitur TF-IDF memiliki performa yang lebih baik dibandingkan dengan Bag of Words. Model terbaik yang ditemukan merupakan model SVM dengan ekstraksi fitur TF-IDF dengan akurasi, presisi, recall, dan F1-measure secara berturut-turut adalah 64.22%, 67.4%, 64.57%, dan 65.19%.
","Social media has grown rapidly all over the world over the past decade. Based on Hootsuite statistical data in 2020, the majority of internet users are also social media users. One of the most popular social media in Indonesia is Twitter. Twitter is often used by users to conduct discussions and conversations in the form of tweets. This conversations and discussions contains various data that can be analyzed such as emotions to find out the responses, feelings, or opinions of people on certain topics.

This study compares the performance of TF-IDF and Bag-of-Words feature extraction on emotional classification problems using the Indonesian language twitter dataset. This study also uses K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) as classification methods.

After doing this research, it was concluded that overall, in the models with the best parameter, TF-IDF feature extraction has a better performance than Bag of Words. The best model found is the SVM model with TF-IDF feature extraction with accuracy, precision, recall, and F1-measure are 64.22%, 67.4%, 64.57%, and 65.19%, respectively.
",,,168622,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212848,,RIZKIAJI PUTRO,The Digitization of the Indonesian Pasar Market Through an E-commerce Based Mobile App Solution,,,"E-Grocery, Traditional Pasar, Mobile Application, Design Thinking, UI/UX, Minimum Viable Product","Mardhani Riasetiawan, SE Ak, M.T.",2,3,0,2022,1,"Traditional Indonesian Pasar has become a symbolic representation of the hardworking backbone of the Indonesian economy and is synonymous with our national identity. In an era where the digitization and implementation of innovative technologies is becoming more prevalent, Indonesian pasar must be willing to adapt to the changing times. Hence, A mobile application will be developed in an attempt to streamline the process of purchasing products from a traditional pasar and make it more accessible and convenient to users. By creating a catalogue of products and traditional pasars shown around their area, Users are able to purchase products and even negotiate for better prices similar to their real-life pasar counterparts, all in an effort to make the transition from physical to digital more convenient for old and new customers alike. 
Early Survey and Final Targeted Surveys results show that the creation of the mobile application has ultimately been a pleasant and endeavor for the users. Ultimately leading to a net positive response in terms of convenience and accessibility and achieving significant steps towards innovating in the Indonesian pasar market through an e-grocery mobile application. However, features such as user reviews and an order/delivery logistics system be considered in future reiterations of the application; in order to encompass a more complete flow of the user journey and transforming the way we currently think of Traditional Pasar.","Traditional Indonesian Pasar has become a symbolic representation of the hardworking backbone of the Indonesian economy and is synonymous with our national identity. In an era where the digitization and implementation of innovative technologies is becoming more prevalent, Indonesian pasar must be willing to adapt to the changing times. Hence, A mobile application will be developed in an attempt to streamline the process of purchasing products from a traditional pasar and make it more accessible and convenient to users. By creating a catalogue of products and traditional pasars shown around their area, Users are able to purchase products and even negotiate for better prices similar to their real-life pasar counterparts, all in an effort to make the transition from physical to digital more convenient for old and new customers alike. 
Early Survey and Final Targeted Surveys results show that the creation of the mobile application has ultimately been a pleasant and endeavor for the users. Ultimately leading to a net positive response in terms of convenience and accessibility and achieving significant steps towards innovating in the Indonesian pasar market through an e-grocery mobile application. However, features such as user reviews and an order/delivery logistics system be considered in future reiterations of the application; in order to encompass a more complete flow of the user journey and transforming the way we currently think of Traditional Pasar.",,,164039,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
216689,,BRYAN DWISON,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN PESERTA PELATIHAN KARYAWAN DENGAN METODE WASPAS (STUDI KASUS PT. CLOUDARON JAYA INDONESIA),,,"Sistem Pendukung Keputusan, WASPAS, Pelatihan Karyawan","Dr. Sri Mulyana, M.Kom.",2,3,0,2022,1,"Dalam manajemen karyawan, dari setiap divisi pada umumnya melakukan
pelatihan untuk beberapa karyawan dengan tujuan untuk memperluas
pengalaman, wawasan, meningkatkan produktivitas kerja serta kemampuan
karyawan. Jumlah karyawan dan jenis bidang pekerjaan bervariasi, sedangkan
dana yang disiapkan juga terbatas. Hal ini menyebabkan perusahaan harus
mengatur strategi dan memutuskan peserta yang ditugaskan untuk mengikuti
pelatihan dengan tepat, terkadang hasil penilaian tidak sesuai dengan harapan,
karyawan yang seharusnya tidak berhak mengikuti pelatihan terpilih mengikuti
pelatihan tersebut dan sebaliknya.
Untuk membantu perusahaan dalam memilih peserta pelatihan karyawan,
dibutuhkan sebuah Sistem Pendukung Keputusan yang dapat digunakan untuk
melakukan penilaian terhadap karyawan dalam pemilihan peserta pelatihan.
Metode yang akan digunakan adalah Weighted Aggregated Sum Product
Assesment (WASPAS). Sistem pendukung keputusan yang dimaksudkan dapat
menghasilkan sistem penilaian dengan akurat dan tepat sasaran dalam melakukan
penilaian karyawan serta menjaga kualitas sumber daya manusia agar dapat
mengikuti perkembangan dan meningkatkan kompetensi sumber daya manusia.
Hasil dari penelitian yang telah dilakukan adalah Sistem Pendukung
Keputusan pemilihan peserta pelatihan karyawan berbasis web menggunakan
metode WASPAS, sistem ini mampu memberikan rekomendasi karyawan yang
yang memenuhi kualifikasi untuk dapat mengikuti pelatihan karyawan
berdasarkan penilaian yang diberikan pada setiap kriteria penilaian.","In employee management, each division generally conducts training for
several employees with the aim of expanding experience, insight, increasing work
productivity and employee capabilities. The number of employees and the type of
field of work vary, while the funds prepared are also limited. This causes the
company to have to set a strategy and decide on the participants who are assigned
to attend the training appropriately, sometimes the assessment results are not in
accordance with expectations, employees who should not be entitled to take part in
the training are selected to attend the training and vice versa.
Therefore, a Decision Support System is needed that can be used to assess
employees in the selection of trainees. The method used is Weighted Aggregated
Sum Product Assessment (WASPAS). The intended decision support system can
produce an accurate and targeted assessment system in conducting employee
assessments and maintaining the quality of human resources in order to follow
developments and improve the competence of human resources.
The result of the research that has been carried out is the Decision Support
System for selecting web-based employee trainees using the WASPAS method, this
system is able to provide recommendations for employees who meet the
qualifications to be able to take part in employee training based on the assessment
given on each assessment criterion.",,,167851,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214386,,Annisa Titi Ramadhani,ANALISIS PEMODELAN TOPIK PADA LATENT DIRICHLET ALLOCATION (LDA) UNTUK SISTEM AUTO-TAGGING PADA BERITA ONLINE,,,"auto-tagging, topic modelling, LDA, Latent Dirichlet Allocation, Gibbs Sampling, Confusion Matrix, tagging","I Gede Mujiyatna, S.Kom., M.Kom.; Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2022,1,"Tagging atau penandaan menjadi salah satu metode yang populer dalam melakukan anotasi dan mengorganisir konten. Salah satu konten yang paling sering dimanfaatkan dalam tagging adalah teks artikel berita. Teks artikel berita menjadikan sarana untuk mendapatkan informasi terbaru namun terkadang penulis konten artikel berita melakukan penandaan secara manual. Hal tersebut dapat menyulitkan penulis jika dilakukan pada kumpulan artikel berita dalam jumlah besar. Oleh karena itu, diperlukan metode penandaan secara otomatis atau auto-tagging yang dapat memudahkan dalam melakukan penandaan pada teks artikel berita. Penelitian ini bertujuan untuk mengimplementasikan pemodelan topik dengan metode Latent Dirichlet Allocation (LDA), yaitu salah satu metode yang dibuat oleh Blei untuk mencari topik-topik apa saja yang terdapat dalam kumpulan dokumen dalam jumlah besar. Penelitian ini melakukan pemodelan topik dengan LDA sebagai penandaan otomatis pada 100 teks artikel berita berdasarkan hasil model LDA yang didapatkan. Hasil dari model LDA berupa sekumpulan topik yang setiap topik terdiri dari sekumpulan kata kunci. Metode yang digunakan dalam penelitian ini sebagai penandaan otomatis dengan mengimplementasikan model LDA dengan mencari topik yang dominan pada setiap artikel berita sehingga dari metode ini didapatkan beberapa kata kunci dari hasil pendistribusian topik. Pengujian dilakukan dengan validasi model LDA dan mendapatkan jumlah topik yang optimal sebanyak 16 topik. Evaluasi hasil dilakukan dengan menggunakan perhitungan confusion matrix untuk mengukur nilai akurasi, presisi, recall, dan f1-measure. Hasil penelitian ini mendapatkan nilai akurasi 49%, nilai presisi 46%, nilai recall 94,70%, dan nilai f1-score 62% terhadap hasil dari implementasi LDA sebagai auto-tagging.","Tagging becomes a popular technique to annotate and organizing content. An example of content that is often used in tagging is news article text. News article text is used as a tool for getting the latest information. Nevertheless, news article content writers often tag the content manually. This can also be troublesome for writers if it is done on a large collection of articles. Therefore, this research generates an automatic tagging method that can make an easier way to tag the news article. This research proposed implementing a topic modelling using Latent Dirichlet Allocation (LDA), a method developed by Blei to determine what kind of topics are in a large collection of documents. This research also doing a topic modelling with LDA as an auto-tagging on 100 news articles based on a result that generated from LDA model. The results are some topics in which each topic has its own keywords. The method in this research is used to discover the dominant topics of each news article so that this method can get the keywords of topic distribution.
In this research, the test is conducted by validate the LDA model and be able to get 16  topics as the most optimal number of topic.The LDA-tagging result is evaluated with confusion matrix method by measure the values of accuracy, precision, recall, and f1-measure. In this research, the result shows values of 49% for accuracy, precision with 46%, recall with 94,70%, and f1-score with 62% over the LDA-tagging result.",,,165429,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212596,,SETYAWAN PUTRA S,Optimasi Peringkasan Teks Otomatis Unsupervised Secara Ekstraktif Menggunakan Integer Linear Programming,,,"Peringkasan Teks Otomatis, ILP, Ekstraktif, Unsupervised ","Yunita Sari, S.Kom., M.sc., Ph.D; Azhari, Drs., MT., Dr",2,3,0,2022,1,"Perkembangan media penyebaran informasi yang begitu masif merubah kebiasaan masyarakat untuk selalu mencari informasi melalui media digital. Dengan banyaknya informasi teks yang tersedia, diperlukan sebuah sistem peringkasan teks otomatis yang akan mempermudah masyarakat dalam mengekstrak informasi dari sebuah teks dengan lebih efektif, yang akan menghemat waktu dan tenaga. Sistem ini secara otomatis akan menghasilkan ringkasan yang mengandung pokok bahasan dari sebuah data teks.
Penelitian ini mencoba mengimplementasikan Integer Linear Programming pada berbagai sistem peringkasan teks otomatis ekstraktif dengan pendekatan unsupervised yang sudah ada. INDOSUM sebagai sebuah penelitian mengenai peringkas teks otomatis beserta dengan dataset nya akan dijadikan baseline, yang akan dibandingkan performanya, sebelum dan sesudah pengimplementasian Integer Linear Programming. Empat algoritma akan diuji pada penelitian ini, yakni LEAD-3, Sumbasic, Latent Semantic Analysis dan TextRank.
Evaluasi sistem ini dilakukan menggunakan skor F1 dari metrik ROUGE. Dari pengujian keempat algoritma yang disebutkan diatas, didapatkan rata-rata kenaikan pada sistem yang ditambahkan ILP, berturut-turut sebesar 6.14%, 20.14% dan 7.44% pada ROUGE-1, ROUGE-2 dan ROUGE-L.
","The development of information dissemination media that is so massive has changed people's habits to always seek information through digital media. With a lot of text information available online, we need an automatic text summarization system that will make it easier for people to extract information from a text more effectively, which will save time and effort. This system will automatically generate a summary containing the subject matter of a data text.
This study tries to implement Integer Linear Programming on various extractive automatic text summarization systems with an existing unsupervised approach. INDOSUM as a study on automatic text summarization along with its dataset, will be used as a baseline against which the performance will be compared, before and after the implementation of Integer Linear Programming. Four algorithms will be tested in this study, namely LEAD-3, Sumbasic, Latent Semantic Analysis and TextRank.
Evaluation of this system is carried out by using the F1 score of the ROUGE metric. From testing the four algorithms mentioned above, the average increase in the system added with ILP was 6.14%, 20.14% and 7.44% in ROUGE-1, ROUGE-2 and ROUGE-L, respectively.
",,,163684,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212853,,ADIVA VIDYANI ERIZA,Hate Speech Detection in Indonesian Using Complement Naive Bayes,,,"Complement Naive Bayes, Hate Speech Detection, Natural Language Processing, Classification","Nur Rokhman, S.Si., M.Kom, Dr.",2,3,0,2022,1,"Ujaran kebencian telah menjadi masalah yang meluas seiring penggunaan media sosial yang terus meningkat di Indonesia saat ini. Untuk mengatasi masalah ini dengan cepat, diperlukan otomatisasi pendeteksian ujaran kebencian. Adapun masalah dalam pendeteksian ujaran kebencian adalah bahwa data ujaran kebencian jauh lebih sedikit dibandingkan dengan data yang bukan ucapan kebencian, sehingga peluang untuk menghasilkan dataset yang tidak seimbang meningkat, dan dapat menyebabkan kesalahan klasifikasi lebih besar. Complement Naive Bayes classifier sering digunakan untuk menyelesaikan kasus-kasus ketidakseimbangan dataset ini.
Complement Nave Bayes diciptakan dengan tujuan untuk memecahkan masalah saat menggunakan Multinomial Naive Bayes, di mana ia tidak bekerja dengan baik dalam kasus dataset yang tidak seimbang. Oleh karena itu dalam penelitian ini kinerja pengklasifikasi Complement Naive Bayes dibandingkan dengan kinerja pengklasifikasi Multinomial Naive Bayes dalam mengklasifikasikan data ujaran kebencian yang tidak seimbang, dan model divalidasi menggunakan K-Fold Cross Validation.
Dalam eksperimen ini, meskipun skor akurasi mencetak 1% lebih rendah dibandingkan dengan pengklasifikasi Multinomial Naive Bayes, pengklasifikasi Complement Nave Bayes mencetak skor rata-rata recall tertinggi sebesar 89%, yaitu 25% lebih tinggi dibandingkan dengan pengklasifikasi Multinomial Naive Bayes. Hal ini menunjukkan bahwa pengklasifikasi Complement Naive Bayes dapat mengklasifikasikan ujaran kebencian lebih baik, dan memiliki peluang lebih kecil untuk gagal mendeteksi ujaran kebencian yang sebenarnya.","Hate speech has become a widespread problem as the use of social media continues to increase in Indonesia today. To quickly tackle this problem, the automation of detecting the hate speech is needed. The problem in hate speech detection is that there is way less data of hate speech compared to non-hate-speech, hence higher chance of creating an imbalanced dataset that can cause misclassification. Complement Naive Bayes classifier is often used to solve these cases of imbalanced dataset.
Complement Naive Bayes was invented with the purpose of solving the problem when using the Multinomial Naive Bayes, in which it does not perform well in the case of imbalanced dataset. Therefore in this research, the performance of Complement Naive Bayes classifier is compared to the performance of Multinomial Naive Bayes classifier in classifying imbalanced hate speech data, and the models are validated using the K-Fold Cross Validation.
In this experiment, despite the 1% lower accuracy score compared to the Multinomial Naive Bayes classifier, the Complement Naive Bayes classifier scored the highest of 89% in average recall score, that is 25% higher compared to Multinomial Naive Bayes classifier. This indicates that Complement Naive Bayes classifier can classify hate speech better, and has a lower chance of letting the actual hate speech go undetected.",,,163931,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
210550,,KHRESNA PANDU I,Pengenalan Bahasa Isyarat Indonesia menggunakan MediaPipe dengan Model Random Forest dan Multinomial Logistic Regression,,,"Pengenalan Bahasa Isyarat, Bahasa Isyarat Indonesia, SIBI, MediaPipe.","Wahyono, Ph.D",2,3,0,2022,1,"Menurut World Federation of the Deaf, terdapat lebih dari 300 bahasa isyarat di seluruh dunia dan 70 juta tunarungu menggunakannya, sedangkan berdasarkan Kementrian Kesehatan (2014) di Indonesia terdapat 2.5 juta tunarungu. Di sisi lain jumlah masyarakat umum yang mempunyai kemampuan untuk memahami dan menggunakan bahasa isyarat sangat terbatas. Hal ini tentu menjadi masalah karena penutur bahasa isyarat tidak dapat berkomunikasi dengan mudah dengan masyarakat umum. Terdapat beberapa penelitian sebelumnya mengenai pengenalan bahasa isyarat Indonesia tetapi memiliki keterbatasan dimana dibutuhkan perangkat khusus seperti Leap Motion Controller atau depth camera. Penelitian ini bertujuan membuat model pembelajaran mesin random forest dan logistic regression yang dapat melakukan pengenalan bahasa isyarat Sistem Bahasa Isyarat Indonesia (SIBI) menggunakan kamera RGB biasa dengan kerangka kerja MediaPipe. Model random forest memiliki asimtot pada jumlah tree 100 dengan nilai akurasi mencapai sekitar 97%. Sedangkan model logistic regression memiliki asimtot pada maksimum iterasi 250 dan L2 regularisasi dengan nilai akurasi mencapai sekitar 96%.","According to the World Federation of the Deaf, there are more than 300 sign languages worldwide and 70 million deaf use them, based on the Ministry of Health (2014) in Indonesia there are 2.5 million deaf people. On the other hand, the number of the general public who can understand and use sign language is very limited. This becomes a problem because sign language speakers can&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;t communicate easily to the general public. There are several previous studies regarding Indonesian language recognition but has limitations where it is necessary to a particular device such as the Leap Motion Controller or depth camera. This research aims to create a random forest
machine learning model and logistic regression that can perform the sign language recognition of the Indonesian Sign Language System (SIBI) using a regular RGB camera with the MediaPipe framework. Our Random forest models got an asymptote at 100 trees with an accuracy of 97%. On the other hand, the logistic regression model has an asymptote of maximum iterations at 250 and L2 regularization that got accuracy around 96%.",,,161702,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212342,,RETNO WATI,KLASIFIKASI UCAPAN NOMOR LANTAI DAN PERINTAH SEDERHANA PADA PENGGUNA ELEVATOR MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK DAN RANDOM FOREST,,,"Klasifikasi ucapan, Convolutional Neural Network, Random Forest, Mel Frequency Spectral Coefficient beserta fitur dinamisnya","Afiahayati, S.Kom., M.Cs., Ph.D.",2,3,0,2022,1,"Penggunaan elevator dapat memberikan keuntungan, di antaranya dapat menghemat waktu dan tenaga serta pengguna tidak perlu kerepotan untuk memindahkan barang &acirc;€“ barang berat dari suatu lantai ke lantai yang lain. Namun, terkadang beberapa penyandang disabilitas yang memiliki keterbatasan fisik dapat mengalami kesulitan dalam mengoperasikan tombol &acirc;€“ tombol pada elevator. Di sisi lain, pada masa pandemi COVID-19, tombol elevator dapat berpotensi sebagai media penularan COVID-19 melalui kontak fisik ketika pengguna menggunakan tombol elevator yang telah terkontaminasi virus corona SARS-CoV-2. Salah satu solusi yang dapat ditawarkan untuk mengatasi permasalahan tersebut adalah model pengenal ucapan otomatis yang dapat mengenali dan mengklasifikasikan ucapan nomor lantai dan perintah sederhana pada pengguna elevator. 
Penelitian ini bertujuan untuk mendesain, membangun, dan menganalisis kombinasi arsitektur pengenal ucapan yang mampu mengenali dan mengklasifikasikan ucapan nomor lantai dan perintah sederhana pada pengguna elevator. Metode klasifikasi yang digunakan pada penelitian ini adalah Convolutional Neural Network (CNN) yang dikombinasikan dengan Random Forest. Penelitian ini juga menggunakan Mel Frequency Spectral Coefficient (MFSC) beserta fitur dinamisnya yaitu delta dan double delta untuk mengekstrak fitur audio.
Pada penelitian ini, kombinasi model CNN dan Random Forest yang diusulkan untuk mengklasifikasikan ucapan nomor lantai den perintah sederhana dapat menghasilkan akurasi uji dengan capaian sebesar 96,5%.
","The use of an elevator can provide benefits, including saving time and energy and user do not need to be bothered to move heavy items from one floor to another. But sometimes people with disabilities who have physical limitations can have difficulty operating the buttons of the elevator. On the other hand, during the COVID-19 pandemic, the elevator button could be a medium for transmitting COVID-19 through physical contact when the user uses the elevator button contaminated with SARS-CoV-2 coronavirus. One solution that can be offered to overcome these problems is an automatic speech recognition that can recognize and classify floor numbers and simple commands speech for elevator users.
This study aims to design, build, and analyze a combination of speech recognition architecture that can recognize and classify floor numbers and simple commands speech for elevator users. The classification method used in this study is Convolutional Neural Network (CNN) combined with Random Forest. This study also uses the Mel Frequency Spectral Coefficient (MFSC) with its dynamic features, namely delta and double delta to extract audio features.
In this study, the model combination of CNN and Random Forest proposed to classify floor numbers and simple commands speech can produce a test accuracy of 96,5%.
",,,163390,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
220024,,SALSABILA Q,Pengembangan Metode Pengambilan Keputusan pada Multi-Agent Reinforcement Learning untuk Multi-Depot Vehicle Routing Problem with Time Window,,,"Vehicle Routing Problem, Reinforcement Learning, Pointer Network, Transformer, Multi-Agent","Suprapto, Drs., M.Kom., Dr.",2,3,0,2022,1,"Permasalahan Multi-Depot Vehicle Routing Problem with Time Window (MDVRPTW) yang merupakan bagian dari Vehicle Routing Problem (VRP) sudah ada sejak lama dan merupakan bagian dari permasalahan NP-hard. Modified Pointer Network (MPN) serta Mutli-Agent Attention Model (MAAM) dapat menyelesaikan Capacitated VRP serta Multi-VRP with Soft Time Window dengan sangat baik. Pada penelitian ini, kami memodifikasi MPN dan MAAM sehingga arsitektur dapat menerima permasalahan MDVRPTW dan dapat menerima jumlah kapasitas kendaraan yang berbeda dengan mengubah kedua arsitektur menjadi multi-agent. Dengan hyperparameter yang sama, arsitektur MPN dan MAAM dengan formulasi decoding multi-agent terbukti memiliki performa yang lebih baik dibandingkan dengan single agent sebanyak 18 dari 20 dataset.","Multi-Depot Vehicle Routing Problem with Time Window (MDVRPTW), part of Vehicle Routing Problem (VRP) is already exists for a long time and is one of NP-hard problem. Modified Pointer Network (MPN) and Multi-Agent Attention Model (MAAM) are able to solve Capacitated VRP and Multi-VRP with Soft Time Window respectively with a good performance. In this research, we modify MPN and MAAM so that the architectures are able to recieve MDVRPTW with difference in total vehicles by modifying both architecture to be a multi-agent. With the same hyperparameter, MPN and MAAM with multi-agent decoding formulation shown outperforming MPN and MAAM with single agent in 18 out of 20 dataset.",,,171212,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211323,,VINCENT JUNITIO UNGU,Analisis Sentimen Berbasis Aspek Menggunakan Embeddings from Language Models (ELMo) dan Gated Recurrent Unit (GRU) pada Data Ulasan Restoran,,,"analisis sentimen berbasis aspek, aspect category detection, aspect category polarity classification, Embeddings from Language Models, Gated Recurrent Unit ","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Ulasan yang ditulis seseorang terhadap pelayanan di suatu restoran dapat menjadi referensi bagi pelanggan lainnya untuk mengunjungi restoran. Analisis ulasan restoran perlu dilakukan untuk memahami tingkat kepuasan dari pelayanan yang diberikan. Oleh karena itu, diperlukan suatu model yang mampu melakukan analisis sentimen berbasis aspek yang mampu menunjukkan performa yang bagus.
Penelitian ini melakukan analisis sentimen berbasis aspek pada data restoran. Pada penelitian ini terdapat dua subtugas yang akan dilaksanakan, yaitu aspect category detection dan aspect category polarity classification. Pada aspect category detection, aspek yang akan dipertimbangkan adalah aspek ambience, anecdotes/miscellaneous, food, price, dan service. Pada aspect category polarity classification, hanya polaritas positif dan negatif yang akan dipertimbangkan. Penelitian ini akan menggunakan Embeddings from Language Models (ELMo) dan model klasifikasi Gated Recurrent Unit (GRU).
Pada subtugas aspect category detection, nilai f1 score tertinggi yang didapatkan adalah 86%. Pada subtugas aspect category polarity classification, nilai f1 score tertinggi yang didapatkan adalah 89%. Penelitian ini belum dapat menyimpulkan secara pasti bahwa penggunaan ELMo dan GRU mampu menangkap keterkaitan antar kata lebih baik daripada ELMo dan CNN.","A review written by someone about the service at a restaurant can be a reference for other customers to visit the restaurant. Analysis of restaurant reviews needs to be done to understand the level of satisfaction from the services provided. Therefore, a model capable of performing aspect-based sentiment analysis with good performance is needed.
In this research, aspect-based sentiment analysis is conducted. There are two subtasks to be carried out, namely aspect category detection and aspect category polarity classification. In the aspect category detection, the aspects considered are ambience, food, price, service, and anecdotes/miscellaneous. In the aspect category polarity classification, only positive and negative polarities are considered. This study uses deep contextualized word embedding Embeddings from Language Models (ELMo) and Gated Recurrent Unit (GRU) classification model.
In the aspect category detection, the highest f1 score obtained was 86%. In the aspect category polarity classification, the highest f1 score obtained was 89%. This study has not been able to conclude with certainty that the use of ELMo and GRU is able to capture the relationship between words better than ELMo and CNN.",,,162405,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
216445,,DIAZ SAUFA YARDHA,Komparasi Fitur Pada Situs Web E-Commerce Dengan Metode Automation Testing Menggunakan Robot Framework,,,"e-commerce, website, robot framework, automation testing","Dr. Sri Mulyana, M.Kom.",2,3,0,2022,1,"Terdapat berbagai jenis e-commerce berbasis website yang bisa diakses oleh masyarakat. Setiap e-commerce tentunya memiliki tujuan yang sama, yaitu untuk mendapatkan barang tertentu. Untuk mempermudah pencarian barang, pihak pengembang e-commerce menyediakan fitur membantu pencarian untuk mempersempit kriteria produk 
agar sesuai dengan keinginan pengunjung. Akan tetapi, terdapat perbedaan kinerja dari setiap fitur sehingga mempengaruhi hasil pencarian dan tentunya kepuasan pengguna. 

Untuk melakukan percobaan di berbagai website dan dilakukan berulang kali, dibutuhkan program automasi untuk mendapatkan hasil yang optimal sehingga dipilih robot framework untuk membantu mengimitasi perilaku user sekaligus mengambil data hasil pencarian setelah dikenakan fitur-fitur pembantu pencarian terhadap beberapa 
produk. 

Hasil pengujian menunjukkan bahwa hanya satu fitur yang menampilkan hasil 100% sesuai dengan fitur pembantu pencarian yang diaktifkan, yaitu filter berdasar harga terendah yang dimasukkan pengguna. Selain fitur tersebut, e-commerce yang diuji memiliki hasil yang fluktuatif akurasinya. Dalam pengaktifan fitur tertentu pun pengguna membutuhkan waktu lebih untuk menunggu website untuk merespon, seperti saat membuka website dan melakukan pencarian suatu produk.","There are a lot of websites-based e-commerce that can be accessed by people from Indonesia. Each and every e-commerce has the same function, that is to search for a product. To make the product finding easier, the e-commerce developer makes a searching option to narrow down the criteria of product for the user. But for each and every of those features, there are performance differences that can affect search results and the users' satisfaction.

We need to use an automated program so that we can do tests on a couple of websites, a couple of times and at the same time get an optimal result. For that reason, we choose Robot Framework to imitate users' behavior and get the data from search results that have been influenced by the search filter features on a couple of products. 

The results show that only one search filter feature can show the products according to the filter that has been selected, that filter is filter by lowest price. The other features that have been tested on e-commerce websites have varying accuracy. When activating the search filter feature, users need to wait for a dozen seconds for the website to respond, for example, when the user wants to open the website or search for a product.",,,167602,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213118,,LINUS RANDU DANARDYA,DATA AUGMENTATION EFFECT ON MULTI-LABEL EMOTION CLASSIFICATION OF INDONESIAN TWEETS USING SVM AND LSTM,,,"Augmentation, Natural Language Processing (NLP), Support Vector Machine (SVM), Long Short-Term Memory (LSTM), Bidirectional Encoder Representations Transformers (BERT)","Edi Winarko, M.Sc., Ph.D.",2,3,0,2022,1,"Twitter adalah salah satu platform di internet tempat orang mencurahkan isi hatinya. Ada banyak hal pribadi sehari-hari yang diposting. Secara eksplisit Tweet ini mengandung emosi penulis yang dapat menunjukkan keadaan pikiran mereka. Tweet ini sangat penting untuk penelitian emosi dalam upaya mendapatkan pemahaman yang lebih baik tentang orang. Dengan memahami keadaan emosional orang, orang dapat membuka kunci untuk mengelola respons emosional mereka dengan lebih baik dan dalam jangka panjang meningkatkan kesehatan mental dan kesejahteraan mereka secara keseluruhan. Tetapi karena orang mampu mengalami banyak emosi sekaligus, berbicara tentang emosi manusia adalah topik yang sangat rumit. Penelitian ini bertujuan untuk membuat pendekatan augmentasi data yang dapat diterapkan pada kicauan multi-label Indonesia, karena saat ini dataset seperti itu masih sedikit atau tidak ada sama sekali. Metode augmentasi yang diusulkan menggunakan Bidirectional Encoder Representations Transformers (BERT) untuk menukar kata pada tweet ke sinonimnya. Dua pengklasifikasi Support Vector Machine (SVM) dan Long Short-Term Memory (LSTM) kemudian mencoba mengklasifikasikan kumpulan data yang ditambah. Efektivitas augmentasi kemudian dievaluasi dengan membandingkan kinerja pengklasifikasi yang dilatih pada kumpulan data asli dengan pengklasifikasi yang dilatih pada kumpulan data yang ditambah. Perbandingan lain juga dilakukan untuk melihat efek augmentasi pada berbagai teknik embedding yang digunakan. Hasil evaluasi menunjukkan augmentasi yang diusulkan meningkatkan kinerja classifier hingga 3,36% untuk SVM mikro F1 dan hingga 3,84% untuk LSTM mikro F1. Augmentasi yang diusulkan juga dapat bekerja dengan penyematan FastText yang mendorong kinerja lebih tinggi hingga peningkatan 4,52% dalam mikro F1 untuk LSTM.","Twitter is one of the platforms on the internet where people go to pour their heart out. There are a lot of personal, everyday-things that are posted. Explicitly these Tweets contain emotions of the writer that can show what state of mind they are in. These tweets are essential for emotion research in pursue of gaining a better understanding of people. By understanding people's emotional states, people can unlock the keys to better managing their emotional responses and in the long-term improving their mental health and overall well-being. But since people are capable of experiencing numerous emotions at once, talking about human emotion is a very complicated subject. This research intends to create a data augmentation approach that can be applied to multi-label Indonesian tweets, as there is currently little to no such dataset. The proposed augmentation method utilizes Bidirectional Encoder Representations Transformers (BERT) to swap words on a tweet to their synonym. Two classifiers of Support Vector Machine (SVM) and Long Short-Term Memory (LSTM) then try to classify the augmented dataset. The effectiveness of augmentation is then evaluated by comparing the performance of classifiers trained on the original dataset with that of classifiers trained on the augmented dataset. Another comparison is also done to see the augmentation effect on different embedding techniques used. Evaluation result shows proposed augmentation improve classifier performance up to 3.36% for SVM micro F1 and up to 3.84% for LSTM micro F1. Proposed augmentation can also work with FastText embedding pushing the performance even higher to 4.52% improvement in micro F1 for LSTM.",,,164205,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212863,,MUHAMMAD ZAINAL A,Motorcycle Parking Slots Classification using AlexNet Based Architecture as Part of Deep Learning CNN,,," Image Classification, Motorcycle Parking Slot, Machine Vision, Parking Space Classification, Deep Learning, Convolutional Neural Network, Image Processing","Dr-Ing, MHD. Reza M.I. Pulungan, M. Sc.",2,3,0,2022,1,"Ruang parkir dan infrastruktur jalan telah menjadi isu yang sangat memprihatinkan di
berbagai belahan dunia. Banyak negara berkembang telah menunjukkan pertumbuhan
penduduk yang signifikan khususnya di daerah metropolitan dan perkotaan yang
mengakibatkan tingginya kebutuhan kendaraan pribadi. Pertumbuhan jumlah kendaraan
pribadi telah menyebabkan kesulitan untuk menemukan tempat parkir pada jam-jam sibuk. Hal
ini menyebabkan dampak negatif bagi kota dan pengemudi itu sendiri, seperti polusi,
kemacetan lalu lintas, kecelakaan lalu lintas, pemborosan waktu dan bahan bakar, dan
sebagainya. Solusi yang baik dari sistem parkir pintar dapat dibuat yang mencakup klasifikasi
slot parkir dengan menggunakan deep learning CNN dari gambar yang diambil dari video pada
kamera CCTV yang terekam. Solusi ini dapat memecahkan masalah, memanfaatkan sumber
daya parkir dengan lebih baik, dan selanjutnya dapat digunakan untuk memandu pengemudi
untuk menemukan tempat parkir terdekat secara efisien. Metode klasifikasi slot parkir ini
merupakan metode yang efektif dibandingkan dengan metode yang paling banyak digunakan
saat ini yang menggunakan sensor yang tertanam pada setiap ruang parkir untuk
mengklasifikasikan status parkir yang umumnya bisa sangat mahal untuk sebuah instalasi.
Tesis ini berfokus pada teknologi berbasis machine-vision yang digunakan untuk klasifikasi
slot parkir sepeda motor.","Parking spaces and road infrastructures has been a widely concerning issue in many
different parts of the world. Many developing countries has shown a significant population
growth specifically in the metropolitan and urban areas that results to a high demand of private
vehicles. The growth of the amount of private vehicle has caused difficulty to find a parking
space during the peak hours. This caused negative impacts to the cities and drivers themselves,
such as pollution, traffic congestion, traffic accidents, waste of time and fuel, and so on. A
good solution of a smart parking system can be made which includes parking slot classification
by using deep learning CNN from an image taken from a video on a recorded CCTV camera.
This solution might solve the problem, make better use of parking resources, and further be
used to guide drivers to find the nearest possible parking space efficiently. This method of
parking slots classification is an effective method compared to the most widely used today that
uses an embedded sensors on each parking space to classify the parking status which generally
can be very costly for an installation. This thesis focuses on a machine-vision-based technology
used for motorcycle parking slot classification.",,,163912,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
220031,,FARHAN RENALDI N,Center Loss Function Untuk Meningkatkan Performa Deteksi Masker Wajah Menggunakan Faster Regional Convolutional Neural Network,,,"center loss, deteksi masker, deteksi objek, Faster-RCNN, masker wajah","Dr. Agus Sihabuddin, S.Si., M.Kom.",2,3,0,2022,1,"Sistem deteksi masker wajah telah banyak dibangun oleh peneliti-peneliti sebagai salah satu upaya pengawasan pemakaian masker di kala pandemi COVID-19. Namun, sebagian besar sistem deteksi masker masih memiliki kekurangan, yaitu kesulitan dalam mengkategorikan penggunaan masker wajah yang tidak benar dengan penggunaan masker wajah yang benar. Oleh karena itu, kekuatan diskriminatif model deteksi masker perlu ditingkatkan agar dapat lebih mudah mengkategorikan kedua kelas tersebut.
Salah satu cara meningkatkan performa sistem deteksi yaitu dengan memilih loss function yang tepat. Arsitektur Faster-RCNN pada umumnya menggunakan softmax loss sebagai loss function untuk klasifikasi objek pada sistem deteksi. Pada penelitian ini, akan dibangun model sistem deteksi masker menggunakan arsitektur Faster-RCNN dengan modifikasi loss function menggunakan center loss. Center loss yang digunakan diharapkan mampu meningkatkan performa akurasi sistem deteksi masker dalam mendeteksi penggunaan masker yang benar, tidak benar, dan tidak menggunakan masker, dimana center loss merupakan loss function untuk memperjelas batasan antar-kelas dan mendekatkan fitur-fitur intra-kelas yang dapat meningkatkan kemampuan diskriminatif model.
Pada penelitian ini, model Faster-RCNN dengan center loss dengan nilai lambda sebesar 0.0003 dan lr_center sebesar 0.25 menghasilkan mAP latihan dan validasi sebesar 76.95% dan 66.60%, lebih baik daripada model dengan softmax loss dengan mAP sebesar 75.01% dan 64.65%. Selain itu, model dengan center loss memiliki kemampuan diskriminatif yang lebih baik, yang ditandakan dengan kerapatan antar kelas yang lebih padat dan jarak antar kelas yang lebih renggang pada distribusi fitur mendalamnya.","Many researchers have built face mask detection systems for monitoring the usage of face masks during the COVID-19 pandemic. However, most of the mask detection systems still have drawbacks, namely the difficulty in categorizing the incorrect usage of face mask with the correct one. Therefore, the discriminatory power of the mask detection model needs to be improved in order to more easily categorize the two classes.
One way to improve the performance of the detection system is to choose the right loss function. Faster-RCNN architecture generally uses softmax loss as the loss function for classifying objects in detection systems. In this study, a mask detection system model will be built using the Faster-RCNN architecture with modified loss function using center loss. The center loss used is hypothesized to improve the accuracy of the mask detection system in detecting the use of masks that are correct, incorrect, and not using masks, whereas the center loss is a loss function to clarify boundaries between classes and bring together intra-class features that can improve the discriminatory ability of the model.
In this research, the Faster-RCNN model with center loss with lambda value of 0.0003 and lr_center of 0.25 produced training and validation mAPs of 76.95% and 66.60% respectively, better than the model with softmax loss with mAPs of 75.01% and 64.65%. In addition, models with center loss have better discriminative power, which is indicated by denser intra-class compactness and inter-class distance that are more sparse in the distribution of deep features.",,,171189,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213632,,ANINDITA K,Predicting Stock Price Using Technical Indicators With Convolutional Neural Network-Long Short Term Memory (CNN-LSTM),,,"stock price prediction, CNN-LSTM, technical indicators, trend-following, oscillator","Azhari, Drs., MT., Dr",2,3,0,2022,1,"Jumlah investor pada pasar modal Indonesia telah meningkat secara signifikan sejak 2020. Bagi para investor, prediksi harga saham merupakan salah satu hal yang penting dalam membuat keputusan di pasar saham.
Dalam studi ini, model Deep Learning CNN-LSTM diaplikasikan untuk kasus time series untuk memprediksi harga saham. Riset ini menggunakan data harga saham historis dari salah satu perusahaan terbesar di Indonesia, Bank BCA. Data harga saham di ekstraksi untuk mendapatkan indikator-indikator teknikal untuk dapat dipisahkan ke dalam beberapa set data tipe indikator yang berbeda; trend-following, oscillator, dan keduanya. Riset ini juga mengidentifikasi set data terbaik untuk masing-masing model dalam memprediksi harga saham. Performa dari model untuk masing-masing set data diukur dengan beberapa metriks; Mean Absolute Error, Root Mean Squared Error, Mean Absolute Percentage Error dan R-squared. Student's t-test juga digunakan untuk melihat signifikansi CNN-LSTM dibanding model lainnya dalam memprediksi harga saham.
Studi ini membuktikan bahwa CNN-LSTM berhasil dalam menaikkan performa prediksi dibanding LSTM dan CNN","Jumlah investor pada pasar modal Indonesia telah meningkat secara signifikan sejak 2020. Bagi para investor, prediksi harga saham merupakan salah satu hal yang penting dalam membuat keputusan di pasar saham.
Dalam studi ini, model Deep Learning CNN-LSTM diaplikasikan untuk kasus time series untuk memprediksi harga saham. Riset ini menggunakan data harga saham historis dari salah satu perusahaan terbesar di Indonesia, Bank BCA. Data harga saham di ekstraksi untuk mendapatkan indikator-indikator teknikal untuk dapat dipisahkan ke dalam beberapa set data tipe indikator yang berbeda; trend-following, oscillator, dan keduanya. Riset ini juga mengidentifikasi set data terbaik untuk masing-masing model dalam memprediksi harga saham. Performa dari model untuk masing-masing set data diukur dengan beberapa metriks; Mean Absolute Error, Root Mean Squared Error, Mean Absolute Percentage Error dan R-squared. Student's t-test juga digunakan untuk melihat signifikansi CNN-LSTM dibanding model lainnya dalam memprediksi harga saham.
Studi ini membuktikan bahwa CNN-LSTM berhasil dalam menaikkan performa prediksi dibanding LSTM dan CNN",,,164743,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218242,,EMMANUEL BAYU SETYAJI,Verifikasi Tanda Tangan Menggunakan Siamese Convolutional Neural Network,,,"Tanda Tangan, Deep Learning, Siamese Convolutional Neural Network, Verifikasi Tanda Tangan","Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.; Edi Winarko, Drs. M.Sc., Ph.D ",2,3,0,2022,1,"Tanda tangan merupakan lambang nama yang dituliskan dengan tangan oleh orang itu sendiri sebagai penanda pribadi bahwa orang tersebut telah menerimanya. Hal ini menandakan bahwa tanda tangan merupakan sesuatu yang penting untuk diperhatikan. Di sisi lain, hal tersebut bisa menimbulkan adanya tindak kejahatan yaitu pemalsuan tanda tangan. Verifikasi tanda tangan perlu dilakukan agar bisa membuktikan bahwa tanda tangan tersebut merupakan tanda tangan yang dihasilkan oleh asli dari orang itu sendiri atau tiruan dari orang lain. 
Penelitian ini membuat sebuah model yang mampu memverifikasi apakah tanda tangan bersifat asli atau tiruan. Pembuatan model dilakukan dengan arsitektur siamese convolutional neural network dengan mempertimbangkan beberapa hyperparameter yaitu batch size dan learning rate. Data yang digunakan merupakan sebuah public dataset yang terdapat pada Kaggle yang menyederhanakan dataset kompetisi ICDAR 2011 Dutch Signature Dataset. Data akan melewati beberapa tahap prapemrosesan yaitu grayscaling, normalisasi, binarization, dan resizing. Hasil performa dari model akan diuji menggunakan metrik akurasi, false acceptance rate, dan false rejection rate.
Penelitian ini menghasilkan sebuah model yang memiliki performa berupa akurasi sebesar 91,23%, FAR sebesar 4,63%, dan FRR sebesar 12,99% dengan arsitektur siamese convolutional neural network dengan hyperparameter sebanyak 64 untuk batch size dan learning rate sebesar 0,001.","Signature is a person's sign that is handwritten by the person itself as a sign that the person has accept it. Based on that, signature is something that needs to be look at carefully. Therefore, there are some cases which leads to people creating forged signature. Signature verification is needed to verify the authenticity of the signature to prove that it was written by the original author or faked by other author.
This research will develop a model to classify a signature whether it is written original and forged. The model will be created using the siamese convolutional neural network algorithm with also taking various kinds of hyperparameters into account. The data that will be used in this research is a public dataset from Kaggle that simplifies the ICDAR 2011 Dutch signature dataset. The data will go through some preprocessing methods such as grayscaling, normalization, binarization, and resizing. The evaluation for the model will be tested using accuracy, false acceptance rate, and false rejection rate.
This research developed a model that has 91,23% accuracy, 4,63% FAR, dan 12,99% FRR performance using the siamese convolutional neural network with 64 batch size and 0,001 learning rate.",,,169454,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
210563,,PASKALIS HENRY S,Deteksi Jamur pada Roti Menggunakan Segmentasi Citra Berbasis Klaster,,,"pengolahan citra digital, segmentasi citra, deteksi jamur pada roti, neural network, GLCM","Wahyono, S. Kom., Ph.D.",2,3,0,2022,1,"Jamur pada roti dapat menyebabkan keracunan makanan dan berbagai macam penyakit yang dapat berakibat fatal. Jamur pada roti merupakan masalah yang serius pada rantai pasok pangan. Jamur, khususnya pada awal pertumbuhannya, tidak mudah terlihat oleh manusia, sehingga akan kurang efektif jika manusia perlu memperhatikan secara seksama untuk setiap roti pada rantai pasok pangan. Pengolahan citra digital dapat diaplikasikan pada permasalahan tersebut, supaya bisa mendeteksi jamur pada permukaan roti secara cepat dan akurat. Metode pengolahan citra digital yang digunakan pada penelitian ini adalah segmentasi citra berbasis klaster.

Algoritma segmentasi citra yang digunakan adalah far enhanced clustering algorithm (FECA), dengan perubahan pada pemilihan pusat klaster awal untuk mempersingkat waktu komputasi. Penerapan FECA sebelumnya oleh Mishra et al. (2019) memiliki rata-rata waktu komputasi 5,3494 detik untuk segmentasi citra RGB dengan jumlah klaster 3, dan 4,0078 detik untuk segmentasi citra greyscale dengan jumlah klaster 2. Hasil segmentasi citra dikonversi ke format HSI, dari citra tersebut dibuat GLCM untuk ekstraksi fitur, yang kemudian digunakan sebagai input pada neural network.

Hasil segmentasi menggunakan citra RGB dengan jumlah klaster 3 memiliki nilai evaluasi SC, RMSE, MSE, PSNR, dan NAE lebih baik daripada segmentasi menggunakan citra greyscale dengan jumlah klaster 2. Segmentasi citra RGB dengan jumlah klaster 3 pada penelitian ini memiliki rata-rata waktu komputasi 3,4657 detik, dan pada segmentasi citra grayscale dengan jumlah klaster 2 memiliki rata-rata waktu komputasi 2,1364 detik. Model neural network yang dihasilkan memiliki nilai accuracy 0,8054, precision 0,7886, recall 0,6181, dan F1 0,6819.","Mold on bread can cause food poisoning and various diseases that can be fatal. Mold on bread is a serious problem in the food supply chain. Molds, especially at the beginning of their growth, are not easily visible to humans, so it will be less effective if humans need to pay close attention to each bread in the food supply chain. Digital image processing can be applied to these problems, so that it can detect mold on the surface of the bread quickly and accurately. The digital image processing method used in this research is cluster-based image segmentation.

The image segmentation algorithm used is far enhanced clustering algorithm (FECA), with changes to the initial cluster center selection to shorten computation time. The previous application of FECA by Mishra et al. (2019) has an average computation time of 5,3494 seconds for RGB image segmentation with 3 clusters, and 4,0078 seconds for greyscale image segmentation with 2 clusters. The image segmentation results are converted to HSI format, GLCM is made from the image for feature extraction, which is then used as input to the neural network.

The results of segmentation using RGB images with the 3 clusters have evaluation values of SC, RMSE, MSE, PSNR, and NAE better than the segmentation using grayscale images with the 2 clusters. RGB image segmentation with 3 clusters has an average computation time of 3,4657 seconds, and the grayscale image segmentation with 2 clusters has an average computation time of 2,1364 seconds. The resulting neural network model has an accuracy value of 0,8054, 0,7886 precision, 0,6181 recall, and 0,6819 F1.",,,161755,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
208004,,ABDUL HAMID UMAR,CUBATURE KALMAN FILTER DAN RECURRENT NEURAL NETWORK UNTUK PREDIKSI NILAI TUKAR MATA UANG,,,"peramalan, nilai tukar mata uang, neural network, RNN, Kalman Filter, Cubature Kalman Filter","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Nilai tukar mata uang memiliki likuiditas dan laju pergerakan harga yang tergolong
tinggi. Oleh karena itu, banyak ahli berupaya untuk memprediksi pergerakan nilai tukar
mata uang. Neural network merupakan salah satu metode yang digunakan untuk untuk
memprediksi nilai tukar mata uang karena kemampuannya untuk memprediksi data nonlinear. Recurrent Neural Network (RNN) merupakan salah satu jenis jaringan saraf tiruan
yang sering digunakan pada data dengan tipe time series.
Banyak penelitian mengusulkan Unscented Kalman Filter (UKF) sebagai metode
pembelajaran neural network. Salah satu kelemahan algoritma UKF adalah kecepatan
komputasinya yang lama akibat penggunaan metode Unscented Transform (UT) dalam
menentukan titik &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; titik sigma. Oleh karena itu, penelitian ini akan menggunakan
Cubature Kalman Filter (CKF) yang menggunakan aturan spherical &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; radial cubature
untuk menentukan titik &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; titk sigma. Algoritma CKF akan digunakan untuk melatih bobot
pada RNN. Data yang digunakan adalah data nilai tukar mata uang USD &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; IDR dari 1
Januari 2017 hingga 1 Januari 2021. Untuk menguji akurasi dari model CKF-RNN, akan
digunakan metode Mean Absolute Error (MAE), Mean Absolute Percentage Error
(MAPE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), dan
Directional Statistics (Dstat).
Model CKF &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; RNN pada penelitian ini menghasilkan nilai MSE 17842.095, RMSE
133.574, MAE 88.231, MAPE 0.591 %, Dstat 75.5 %, dan runtime 4794.373 ms. Dari hasil
tersebut algoritma CKF memiliki runtime yang lebih cepat dari algoritma UKF dengan
selisih runtime 9033.242 ms.
","Currency exchange rates have characteristics such as high fluctuation and liquidity
rates. Many research have been done in order to accurately predict future currency
exchange rates. One of the most used methods is neural network because of its ability to
predict nonlinear data.
Previous studies have proven Unscented Kalman filter (UKF) as an efficient learning
algorithm for neural network. One of the disadvantages of UKF is the long computation
time due to the unscented transform step used to find sigma points. In this research,
Cubature Kalman filter (CKF) is proposed as the learning algorithm for Recurrent Neural
Network. CKF uses spherical-radial cubature rule in order to find sigma points. This
research uses historical data USD-IDR exchange rates from January 1st 2017 to January
1st 2021. Performance measurements of the model will be calculated using mean squared
error, root mean squared error, mean absolute error, mean absolute percentage error,
directional statistics, and its runtime.
Using the CKF-RNN model, this study produces MSE value of 17842.095, RMSE
value of 133.574, MAE value of 88.231, MAPE value of 0.591 %, Dstat value of 75.5 %,
and 4794.373 ms runtime. From those values, the proposed method has a faster runtime
than the UKF method by 9033.242 ms.
",,,159204,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219524,,Adira Kurniawan Firdaus,Komparasi Metode Multiclass Classifier untuk Pengkategorian Artikel Berita Keterbukaan Pemerintah,,,"News Categorization,SVM,RFC,Naive Bayes","Dr. Agus Sihabuddin, S.Si., M.Kom;Drs. Bambang Nurcahyo  Prastowo, M.Sc",2,3,0,2022,1,"Open Government Indonesia (OGI) adalah suatu inisiatif untuk mengkonsolidasikan gerakan keterbukaan pemerintah dalam mendorong pembuatan kebijakan yang transparan dan inklusif, serta mendorong keterlibatan aktif dari warga negara sesuai dengan prinsip keterbukaan pemerintah. Dalam penelitian ini, akan diteliti potensi penggunaan algoritma untuk membangun model pengkategori berita keterbukaan pemerintah sesuai dengan prinsip keterbukaan pemerintah.
Klasifikasi dokumen berita berdasarkan kategori di penelitian ini akan membandingkan algoritma klasifikasi Multinomial Naive Bayes (MNB), Random Forest Classifier (RFC), dan Support Vector Machine (SVM). Pengujian dilakukan pada hasil ekstraksi fitur TF-IDF dari teks berita keterbukaan pemerintah berjumlah 1500 yang dibagi menjadi 5 kategori.
Hasil penelitian ini menunjukkan penggunaan Grid Search K-Fold Cross Validation untuk mengoptimalkan pengklasifikasi menghasilkan hasil yang kompetitif di antara ketiganya. Implementasi SVM tanpa stemming memiliki hasil akurasi dengan performa terbaik dengan akurasi sebesar 88,00%, presisi 87,98, recall sebesar 87,66%, dan F1-score sebesar 87,75%. Penggunaan RFC dengan stemming memiliki performa lebih baik dibandingkan implementasi tanpa stemming dengan nilai terbaik yaitu 85,67%, presisi sebesar 84,52%, recall sebesar 84,52%, dan F1-score sebesar 84,45%.
","Open Government Indonesia (OGI) is an initiative to consolidate open government movement in guiding transparent and inclusive policy making, and driving active public participation based on open government principles. In this research, the possibilities to develop open government news categorization model based on open governments principles will be explored.
By classifying based on categories, this research will compare Multinomial Naive Bayes (MNB), Random Forest Classifier (RFC), and Support Vector Machine (SVM) as multiclass classifier. Research will be applied on TF-IDF features extraction from 1500 open government news divided to 5 categories.
The results of the training show that by utilizing Grid Search K-Fold Cross Validation to optimize classifiers, competitive results can be achieved between the three. Implementation of SVM without stemming shows best performance with accuracy score of 88,00%, precision of 87,98%, recall of 87,66%, and F1-score of 87,75%. Usage of RFC with stemming shows better result than without stemming with accuracy score of 85,67%, precision of 84,52%, recall of 84,52%, and F1-score of 84,45%.",,,170715,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
215945,,GARDA RIZKYAMIEN,Classification Defect and Non-Defect of Green bean Coffee  using Convolutional Neural Network,,,"Green Bean Coffee, Deep learning, CNN, Digital Image Processing","Wahyono, Ph.D",2,3,0,2022,1,"Kopi telah menjadi salah satu minuman terpenting yang diperdagangkan di pasar dunia,
dan Indonesia dikenal sebagai salah satu dari lima negara penghasil kopi terbesar di dunia, dan
perkebunan kopi telah menjadi kegiatan pertanian di seluruh negeri.
Kopi berkualitas telah menjadi salah satu nilai tertinggi dalam hal pangsa pasar
di seluruh dunia. Namun, klasifikasi kopi biji hijau telah diperiksa secara manual
melalui visual. Membuat model deep learning yang dapat membantu melakukan klasifikasi terhadap
kopi biji hijau yang cacat dan tidak cacat itu akan menjadi tujuan utama dari
riset. Pendekatannya adalah dengan menggunakan beberapa metode pada datasetnya, seperti preprocessing,
ekstraksi fitur dan model Convolutional Neural Network yang bertindak sebagai classifier.
Untuk menyimpulkan penelitian, diharapkan model CNN dapat mengklasifikasikan cacat
dan biji kopi hijau non-cacat.
Pada akhir penelitian, ditunjukkan bahwa model CNN terbaik memberikan 86%
akurasi klasifikasi terhadap data dalam data uji. Menggunakan penyetelan hyper yang berbeda
parameter.","Coffee has become one of the most important beverages traded in world markets, 
and Indonesia is known as one of the top five coffee producing country in the world, and 
coffee plantation has become an agriculture activity around the nation. 
Quality coffee has become one of the highest values when it comes to market share 
worldwide. However, classification of green bean coffee has been inspected manually 
through visual. Creating a deep learning model which can help perform classification to 
those defects and non-defects green bean coffee will be the main objective of the 
research. The approach is to use several methods on its datasets, such as preprocessing, 
feature extraction and Convolutional Neural Network model that acts as the classifier. 
To conclude the research, it is anticipated that the CNN model can classify the defects 
and non-defects green coffee beans.
By the end of the research, it is shown that the best CNN model gives 86% of 
classification accuracy towards data in the data test. Using different tuning of hyper 
parameters.",,,167013,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211082,,ACHMAD JODHY AL A E,IMPLEMENTASI REINFORCEMENT LEARNING SEBAGAI SISTEM REKOMENDASI UNTUK MENU DIET,,,"Reinforcement Learning, Markov Decision Process, Recommendation System, Diet, Meals","Drs. Retantyo Wardoyo M.Sc., Ph.D.",2,3,0,2022,1,"Dalam melakukan diet, tidak boleh memilih program diet secara sembarang. Dikarenakan program-program diet yang beredar belum tentu sesuai dan dapat berhasil. Faktor keberhasilan diet dipengaruhi dengan kondisi tubuh dan kesesuaian menu diet yang ditawarkan dengar user. Sehingga untuk membuat program diet yang dapat berhasil, perlu dibuat sebuah program diet yang personal dan beradaptasi dengan user.
Pada penelitian ini dibangun sistem rekomendasi menu diet menggunakan metode Reinforcement Learning berbasis Markov Decision Process. Dengan sistem rekomendasi sebagai agen pembelajar, state untuk menyimpan nilai bobot kesukaan dan faktor perubahan berat badan, dan action berupa pilihan rekomendasi menu makanan untuk tiap jam makan.
Sistem ini kemudian diujikan kepada empat responden. Responden diminta untuk mengikuti diet selama 21 hari, dengan tujuan menurunkan berat badannya untuk mendekati berat badan idealnya. Hasil dari pengujian ini dianalisa berdasarkan variasi pilihan menu makanan yang dihasilkan, ketertarikan responden dalam memilih, dan perubahan berat badan.","In choosing a diet program, we can't choose it as we like. Because an established diet program may not be suitable or even successful for the user. For a diet program to succeed, it is affected by the user's body condition and how suitable the diet is to the user. Therefore, to create a diet program that can be successful, the diet program itself needs to adapt to the user.
This research has been conducted by building a recommendation system using the Reinforcement Learning method based on Markov Decision Process. With the recommendation system itself as the learning agent, state for storing likeness value and weight factor, and action as the recommendation meals for each mealtime.
Then this system is being tested on four respondents. Each respondent was asked to follow the diet program for 21 days, with a goal to reduce their body weight to get their ideal weight. The tested result is analysed by the variety of meals that have been generated, respondent interest in picking meals, and weight changes",,,164767,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214922,,M RETIAN FAALI,PREDIKSI HASIL PERTANDINGAN DOTA 2 BERDASARKAN PEMILIHAN HERO MENGGUNAKAN RANDOM FOREST DAN ADABOOST ,,,"prediksi, MOBA, DOTA 2, hero, Random Forest, AdaBoost","Sri Mulyana, Drs., M.Kom.",2,3,0,2022,1,"Permainan MOBA (Multiplayer Online Battle Arena) merupakan salah satu genre permainan online yang populer dikalangan para penggemar game. DOTA 2 adalah salah satu permainan MOBA yang  hingga saat ini memiliki peminat yang masih tinggi. Permainan ini mengalami pembaharuan setiap jangka waktu tertentu, salah satu pembaharuan yang terjadi adalah perubahan spesifikasi hero atau karakter dalam permainan. DOTA 2 memiliki 119 hero atau karakter dengan karakteristik berbeda yang dapat dipilih sebelum permainan dimulai, tiap tim dapat memilih masing-masing lima hero dari semua hero yang tersedia. Pemilihan hero ini dapat memengaruhi hasil pertandingan.
Pada penelitian ini dilakukan percobaan untuk melakukan prediksi hasil pertandingan DOTA 2 berdasarkan pemilihan hero atau karakter yang sudah mengalami perbaharuan pada versi 2.72d. Metode yang digunakan adalah metode Random Forest dan AdaBoost dengan Decision Tree sebagai estimator dasar. Dataset didapat dari pengumpulan data yang dilakukan menggunakan Steam Web API.
Dari pengujian prediksi hasil pertandingan DOTA 2 didapatkan metode Random Forest memberikan hasil akurasi yang lebih baik dibandingkan dengan metode AdaBoost dengan akurasi 56.6%, sedangkan metode AdaBoost menghasilkan akurasi sebesar 52.7%. Hasil ini bisa dikatakan masih rendah. Akurasi yang masih rendah ini dapat diartikan bahwa pemilihan hero saja tidak dapat menentukan hasil pertandingan.
","MOBA (Multiplayer Online Battle Arena) games are one of the most popular online game genres among gamers. DOTA 2 is one of the MOBA games that currently have pretty high enthusiast. This game is updated every certain period of time, one of the update occur is a change in the specifications of the heroes or characters in the game. DOTA 2 has 119 heroes or characters with different characteristics that can be selected before the game starts, each team can choose five different heroes from all the available heroes. This heroes choice can affect the outcome of the match.
In this research, an attempt to predict the result of DOTA 2 match based on heroes or characters selection that had been updated in version 7.27d. The method being used are Random Forest and AdaBoost methods with Decision Tree as base estimator. The dataset is obtained from data collection using Steam Web API.
The outcomes from testing the match result prediction of the DOTA 2 match, it was found that the Random Forest method gave a better accuracy results than the AdaBoost method with an accuracy of 56.6%, while the AdaBoost method gave an accuracy of 52.7%. This results can be said still low. This low accuracy can be interpreted that the selection of the heroes alone cannot determine the results of the match.
",,,165993,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212107,,ALFA NADHYA MAIMANAH,Komparasi Metode Ekstraksi Fitur Tekstur untuk Asesmen Tingkat Keparahan Jerawat pada Citra Wajah Manusia,,,"Jerawat, GLCM, Filter Gabor,Gaussian Mixture Model","Wahyono, S.Kom., Ph.D.; Faizal Makhrus, S.Kom., M.Sc., Ph.D.",2,3,0,2022,1,"Meningkatnya antusiasme dan kesadaran masyarakat terhadap kesehatan kulit beberapa tahun belakangan ini menyebabkan industri kecantikan tumbuh secara signifikan. Salah satu penyakit kulit yang menjadi perhatian utama adalah jerawat atau Acne Vulgaris. Metode penentuan tingkat keparahan jerawat yang umum dilakukan adalah melalui penghitungan manual oleh ahli dermatologi yang tidak efektif dari segi waktu dan tenaga. Untuk membantu mengatasi hal tersebut penelitian ini mengimplementasikan teknik ekstraksi fitur tekstur untuk mengenali jerawat pada citra wajah pasien yang berasal dari dataset ACNE04. Sebanyak 40 citra digunakan sebagai data latih dan 20 citra sebagai data uji. Eksperimen yang dilakukan adalah membandingkan hasil ekstraksi fitur dari filter Gabor dan Gray Level Co-occurrence Matrix (GLCM). Fitur tersebut menjadi input untuk melakukan klasifikasi blok-blok kandidat jerawat ke kelas jerawat dan bukan jerawat dengan Gaussian Mixture Model (GMM). Hasil klasifikasi diraih paling baik oleh fitur GLCM di channel Cr-YCrCb dengan recall 0,78. Meskipun demikian, nilai akurasi masih rendah yaitu 45,14% karena belum tersedianya anotasi untuk objek dengan fitur menyerupai jerawat sehingga jumlah false positive masih banyak. Diketahui bahwa fitur warna dengan nilai F1-Score 0,58 masih lebih baik untuk mengklasifikasikan jerawat daripada fitur tekstur dengan nilai F1-Score 0,56. Fitur warna juga memiliki waktu komputasi yang lebih cepat, yaitu sekitar 0,3 detik dibandingkan fitur tekstur yang membutuhkan sekitar 1,2 detik. GMM dengan nilai F1-Score 0,56 sudah memiliki performa yang kompetitif sebagai model klasifikasi jika dibandingkan dengan KNN yang memiliki nilai F1-Score 0,54.","The increasing enthusiasm and public awareness of skin health in recent years have led to significant growth in the beauty industry. One major concern regarding skin disease is acne or Acne Vulgaris. The common method to determine acne severity is through manual calculation by dermatologists which is ineffective in terms of time and effort. To help overcome the problem, this research implemented texture feature extraction methods to detect acne on the patient's face image from the ACNE04 dataset. This research used 40 images as training data and 20 images as testing data. The experiment was conducted to compare the results of feature extraction from the Gabor filter and Gray Level Co-occurrence Matrix (GLCM). These features became the input for classifying acne candidate blocks into acne and non-acne classes using the Gaussian Mixture Model (GMM). The GLCM feature achieved the best classification result in the Cr-YCrCb channel with a recall of 0.78. However, the accuracy value was still low at 45.14% because there were no annotations available yet for objects resembling acne so the number of false positives was still high. The color feature with an F1-Score value of 0.58 was still better for classifying acne than the texture feature with an F1-Score value of 0.56. The color feature also had a faster computation time, which was about 0.3 seconds, compared to the texture feature which took about 1.2 seconds. GMM with an F1-Score value of 0.56 already had a competitive performance as a classification model compared to KNN which had an F1-Score value of 0.54.",,,163110,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219284,,R. BINTANG BAGUS PUTRA ANGKASA,IMPLEMENTASI BLOCKCHAIN PADA METERAI DIGITAL BERBASIS NFT DENGAN PEMBUBUHAN MELALUI SMART-CONTRACTS DAN IPFS,,,"Meterai, Blockchain, NFT, Smart-contracts, Ethereum.","Anny Kartika Sari, S.Si., M.Sc., Ph.D. ",2,3,0,2022,1,"Meterai telah berevolusi dari bentuk fisik menjadi elektronik (e-Meterai) yang diterbitkan oleh pemerintah. Perubahan wujud meterai menjadi elektronik memberikan kemudahan tetapi juga membawa kekurangan. E-meterai memiliki sistem yang tersentralisasi, sehingga memungkinkan adanya kegagalan atau penyalahgunaan oleh satu titik yang menjadi tumpuannya. Teknologi seperti blockchain yang merupakan jaringan terdesentralisasi dapat diterapkan ke sistem meterai. Jaringannya yang memiliki banyak node membuatnya dapat selalu diandalkan dan terbebas dari kegagalan karena tidak bertumpu pada satu titik saja. Data yang disimpan secara terdistribusi pada blockchain juga akan meningkatkan kepercayaan, keamanan, transparansi, dan pelacakannya.
	Sistem yang dirancang pada penelitian ini disebut d-Meterai (digital-meterai). Meterai dibuat dalam wujud Non-fungible Token (NFT) yang membuatnya unik secara digital dan keasliannya tertulis di dalam jaringan blockchain. Meterai digital tersebut hanya bisa diterbitkan oleh pemerintah pada jaringan blockchain sebagai NFT terprogram khusus yang memiliki kemampuan untuk dibubuhkan satu kali pada satu dokumen digital yang dikunci dan diunggah ke penyimpanan terdesentralisasi. Jaringan yang digunakan oleh sistem ini adalah Ethereum dengan bahasa pemrograman Solidity dan framework Hardhat. Sedangkan aplikasi client-side dibuat dengan framework Next.js.
Hasil dari penelitian ini adalah terwujudnya implementasi blockchain pada sistem pemeteraian yang meliputi seluruh prosesnya dari penerbitan, pembelian, pembubuhan, verifikasi dan pengendalian akses terhadap dokumen. Sistem telah diuji dan menunjukan tingkat desentralisasi yang diperlukan telah tercapai. Hasil pengujian dari segi biaya dan kecepatan operasi juga telah dilakukan dan menunjukan hasil yang memuaskan. Keamanan privasi dokumen yang disimpan pada IPFS juga telah berhasil dipertahankan sehingga percobaan untuk melihat isi suatu dokumen hanya mendapatkan baris teks terenkripsi saja
","Stamp duty (meterai) has evolved from physical to electronic (e-Meterai) forms issued by the government. Changing the form of stamp duty into electronic provides convenience but also brings drawbacks. E-meterai has a centralized system, thus allowing for failure or misuse by a single point on which it is based. Technologies like blockchain which is a decentralized network can be applied to a seal system. Its network which has many nodes makes it always reliable and free from failures because it is not based on just one point. Data stored in a distributed manner on the blockchain will also increase its trust, security, transparency, and traceability.
The system designed in this study is called d-Meterai (digital stamp duty). The stamp duty is created in the form of a Non-fungible Token (NFT) which makes it digitally unique and its authenticity is inscribed on the blockchain network. Such digital stamp duty can only be issued by governments on a blockchain network as specially programmable NFTs that have the ability to be affixed once to a single locked digital document and uploaded to a decentralized repository. The network used by this system is Ethereum with the Solidity programming language and the Hardhat framework. Meanwhile, client-side applications are built using the Next.js framework.
The result of this research is the implementation of blockchain in the stamp duty system which covers the entire process from issuing, purchasing, affixing, verifying and controlling access to documents. The system has been tested and shows the required level of decentralization has been achieved. Test results in terms of cost and speed of operation have also been carried out and show satisfactory results. The privacy security of documents stored on IPFS has also been successfully maintained so that attempts to view the contents of a document only get lines of encrypted text.
",,,170533,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211093,,EDRICK ALVARO LESLIE,"Perbandingan Algoritma Genetika, Ant Colony Optimization (ACO), dan Algoritma Greedy untuk Keputusan Pembagian Sumber Daya pada Vehicular Fog Computing (VFC)",,,"pembagian sumber daya, perbandingan algoritma, algoritma genetika, ant colony optimization, ACO, algoritma greedy, komputasi kabut kendaraan","Dr.-Ing. Mhd. Reza M.I. Pulungan, S.Si., M.Sc.",2,3,0,2022,1,"Kendaraan saat ini telah banyak terdapat di dalam kota-kota besar. Untuk meningkatkan efektivitas dalam hal komputasi, maka sumber daya yang menganggur pada kendaraan dapat dimanfaatkan untuk membantu komputasi berjalan lebih cepat. Pembagian sumber daya, melalui sebuah algoritma, dilakukan untuk memutuskan kendaraan mana yang akan membantu proses komputasi untuk menghindari permasalahan saat kendaraan akan menyumbangkan sumber daya kepada sumber komputasi terdekat. Diperlukan sebuah algoritma yang menghasilkan hasil optimal dengan waktu yang relatif singkat dengan hasil yang lumayan baik agar waktu komputasi tidak terbuang saat pembagian saat pembagian sumber daya.
Penelitian ini membandingkan kinerja dan kecepatan kerja dari algoritma genetika, ant colony optimization (ACO), dan algoritma greedy dalam konteks pembagian sumber daya pada komputasi kabut kendaraan. Perbandingan dilakukan dengan seberapa banyak kontribusi kendaraan dalam menyumbangkan sumber daya mereka untuk membantu komputasi. Waktu komputasi setiap algoritma juga dibandingkan agar diketahui hasil kerja algoritma dengan kinerja yang tinggi, tetapi dalam waktu yang relatif cepat. Perbandingan juga dilakukan untuk melihat pengaruh kendaraan terhadap kinerja algoritma.
Hasil penelitian ini adalah algoritma genetika menghasilkan solusi terbaik dengan waktu terlama. Sebaliknya, algoritma greedy merupakan algoritma tercepat dengan solusi terburuk. ACO menghasilkan solusi terbaik kedua dan waktu tercepat kedua. Jumlah kendaraan mempengaruhi solusi dan waktu komputasi, sehingga algoritma genetika dan ACO digunakan secara bergantian untuk kondisi yang ramai dan sepi. Pergantian dilakukan dengan menggunakan batas antara jam kerja dan jam istirahat karena waktu tersebut mempengaruhi jumlah kendaraan.","Vehicles are now widely available in big cities. To increase the effectivity in terms of computation, idle resources in vehicles can be utilized to help computation run faster. Resource pooling, through an algorithm, is carried out to decide which vehicle will be helping computation process to avoid problems when vehicle will donate their resources to the nearest computing source. An algorithm is needed to produce optimal results in a relatively short time with fairly good results so that computational time is not wasted when sharing resources.
This study compares the performance and the working speed of genetic algorithm, ant colony optimization (ACO), and greedy algorithm in the context of resource pooling in vehicle fog computing. Comparisons ware made with how much the vehicles can contribute in donating their resources to help with computation. The computation time of each algorithm is also compared to find out the results of the work of the algorithm with high performance, but in a relatively short time. Comparisons were also made to see the effect of vehicle towards algorithm performance.
The results of this study are genetic algorithm produces the best solution with the longest time. On the other hand, greedy algorithm is the fastest algorithm with the worst solution. ACO has the second best solution with the second fastest time. The number of vehicles affect the solution and computational time so genetic algorithm and ACO are used interchangeably for crowded and quiet condition. Changes are made by using the transitition between working hours and resting hours because those hours affect the number of vehicles.",,,162233,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212119,,GENNARDO,KLASIFIKASI JUDUL BERITA CLICKBAIT BERBAHASA INDONESIA MENGGUNAKAN BERT EMBEDDING,,,"Kata kunci: Klasifikasi, Clickbait, Judul Berita, Bahasa Indonesia, Bidirectional Encoder Representations from Transformers Embedding, Bidirectional Long Short-Term Memory, Convolutional Neural Network","Bapak Drs. Edi Winarko, M.Sc., Ph.D. M.Cs.;Ibu Diyah Utami Kusumaning Putri, S.Kom., M.Sc.",2,3,0,2022,1,"Perkembangan teknologi telah mempermudah berbagai aspek kehidupan saat ini, salah satunya adalah pencarian informasi. Permasalahan terdapat pada konten-konten clickbait yang beredar. Konten sejenis ini dapat menyebabkan miskomunikasi informasi yang mana dapat berlanjut pada masalah lain seperti pengalihan fokus dan menurunkan popularitas konten lain yang tidak bersifat clickbait.
Penelitian ini mengembangkan model berbasis transfer learning yang mempunyai performa yang lebih baik dalam mengklasifikan judul berita Berbahasa Indonesia yang bersifat clickbait dibandingkan penelitian serupa yang menggunakan deep learning. Model yang dikembangkan adalah IndoBERT (Bidirectional Encoder Representations from Transformers Embedding) yang ditambahkan classifier berbeda, yaitu Fully-Connected-Layer tanpa hidden layer, Bidirectional Long Short-Term Memory (Bi-LSTM), dan Convolutional Neural Network (CNN). Data ini diperoleh dari dua belas sumber berita lokal yang sudah dilabeli clickbait atau tidak, yaitu detikNews, Fimela, Kapanlagi, Kompas, Liputan6, Republika, Sindonews, Tempo, Tribunnews, Okezone, Wowkeren, dan Posmetro-Medan.
Hasil eksperimen menunjukkan bahwa ketiga model yang dikembangkan mempunyai akurasi yang lebih baik dibandingkan penelitian serupa yang menggunakan deep learning untuk data yang sama. Hasil perbandingan menunjukkan bahwa dari segi nilai F1, model IndoBERT yang ditambah BiLSTM mempunyai performa tebaik dengan nilai F1 76.86%, diikuti model IndoBERT yang ditambah CNN dengan nilai F1 76.67%, dan IndoBERT yang ditambah Fully Connected Layer dengan nilai F1 76.53%.","Technological Improvement has made it easy for everyone to search for information. Problems arise when there exists clickbait content. This type of content might cause miscommunication which will then cause more problems like focus shift and lower other non-clickbait content's popularity.
This research aims to develop transfer learning models that will perform better than other deep learning model in classifying clickbait Indonesian Headlines, especially the one using the same dataset. Those models are based on IndoBERT with different additional classifiers, Fully Connected Layer without Hidden Layer, Bidirectional Long Short-Term Memory, and Convolutional Neural Network. The data that we use is obtained from twelve news headlines in Indonesia that have been labeled as clickbait or non clicbait. They are detikNews, Fimela, Kapanlagi, Kompas, Liputan6, Republika, Sindonews, Tempo, Tribunnews, Okezone, Wowkeren, dan Posmetro-Medan.
The experiment shows that all three models have better accuracy than another experiment that utilizes deep learning on the same data. The comparison in terms of F1 Score shows that, IndoBERT with BiLSTM has the best performance with 76.86% F1, followed by IndoBERT with CNN in the second place with 76.67% F1, and IndoBERT with Fully Connected Layer with 76.53% F1.",,,163148,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219034,,AZIZ ANWAR,Klasifikasi Kualitas Air Menggunakan Fuzzy LNF-FE dan ANN,,,"Klasifikasi Kualitas air, Logika Fuzzy, ANN","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.",2,3,0,2022,1,"Kualitas air merupakan ukuran kesesuaian kondisi air terhadap kebutuhan tumbuhan, hewan, dan manusia. Sekarang ini untuk menentukan kualitas air dilakukan perhitungan secara manual. Perhitungan secara manual memiliki kesulitan seperti pemilihan cara menentukan metode kualitas air, kesulitan dalam perhitungan, dan penentuan baku mutu air. 
Penelitian ini menggunakan logika fuzzy untuk menangani data kandungan air yang bersifat tidak pasti (uncertainty) karena sulit mengukur kandungan air dan kandungan air dapat berubah dengan cepat. ANN digunakan karena mampu melakukan prediksi dari sekumpulan data. Penelitian ini menggunakan dataset kualitas air dengan 9 fitur kandungan air dengan label potable dan not potable. Metode Linguistic Neuro Fuzzy with Feature Extraction (LNF-FE) yang menggabungkan logika fuzzy dan ANN digunakan untuk melakukan klasifikasi kualitas air. 
Penelitian ini akan membandingkan hasil dari ANN, ANN dengan fuzzifikasi, serta ANN dengan fuzzifikasi dan ekstraksi fitur PCA dengan menggunakan metriks evaluasi akurasi, presisi, recall, dan f1-score dari lima kali percobaan. Hasil penelitian memperoleh rata-rata akurasi ANN, ANN dengan fuzzifikasi, serta ANN dengan fuzzifikasi dan ekstraksi fitur PCA secara berturut-turut adalah 66.36%, 66.04%, dan 66.52%.","Water quality is a measure of the suitability of water conditions to the needs of plants, animals and humans. Currently, to determine water quality calculations are carried out manually. Manual calculations have difficulties such as choosing how to determine water quality methods, difficulties in calculations, and determining water quality standards. 
This study uses fuzzy logic to handle uncertain water content data because it is difficult to measure water content and water content can change quickly. ANN is used because it is able to make predictions from a set of data. This study uses a water quality dataset with 9 features of water content labeled as portable and non-potable. The Linguistic Neuro Fuzzy with Feature Extraction (LNF-FE) method which combines fuzzy logic and ANN is used to classify water quality.
This study will compare the results of ANN, ANN with fuzzification, and ANN with fuzzification and PCA feature extraction using evaluation metrics of accuracy, precision, recall, and f1-score from five trials. The results showed that the average accuracy of ANN, ANN with fuzzification, and ANN with fuzzification and PCA feature extraction were 66.36%, 66.04%, and 66.52%, respectively.",,,170234,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212891,,DEWA NYOMAN DHARMA T,Sistem Pendukung Keputusan Penentuan Penerima Kartu Indonesia Pintar Menggunakan Metode AHP-Promethee,,,"Kartu Indonesia Pintar, sistem pendukung keputusan, AHP, Promethee","Dr. Sri Mulyana, M.Kom",2,3,0,2022,1,"Kartu Indonesia Pintar (KIP) merupakan salah satu program pemberian bantuan kepada siswa untuk tetap menjaga mutu pendidikan di Indonesia. Bantuan pendidikan berkontribusi positif terhadap peningkatan dan motivasi belajar siswa, namun dalam penerapan pemberian program bantuan sosial masih terkendala berbagai permasalahan diantaranya target penerima bantuan sosial yang tidak tepat sasaran dan data penerima bantuan belum terintegrasi. Maka dari itu implementasi suatu sistem pada permasalahan bantuan pendidikan sangat diperlukan untuk meminimalisir permasalahan tersebut. 
Penelitian ini dilakukan dengan membangun Sistem Pendukung Keputusan (SPK) untuk penentuan penerima Kartu Indonesia Pintar menggunakan metode AHP-Promethee. Data kriteria disesuaikan dengan kriteria yang digunakan pada Desa Antap sedangkan data alternatif merupakan masyarakat Desa Antap. Sistem pendukung keputusan dikembangkan berbasiswa website dengan tujuan agar mudah dalam pengoperasiannya oleh pengguna.
Hasil dari sistem berupa rekomendasi keluarga yang layak ataupun tidak layak untuk mendapatkan Kartu Indonesia Pintar beserta ranking dan skornya. Hasil rekomendasi pada setiap tipe preferensi memiliki urutan yang sama kecuali pada tipe quasi. Hasil pengujian, pada setiap tipe preferensi berhasil mendapatkan akurasi sebesar 80%. Berdasarkan hasil pengujian dapat disimpulkan bahwa SPK dengan metode AHP-Promethee dapat digunakan untuk penentuan penerima Kartu Indonesia Pintar.","Kartu Indonesia Pintar (KIP) is one of the social assistance to students to maintain the quality of education in Indonesia. educational assistance contributes positively to the improvement and motivation of students learning. However, in its implementation, social assistance programs are still constrained by various problems, such as the target of recipients of social assistance is not well-targeted and data on recipients is not integrated. Therefore implementing a system of educational aid is needed to minimize these problems.
This research is conducted by developing a Decision Support System (DSS) in determination of Kartu Indonesia Pintar using the AHP-Promethee method. The criteria data is adjusted to the rules used in Desa Antap and the alternatives data is using society in Desa Antap. Decision support system is built based on a website to make it easy to operate by users.
The system results are in the form of family recommendations that are eligible or not to get Kartu Indonesia Pintar with its ranking and score. The results of recommendations for each preference type have same order except for quasi type. Whereas for the test results, each preference type managed to get an accuracy of 80%. Based on the test results, it can be concluded that DSS with AHP-Promethee method can be used to determine the recipients of the Kartu Indonesia Pintar.",,,163904,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
209822,,M VERDY RIZALDI N,Pendeteksian Lubang Jalan Menggunakan Semantic Segmentation dengan Arsitektur Attention Residual U-Net,,,"semantic segmentation, ARU-Net, U-Net, lubang jalan, computer vision ","Andi Dharmawan, S.Si., M.Cs., Dr.; Muhammad Alfian Amrizal, B.Eng., M.I.S., Ph.D.",2,3,0,2022,1,"Sistem pendeteksian lubang jalan otomatis dapat menjadi langkah mitigasi dari berbagai masalah yang diakibatkan lubang jalan. Semantic segmentation merupakan salah satu metode untuk mendeteksi lubang jalan. ARU-Net dapat diimplementasikan untuk melakukan semantic segmentation pada lubang jalan. Penelitian sebelumnya menunjukkan bahwa ARU-Net memiliki performa yang lebih baik dibandingkan U-Net pada dataset segmentasi citra radiografi tulang lengan. Namun, ARU-Net memiliki kompleksitas dan bobot komputasi yang lebih tinggi dibandingkan U-Net karena penggunaan attention residual block. Dilakukan penelitian untuk mengetahui besaran peningkatan performa ARU-Net dibandingkan U-Net dan dampak bobot komputasi ARU-Net yang lebih tinggi pada kecepatan pemrosesan model. Hasil yang didapatkan menunjukkan bahwa ARU-Net memiliki performa yang lebih baik dibandingkan U-Net, dengan peningkatan pada mIoU, accuracy, precision, recall, dan F1-score secara berturut-turut sebesar 17%, 1%, 13%, 7%, dan 11% dari U-Net. Namun, kecepatan pemrosesan ARU-Net 40-75% lebih lambat dibandingkan U-Net (besaran penurunan kecepatan berbeda-beda pada tiap GPU). ","Pothole detection system could be a solution to various problems caused by damaged roads. Semantic segmentation is one of the methods to detect potholes. ARU-Net could be implemented to do semantic segmentation to detect potholes. Previous study shows that ARU-Net performs better than U-net on wrist reference bone dataset. However, ARU-Net has higher complexity and computing weight because of the use of attention residual block. Research was done to study the significance of performance increase when using ARU-Net compared to U-Net and the magnitude of the impact caused by the higher computational weight of ARU-Net on the processing speed of the model. The result shows that ARU-Net has a better performance than U-Net, with the increase of 17%, 1%, 13%, 7%, and 11% on mIoU, accuracy, precision, recall, and F1-score respectively when compared to the U-Net. However, the study also show that ARU-Net is 40-75% slower than U-Net (the magnitudes of processing speed difference vary between each GPU). ",,,161005,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219038,,GABRIELLA C K,User-Centred Design Approach to User Experience (UX) Analysis in Universitas Gadjah Mada's eLok Courses using Attitudinal and Behavioural Research,,,"User Experience (UX), Usability, User Interface (UI), Learning Management System (LMS), eLok, User-Centred Design, Usability Testing, System Usability Scale","Mardhani Riasetiawan, M.T., Dr",2,3,0,2022,1,"Sebagian besar penelitian Pengalaman Pengguna (User Experience) dilakukan dengan metode attitudinal, baik di dunia akademia maupun industri. Di sisi lain, pendekatan berbasis data behavioral masih memiliki praktik yang terbatas. Meskipun keduanya sama penting, akan tetapi jika seseorang hanya menggunakan satu pendekatan dalam penelitiannya dapat menimbulkan potensi masalah karena subjektivitas. Dalam skripsi ini, penulis akan mencoba untuk meningkatkan User Experience pada Learning Management System, eLok, dengan menggabungkan pendekatan sikap dan perilaku. Hal ini dilakukan dengan mengadopsi kerangka User-Centered Design yang berfokus pada perbaikan permasalahan yang ada dengan menggunakan 2 (dua) jenis evaluasi: attitudinal dengan System Usability System dan behavioral dengan Usability Testing.

Untuk mengakomodir tujuan tersebut, penulis bertujuan untuk menggunakan Moderated Usability Testing (UT) untuk menggali permasalahan yang mendasarinya. Kemudian kita dapat melihat keselarasan antara apa yang dikatakan pengguna dan apa yang dilakukan pengguna dengan Usability Testing. Mengacu pada framework yang digunakan, User-Centred Design, kita akan mulai dengan pengumpulan data, mendefinisikan masalah dengan pemetaan afinitas, diagram MoSCoW, dan How Might We Framework -  kemudian memvalidasi hasilnya dengan Maze.

Hasil dari penelitian ini menunjukkan peningkatan di semua parameter yang digunakan: efektivitas, efisiensi, dan kepuasan pengguna. Efisiensi berbasis waktu telah menunjukkan peningkatan yang tinggi dari 0,05 tugas/detik menjadi 0,12 tugas/detik; menghasilkan peningkatan 58,33%. Di sisi lain, kepuasan pengguna terhadap Skala Kegunaan Sistem memiliki peningkatan terkecil sebesar 1,08%.","Most User Experience (UX) research is done in an attitudinal manner, both in the academia and industrial world. On the other hand, a behavioural-based approach still has limited practice. While both are equally important, a potential problem may occur if one only uses an approach with the possibility of misaligned results between the two. In this research, the author will attempt to enhance the User Experience in Learning Management System, eLok, by combining both approaches of attitudinal and behavioural. This is done by adopting the framework of User-Centred Design which focuses on fixing the existing problems while using 2 (two) types of evaluation: attitudinal with System Usability System and behavioural with Usability Testing.

To accommodate this purpose, the author aims to use a moderated Usability Testing (UT) to dig dive the underlying problem. Then we may see the alignment between what user says and what users do by unmoderated Usability Testing. Referencing to the framework used, User-Centred Design, we will begin with data collection, defining the problem by affinity mapping, MoSCoW diagram, and How Might We Framework - then validate the result with Maze.

The output of this research shows improvement in all parameters used: effectivity, efficiency, and users satisfaction. The average time needed to complete a task as a measure has shown remarkable improvement from 0.05 task/second to 0.12 task/second; accounting for a 58.33% improvement. On the other hand, the users' satisfaction with System Usability Scale resulted in the smallest improvement of 1.08%.",,,170240,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
215717,,LIA MONICA,"Predicting Monthly Sales Target Using Machine Learning Approach, Case Study at Danone Indonesia",,,"Sales, Prediction, Predicting, Machine Learning, Cloud, Industry, Amazon SageMaker Canvas, IBM Machine Learning, Google AI Platform, Microsoft Azure Machine Learning, regression, time-series forecasting","Mardhani Riasetiawan, SE., Akt., M.T. Dr.",2,3,0,2022,1,"Kemajuan teknologi saat ini telah berdampak pada industri. Menyadari hal ini, semua pelaku bisnis harus mampu membuat strategi terbaik untuk memenangkan pasar. Mempertimbangkan betapa pentingnya peran teknologi, tidak mengherankan jika industri mulai beralih dari cara manual ke otomatis. Salah satu hal yang dapat diotomatisasi adalah forecasting menggunakan machine learning. Hal ini bisa menghemat banyak tenaga, waktu, dan upaya untuk menciptakan rencana produk permintaan yang bergerak cepat. Prediksi menggunakan pembelajaran mesin adalah cara populer yang banyak digunakan oleh peneliti dalam pekerjaan mereka.

Skripsi ini menyajikan pendekatan baru untuk pembelajaran mesin berbasis cloud platform. Memanfaatkan layanan komputasi awan, pembelajaran mesin telah memperluas fiturnya menjadi Machine Learning as a Service (MLaaS) dengan model dan evaluasi otomatis sehingga memudahkan semua orang untuk menerapkan pembelajaran mesin dalam pekerjaan mereka. Melalui langkah-langkah sederhana, platform akan menampilkan model, evaluasi, dan bahkan hasilnya.

Penelitian ini bertujuan untuk mempresentasikan prediksi penjualan bulanan kepada tim sales di Danone Indonesia; Oleh karena itu, target dapat disesuaikan di setiap wilayah sebelum menjalankan strategi agar mendapat hasil yang akurat. Pekerjaan ini terbatas pada level toko dan beberapa variabel eksternal tidak diperhitungkan. Pengembangan di kemudian hari diperlukan untuk meningkatkan prediksi dengan beberapa faktor eksternal yang dapat meningkatkan akurasi.

Kata kunci: Penjualan, Prediksi, Memprediksi, Pembelajaran Mesin, Cloud, Industri, Amazon SageMaker Canvas, IBM Machine Learning, Google AI Platform, Microsoft Azure Machine Learning, regresi, time-series forecasting","The advancement of technology these days has been an impact on the industry. Aware of this, all business players should put the best strategy to win the market. Considering how essential tech plays a role, it is not surprising that the industry started shifting from manual methodology to automated. One of the things that can be automated is forecasting using machine learning. It could save much work, time, and effort to roll out plan the fast-moving demand products. Forecasting using machine learning itself is a popular methodology that many researchers use in their works.  

This thesis presents a new approach to machine learning based on cloud platform. Leveraging cloud computing service, machine learning has extended its feature becomes Machine Learning as a Service (MLaaS) with its automated model and evaluations, making it easier for everyone to implement machine learning in their works. Through simple steps, the platforms will show the models, evaluation, and even the result.

This study aims to present on-time monthly sales predictions to the sales team in Danone Indonesia; hence, the target could be adjusted in each region before executing the strategy. This work is reflected up to the store level, and some external variables are not accounted. Future development works are needed to improve the prediction with some external factors that could improve the accuracy. 

Keywords: Sales, Prediction, Predicting, Machine Learning, Cloud, Industry, Amazon SageMaker Canvas, IBM Machine Learning, Google AI Platform, Microsoft Azure Machine Learning, regression, time-series forecasting",,,167079,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211880,,MICHAEL RAFEL H S,Reflection Removal pada Hasil Fotografi dengan Arsitektur Convolutional Neural Network U-Net,,,"Fotografi, Reflection Removal, Single Image Reflection Removal, Denoising, Convolutional Neural Network, U-Net","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.; Wahyono, Ph.D.",2,3,0,2022,1,"Ketika melakukan kegiatan fotografi yang melibatkan kaca atau benda transparan, hasil foto yang didapatkan terkadang mengandung refleksi. Refleksi tersebut merupakan noise. Refleksi pada citra dapat mengurangi kualitas citra dan mengganggu task lain di bidang computer vision seperti klasifikasi dan segmentasi. Oleh karena itu, refleksi tersebut perlu dihilangkan. Akan tetapi, kegiatan untuk menghilangkan refleksi tersebut (reflection removal) dapat dikatakan sulit untuk dilakukan karena komponen refleksi pada citra sulit ditentukan.

Pada penelitian ini, reflection removal dilakukan dengan input berupa satu citra yang mengandung refleksi (single image reflection removal). Adapun arsitektur yang digunakan adalah arsitektur U-Net. Implementasi arsitektur U-Net dilakukan dengan melakukan training dan validasi dengan 27 kombinasi untuk mendapatkan model terbaik yang akan digunakan untuk melakukan testing. Dalam penelitian ini, didapatkan nilai PSNR (Peak Signal to Noise Ratio) sebesar 15,0982 dan model dengan SSIM (Structural Similarity Index Measure) terbaik dengan nilai SSIM sebesar 0,5262.","Doing photograph through glass or transparent object sometimes result in image which contains reflection from glass or transparent object. The reflection is a noise. The reflection can decrease image quality and affect the result of another task in computer vision such as classification and segmentation. Therefore, the reflection must be vanished. But, the task for vanishing reflection of image (reflection removal) is considered as a difficult task because the reflection component of the images is hard to determine.

In this research, reflection removal was held using single image as input (single image reflection removal) and the architecture used is U-Net. In this research, U-Net is implemented using 27 combinations of parameters for training and validation to get best model which be used for testing. The PSNR (Peak Signal to Noise Ratio) value obtained in this research is 15,0982 and the SSIM (Structural Similarity Index Measure) value obtained is 0,5262.",,,162905,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213684,,MUHAMMAD ALDIHAN B,THE COMPARISON AND EVALUATION OF IMPLEMENTING DIFFERENT ALGORITHMS IN A MULTI ORIENTATION FACE RECOGNITION SYSTEM,,,"Deep Learning, Convolutional Neural Networks, CNN, Linear Discriminant Analysis, LDA, Principal Component Analysis, PCA, Eigenface","Wahyono, S. Kom., Ph.D.",2,3,0,2022,1,"Pengenalan wajah telah meningkat selama beberapa tahun terakhir karena cukup bermanfaat bagi sektor mana pun yang menangani data atau informasi klien. Namun, banyak dari sistem pengenalan wajah yang digunakan secara umum saat ini terbatas pada sudut mana sistem dapat mengenali wajah. Ini mungkin tidak cukup karena sifat kemajuan teknologi.
Penelitian ini membahas perlunya penelitian lebih lanjut, kajian dan implementasi sistem pengenalan wajah multi orientasi yang mampu mendeteksi wajah dari berbagai sudut subjek. Penelitian ini mengimplementasikan beberapa (4) algoritma yang sudah ada untuk pengenalan wajah dan membandingkan kinerjanya satu sama lain, dan terhadap kriteria untuk memahami keadaan algoritma ini saat membandingkan subjek multi-sudut bila dibandingkan dengan tampilan depan subjek. Penelitian ini dilakukan dalam bahasa pemrograman Python dan menggunakan beberapa perpustakaan seperti OpenCV. Penelitian ini memfokuskan dirinya di sekitar Database Color FERET.","Face recognition has been on the rise over the last few years as it is quite beneficial to any sector that is handling data or client information. However many of today&acirc;€™s general-use face recognition systems are limited in which angles the system can recognize the face from. This may not be sufficient due to the nature of how technology is advancing.
This research discusses the need for further research, study and implementation of a multi orientational face recognition system that is capable of detecting faces from multiple angles of the subject. The research implements multiple (4) already existing algorithms for face recognition and comparing their performance against each other, and against a criteria to comprehend the current state of these algorithms when comparing multi-angled subjects when compared to a frontal view of the subject. The research was conducted within the Python programming language and utilizes multiple libraries such as OpenCV. The research focuses itself around the Color FERET Database.",,,164615,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
209077,,ADI NURSETYA PRATAMA,AUTHOR OBFUSCATION UNTUK BAHASA INDONESIA MENGGUNAKAN WORD REPLACEMENT WORD EMBEDDING,,,"author obfuscation, author attribution, word embedding, Word2Vec, Glove, FastText","Edi Winarko, Drs., M.Sc., Ph.D;Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2022,1,"Author obfuscation merupakan suatu cara untuk melakukan modifikasi dokumen dengan mengubah writing style dari dokumen. Author obfuscation merupakan salah satu cara untuk menjaga anonimitas author terhadap authorship attribution. Authorship attribution merupakan proses untuk mengidentifikasi suatu penulis dari sebuah dokumen yang diberikan, berdasarkan kumpulan dokumen dari penulis yang sudah diketahui. Hal ini tentunya merupakan ancaman bagi kebebasan berpendapat dan privasi. Untuk melawan ancaman tersebut, metode author obfuscation diusulkan untuk memodifikasi suatu teks supaya penulisnya sulit diidentifikasi tanpa mengaburkan topik utamanya

Pada penelitian ini, model author obfuscation dibuat berbasis word embedding untuk memodifikasi artikel berita berbahasa Indonesia. Pada model ini, setiap artikel akan mengalami pra pemrosesan berupa tokenisasi dan PoS tagging. Selanjutnya, kata yang memiliki PoS tag berupa kata kerja dan kata benda akan diubah menggunakan kata yang telah dihasilkan oleh model word embedding. Kata yang digunakan sebagai pengganti, didapat berdasar nilai cosine similarity yang paling mendekati terhadap kata kerja dan kata benda yang akan diganti. Adapun word embedding yang digunakan dalam penelitian ini adalah Word2Vec, Glove, dan FasText. Selanjutnya, susunan kata dan kalimat digabungkan kembali menjadi artikel utuh untuk dilanjutkan proses evaluasi.

Model tersebut dievaluasi berdasarkan aspek safety, soundness, dan sensibleness. Dari aspek safety model FastText mendapat hasil paling baik karena dapat menurunkan akurasi model authorship attribution sebesar 0,1150. Untuk aspek soundness model FastText mendapat hasil paling baik dengan kemiripan artikel hasil obfuskasi dengan artikel asli sebesar 0,9935. Namun, untuk aspek sensibleness yang dievaluasi secara manual, model Word2Vec yang mendapat hasil paling baik sebesar 2,756 dari skala 1-5. Dari hasil evaluasi ketiga aspek, model FastText yang paling baik meskipun dari segi tata bahasa dan pemilihan diksi masih belum optimal.
","Author obfuscation is a way to modify a document by changing the writing style of the document. Author obfuscation is one way to maintain author anonymity against authorship attribution. Authorship attribution is the process of identifying an author of a given document, based on a document set of known authors. This is certainly a threat to freedom of expression and privacy. To counter this threat, the author obfuscation method is proposed to modify a text so that the author is difficult to identify without obscuring the main topic

In this research, the author obfuscation model is developed based on word embedding to modify Indonesian news articles. In this model, each article will undergo pre-processing in the form of tokenization and PoS tagging. Furthermore, words that have PoS tags in the form of verbs and nouns will be changed using words that have been generated by the word embedding model. The word used as a substitute is obtained based on the cosine similarity value that is closest to the verb and noun to be replaced. The word embedding used in this research are Word2Vec, Glove, and FasText. Furthermore, the arrangement of words and sentences are recombined into a complete article to be continued with the evaluation process.

The model is evaluated based on aspects of safety, soundness, and sensibleness. From the safety aspect, the FastText model gets the best results because it can reduce the accuracy of the authorship attribution model by 0.1150. For the soundness aspect, the FastText model got the best results with the similarity of the obfuscated article with the original article of 0.9935. However, for the sensibleness aspect that was evaluated manually, the Word2Vec model that got the best results was 2,756 from a scale of 1-5. From the evaluation results of the three aspects, the FastText model is the best, although in terms of grammar and diction selection it is still not optimal.
",,,162030,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213942,,MUHAMMAD SEIF R U,Badminton Match Prediction Using Genetic Algorithm,,,"Sport Prediction, Genetic Algorithm, Interaction Terms, Interpretable Machine Learning.","Dr. Azhari, MT.",2,3,0,2022,1,"Prediksi pemenang olahraga memberikan wawasan yang berguna bagi penyelenggara, media, dan industri  pertaruhan. Dibanding sepakbola, bulu tangkis kurang mendapat perhatian dalam prediksi olahraga. Tujuan dari penelitian ini adalah untuk menemukan model yang cocok untuk memprediksi pemenang pertandingan bulu tangkis yang tidak hanya bekerja baik, tetapi juga mudah dapat diinterpretasikan untuk mengidentifikasi kombinasi fitur/prediktor. Model pembelajaran mesin yang mudah ditafsirkan dapat membantu prediktor memahami model dan berpotensi merumuskan strategi untuk memaksimalkan peluang memenangkan permainan. Sebagai model klasifikasi yang dapat diinterpretasi, regresi logistik dipilih dalam penelitian ini untuk dikembangkan menggunakan kombinasi fitur interaksi yang dipilih dari algoritma genetika. Metode yang diusulkan dalam penelitian ini tidak hanya mampu meningkatkan hasil model regresi logistik normal, tetapi juga mengungguli model pohon klasifikasi lainnya yang kurang dapat diinterpretasikan dalam hal prediksi pertandingan bulu tangkis.
","The prediction of sports winners provides useful insights to organizers, media audiences, and the betting market industry. Whereas in the football dominant market, badminton has been receiving way less attention in sports prediction. The goal of this research is to find a suitable model for predicting the winner of a badminton match that is not only well performing, but also interpretable enough to identify combinations of features/key predictors. Interpretable machine learning models can help predictors understand the model and potentially formulate strategies for maximizing the chances of winning a game. As an interpretable classification model, logistic regression is chosen in this research to be developed using combinations of interaction features selected from genetic algorithms. The proposed  methods in this study were able to not only improve the performance of a normal logistic regression model, but also outperform other well performing classification tree models that were less interpretable in the case of badminton match prediction.",,,164840,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207287,,RIDZKI RACHMADEPUTRA,ANEMIA CLASSIFICATION MODEL USING K-NEAREST NEIGHBOR METHOD,,,"Anemia Classification,KNN(k-Nearest Neighbor), Machine Learning.","Afiahayati, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Anemia merupakan masalah penyakit serius yang biasa ditemukan di negara berkembang seperti Indonesia sehingga penerapan teknologi klasifikasi anemia sangat penting dalam bidang kesehatan. Sistem klasifikasi yang dilakukan pada penelitian ini adalah untuk mengklasifikasi 4 kelas anemia dengan 4 kasus yang berbeda. Fokus utama dari penelitian ini adalah bagaimana mencari performa terbaik dari model yang digunakan, diusulkan pembelajaran mesin yang dapat membantu mengklasifikasi anemia menggunakan algoritma K-Nearest Neighbor dengan Euclidean Distance sebagai metric parameter. KNN digunakan untuk melakukan tugas klasifikasi, kemudian hasil performa dari masing-masing kasus dibandingkan dan mencari hasil yang terbaik.
Hasil penelitian menggambarkan akurasi dan sensitiviti dari masing-masing kasus yang dilakukan. Hasil akurasi terbaik yaitu 89,2% didapatkan di kasus dengan 2 kelas (BTT_HbE, DB), dan sensitivity terbaik didapatkan di kasus 4 kelas dan 3 kelas dengan nilai 0,97.
","Anemia is a serious disease problem that is commonly found in developing countries such as Indonesia, so the application of anemia classification technology is very important in the health sector. The classification system used in this study is to classify 4 classes of anemia with 4 different cases. The main focus of this research is how to find the best performance of the model used, proposed machine learning that can help classify anemia using the K-Nearest Neighbor algorithm with Euclidean Distance as a metric parameter. KNN is used to perform classification tasks, then the performance results of each case are compared and look for the best results.
The results of the study describe the accuracy and sensitivity of each case carried out. The best accuracy results, namely 89.2%, were obtained in cases with 2 classes (BTT_HbE, DB), and the best sensitivity was obtained in cases with 4 classes and 3 classes with a value of 0.97.
",,,158571,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211127,,RAYMOND LUIS,Klasifikasi Daerah Asal Musik Tradisional menggunakan Convolutional Neural Network (CNN) dan Mel Frequency Cepstrum Coefficients (MFCC)),,,"musik tradisional, audio classification, Convolutional Neural Network, CNN, Mel-Frequency Cepstral Coefficients ","Nur Rokhman, S.Si., M.Kom., Dr.",2,3,0,2022,1,"	Musik tradisional Indonesia merupakan warisan budaya Indonesia yang sering dilupakan oleh masyarakat modern. Banyak masyarakat yang tidak mengetahui dari daerah mana musik tradisional tersebut berasal. Hal ini menjadi permasalahan karena banyaknya musik tradisional yang kehilangan indentitas. Teknologi Deep Learning dapat menjadi solusi permasalahan klasifikasi musik tradisional ini.	Topik pengklasifikasian musik tradisional dipilih karena sedikit penelitian yang menggunakan topik ini sebelumnya.
	Penelitian ini akan melakukan klasifikasi musik tradisional berdasarkan daerah asal menggunakan data dari Youtube dengan metode ekstraksi fitur Mel-Frequency Cepstral Coefficients  (MFCC) dan model klasifikasi Convolutional Neural Network (CNN). Terdapat 7 provinsi yang akan digunakan sebagai label klasifikasi, yaitu Riau, Papua, Daerah Khusus Ibukota Jakarta, Daerah Istimewa Yogyakarta, Sumatera Utara, Jawa Barat, dan Sulawesi Selatan.
	Sistem klasifikasi yang dihasilkan dalam penelitian ini menghasilkan akurasi klasifikasi yang baik dengan nilai 74.03%.","Traditional Indonesian music is an Indonesian cultural heritage that is often forgotten by modern society. Many people do not know which area the traditional music came from. This is a problem because of the large amount of traditional music that loses its identity. Deep Learning technology can be a solution to this traditional music classification problem. The topic of traditional music classification was chosen because there's only few researches using this topic before.
	This research will classify traditional music based on the area of origin using data from Youtube with the extraction method of the Mel-Frequency Cepstral Coefficients (MFCC) feature and the Convolutional Neural Network (CNN) classification model. There are 7 provinces that will be used as classification labels, namely Riau, Papua, Special Capital District  of Jakarta, Special Region of Yogyakarta , North Sumatra, West Java, and South Sulawesi.
	The classification system produced in this study produced good classification accuracy with a value of 74.03%.",,,162263,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219321,,NAUFAL ABDILLAH,Identifikasi Fokus Pengemudi Menggunakan Estimasi Pose dan Algoritma Pembelajaran Mesin,,,"Driver Drowsiness, Computer Vision, Machine Learning, Pose Estimation","Wahyono, S. Kom., Ph.D.",2,3,0,2022,1,"Sistem pengawasan pengemudi menjadi salah satu pengembangan teknologi topikal sebagai upaya pencegahan kecelakaan di kendaraan. Disebut oleh Choi and Kim (2014) ditemukan lebih dari 30% kematian karena kecelakaan di kendaraan berhubungan dengan kantuk pengemudi. Sedangkan kecelakaan juga sering disebabkan oleh distraksi selain kantuk. Di sisi lain, untuk teknologi kendaraan otonom saat ini bisa maju ke level selanjutnya, perlu dibuat sistem yang memberikan keadaan pengemudi sebagai input (Canas et al., 2021). Sistem identifikasi fokus pengemudi rata-rata menggunakan perangkat yang dinilai kurang terjangkau dibandingkan kamera RGB seperti kamera kedalaman atau sensor fisiologis. Analisis aktivitas manusia dari citra RGB memberikan tantangan baru yaitu variasi citra yang luas seperti pencahayaan dan bayangan. Salah satu alternatif dari tantangan tersebut adalah ekstraksi pose. Penelitian ini mencoba membuat alternatif model pembelajaran mesin Logistic Regression dengan input pose hasil ekstraksi framework MediaPipe.","The driver monitoring system is one of the topical technology developments to prevent road accidents. Choi and Kim (2014) found that more than 30% of road accident deaths are related to driver drowsiness while accidents are also often caused by distractions other than drowsiness. On the other hand, for the current autonomous vehicle technology to advance to the next level, the driver's state must be an input to the systems (Canas et al., 2021). Driver's focus identification systems on average use relatively expensive devices than RGB cameras such as depth cameras or physiological sensors. Human activity analysis using RGB images provides a new challenge, namely a wide variety of image quality such as lighting and shadows. One alternative to the challenge is pose extraction. This study tries to create an alternative Logistic Regression machine learning model with pose input from the extraction of the MediaPipe framework.",,,170561,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213692,,M HAIDAR AZHAR,Data Splitting Method of Stacking with Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) for Predicting Flood in Special Capital Region of Jakarta,,,"Flood prediction, Climate data, Data splitting, Support Vector Machine, K-Nearest Neigbors, Stacked Generaliztaion","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Banjir selalu menjadi jenis bencana alam yang paling sering terjadi di Indonesia. Oleh karena itu, pemerintah dan para ahli berupaya untuk mengurangi kerugian akibat peristiwa tersebut dengan memperkirakan apakah akan terjadi banjir atau tidak. Berbagai metode dapat digunakan untuk memprediksi manifestasi banjir, salah satunya adalah dengan menggunakan ensemble yang disebut stacked generalization atau stacking. Meskipun studi tentang stacking cukup umum, diyakini bahwa metode data splitting masih sangat sedikit yang tidak diketahui untuk digunakan. Dengan demikian, teknik data splitting stacking dapat memperbaiki model prediksi menjadi lebih akurat. 

Stacking dilakukan dengan menggunakan dua model sebagai dua pembelajar utama, Support Vector Machine (SVM) dan K-Nearest Neighbors (KNN). Kedua model ini dilatih dan diuji dengan menggunakan data data iklim Jakarta Indonesia dari tahun 2017 hingga 2021 dan data curah hujan tahunan tambahan dari provinsi Kerala di India, dengan kondisi iklim yaitu suhu minimum, suhu maksimum, suhu rata-rata, kelembaban rata-rata, curah hujan, kecepatan angin maksimum, dan kecepatan angin rata-rata sebagai prediktor. Selain itu, perbandingan tanpa data splitting juga dilakukan untuk membedakan kedua metode tersebut.

Stacked generalization menggunakan dataset Jakarta dan Kerala dengan pemisahan data menghasilkan akurasi masing-masing sebesar 95,62 dan 90,90. Sebaliknya, metode non-spitting menghasilkan akurasi sebesar 93,16 untuk dataset Jakarta dan 95,83 untuk dataset Kerala. Hasil menunjukkan bahwa stacked generalization menunjukkan akurasi yang lebih tinggi dan berkinerja lebih baik daripada SVM dan KNN untuk metode splitting dan non-splitting dengan metode data splitting stacking meningkatkan akurasi lebih signifikan daripada pendekatan non-splitting.","Flood occurrence has always been the most common type of natural disaster that frequently happens in Indonesia. Therefore, government and experts attempt to lower the damages of the event by predicting whether flood is happening or not. Various methods can be used to predict the manifestation of flood, one method is by using ensemble called stacked generalization or stacking. Although studies regarding stacking is quite common, it is believed that the data splitting method is still very much little to unknown to be used. Thus, the data splitting stacking technique can improve the prediction model to be more accurate.

The stacking is done by using two models as the two main learners, Support Vector Machine (SVM) and K-Nearest Neighbors (KNN). These two models train and test by employing the data of Indonesia&acirc;€™s Jakarta climate data from 2017 to 2021 and an additional yearly rainfall data from Kerala province in India, with climate conditions that are minimum temperature, maximum temperature, average temperature, average humidity, rainfall, maximum wind speed, and average wind speed as predictors. Moreover, comparison without data splitting is also conducted to differentiate both methods.

Stacked generalization using Jakarta and Kerala dataset with data splitting resulted an accuracy of 95.62 and 90.90 of accuracy, respectively. In contrast, the method of non-spitting produced an accuracy of 93.16 for Jakarta dataset and 95.83 for Kerala dataset. The result shows that stacked generalization has higher accuracy that indicated that it performs better than SVM and KNN for both splitting and non-splitting method with the splitting method stacking increased the accuracy more significantly rather than the non-splitting approach.",,,164641,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211133,,HANUN FADHIL I S,PERFORMANCE COMPARISON: LINEAR REGRESSION &amp; LOGISTIC REGRESSION AS NETWORK INTRUSION DETECTOR,,,"network intrusion detection, network traffic, machine learning, linear Regression, logistic Regression","Mardhani Riasetiawan, SE., Akt., MT., Dr.",2,3,0,2022,1,"Seiring kemajuan teknologi, jenis-jenis ancaman dan serangan baru pun akan muncul juga. Keamanan dan privasi selalu menjadi perhatian utama dalam teknologi. Dalam perangkat yang saling terhubung seperti IoT, di mana perangkat terhubung satu sama lain dalam jaringan yang besar dan kompleks, sangat penting untuk menerapkan keamanan jaringan yang baik. Network Intrusion Detection System (NIDS) adalah salah satu metode untuk memperkuat keamanan jaringan dengan mengklasifikasikan apakah lalu lintas jaringan dianggap normal atau berbahaya. Namun, penerapan NIDS konfensional terbukti tidak mampu mengimbangi serangan yang terus berkembang. Untuk mencegah hal ini, NIDS perlu menjadi lebih adaptif terhadap jangkauan serangan yang lebih luas. Salah satu metode yang dapat dilakukan adalah dengan membiarkan sistem mempelajari pola serangan yang dapat dilakukan dengan mengimplementasikannya dengan pembelajaran mesin.

Dalam penelitian ini, perbandingan antara dua metode regresi pembelajaran mesin, yaitu regresi linear dan regresi logistik dilakukan untuk menentukan model mana yang berkinerja lebih baik ketika diterapkan sebagai detektor intrusi jaringan. Model dirancang untuk mengklasifikasikan apakah lalu lintas jaringan tertentu diklasifikasikan sebagai lalu lintas normal atau anomali dari perilakunya.

Empat metrik yaitu skor akurasi, tingkat presisi rata-rata, tingkat ingatan rata-rata, dan skor F1 digunakan untuk mengukur dan membandingkan kinerja kedua model. Hasil keseluruhan menunjukkan bahwa regresi logistik memberikan keamanan yang lebih tinggi daripada regresi linier, namun regresi linier memberikan aksesibilitas yang lebih tinggi.","As the advancement of technology goes on, more advanced and a variety of attacks will also follows. Security and privacy are always a top concern in technology. In interconnected devices such as in IoT, where devices are connected with one another in a large and complex network, it is crucial to implement a good network security. A network intrusion detection system (NIDS) is one of the methods to strengthen network security by classifying if a network traffic is considered normal or malicious. However, implementation of traditional NIDS are proven to not able to keep up with more sophisticated attacks. To prevent this, NIDS needs to become more adaptive to wider range of attacks. One of the methods that can be done is by allowing the system to learn the pattern of the attacks, which can be done by implementing it with machine learning.

In this paper, a comparison between the two machine learning regression methods, which are linear regression and logistic regression is conducted to determine which model performs better when applied as a network intrusion detector. The model is designed to classify whether a certain network traffic is classified as a normal or anomalous traffic from the behaviour inspected.

Four metrics which are accuracy score, average precision rate, average recall rate, and F1 score are used to measure and compare the two model's performance. The overall result shows that the logistic regression provides higher security than the linear regression, however linear regression is able to provides higher accessibility.",,,162283,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
215487,,FADLI MAULANA M,PERBANDINGAN KINERJA ALGORITMA NAIVE BAYES CLASSIFICATION DAN RANDOM FOREST CLASSIFICATION DALAM MENDETEKSI WEB SHELL,,," Indentifikasi Web Shell, PHP opcode, Na&Atilde;ƒ&Acirc;&macr;ve Bayes Classification, Random Forest Classification.","I Gede Mujiyatna, S.Kom., M.Kom",2,3,0,2022,1,"Web shell merupakan salah satu kode program yang digunakan oleh hacker
untuk melakukan eksploitasi pada laman web yang ditulis menggunakan bahasa
pemrograman tertentu, contohnya menggunakan bahasa pemrograman PHP. Isi
dari Web Shell tersebut dinamis tergantung dari pembuatnya, sehingga tiap masingmasing Web Shell merupakan sebuah script yang unik. Agar Web Shell mudah
untuk diidentifikasi, perlu dikalukan konversi terlebih dahulu menjadi bentuk
bahasa tingkat rendah atau opcode agar memiliki standar yang sama.
Beberapa algoritma yang dapat melakukan identifikasi Web Shell adalah
Na&Atilde;ƒ&Acirc;&macr;ve Bayes dan Random Forest. Algoritma Na&Atilde;ƒ&Acirc;&macr;ve Bayes bekerja dengan
menggunakan probabilitas dengan mempertimbangkan semua fitur secara
independen satu sama lain untuk melakukan klasifikasi. Hal yang berbeda
dilakukan pada algoritma Random Forest karena menggabungkan banyak Decision
Tree, salah satu tujuannya adalah untuk mengurangi overfitting. Algoritma
Decision Tree sendiri dapat melihat tingkat impuritas dari sebuah fitur dengan
harapan dapat melihat fitur yang penting, tidak seperti pada Na&Atilde;ƒ&Acirc;&macr;ve Bayes yang
melihat fitur secara independen. Namun, kedua algoritma tersebut baik Na&Atilde;ƒ&Acirc;&macr;ve Bayes
maupun Random Forest belum diketahui algoritma manakah yang memiliki kinerja
terbaik dalam mendeteksi Web Shell, sehingga penelitian ini akan membandingkan
kinerja dari kedua algoritma tersebut.
Kedua algoritma baik Na&Atilde;ƒ&Acirc;&macr;ve Bayes maupun Random Forest memiliki
performa deteksi yang baik yaitu di atas 90%. Sebagai perbandingan, performa
deteksi Random Forest lebih baik dibandingkan dengan Na&Atilde;ƒ&Acirc;&macr;ve Bayes karena
memiliki skor akurasi, presisi, recall, dan f1 yang lebih tinggi. Namun, Na&Atilde;ƒ&Acirc;&macr;ve Bayes
lebih baik dibandingkan dengan Random Forest dalam hal waktu eksekusi karena
memiliki waktu eksekusi yang lebih kecil. Dari hasil penelitian juga didapatkan
bahwa Random Forest lebih sensitif dibandingkan Na&Atilde;ƒ&Acirc;&macr;ve Bayes karena memiliki
selisih skor recall yang cukup besar yaitu 6.46% dibandingkan selisih skor presisi
yang hanya sebesar 1.12%.","Web shell is one of the program codes used by hackers to exploit web pages
written using a particular programming language, for example using the PHP
programming language. The content of the Web Shell is dynamic depending on the
author, so each Web Shell is a unique script. In order for the Web Shell to be easily
identified, it is necessary to convert it first into a low-level language form or opcode
so that it has the same standard.
Some algorithms that can identify Web Shell are Na&Atilde;ƒ&Acirc;&macr;ve Bayes and Random
Forest. The Na&Atilde;ƒ&Acirc;&macr;ve Bayes algorithm works by using probabilities by considering all
features independently of each other to perform classification. Different things are
done in the Random Forest algorithm because it combines many Decision Tree, one
of the goals is to reduce overfitting. The Decision Tree algorithm itself can see the
impurity level of a feature in the hope of seeing important features, unlike Na&Atilde;ƒ&Acirc;&macr;ve
Bayes which sees features independently. However, both Na&Atilde;ƒ&Acirc;&macr;ve Bayes and Random
Forest algorithms are not yet known which algorithm has the best performance in
detecting Web Shell, so this research will compare the performance of the two
algorithms.
Both Na&Atilde;ƒ&Acirc;&macr;ve Bayes and Random Forest algorithms have good detection
performance which is above 90%. In comparison, Random Forest detection
performance is better than Na&Atilde;ƒ&Acirc;&macr;ve Bayes because it has higher accuracy, precision,
recall, and f1 scores. However, Na&Atilde;ƒ&Acirc;&macr;ve Bayes is better than Random Forest in terms
of execution time because it has a smaller execution time. The results also show
that Random Forest is more sensitive than Na&Atilde;ƒ&Acirc;&macr;ve Bayes because it has a fairly large
recall score difference of 6.46% compared to the precision score difference of only
1.12%.",,,166819,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217791,,ALDI TRI MARGIYONO,Perbandingan Seleksi Fitur Information Gain dan Query Expansion Ranking untuk Analisis Sentimen pada Data Twitter Menggunakan Metode Naive Bayes (Studi Kasus: Pembelajaran Tatap Muka),,,"analisis sentimen, information gain, naive bayes, Pembelajaran Tatap Muka, query expansion ranking, Twitter","Aina Musdholifah, S.Kom., M.Kom., Ph.D; Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2022,1,"Pembelajaran Tatap Muka (PTM) kembali diberlakukan setelah setahun
lebih melaksanakan Pembelajaran Jarak Jauh (PJJ) melalui dunia maya sebagai
upaya mencegah penularan virus COVID-19 (Coronavirus Disease 2019).
Pemberlakuan PTM saat pandemi COVID-19 belum benar-benar berakhir ini pun
tak lepas dari komentar masyarakat yang disampaikan melalui berbagai media
sosial, salah satunya adalah Twitter. Komentar-komentar di Twitter (tweet)
tersebut dapat digunakan untuk mengetahui pandangan masyarakat terhadap
kebijakan PTM dengan melakukan analisis sentimen. Analisis sentimen sering
dilakukan dengan pendekatan machine learning seperti naive bayes, tetapi
memiliki masalah dengan besarnya dimensi fitur, sehingga perlu dilakukan seleksi
fitur. Beberapa contoh metode seleksi fitur adalah information gain dan query
expansion ranking.
Penelitian ini melakukan perbandingan metode seleksi fitur information
gain dan query expansion ranking dengan rasio 25%, 50%, dan 75% berdasarkan
performa yang dihasilkan dalam analisis sentimen dengan metode naive bayes
untuk data Twitter berbahasa Indonesia.
Setelah penelitian ini dilakukan, diperoleh kesimpulan bahwa secara
keseluruhan performa model naive bayes yang menggunakan seleksi fitur query
expansion ranking lebih baik daripada information gain dengan performa terbaik
dicapai saat menggunakan rasio 75%. Pada rasio tersebut, model dengan query
expansion ranking mendapat nilai rata-rata akurasi sebesar 81,70%, presisi
87,10%, recall 75,73%, dan f1-measure 80,38%, sedangkan model dengan
information gain mendapat nilai rata-rata akurasi sebesar 81,64%, presisi 87,93%,
recall 73,94%, dan f1-measure 79,84%.","Offline learning (Pembelajaran Tatap Muka/PTM) was again 
implemented after more than a year the online learning (Pembelajaran Jarak
Jauh/PJJ) was implemented through cyberspace as an effort to prevent the
transmission of the COVID-19 (Coronavirus Disease 2019) virus. The
implementation of PTM when the COVID-19 pandemic has not really ended can
not be separated from public comments submitted through various social media,
one of which is Twitter. The comments on Twitter (tweet) can be used to find out
the public's views on PTM policies by conducting sentiment analysis. Sentiment
analysis is often done with a machine learning approach such as naive bayes, but
has problems with the size of the feature dimensions, so feature selection is
necessary. Some examples of feature selection methods are information gain and
query expansion ranking. 
This research compares the information gain and query expansion ranking 
feature selection method with ratio of 25%, 50%, and 75% based on the
performance generated in sentiment analysis using the naive bayes method for
Indonesian-language Twitter data. 
 
After this research is done, it was concluded that the overall performance 
of the naive bayes model using the query expansion ranking feature selection was
better than information gain with the best performance achieved when using ratio
of 75%. At this ratio, the model with the query expansion ranking got an average
accuracy score of 81.70%, precision 87.10%, recall 75.73%, and f1-measure
80.38%, while the model with information gain got an average accuracy score of
81.64%, precision 87.93%, recall 73.94%, and f1-measure 79.84%.",,,169016,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207296,,ZAKY SYIHAB HATMOKO,CLUSTERING-BASED COMIC RECOMMENDER SYSTEM USING COLLABORATIVE FILTERING METHOD,,,"clustering, collaborative filtering, comics, DBSCAN, K-Means, recommender system","Aina Musdholifah, S.Kom, M.Kom, Ph.D",2,3,0,2022,1,"Pandemi COVID-19 telah lama membatasi aktivitas warga global di rumah mereka. Situasi ini dapat menyebabkan kebosanan dan stres. Membaca komik bisa menjadi hiburan dan mengurangi stres masyarakat. Namun, menemukan komik yang bagus untuk dibaca bisa jadi menantang. Dengan demikian, membuat Sistem Rekomendasi Komik dapat membantu orang untuk menemukan komik pilihan mereka.
Sistem Rekomendasi Komik dibangun menggunakan Item-based Collaborative Filtering untuk memberikan rekomendasi komik berdasarkan prediksi nilai kemiripan antar komik. Item-based Collaborative Filtering memiliki masalah scalability dan sparsity. Metode clustering: K-Means dan DBSCAN diimplementasikan untuk mengatasi masalah ini.
Evaluasi offline dan survei pengguna dilakukan untuk membandingkan kinerja K-Means dan DBSCAN. Evaluasi offline mengukur dua parameter: MAE dan running time. Hasil evaluasi offline menunjukkan bahwa DBSCAN memiliki akurasi yang lebih baik daripada K-Means, namun K-Means memiliki running time yang lebih cepat. Survei pengguna dilakukan kepada 30 peserta, dan Sistem Rekomendasi Komik memiliki skor rata-rata di atas 4 (skala 1-5) dalam parameter relevansi, kebaruan, kebetulan, dan peningkatan keragaman rekomendasi.","The COVID-19 pandemic has limited the activities of global citizens in their homes for a long time. This situation can cause boredom and stress. Reading comics can act as entertainment and reduce people's stress. However, finding a good comic to read can be challenging. Thus, creating a Comic Recommender System can help people to find their preferred comic.
The Comic Recommender System is built using Item-based Collaborative Filtering to give a comic recommendation based on the prediction of similarity value between comics. Item-based Collaborative Filtering has scalability and sparsity issues. Clustering methods: K-Means and DBSCAN are implemented to solve these problems.
Offline evaluation and user survey are conducted to compare the performance of K-Means and DBSCAN. The offline evaluation measures two parameters: MAE and running time. The offline evaluation result shows that DBSCAN has better accuracy than K-Means, but K-Means has a faster running time. The user survey was conducted to 30 participants, and the Comic Recommender System has an average score above 4 (1-5 scale) in relevance, novelty, serendipity, and increasing recommendation diversity parameters.",,,158556,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207041,,ARDACANDRA S,EVALUASI KUALITAS GERAKAN DARI REKAMAN PERMAINAN OTHELLO DENGAN METODE CONVOLUTIONAL NEURAL NETWORK,,,"Othello, CNN, Pengolahan Citra Digital","Wahyono, Ph.D.",2,3,0,2022,1,"Othello adalah sebuah deterministic zero-sum game dengan informasi sempurna untuk dua pemain yang dimainkan pada papan berukuran 8x8. Sudah banyak penelitian sebelumnya yang berupaya untuk melatih agen yang dapat bermain Othello
dengan berbagai metode. Namun, seorang pemain Othello tidak dapat dengan mudah menggunakan agen-agen tersebut untuk mengevaluasi kualitas gerakan-gerakan
yang ia buat pada sebuah permainan Othello. Kombinasi dari agen bermain Othello dan metode pengolahan citra digital dapat mempermudah seorang pemain Othello
menemukan kesalahan-kesalahan dan gerakan-gerakan terbaik dari sebuah rekaman
permainan Othello.

Penelitian ini melatih sebuah model yang belajar bermain Othello dari gerakangerakan pemain profesional, lalu menggunakan model tersebut untuk mengevaluasi
kualitas gerakan-gerakan dari rekaman video. Model belajar cara bermain Othello
dari dataset WThor dengan metode Convolutional Neural Network. Metode prapemrosesan citra yang terdiri dari Canny Edge Detection, operasi closing, Hough Transform, dan thresholding digunakan untuk mengubah citra papan Othello ke bentuk
matriks 8x8 agar dapat dijadikan input ke model CNN.

Hasil yang diperoleh adalah program kombinasi model CNN dan metode prapemrosesan berhasil mengevaluasi kualitas gerakan-gerakan pada rekaman video permainan Othello secara efektif. Model CNN mencapai akurasi prediksi gerakan 59.37%
dan akurasi top 3 88.83%, dan memperoleh persentase kemenangan 100% melawan
agen random, 90% melawan agen positional, dan 97% melawan agen mobility. Metode prapemrosesan dapat mendeteksi seluruh perubahan kondisi papan pada rekaman
permainan Othello dengan tepat dan menjadikannya sebagai input ke model CNN
untuk mendapat nilai evaluasinya.","Othello is a deterministic zero-sum game with perfect information for two
players played on an 8x8 board. There have been many previous studies that have
attempted to train agents that can play Othello with various methods. However, an
Othello player cannot easily use these agents to evaluate the quality of the moves he
makes in an Othello game. The combination of an Othello agent and digital image
processing methods can make it easier for an Othello player to find blunders and the
best moves from an Othello play recording.

This study trained a model that learned to play Othello from the moves of
professional players, and then used the model to evaluate the quality of the moves in
the video recording. The model learned how to play Othello from the WThor dataset
with the Convolutional Neural Network method. The image preprocessing method
consisting of Canny Edge Detection, closing, Hough Transform, and thresholding
operations is used to convert the Othello board image into an 8x8 matrix so that it can
be used as input to the CNN model.

The results obtained are that the combination of the CNN model and the preprocessing method has succeeded in evaluating the quality of the moves in the video
recording of the Othello game effectively. The CNN model achieved a movement
prediction accuracy of 59.37% and a top 3 accuracy of 88.83%, and obtained a winning percentage of 100% against random agents, 90% against positional agents, and
97% against agents mobility. The preprocessing method is able detect all changes in
the condition of the board in the Othello game recording accurately and make it as
input to the CNN model to get its evaluation value.",,,158430,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217794,,AMMAR PASHA ALFARISY, THE DEVELOPMENT OF WHATSAPP GATEWAY TO ENABLE REAL TIME PAYMENT NOTIFICATION VIA WHATSAPP MESSAGE AT BERSEDEKAH.COM,,,"Notifikasi pembayaran, gateway pembayaran, API WhatsApp / Payment notification, payment gateway, WhatsApp API","Medi, Drs., M.kom",2,3,0,2022,1,"Notifikasi pembayaran merupakan fitur wajib dalam proses pembayaran online. Dengan kemungkinan adanya kegagalan pembayaran dalam transaksi online, diperlukan konfirmasi kepada pembayar bahwa pembayaran online telah berhasil. Notifikasi pembayaran online biasanya dikirim melalui email, namun saat ini respon orang membaca email saat ini lebih lama dibandingkan respon orang membaca pesan WhatsApp.

Dalam penelitian ini dilakukan studi kasus pada Bersedekah.com, sebuah perusahaan portal agregator yang terintegrasi secara online dan real-time dengan situs penggalangan dana resmi dari berbagai lembaga amil zakat, nadzir wakaf, dan lembaga sosial yang telah memiliki izin. 

Tujuan dari penelitian ini adalah untuk mengembangkan aplikasi WhatsApp gateway yang dapat menerima notifikasi pembayaran dari payment gateway Bersedekah.com kemudian meneruskan notifikasi tersebut ke sistem WhatsApp API sehingga pelanggan atau pembayar dapat menerima informasi status pembayaran pada aplikasi WhatsApp mereka. Selain itu, sistem harus dapat menerima pemberitahuan status pengiriman pesan dari sistem WhatsApp API dan kemudian meneruskannya ke sistem gateway pembayaran Bersedekah.com.

Aplikasi WhatsApp gateway terdiri dari 5 komponen utama yaitu modul untuk menerima notifikasi dari payment gateway, modul untuk mendapatkan token dari sistem WhatsApp, modul untuk mengirim notifikasi ke sistem WhatsApp, modul untuk menerima status notifikasi dari WhatsApp, dan modul untuk mengirimkan status notifikasi ke Bersedekah.com. Aplikasi ini dibangun melalui konektivitas REST-API melalui Bersedekah.com API dan WhatsApp API. Dari penelitian ini didapatkan bahwa fungsi dari aplikasi WhatsApp gateway berfungsi, dimana 11 dari 12 pesan berhasil dan hanya 1 pesan yang gagal. Pesan yang gagal bukan karena sistem, tetapi karena nomor yang tidak valid. Pengujian lainnya dilakukan dengan mengirimkan notifikasi sebanyakan 100 kali ke 11 nomor WhatsApp yang valid untuk melihat seberapa cepat pengguna menerima notifikasi pembayarannya. Dapat disimpulkan bahwa 100% pesan terkirim, dimana 90% notifikasi berada dalam rentang dibawah 2 menit atau dengan rata-rata 83,1 detik. Perlu diwaspadai karena ada 10% pengguna yang masih mendapatkan notifikasi lebih dari 2 menit,bahkan 9% dari 10% mendapatkan notifikasi status pembayaran lebih dari 6 menit.

","Payment notification is a mandatory feature in the online payment process. With the possibility of a payment failure in an online transaction, a confirmation to the payer is needed that the online payment has been successful. Payment notification is usually sent through email, but nowadays people's responses to reading emails are currently longer than people's responses to reading WhatsApp messages.

In this research, a case study is made on Bersedekah.com, an aggregator portal company that is integrated online and in real-time with official fundraising sites from various amil zakat institutions, nadzir waqf, and social institutions that already have permits.

The purpose of this research is to develop WhatsApp gateway application that can receive payment notification from the Bersedekah.com's payment gateway and then forward the notification to the WhatsApp API system so that customers or payers can receive payment status notification on their WhatsApp application. Moreover, the system must be able to receive notification of message delivery status from WhatsApp API system and then forward it to the Bersedekah.com's payment gateway system.

The WhatsApp Gateway application consists of 5 main components, namely a module for receiving notifications from the payment gateway, a module for getting tokens from the WhatsApp system, a module for sending notifications to the WhatsApp system, a module for receiving notifications status from WhatsApp, and a module for sending notification status to Bersedekah.com. This application is built through REST-API connectivity via the Bersedekah.com API and WhatsApp API.

From this research, it is found that the function of the WhatsApp gateway application is working, where 11 of 12 messages is success and only 1 message failed. The failed message is not because of the system, but it is because of invalid number. Another testing is conducted in sending notification 100 times to 11 valid WhatsApp numbers to see how fast the user to receive their payment notification. It can be concluded that 100% messages is delivered, where 90% of the notifications are within the range of under 2 minutes or with an average of 83,1 seconds. It needs to be a concern because there are 10% of the users who still get the notification for more than 2 minutes, even 9% of 10% get notification of payment status in more than 6 minutes.",,,169033,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218306,,IVAN LIU NARDO S,Multiclass Chat Topic Classification pada Dataset Fallback Chatbot dalam Domain Telekomunikasi Menggunakan Temporal Convolutional Network (TCN),,,"Chatbot Fallback, Gibbs Sampling Dirichlet Multinomial Mixture, Temporal Convolutional Network, FastText, IndoBERT.","Anny Kartika Sari, S.Si., M.Sc., Ph.D; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2022,1,"Layanan digital berupa chatbot dalam domain telekomunikasi masih memiliki kekurangan di mana chatbot kerap tidak mengerti pesan yang dikirimkan oleh pelanggan sehingga terjadi fallback, yaitu chatbot hanya dapat menjawab dengan pesan default. Oleh sebab itu, diperlukan suatu pengembangan arsitektur yang dapat mengategorikan pesan-pesan pelanggan yang bersifat teks pendek serta tidak baku yang menyebabkan pesan-pesan tersebut tidak dapat dimengerti oleh chatbot.
Dalam penelitian ini, dikembangkan model Temporal Convolutional Network (TCN) untuk klasifikasi multiclass fallback chat untuk meminimalisir terjadinya fallback. Dataset pesan-pesan pelanggan yang digunakan pada penlitian ini menggunakan bahasa Indonesia. Pesan-pesan fallback tentunya masih belum memiliki label sehingga digunakan metode Gibbs Sampling Dirichlet Multinomial Mixture untuk melakukan topic modelling dengan tujuan memberikan label kepada setiap pesan.  Pesan yang sudah diberikan label kemudian diklasifikasi menggunakan arsitektur TCN dengan model word embedding berupa FastText dan IndoBERT.
Tugas topic modelling menggunakan metode Gibbs Sampling Dirichlet Multinomial Mixture yang digunakan setelah diverifikasi secara manual mampu menghasilkan nilai akurasi sebesar 83,01% dengan 16.602 pesan memiliki label yang benar dan 3.398 pesan memiliki label yang salah. Tugas klasifikasi teks menggunakan arsitektur TCN &acirc;€“ FastText yang dikembangkan mampu menghasilkan nilai F1-Score sebesar 93,49% saat training dan 95,51% saat testing. Sedangkan arsitektur TCN &acirc;€“ IndoBERT yang dikembangkan mampu menghasilkan nilai F1-Score sebesar 93,1% saat training dan 93,44% saat testing. Dari sisi waktu training model, arsitektur TCN &acirc;€“ FastText mampu melakukan training &Acirc;&plusmn;15 kali lebih cepat dibandingkan arsitektur TCN &acirc;€“ IndoBERT.
","There is still a shortage of chatbot-style digital services in the telecommunications sector, as chatbots often fail to understand the messages sent by customers, leading to fallbacks where chatbots can only reply with default standard messages. As a result, it is essential to develop an architecture that can classify customer messages that contain short and irregular language, making them difficult for chatbots to understand.
In this research, a Temporal Convolutional Network (TCN) model was developed for the classification of multiclass fallback messages in order to lessen the risk of fallback. The customer message dataset that was utilized in this research is in Indonesian. Obviously, the backup messages still lack labels, thus topic modelling is done using the Gibbs Sampling Dirichlet Multinomial Mixture approach in order to label each message. Once messages have been labelled, they are then categorized using the TCN architecture utilizing FastText and IndoBERT as word embedding models.
The topic modelling task utilizing Gibbs Sampling Dirichlet Multinomial Mixture approach, after being manually validated, was able to produce an accuracy result of 83.01%, with 16,602 messages having the correct label and 3,398 messages having the incorrect label. As for the text classification task, the TCN - FastText architecture was able to obtain F1-Score values of 93.49% during training and 95.51% during testing. The TCN - IndoBERT architecture, on the other hand, was able to generate F1-Score values of 93.1% during training and 93.44% during testing. The TCN - FastText architecture can train models approximately 15 times quicker than the TCN - IndoBERT architecture.",,,169462,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212931,,DANIEL SURANTA S,PENGGUNAAN SINGLE SHOT MULTIBOX DETECTOR (SSD) UNTUK MENDETEKSI KERUSAKAN DNA PADA CITRA COMET ASSAY,,,"Comet assay, buccal mucosa, deteksi objek, Single Shot Multibox Detector (SSD)","Afiahayati, S.Kom., M.Cs, Ph.D",2,3,0,2022,1,"DNA damage analysis atau analisis kerusakan sel DNA adalah kegiatan untuk mendapatkan informasi penting dari sebuah sel agar dapat digunakan dalam berbagai kegiatan medis seperti pengobatan suatu penyakit. Comet assay merupakan salah satu metode untuk melakukan DNA damage analysis, dimana sel comet perlu dideteksi terlebih dahulu untuk mendapatkan informasinya. Saat ini, metode comet assay masih sering dilakukan secara manual oleh pengamat profesional sehingga membutuhkan waktu lama. Selain itu, terdapat juga perangkat lunak yang bisa digunakan untuk melakukan analisis, tetapi membutuhkan biaya yang besar.
Penelitian ini mencoba melakukan deteksi objek untuk memudahkan analisis comet assay dengan metode Single Shot Multibox Detector (SSD). Penerapan SSD ini menggunakan beberapa variasi parameter model dengan hasil nilai Average Precission (AP) tertinggi 33,8% dan Average Precision (AR) 46,3% untuk satu model yang sama. Untuk proses pengujian sendiri, membutuhkan waktu sekitar 0,181 sampai 0,2 detik untuk setiap gambar. Hal ini menunjukkan bahwa SSD bisa digunakan untuk metode comet assay pada DNA damage analysis. Penggunaan metode SSD untuk deteksi comet assay buccal mucosa ini bisa menggantikan peran manusia dalam kegiatan comet assay, tetapi masih dibutuhkan pengembangan agar nilai AP dan AR bisa lebih optimal dan mampu diandalkan dalam dunia kesehatan.","DNA damage analysis is an activity to obtain important information from a cell so that it can be used in various medical activities such as treating a disease. Comet assay is one of the methods to perform DNA damage analysis, where comet cells need to be detected first to get the important information. Currently, this comet assay method is still often done manually by professional observers, so it takes a long time. Moreover, there is also software that can be used to perform analysis, but it costs a lot.
This research tries to detect objects to ease comet assay analysis with the Single Shot Multibox Detector (SSD) method. The implementation of this SSD uses several variations of model parameters with the highest Average Precision (AP) of 33.8% and Average Precision (AR) of 46.3% for the same model. The testing process takes about 0.181 to 0.2 seconds for each image. This shows that SSD can be used for the comet assay method in DNA damage analysis. The SSD method for detecting the comet assay of the buccal mucosa can replace the role of humans in comet assay activities, but development is still needed so that the AP and AR values can be more optimal and reliable in the medical world.",,,163916,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
208326,,FAZHLUR BANNA DAYU N,Creating A Dota 2 Custom Game Modifier Template Using Dota LUA API,,,"game, custom, user tools","M Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2022,1,"Video games and Esports have become a massive market for entertainment with numerous and a wide range of genres and variety of selections of video games, the selection of entertainment is quite vast. Despite its growth and broad reach, understanding on this market are often limited to bias and speculation, the reason is that previously, there was a lack of understanding of game development process and the general people&acirc;€™s intimidation of programming languages. In recent years, game development companies made some of their games open source and they release developer&acirc;€™s log to the public on their website and social media platforms for the public to read and study from. Moreover, there are game engines that are available to the public for them to try their hands on game development and programming such as Unity and Unreal Engine that are free to download and publish their games on, with conditions attach to them of course depending on which game engine they use. This helps both the gaming industry and the consumers to start creating their own games and attracting newcomers into the game development industry by providing them with easy to access tools.
In this research, we offer an entry way for the public to get into game development, specifically the Dota 2 player base, by creating a template for them to create their own custom games in the Dota 2 game. Our template is a modifiable pre-made code that you can overlay in your custom game directory to ease your custom game creation process. Our template will have separate codes for different aspects of the template and also simplified explanation on the function of the code in them, to make it easier to find and modify the specific code or value that you want to modify. In this research and creation of our template, we hope that more people get into game development and make it less daunting to get into programming as a whole.
The result of this research shows that our template functions as we hoped it will albeit encountering a few obstacles along the research process. The feedback from the users is well received and will be used as future improvements to our template.
","Video games and Esports have become a massive market for entertainment with numerous and a wide range of genres and variety of selections of video games, the selection of entertainment is quite vast. Despite its growth and broad reach, understanding on this market are often limited to bias and speculation, the reason is that previously, there was a lack of understanding of game development process and the general people&acirc;€™s intimidation of programming languages. In recent years, game development companies made some of their games open source and they release developer&acirc;€™s log to the public on their website and social media platforms for the public to read and study from. Moreover, there are game engines that are available to the public for them to try their hands on game development and programming such as Unity and Unreal Engine that are free to download and publish their games on, with conditions attach to them of course depending on which game engine they use. This helps both the gaming industry and the consumers to start creating their own games and attracting newcomers into the game development industry by providing them with easy to access tools.
In this research, we offer an entry way for the public to get into game development, specifically the Dota 2 player base, by creating a template for them to create their own custom games in the Dota 2 game. Our template is a modifiable pre-made code that you can overlay in your custom game directory to ease your custom game creation process. Our template will have separate codes for different aspects of the template and also simplified explanation on the function of the code in them, to make it easier to find and modify the specific code or value that you want to modify. In this research and creation of our template, we hope that more people get into game development and make it less daunting to get into programming as a whole.
The result of this research shows that our template functions as we hoped it will albeit encountering a few obstacles along the research process. The feedback from the users is well received and will be used as future improvements to our template.
",,,159585,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218566,,JOVAN JODHITYA,SISTEM TANYA JAWAB BERBASIS ATURAN DALAM PEMBANGKITAN JAWABAN PADA DATASET INFORMASI FILM,,,"Sistem Tanya Jawab, Rule-Based, Information Retrieval, Closed-domain","Azhari, Drs., M.T., Dr",2,3,0,2022,1,"Metode Rule-based dalam sistem tanya jawab merupakan salah satu metode yang dapat dikembangkan untuk sistem tanya jawab bertipe closed-domain, dalam metode rule-based, sistem dikembangkan secara spesifik berdasarkan domain tertentu sehingga dalam pengembangannya dilakukan pembuatan ketentuan-ketentuan spesifik berdasarkan pola pertanyaan yang dapat diajukan terhadap sistem.
Pada penelitian ini akan dikembangkan sistem tanya jawab metode rule-based untuk tipe basis data relational model untuk dataset informasi film. Rancangan rule yang dikembangkan bekerja dengan melakukan pencarian informasi-informasi yang tersedia dari input pertanyaan, lalu berdasarkan informasi yang diperoleh dilakukan pengambilan data dari database, lalu informasi tersebut disajikan kepada user secara langsung, user hanya perlu memasukkan input berupa satu kalimat saja dalam Bahasa Indonesia.
","Rule-based is one of the methods that can be developed for closed-domain type question answering systems. In rule-based method, the system is developed based on a specific domain, so specific rules are created based on the patterns of questions that could be asked.
In this research, a rule-based question answering system will be developed for movie dataset, a dataset in the form of relational model. The rules that would be developed operate by searching for every available information from the input, then based on the obtained information, data is extracted from database, and the information will be presented to the user directly. The user only needs to enter a single sentence in Indonesian as input.",,,169743,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211149,,VIGA LAKSA HARDJANTO,Pengenalan Teknik Tabuhan Instrumen Gamelan Bonang Barung Menggunakan Convolutional Neural Network,,,"Gamelan, Mel Spektrogram, MFCC, CNN.","Wahyono, Ph.D",2,3,0,2022,1,"Keberagaman budaya Indonesia menjadi hal yang menarik untuk diulas dan ditinjau lebih lanjut. Salah satunya, ialah seni karawitan yang menggunakan alat musik gamelan sebagai medianya dengan beragam teknik permainan pada tiap instrumennya. Hal ini dapat ditemukan pada instrumen bonang barung yang memiliki sedikitnya tiga belas teknik tabuhan yang berbeda. Ternilai tidaklah mudah bagi pemula untuk mempelajari seni karawitan. Maka dari itu, sebagai upaya dalam membantu pemula mempelajari karawitan dengan dukungan keterbaruan kemampuan sistem dalam mengolah data, adanya penelitian ini juga diharapkan bisa memperkaya penelitian dibidang klasifikasi audio. 
Jenis esktraksi fitur seperti mel spektrogram dan MFCC diujikan terhadap arsitektur CNN. Selain itu, proses pembersihan noise dan penyamarataan level kenyaringan terhadap raw data diterapkan dengan tujuan untuk mendapatkan kualitas audio yang lebih baik. Akhir dari penelitian ini menunjukkan bahwa fitur MFCC lebih unggul pada data audio yang memuat noise, yakni mencapai akurasi sebaik 99%, sedangkan Mel Spektrogram unggul pada data audio yang bersih dari noise dengan akurasi sebaik 98%, keduanya merupakan hasil terbaik pada pengaturan hyperparameter yang paling optimal.","The diversity of Indonesian culture is an interesting thing to be reviewed and reviewed further. One of them, is the art of karawitan which uses gamelan musical instruments as its medium with various playing techniques on each instrument. This can be found on the bonang barung instrument which has at least thirteen different wasp techniques.
It is priceless for beginners to learn the art of karawitan. Besides wanting to help beginners learn karawitan with the support of up-to-date system capabilities in processing data, this research is also expected to enrich research in the field of audio classification. 
Types of feature extractions such as mel spectrogram and MFCC were tested on the CNN architecture. In addition, the process of cleaning noise and levelling the loudness level of the raw data is applied with the aim of getting better audio quality. Apparently, at the best hyperparameter settings, it was found that the MFCC feature is better for audio data containing noise, which achieves an accuracy as good as 99%, while Mel Spectrogram excels at noise-free audio data with an accuracy as good as 98%.",,,162269,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
208847,,FRANCISCA C S,Pemanfaatan Data Augmentation Untuk Meningkatkan Akurasi Pendeteksian Cyberbullying Pada Tweet Bahasa Indonesia,,,"data augmentation, data-centric, prediction, cyberbullying detection, Twitter, long short-term memory, support vector machine, machine learning","Edi Winarko, Drs., M. Sc., Ph. D",2,3,0,2022,1,"Teknologi serta internet banyak membawa perkembangan ke penggunanya, baik positif maupun negatif. Hampir semua aktivitas dapat dilakukan di internet. Namun, pengguna dapat menggunakan data diri palsu di dunia maya, sehingga tindakan mereka akan sangat sulit dilacak. Semua orang memiliki potensi melakukan pemalsuan data, bahkan di Indonesia sekalipun. Hal ini menjadi salah satu yang mendorong peningkatan angka kejahatan online, tak terkecuali cyberbullying.

Penulis melakukan penelitian terhadap cyberbullying pada data tweet dalam Bahasa Indonesia. Fokus penelitian ini adalah data-centric,  yang berusaha meningkatkan kualitas data sebelum dilakukan klasifikasi, dalam rangka meningkatkan performa model pendeteksi perilaku cyberbullying pada komunitas online, khususnya dalam Bahasa Indonesia. Aplikasi data-centric yang dilakukan pada penelitian ini adalah dengan memanfaatkan data augmentation untuk menanggulangi masalah keterbatasan data. Tujuan dilakukan data augmentation adalah untuk meningkatkan jumlah data point pada dataset sebelum digunakan untuk pelatihan mesin. Metode data augmentation  yang digunakan adalah character augmentation, word augmentation dan backtranslation. Hasil dari masing-masing data augmentation kemudian diklasifikasi menggunakan deep learning LSTM dan machine learning SVM.

Secara keseluruhan, hasil dari penelitian ini menunjukkan data augmentation memberikan peningkatan performa baik untuk LSTM maupun SVM. Akurasi tertinggi pada pemodelan LSTM menghasilkan nilai akurasi 89,41% dengan peningkatan 6,66% dan skor F1 88,98% dengan peningkatan 7,01%. Sedangkan untuk pemodelan SVM, hasil nilai akurasi 92,16% dengan peningkatan 2,75% dan skor F1 92,42% dengan peningkatan 3,44%.","Technology and the internet have brought many developments to its users, both positive and negative. Almost all activities can be done on the internet. However, users can use fake personal data on the internet, thus their actions will be very difficult to track. Everyone has the potential to falsify data, even in Indonesia. This matter of fact has become one of the reasons for the increase in the number of online crimes, including cyberbullying.

The author conducted this research on cyberbullying tweets in Indonesian. The focus of this research is data-centric, which seeks to improve the quality of the data prior to classification, in order to improve the performance of the cyberbullying detection model in online communities, especially in Indonesian. The data-centric application carried out in this study is data augmentation to overcome the problem of data limitations. The purpose of data augmentation is to increase the number of data points in the dataset before being used for machine training. Data augmentation methods used are character augmentation, word augmentation and backtranslation. The results of each augmentation data are then classified using deep learning LSTM and machine learning SVM.

Overall, the results of this study show that data augmentation provides improvement for both LSTM and SVM performances. The result  in LSTM modeling produces an accuracy value of 89,41% with an increase of 6.66% and a F1-score of 88.98% with an increase of 7.01%. As for the SVM modeling, the accuracy value is 92.16% with an increase of 2.75% and a F1-score of 92.42% with an increase of 3.44%.",,,160173,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213204,,MUHAMMAD KHALIFA U,TRANSFER LEARNING IMPLEMENTATION ON SUNDANESE SCRIPT RECOGNITION USING CONVOLUTIONAL NEURAL NETWORK,,,"Transfer learning, Classification, CNN, Model deployment","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Dengan bertumbuhnya jumlah data, penggunaan data pun mulai berkembang. Visualisasi data, analisis data hingga pembelajaran mesin. Penggunaan data pada metode pembelajaran mesin berbeda terhadap masalah dan tipe data. klasifikasi adalah salah satu pengaplikasian paling umum untuk pembelajaran mesin yang dapat diaplikasikan dengan menggunakan bebagai jenis data seperti data dalam bentuk tabular dan data citra. Secara umum, setiap permasalahan yang akan dipecahkan membutuhkan algoritma spesifik. sebagai terobosan, transfer learning datang dengan tujuan untuk membuat solusi untuk memecahkan masah berbeda tapi berkaitan.
untuk melihat implementasi dari transfer learning, menggunakan model pembelajaran mesin dengan berbagai jenis dataset untuk memecahkan masalah klasifikasi. menggunakan convolutional neural network untuk hasil tulis tangan menunjukkan kapabilitas transfer learning.
aksara sunda dengan 32 jenis karakter adalah data yang tepat pada permasalahan ini. mdel yang dibuat menggunakan 4 dataset tulis tangan yang berbeda yakni karakter alphabet, karakter devanagari, karakter arab, dan aksara jawa. hasil dari penelitian ini menunjukan bahwa penggunaan metode non transfer learning masih lebih baik daripada metode transfer learning dari segi performa tetapi waktu pelatihan yang dipakai jauh lebih sedikit dengan implementasi transfer learning. Model terbaik untuk transfer learning didapatkan menggunakan aksara arab dengan akurasi 91.86% dan loss 0.2814","With the growth of the number of data, the utilization of it also starts to flourish. Data visualization, data analysis to machine learning. Machine learning method of data usage differs on the problem and the data types. Classification as one of the most common applications of machine learning can be applied into various data types such as tabular data and image data. In general, each problem needs a specific algorithm to be solved. As a breakthrough, Transfer Learning comes with a goal to create a solution to solve different but related tasks.
To see the transfer learning implementation, utilizing machine learning models trained with a variety of dataset to solve a classification using convolutional neural network for handwriting shows the capability of transfer learning. Sundanese script with a variety of 32 characters of difference is a great fit to be used on this problem. 
The model was created using 4 different handwriting dataset which are Alphabet character, Devanagari character and Aksara Jawa. The result shows that the non transfer learning model is better than the transfer learning model performance-wise but the training time spent is lesser by the transfer learning implementation. The best model performed for transfer learning is using the Arabic dataset with the accuracy of 91.86% and loss of 0.2814.
",,,164158,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
214229,,GRAYSON JOVANDI S,Emotion Detection in Messenger Applications Using Support Vector Machine and Natural Language Processing,,,"Messenger application, text processing, support vector machine,natural language processing ,mental health.","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Messenger application is a crucial part of our life. It is a vital part of day-to-day communication. Its has been used from important notifications and meetings to mundane conversation about the weather. Some of the notable ones are WhatsApp Messenger, Line, and Discord. With 5 billion, 500 million, and 100 million users respectively. Discord is extremely popular with the younger generation as it is specifically made for gaming and youth. These demographics are the most likely to experience depression and radicalisation which makes them the focus of this project.

    The goal of this project is to create a tool to help with the mental health of the young people in discord by detecting their emotions from their messages and when necessary, sending them resources that they might need.  To achieve this goal, a bot is created that can process the messages from the users in a discord server. The bot will then process the text, then feeds into an SVMsentiment analysis algorithm which will classify its emotion. After that it is saved into a database. At the start of each week, the bot will then evaluate their emotions and if it passes a certain threshold, the bot will send them online resources that can help them deal with their emotions.

    The result is a robust system that can predict a persos accuracy based on their chatting behaviour in a week at about 85% accuracy. The system can also detect irregularity in a person's chatting behaviour such as typos and acronym and read them accordingly. It can also analyse a person's emotional state, compare it to their past record and send help resources when needed, all from their chatting behaviour.
","Messenger application is a crucial part of our life. It is a vital part of day-to-day communication. Its has been used from important notifications and meetings to mundane conversation about the weather. Some of the notable ones are WhatsApp Messenger, Line, and Discord. With 5 billion, 500 million, and 100 million users respectively. Discord is extremely popular with the younger generation as it is specifically made for gaming and youth. These demographics are the most likely to experience depression and radicalisation which makes them the focus of this project.

    The goal of this project is to create a tool to help with the mental health of the young people in discord by detecting their emotions from their messages and when necessary, sending them resources that they might need.  To achieve this goal, a bot is created that can process the messages from the users in a discord server. The bot will then process the text, then feeds into an SVMsentiment analysis algorithm which will classify its emotion. After that it is saved into a database. At the start of each week, the bot will then evaluate their emotions and if it passes a certain threshold, the bot will send them online resources that can help them deal with their emotions.

    The result is a robust system that can predict a person's accuracy based on their chatting behaviour in a week at about 85% accuracy. The system can also detect irregularity in a person's chatting behaviour such as typos and acronym and read them accordingly. It can also analyse a person's emotional state, compare it to their past record and send help resources when needed, all from their chatting behaviour.
",,,165256,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219353,,MUH ADI PRASETYO,COMPARING HISTOGRAM OF ORIENTED GRADIENT BY COLOR SPACES FOR HUMAN CLASSIFICATION,,,"Human Classification, HOG (Histogram of Oriented Gradients), SVM (Support Vector Machine), Machine Learning","Medi, Drs., M.Kom",2,3,0,2022,1,"Manusia adalah suatu objek yang penting dalam deteksi objek, maka dari itu teknologi deteksi manusia sangat penting dalam semua teknologi deteksi objek yang bisa mendeteksi dan menunjuk objek. klasifikasi objek sangatlah penting karena klasifikasi objek adalah bagian inti dari deteksi objek, maka dari itu klasifikasi manusia sangatlah penting. riset ini mengklasifikasi apakah terdapat objek manusia dalam suatu gambar atau tidak dengan menggunakan dataset yang diambil dari berbagai kamera pengintai. Inti fokus pada riset ini adalah untuk membandingkan efek pengaruh ruang warna pada klasifikasi manusia menggunakan HOG dan SVM Linear. HOG digunakan untuk mengekstrak fitur gambar untuk diberikan kepada SVM Linear untuk melakukan klasifikasi. Hasil dari setiap kombinasi ruang warna, C parameter, dan normalisasi blok akan dibandingkan untuk mendapatkan kombinasi yang paling optimal.
Hasil dari riset akan menunjukkan berapa banyak hasil klasifikasi positif dan negatif pada kedua positif dan negatif data pada setiap kombinasi ruang warna, C parameter, dan normalisasi blok dan juga skor akurasi, presisi, recall, dan F1. Himpunan data mengandung 572 gambar positif dan 363 data negatif untuk dimasukkan ke HOG untuk mengekstrak fitur dan SVM Linear untuk klasifikasi.
","Humans are an important object for object detection, thus human detection technologies are very important for all object detection technologies which could detect and pinpoint the object. Object classification is very important as it is the core of object detection, thus human classification is very important. This research classifies whether there's a human or not in the image with a dataset taken from various surveillance cameras. The main focus of this research is to compare the effect color spaces have on human classification results by using HOG and Linear SVM. HOG is used to extract the features of the images to be fed into the Linear SVM to do the classification. The results of each color space, C parameter, and block normalization combination will then be compared to find the most optimum combination.
The results of the research will contain how many the positive and negative classification in both positive and negative data result each color space, block normalization, and C parameter combination gives and also its accuracy, precision, recall, and F1 scores. The dataset contains 572 positive images and 363 negative images to be fed into the HOG for feature extraction and SVM Linear for classification.
",,,170580,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213466,,ATHA LAYANA H,Komposisi Musik Piano Dua Kunci menggunakan Algoritma Genetika,,,"Komposisi musik dua kunci, piano, algoritma genetika, musik","Aina Musdholifah, S.Kom., M.Kom., Ph.D.",2,3,0,2022,1,"Piano adalah salah satu alat musik yang sering dimainkan oleh pemain musik dan juga digemari oleh penikmat musik masa kini. Musik diciptakan oleh seseorang yang dinamakan komponis. Komponis nantinya akan memikirkan lagu seperti apa yang akan diciptakan yaitu pertama menentukan genrenya, suasana yang dibawakan, dan pesan yang disiratkan dimana setelahnya komponis akan menentukan beberapa komponen lain seperti seberapa panjang lagunya, tangga nada apa yang digunakan, tempo lagunya, kunci apa saja yang akan digunakan, dan komponen lainnya. 
Komposisi musik piano saat ini dapat dilakukan secara otomatis yaitu dengan menggunakan algoritma genetika, long short-term memory networks (LSTM), recurrent neural network (RNN), dan reinforcement learning (RL). LSTM, RNN, dan RL membutuhkan kumpulan data berupa komposisi musik piano untuk melakukan training model sedangkan pada algoritma genetika, kumpulan data bersifat opsional karena tidak melakukan proses training model dan data hanya sebagai acuan tambahan. Penelitian ini juga membuat komposisi musik piano yang berdurasi maksimal 12 detik, sehingga dengan tidak dibutuhkannya proses training model, data berupa komposisi referensi bersifat opsional, dan panjang durasi komposisi menjadikan algoritma genetika terpilih untuk digunakan.
Penelitian ini mengembangkan komposisi musik piano dari satu kunci menjadi dua kunci menggunakan algoritma genetika. Proses komposisi terdiri dari inisialisasi individu, penghitungan nilai fitness, seleksi, dan mutasi. Keluaran dari program berupa lembaran musik hasil komposisi dan audio permainan komposisi musik tersebut. Penilaian komposisi musik dilakukan dengan memberikan 4 komposisi berbeda kepada beberapa audiens untuk dinilai. Hasil dari penelitian ini adalah komposisi musik piano yang dihasilkan mendapatkan nilai tanggapan cukup dan bagus serta komposisi yang menggunakan komposisi referensi dan kriteria nilai fitness memiliki nilai tanggapan tertinggi yaitu bagus berdasarkan penilaian pendengar.
","Piano is one of the musical instruments that is often played by music players and is also favored by music lovers today. Music is created by someone called a composer. The composer will think about what kind of song will be created, namely first determining the genre, the atmosphere that is brought, and the implied message where after the composer will determine several other components such as how long the song is, what scales are used, the tempo of the song, what keys will be used, and other components.
Currently, piano music composition can be done automatically by using genetic algorithms, long short-term memory networks (LSTM), recurrent neural network (RNN), and reinforcement learning (RL). LSTM, RNN, and RL require data collection in the form of piano music composition to conduct model training, while in genetic algorithms, the data set is optional because it does not carry out the model training process and the data is only as an additional reference. This research also creates a piano music composition with a maximum duration of 12 seconds, so that by not requiring a model training process, the data in the form of a reference composition is optional, and the length of the composition duration makes the genetic algorithm chosen to be used.
This research develops a piano music composition from one key to two keys using a genetic algorithm. The composition process consists of individual initialization, fitness value calculation, selection, and mutation. The output of the program is in the form of sheet music from the composition and audio of the music composition. Assessment of musical compositions is done by giving 4 different compositions to several audiences to be assessed. The result of this research is that the resulting piano music composition gets a sufficient and good response value and a composition that uses reference compositions and fitness value criteria has the highest value based on the listener's assessment.
",,,164335,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217818,,MUHAMMAD LUTFI F,Penggunaan FastText dan Word2Vec untuk Sistem Rekomendasi Kelanjutan Daftar Putar Lagu Otomatis,,,"Sistem Rekomendasi, Musik, Daftar Putar Lagu, Pemrosesan Bahasa Natural, Word Embedding, FastText, Word2Vec","Dr. Sri Mulyana, M.Kom.",2,3,0,2022,1,"Musik dan daftar lagu (playlist) merupakan dua hal yang sedang mengalami peningkatan minat. Playlist lagu biasanya terdiri dari dua hal yaitu judul playlist dan daftar lagu di dalamnya. Automatic playlist continuation (APC) merupakan salah satu tugas dari sistem rekomendasi yang bertujuan untuk menambahkan lagu baru dengan karakter sesuai dengan kondisi playlist pada awalnya. Permasalahan dari berbagai sistem APC yang telah dikembangkan sebelumnya adalah pengolahan judul playlist tidak efektif, cold-start problem, dan kompleksitas sistem.
Penelitian ini mencoba membangun sistem APC dengan input yang sederhana menggunakan arsitektur FastText dan Word2Vec. Kedua arsitektur ini dapat menghasilkan word embedding dari setiap kata (word) yang dipelajari. Word embedding merupakan cara untuk merepresentasikan bentuk kata yang awalnya berupa teks menjadi bentuk vektor. Dengan menggunakan informasi bentuk vektor dari word ini, setiap dua word dapat dicari nilai kesamaannya atau kedekatannya sehingga dapat dijadikan sebagai dasar rekomendasi.
Sistem yang dikembangkan menghasilkan nilai R-Precision sebesar 0,2092, 0,2551, 0,2637, dan 0,2257 dan nilai NDCG sebesar 0,0217, 0,0262, 0,0266, dan 0,0245 untuk kondisi cold-start hanya judul playlist, judul playlist dan 1 track, judul playlist dan 5 track, dan hanya 5 track secara berurutan. Dibandingkan dengan model- model sebelumnya, sistem ini memiliki performa lebih tinggi secara kuantitas yang ditunjukkan oleh nilai R-Precision tetapi lebih rendah secara kualitas pengurutan yang ditunjukkan oleh nilai NDCG.","Music and playlists are two things that are experiencing increasing interest. A song playlist usually consists of two things, namely the title of the playlist and the list of songs in it. Automatic playlist continuation (APC) is a version of a recommendation system whose purpose is to add new songs or songs with suitable characteristics to the original playlist condition. The problem of recent various previously developed APC systems is not efficient at playlist title processing, cold-start problem, and system complexity.
This research tried to build an APC system with simple inputs using FastText and Word2Vec architecture. Both of these architectures can produce word embedding of each word that is trained. Word embedding is a way to represent a word that was originally in text form in vector form. By using the vector form information of this word, every two words can be searched for the similarity or proximity values which can be used as the basis for recommendations.
The developed system resulted in R-Precision values of 0.2092, 0.2551, 0.2637, and 0.2257 and NDCG values of 0.0217, 0.0262, 0.0266, and 0.0245 for cold-start conditions only title, title and 1 song, title and 5 songs, and only 5 songs consecutively. Compared to other previously developed models, this system has better performance at the quantity shown by its R-Precision value but has lower performance at sorting quality shown by its NDCG value.",,,169046,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
219356,,SALFARIZI KAMIL,Effective Image Colorization Method based on Generative Adversarial Network Method and Convolutional Neural Network Method,,,"Image colorization, Generative Adversarial Network, Convolutional Neural Network","Moh. Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2022,1,"Pewarnaan gambar telah digunakan melalui berbagai cara untuk mendapatkan gambar berwarna yang mendekati warna gambar aslinya. Perkembangan teknik pewarnaan gambar telah berkembang dari input manual oleh orang menggunakan alat lukis dan komputer hingga menggunakan cara yang lebih modern seperti pewarnaan gambar otomatis menggunakan kecerdasan buatan. Namun, dengan diperkenalkannya penggunaan kecerdasan buatan untuk mewarnai gambar, banyak metode pewarnaan telah dibuat, masing-masing menghasilkan gambar berwarna yang berbeda.
Hasil seperti itu dapat bervariasi dari warna yang salah, warna objek yang bercampur dengan objek lain yang memiliki warna berbeda, objek yang tidak diwarnai dan banyak lagi. Oleh karena itu, penelitian ini bermaksud untuk menentukan metode antara Generative Adversarial Network (GAN) dan Convolutional Neural Network (CNN) yang menghasilkan hasil yang lebih baik dengan cara membandingkan hasilnya satu sama lain dengan menggunakan gambar yang diperoleh dari COCO maupun gambar pribadi.
Dalam penelitian ini, penulis menentukan keefektifan kedua metode secara visual dan kuantitatif dengan membandingkan perbedaan nilai warna gambar asli dan gambar yang dihasilkan, dengan nilai yang lebih tinggi menentukan akurasi yang buruk dan nilai yang lebih rendah berarti akurasi yang baik. Dari penelitian ini menunjukkan bahwa GAN melakukan pewarnaan gambar 5% lebih baik dari CNN secara kuantitatif dengan nilai masing-masing 118.1481 dan 124.8074, GAN 5% lebih baik dari CNN. Secara visual GAN &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;bekerja lebih baik dalam hal akurasi warna pada gambar yang dihasilkan daripada CNN","Image colorization has been used through different means to achieve a colorized image that is close to the original image colors. The development of image colorization techniques has evolved from manual input by people using painting tools to using computer tools to more modern ways such as automatic image colorization using artificial intelligence. However, with the introduction to using artificial intelligence to colorize images, many methods of colorization have been made, each producing various results. 
	Such results can vary from colors being incorrect, a color of an object blending in with another object that has a different color, an object simply not being colorized and many more. Therefore, this research intends to determine which method between Generative Adversarial Network (GAN) and Convolutional Neural Network (CNN) produces better results by comparing the results to one another by using images obtained from COCO as well as personal images.
	In this research, the author determines the effectiveness of both methods visually and quantitatively by comparing the difference in color value of the original image and generated image, with higher values determining bad accuracy and lower values meaning good accuracy. From this research, it shows that GAN performs 5% better than CNN image colorization quantitatively with a value of 118.1481 and 124.8074 respectively, GAN being 5% better than CNN. Visually GAN performs better in regards to the accuracy of the colors in the generated images than CNN.
",,,170670,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
210653,,RAMA APRILLIANT Q,QR Code for Indonesia&acirc;€™s Shipping System (IUP),,,"QR code, Web application, Web software, software development","Azhari, Drs., M.T., Dr",2,3,0,2022,1,"In general, shipping has been part of humanity&acirc;€™s economy since the dawn of
civilization. Moving items from one place to another is a necessity regardless, whether
the intention is to trade or for a gift. Entering the era of free trade, the importance of
shipping became clearer. For example, the factory can be located in one continent and
the seller of the product in another continent. Yet for all its significance, our country
has yet to utilize its maximum capabilities to boost our Trade aspect. One of the aspects
that I view can be improved is the bureaucratic process of trade customs. Our custom
bureaucratic system can be quite slow in its process. My proposal is to streamline the
export/ shipment process by creating a web application that contains data and
information needed to pass form checking and form approval.
The proposed system is a system to create a Web&acirc;€“based application for shipment
administration purpose. Our system is divided into two main parts, namely user and
admin. The system is divided to reduce the workload for the server and to have the port
officials a system that they can be involved with. The user side will refer to the system
that the user ( shipment sender ) can interact and use. The other part of the system will
consist of administration of the port or port officials. They will set the price of the trade
based on regulation made by the government and creates the bill of lading for each
shipment. This will allow the user a simple process to submit their shipment and allow
the admin to monitor the process.
The System is developed based on the web with JavaScript, NodeJs and
PostgreSQL as the database with an approach to designing an architecture with
integration between modules which communicate with each other. The use of our
Shipping system web application is expected to improve the process where time is more
effectively used to create a shipment; as well as being easier to monitor and control
information and ensuring the accuracy of the items to be shipped.","In general, shipping has been part of humanity&acirc;€™s economy since the dawn of
civilization. Moving items from one place to another is a necessity regardless, whether
the intention is to trade or for a gift. Entering the era of free trade, the importance of
shipping became clearer. For example, the factory can be located in one continent and
the seller of the product in another continent. Yet for all its significance, our country
has yet to utilize its maximum capabilities to boost our Trade aspect. One of the aspects
that I view can be improved is the bureaucratic process of trade customs. Our custom
bureaucratic system can be quite slow in its process. My proposal is to streamline the
export/ shipment process by creating a web application that contains data and
information needed to pass form checking and form approval.
The proposed system is a system to create a Web&acirc;€“based application for shipment
administration purpose. Our system is divided into two main parts, namely user and
admin. The system is divided to reduce the workload for the server and to have the port
officials a system that they can be involved with. The user side will refer to the system
that the user ( shipment sender ) can interact and use. The other part of the system will
consist of administration of the port or port officials. They will set the price of the trade
based on regulation made by the government and creates the bill of lading for each
shipment. This will allow the user a simple process to submit their shipment and allow
the admin to monitor the process.
The System is developed based on the web with JavaScript, NodeJs and
PostgreSQL as the database with an approach to designing an architecture with
integration between modules which communicate with each other. The use of our
Shipping system web application is expected to improve the process where time is more
effectively used to create a shipment; as well as being easier to monitor and control
information and ensuring the accuracy of the items to be shipped.",,,161895,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211677,,BAGASKARA PUTRA W,"Diabetes Diagnosis From Electronic Medical Records Using DBSCAN, K-Means Clustering, and Naive Bayes",,,"Diabetes, DBSCAN, k-means clustering, naive bayes","Lukman Heryawan, S.T., M.T. Ph.D",2,3,0,2022,1,"Pandemi COVID-19 membuat pemerintah menetapkan mandat karantina yang
membatasi aktivitas di luar ruangan bagi warga. Perubahan gaya hidup di masa pandemi
termasuk kurang berolahraga dan perubahan kebiasaan makan mengakibatkan
peningkatan risiko terkena diabetes. Kunci untuk mengatasi diabetes adalah deteksi
dini. Pendekatan pembelajaran mesin dapat digunakan untuk membantu dokter
mengidentifikasi pasien diabetes melalui data rekam medis elektronik. Penelitianpenelitian
yang ada dengan menggunakan metode supervised learning memiliki akurasi
yang rendah yang disebabkan oleh kesalahan data.
Pendekatan hybrid unsupervised-supervised learning dapat mengatasi masalah
ini dengan menggunakan pendekatan unsupervised learning untuk memproses data
terlebih dahulu sehingga pengklasifikasi supervised dapat bekerja lebih baik. Penelitian
ini menggunakan DBSCAN dan k-means clustering dengan tujuan untuk menghilangkan
data outlier dan noise dari dataset Pima Indians Diabetes Database. DBSCAN
mengelompokkan data mentah menggunakan epsilon dan titik minimum yang
ditentukan untuk menghapus data apa pun yang tidak mengelompok. K-means clustering
kemudian mengelompokkan data yang dihasilkan untuk mengelompokkan data
serupa di mana data yang berbeda dari mayoritas dalam cluster dianggap sebagai
noise dan dihapus. Data bersih diterapkan pada pengklasifikasi Naive Bayes.
Dari percobaan, metode yang diusulkan memperoleh akurasi rata-rata 98,0%,
precision rata-rata 99,3%, recall rata-rata 97,9%, dan skor F1 98,6%. Metode yang
diusulkan memiliki performa lebih baik dibanding metode-metode penelitian sebelumnya
dengan pendekatan serupa.","The COVID-19 pandemic has led to the government establishing a quarantine
mandate that limits outdoor activities for citizens. The change in lifestyle during
the pandemic era including less exercise and changes in eating habits resulted in a
surge in the risk of people contracting diabetes. The key to treating diabetes is early
detection. A machine learning approach can be used to help doctors identify patients
having diabetes through electronic medical records data. Existing researches using
supervised learning methods have low accuracy that are caused by errors in the data.
A hybrid unsupervised-supervised learning can solve this issue by using unsupervised
learning approaches to preprocess the data so that the supervised classifier
can perform better. This research attempts to use DBSCAN and K-means clustering
for the purpose of removing outliers and noisy data from the Pima Indians Diabetes
Database dataset. DBSCAN clusters the raw data using specified epsilon and minimum
points to remove any data that are not clustered. K-means clustering then
clusters the resulting data to group similar data together where data that are different
from the majority within the clusters are considered noisy and removed. The clean
dataset is applied on a Naive Bayes classifier.
From the experiment, the proposed method obtained an average accuracy of
98.0%, average precision of 99.3%, average recall of 97.9%, and an F1-score of
98.6%. The proposed method yields a better performance when compared to past
researches with similar approaches.",,,162724,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
208350,,KEVIN TITO ALFARISSY,DYNAMIC PRIORITIZATION ON MULTI-THREADED DATA TRANSFER,,,"multithread, data transfer","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D",2,3,0,2022,1,"Distribusi digital dari perangkat lunak, musik, dan video dari semua jenis telah didahulukan daripada distribusi tradisional dari contoh-contoh yang disebutkan. Internet saat ini adalah bagian dari kehidupan sehari-hari, prioritas ini telah mengubah fokus seluruh industri, membangun karier banyak artis dan penghibur independen, dan evolusi media tradisional seperti program berita dan hiburan pindah ke model berdasarkan permintaan dengan berbasis langganan atau pendapatan berbasis iklan elektronik. Relevansi kehidupan digital semakin menguat dalam 2 tahun terakhir karena Covid-19, di mana pandemi memaksa orang untuk melakukan aktivitas sehari-hari dari rumah.

Dalam metode distribusi terdesentralisasi seperti berbagi file Peer-to-Peer (P2P) (protokol komunikasi BitTorrent), di mana file didistribusikan melalui jaringan rekan yang terdesentralisasi, tidak perlu mengunduh ulang seluruh file atau file terkompresi jika satu atau lebih banyak paket yang berisi informasi relevan dari file tertentu rusak selama transfer. Metode ini dibatasi oleh kemampuan unggah orang lain, tetapi pada saat yang sama jika cukup banyak orang yang &quot;menyemai&quot; torrent, pembatasan kemampuan unggah itu didistribusikan di antara &quot;penyemai&quot;. Metode ini populer saat ini, tetapi juga identik dengan distribusi perangkat lunak bajakan dan salinan ilegal film dan serial tv, misalnya, itulah sebabnya beberapa negara telah menutup situs web torrent, negara-negara ini adalah Rusia, Cina, Australia, AS, Malaysia , Portugal, Italia, Afrika Selatan, dan Latvia. Meskipun tindakan torrent itu sendiri tidak ilegal, penggunaannya untuk mendistribusikan salinan ilegal perangkat lunak, film, musik, dll. adalah alasan mengapa negara-negara memilih untuk melakukan tindakan seperti mencegah akses ke situs torrent atau langsung menutup operasi mereka melalui jalur hukum. cara.

Dalam penelitian ini, penulis mencoba untuk mengimplementasikan prioritas dinamis dari transfer data multithreaded yang dihosting di situs file-sharing ke klien lokal. Proses kemudian akan menghasilkan daftar prioritas dan secara dinamis akan mengubah prioritas di tengah proses pengunduhan. Data kemudian akan dipisahkan menjadi &quot;bagian&quot; di mana situs file-hosting akan memberikan bagian yang berbeda dari paket secara bersamaan kepada klien dan mengukur kinerja tindakan tersebut dengan mengukur waktu yang diperlukan untuk menyelesaikan proses pengunduhan dan membandingkannya dengan yang lain. metode.

Hasil penelitian ini cukup dapat diprediksi, sementara waktu pengunduhan massal untuk semua aplikasi berada dalam toleransi yang wajar satu sama lain pada pengunduhan massal (di mana unduhan multithreaded praktis menyerahkan keuntungannya), dan di mana multithreading berperan dengan multithreaded pengunduh, unduhan berurutan memberikan keunggulan multithreading dibandingkan peramban web konvensional. Aplikasi yang dikembangkan untuk percobaan ini tidak memberikan keunggulan dibandingkan perangkat lunak yang tersedia secara komersial dalam hal total waktu untuk menyelesaikan proses pengunduhan, tetapi memiliki keunggulan dalam aspek lain dari proses pengunduhan di mana aplikasi untuk penelitian ini mengelola sumber daya yang tersedia lebih efisien.","Digital distribution of software, music, and videos of all kinds has taken precedence over the traditional distribution of mentioned examples. Internet nowadays is a part of everyday lives, this precedence has revamped an entire industrys focus, built careers of many independent artists and entertainers, and the evolution of traditional media such as news and entertainment programs moved to an on-demand model with subscription-based or electronic advertisement-based revenue. The relevance of digital lives are amplified in the past 2 years due to Covid-19, where the pandemic forced people to do their usual day-to-day activities from home.

In a decentralized distribution method such as Peer-to-Peer (P2P) file sharing (BitTorrent communication protocol), where file(s) were distributed via a decentralized network of peers, it is unnecessary to redownload entire file or compressed files if one or more of the packets containing relevant information of a particular file were corrupted during transfer. This method is limited by the upload capabilities of others, but at the same time if enough people are seeding the torrent the limitation of that upload capabilities are distributed amongst the seeders. This method is popular today, but also synonymous with the distribution of pirated software and illegal copies of movies and tv series, for example, that is why several countries have shut down torrenting websites, these countries are Russia, China, Australia, USA, Malaysia, Portugal, Italy, South Africa, and Latvia. While the act of torrenting itself is not illegal, its use for distributing illegal copies of software, movies, music, etc. are the reason why countries opt to do an action such as preventing access to torrenting sites or outright shut down their operation through legal means.

	In this research, the writer attempts to implement dynamic prioritization of multithreaded data transfer hosted on a file-sharing site into the local client. The process would then produce a prioritization list and would dynamically change the priority in the middle of the download process. The data would then be segregated into parts in which the file-hosting site would provide different parts of the packet simultaneously to the client and measure the performance of such action by measuring the required time to finish the download process and comparing it to other methods. 

	 The result of this research has been quite predictable, while the bulk download time for all applications is within a reasonable tolerance of each other on the bulk download (where multithreaded download practically gives up its advantage), and where multithreading comes into play with the multithreaded downloader, a sequential download gives multithreading advantage against conventional web browsers. The application developed for this experiment does not provide an advantage against the commercially available software in terms of total time to complete the download process, but it does have an advantage in another aspect of the download process where the application for this research managed the resources available more efficiently.",,,159576,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
213214,,ALVIN JANUAR RAMADAN,A Deep Learning Approach with Neural Networks to Recognise Javanese Dialects,,,"javanese, dialect identification, dialects, neural network, deep learning","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Sudah terdapat banyak topik riset berbahasa Jawa di berbagai jurnal dan dunia akademis. Walaupun begitu, bentuk riset yang dilakukan sebagian besar merupakan studi perbandingan antara dua dialek yang berbeda atau di suatu daerah. Terlebih juga, layanan penerjemah juga sudah mulai menawarkan Bahasa Jawa sebagai salah satu bahasa yang didukung dalam layanan tersebut. Dengan lebih dari 98 juta penutur, sudah sepantasnya dibutuhkan sebuah model yang mampu membedakan berbagai macam dialek Bahasa Jawa. Berbagai macam pihak seperti perseorangan dan juga penyedia jasa seperti translator dapat memahami konteks yang berbeda ketika pihak-pihak tersebut memahami dialek yang sedang digunakan.

Riset ini bertujuan untuk membuat sebuah model yang mampu memahami satu-satu dari berbagai macam dialek Bahasa Jawa. Hal seperti ini sangat dimungkinkan dengan menggunakan berbagai macam metode Deep Learning untuk mencari tahu macam-macam pola dari masing-masing dialek Bahasa Jawa. Dengan tersedianya subset dari dataset korpora EVA dari Max Planck Institute yang berfokus kepada pemetaan dialek Bahasa Jawa, riset ini berfokus dalam pembuatan sebuah model jaringan syaraf tiruan yang berbasiskan penelitian dari bahasa-bahasa lainnya.

Berbasis kepada arsitektur jaringan syaraf tiruan dari berbagai macam artikel jurnal terpublikasi, kami membandingkan berbagai macam tipe jaringan syaraf tiruan. Dengan menggunakan beberapa model seperti fully-connected deep neural network, convolutional neural network hingga convolutional recurrent neural network, model yang kami gunakan mampu mengidentifikasi berbagai macam dialek Bahasa Jawa dengan ketepatan validasi mulai dari 30% hingga 97.6%. Dibandingkan dengan berbagai macam artikel jurnal yang menjadi basis dari penelitian kami, performa yang diraih oleh riset ini disebabkan oleh beberapa keputusan dalam tahap preprocessing dan penggunaan lapisan jaringan syaraf tiruan, seperti normalisasi suara, regularisasi Ridge, batch normalisation, dan lapisan Dropout.","The research topic of the Javanese language is abundant in journals and the academic world. However, such research projects turn out to be mostly about comparative studies of either two dialects or in a particular area. Moreover, translation services are also offering Javanese as a supported language. With over 98 million speakers, the need for a Javanese dialect identification model has been more significant than ever. People and service providers such as translators can better understand the context with the dialects in mind.

This research aims to recognise dialects from one of the many available dialects in the Javanese language. It is possible by using deep learning methods to figure out patterns in each of the Javanese dialects. With the readily available subset of the EVA corpora by the Max Planck Institute on Javanese dialect mapping, this research focuses on constructing a neural network model based on breakthroughs in other languages.

By referring to neural network architectures from established journal articles, we compare different types of neural networks. Using several models ranging from fully-connected deep neural networks, convolutional neural networks up to convolutional recurrent neural networks, our inferred models can identify Javanese dialects very well with validation accuracy ranging from 30% up to 97.6%. Compared to articles cited in this article, this is due to several preprocessing steps and layer usage decisions, such as sound normalisation before input, ridge regularisation, batch normalization and dropout layers.",,,164322,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
218592,,LUQMAN BAYU S,Text-based Chatbot for Searching Restaurants With Attributes Filtering According to User Choice Behaviour Using Dialogflow and Web Services,,,"Chatbot, Dialogflow, Restaurant search, Halal tourism, Web services, Search attributes filtering","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2022,1,"Jumlah pengguna yang memanfaatkan chatbot untuk pengalaman pencarian telah populer dalam beberapa tahun terakhir, sebagian besar digunakan untuk memenuhi respons cepat dari pertanyaan singkat. Di antara kelebihan chatbot adalah dapat menjawab respons otomatis untuk produk atau layanan tertentu. Bahkan, populasi umat Islam diprediksi akan meningkat di tahun-tahun mendatang, dengan banyak negara di antaranya Indonesia yang beradaptasi dengan wisata halal untuk pasar Muslim. Untuk sebagian besar aplikasi pengantaran makanan dan restoran berdasarkan permintaan, mungkin sulit bagi pengguna untuk mensegmentasikan restoran yang mereka inginkan yang memenuhi preferensi mereka dalam hal restoran yang menyajikan produk halal dan non-halal. Di atas banyaknya platform informasi restoran, Zomato, sebuah aplikasi dan website berbasis informasi restoran memberikan informasi status halal masing-masing restoran hingga lingkup tertentu.

Pada penelitian ini akan dikembangkan sebuah chatbot dimana pengguna dapat mencari preferensi tertentu dengan menggunakan beberapa atribut untuk penyaringan, terutama jika pengguna akan memiliki restoran yang menyajikan produk halal dan sebaliknya. Pemfilteran atribut akan dilakukan sehingga pengalaman pencarian dapat lebih akurat sesuai permintaan pengguna akhir. Chatbot itu sendiri akan menjadi bot berbasis aturan yang dibangun menggunakan Dialogflow dengan memanfaatkan layanan web untuk pengambilan informasi dari data web-scraped yang disimpan dalam database dan karenanya memiliki kemampuan dalam mengembalikan respons yang dinamis. Tahap akhir dari penelitian ini akan mengevaluasi prototipe berdasarkan beberapa pendekatan evaluasi chatbot yang direferensikan.

Penelitian ini berhasil membangun sebuah chatbot yang dapat memfilter pilihan restoran berdasarkan atribut tertentu. Uji kegunaan dilakukan untuk mengukur efektivitas, efisiensi, dan kepuasan sistem dengan bantuan beberapa peserta untuk menguji sistem. Hasilnya mampu mencapai tingkat keberhasilan 75% dilakukan dengan menggunakan uji akurasi. Sedangkan uji efisiensi mampu menyimpulkan hasil 5,5 kali lebih cepat dengan melakukan uji perbandingan dengan platform yang ada untuk mencapai tujuan yang sama. Terakhir, kepuasan pengguna dilakukan dengan menggunakan System Usability Scale (SUS) yang mencapai skor akseptabilitas 88,75.","The amount of users utilizing chatbot for search experience have been popular in the recent years, they are mostly used to fulfil quick responses from short questions. Among the advantages of chatbot is that it may answer automated responses for a certain product or even service. In fact, the population of Muslims is predicted to increase in the coming years, with many countries among them Indonesia adapting to halal tourism for Muslim markets. For most on-demand food delivery and restaurant applications, it could be difficult for users to segmentate their desired restaurants that fulfils their preferences in terms of the restaurant serving halal and non-halal products. Above the many restaurant information platforms, Zomato, a restaurant information based application and website provides information on each restaurant of its halal status to a certain scope.

In this research, a chatbot will be developed in which users may search for specific preferences using several attributes for filtering, especially if the user would have restaurants serving halal products and vice versa. Attributes filtering will be performed so that the search experience may be more accurate to the end- user&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s request. The chatbot itself will be a rule-based bot built using Dialogflow by utilizing web services for information retrieval from a web-scraped data stored in a database and therefore has the ability in returning dynamic responses. The final stage of the research will be evaluating the prototype according to some referenced chatbot evaluation approaches.

This research has successfully constructed a chatbot that can filter restaurant choices based on certain attributes. Usability test is conducted to measure the system&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s effectiveness, efficiency, and satisfaction with the aid of multiple participants to test the system. The result is able to achieve a 75% success rate performed using accuracy test. While efficiency test is able to conclude a 5,5 times faster result by doing comparison test with an existing platform to achieve a same goal. Finally, user satisfaction is carried out using System Usability Scale (SUS) which achieved a 88,75 acceptability score.
",,,170053,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
210657,,HAMMAM MAHFUZH S,An Automatic Data Mapping for Interoperability of OpenEMR Electronic Medical Records System using FHIR,,,"OpenEMR, electronic medical records, FHIR, interoperability, classification","Lukman Heryawan, S.T., M.T., Ph.D.",2,3,0,2022,1,"Keberadaan Rekam Medis Elektronik (RME) masih menimbulkan tantangan baru. Beberapa di antaranya terkait dengan pertukaran data antar fasilitas kesehatan. Salah satu contoh masalah pertukaran data ini adalah adanya perbedaan struktur data yang digunakan di RME, yang menyebabkan ketidakcocokan. Kompatibilitas data seharusnya menguntungkan, terutama bagi para praktisi medis seperti dokter, sehingga mereka dapat memberikan keputusan yang lebih akurat tentang tindakan medis apa yang harus dilakukan untuk pasien mereka, karena perawatan atau pengobatan yang tepat akan meningkatkan kemungkinan pasien untuk berhasil sembuh dari penyakit mereka. Kesesuaian data antar RME juga dapat disebut sebagai interoperabilitas. Penelitian ini mencoba menerapkan interoperabilitas data kesehatan dengan mengimplementasikan pemetaan otomatis data RME dari salah satu sistem manajemen RME yang bernama OpenEMR, sehingga datanya dapat memenuhi standar FHIR. Secara khusus, classifier untuk mengategorikan data OpenEMR ke dalam tipe FHIR yang sesuai, dibahas dalam penelitian ini. Tiga classifier dikembangkan di Java dan Python, dengan memanfaatkan konsep teknik klasifikasi pembelajaran mesin yang, dalam hal ini, adalah Naive-Bayes dan Decision Tree. Kedua implementasi algoritma pembelajaran mesin tersebut menunjukkan akurasi klasifikasi 100%, yang menghasilkan tambahan penerapan teknik rule-based yang juga menghasilkan akurasi 100%.","The existence of Electronic Medical Records (EMR) still poses new difficulties. Several of them are related to data exchange between healthcare facilities. One example of this data exchange problem would be the difference of data structure used in EMR, which leads to incompatibility. Data compatibility should be advantageous, especially for medical practitioners such as doctors or physicians, so that they can grant a more accurate decision on what treatments should be carried out for their patients, since a precise treatment or medication will increase the chance that patients would successfully heal from their disease. The compatibility of EMR data can also be called interoperability. This research attempts to apply interoperability of healthcare data by implementing an automatic mapper of an EMR data from one EMR management system called OpenEMR so that its data can meet the FHIR standard. Specifically, a classifier to categorize the OpenEMR data into the appropriate FHIR type is discussed in this research. Three classifiers are developed in Java and Python, which utilize the concepts of machine learning classification techniques which, in this case, are Naive-Bayes and Decision Tree. Both implementations of machine learning algorithm showed a classification accuracy of 100%, which resulted in the additional implementation of rule-based technique that also resulted in 100% accuracy.",,,161797,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
217573,,ZUL FAZA MAKARIMA,Klasifikasi Esai Jawaban Pendaftar Iisma 2022 Menggunakan Text Mining,,,"BERT, Klasifikasi Dokumen Teks","Medi, Drs. M.Kom.;Andi Dharmawan, S.Si., M.Cs., Dr.",2,3,0,2022,1,"IISMA atau Indonesian International Student Mobility Awards adalah Salah satu program dari kurikulum Merdeka Belajar - Kampus Merdeka yang mengirimkan mahasiswa dan mahasiswi terbaik bangsa untuk berkuliah di universitas luar negeri selama 1 semester. Dengan meningkatnya pendaftar pada setiap tahunnya, maka beban panitia untuk menyeleksi esai pendaftar pun meningkat. Namun sumber daya manusia dan waktu yang dimiliki panitia sangat terbatas, sehingga pengklasifikasi esai perlu dibuat agar panitia tidak perlu membaca semua esai dan mempercepat proses penyeleksian.
Pada penelitian ini dilakukan klasifikasi esai pendaftar IISMA tahun 2022 menggunakan metode BERT dengan pendekatan 3 label dan 2+2 label dan nilai performa yang dicari adalah akurasi, presisi, recall, dan F1-Score. Data yang digunakan adalah sejumlah 2304 dengan komposisi yang tidak seimbang. Esai yang digunakan adalah esai dengan topik &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;Explain about your health condition (i.e., dental and oral treatment/medical treatment/covid-19 medication/mental health therapy/counseling)!&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;.
Hasil dari penelitian ini adalah performa berupa f1-score untuk model dengan pendekatan 3 label pada masing-masing kelasnya adalah adalah 80% untuk kelas aman, 70% untuk kelas meragukan, dan 50% untuk kelas berisiko. Serta f1- score untuk model dengan pendekatan 2+2 label pada masing-masing kelasnya adalah 88% untuk kelas aman, 70% untuk kelas meragukan, dan 57% untuk kelas berisiko.","IISMA or Indonesian International Student Mobility Awards is one of the programs from the Merdeka Learning curriculum - Merdeka Campus which sends the nation's best students and students to study at foreign universities for 1 semester. With the increase in registrants every year, the committee's burden to select applicants' essays also increases. However, the committee's human resources and time are very limited, so an essay classifier needs to be made so that the committee does not need to read all the essays and speed up the selection process.
In this study, the essay classification of IISMA registrants in 2022 was carried out using the BERT method with a 3 label and 2+2 label approach and the performance values sought were accuracy, precision, recall, and F1-Score. The data used are 2304 with an unbalanced composition. The essay used is an essay with the topic &Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;Explain about your health condition (i.e., dental and oral treatment/medical treatment/covid-19 medication/mental health therapy/counseling)!&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;.
The result of this research is that the performance in the form of f1-score for the model with a 3 label approach in each class is 80% for the safe class, 70% for the doubtful class, and 50% for the risk class. And the f1-score for the model with a 2+2 label approach in each class is 88% for the safe class, 70% for the questionable class, and 57% for the risk class.",,,168768,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212974,,THARIQ ISKANDAR ZMP,Model Klasifikasi Berbasis Multiclass Classification dengan Kombinasi IndoBERT Embedding dan Long Short-Term Memory untuk Tweet Berbahasa Indonesia,,,"Klasifikasi Teks, Tweet Berbahasa Indonesia, IndoBERT, Long Short-Term Memory","Dr. Suprapto, M.I.Kom.",2,3,0,2022,1,"Pencarian tweet pada aplikasi Twitter menggunakan kata kunci atau hashtag terkadang masih kurang akurat ketika menggunakan kata yang memiliki beberapa arti. Untuk itu diperlukan pengembangan model yang dapat mengategorikan tweet sesuai dengan konteksnya. Klasifikasi teks merupakan salah satu tugas pemrosesan bahasa alami yang dapat mengategorikan teks secara otomatis berdasarkan konteksnya dengan bantuan metode machine learning atau deep learning. Sudah terdapat beberapa penelitian tentang pengembangan model klasifikasi teks pada dataset berbahasa Indonesia, namun masih mungkin untuk ditingkatkan akurasinya. Penelitian ini bertujuan untuk meningkatkan performa model klasifikasi teks dari penelitian sebelumnya, dengan mengombinasikan model pre-trained IndoBERT dengan arsitektur Long Short-Term Memory (LSTM) dalam mengklasifikasikan tweet berbahasa Indonesia ke beberapa kategori. Model IndoBERT-LSTM dengan skenario kombinasi hyperparameter terbaik (batch size  sebesar 16, learning rate sebesar 2e-5, dan menggunakan average pooling) berhasil mendapatkan F1-score sebesar 98,90% pada dataset yang tidak termodifikasi (peningkatan 0,70% dari model Word2Vec-LSTM dan  0,40% dari model fine-tuned IndoBERT) dan 92,83% pada dataset yang telah termodifikasi (peningkatan 4,51% dari model Word2Vec-LSTM dan 0,69% dari model fine-tuned IndoBERT). Akan tetapi, peningkatan dari model fine-tuned IndoBERT tidak terlalu signifikan dan model Word2Vec-LSTM memiliki total waktu pelatihan yang jauh lebih cepat.","Searching tweets on the Twitter application using keywords or hashtags is sometimes still less accurate when using words that have multiple meanings. This requires the development of a model that can categorize tweets according to their context. Text classification is one of the natural language processing tasks that can automatically categorize text based on its context with the help of machine learning or deep learning methods. There have been several previous studies regarding the development of a text classification model on Indonesian language datasets, but it is still possible to improve its accuracy. This research aims to improve the performance of the text classification model from previous studies, by combining the IndoBERT pre-trained model with the Long Short-Term Memory (LSTM) architecture in classifying Indonesian-language tweets into several categories. The IndoBERT-LSTM model with the best hyperparameter combination scenario (batch size of 16, learning rate of 2e-5, and using average pooling) managed to get an F1-score of 98.90% on the unmodified dataset (0.70% increase from the Word2Vec-LSTM model and 0.40% from the fine-tuned IndoBERT model) and 92.83% on the modified dataset (4.51% increase from the Word2Vec-LSTM model and 0.69% from the fine-tuned IndoBERT model). However, the improvement from the fine-tuned IndoBERT model is not very significant and the Word2Vec-LSTM model has a much faster total training time.",,,163969,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
211439,,RIZQI PANGESTU,METODE KLASIFIKASI KUALITAS LAS BUSUR MENGGUNAKAN SEMANTIC SEGMENTATION BERBASIS DEEP LEARNING,,,"SemanticSegmentation,WeldDefectDetection,CNN,ResNet,DeepLabV3","Dr. Andi Dharmawan, S.Si., M.Cs; Wahyono, S.Kom, Ph.D.",2,3,0,2022,1,"Las merupakan teknik penyatuan dua buah atau lebih material logam dengan menggunakan suhu yang tinggi untuk mencairkannya. Teknologi las pada zaman sekarang memiliki alat dan teknik yang bermacam-macam. Bahkan untuk mate- rial atau bagian yang berbeda pun memiliki teknik khusus agar memberikan hasil yang baik. Proses pengerjaan las pun sudah banyak yang menggunakan mesin robot yang hasilnya lebih presisi dibandingkan manusia. Dunia pabrik yang memproduksi barang-barang berat tidak luput dari teknologi las, setiap pabrik yang memproduksi barang-barang berat tentunya akan melakukan proses pengelasan secara massal diser- tai dengan quality controlnya.
Penelitian ini berfokus untuk mengembangkan suatu sistem yang menggunak- an semantic segmentation untuk mendeteksi kualitas suatu las berbasis citra kamera RGB secara real time. Pada penelitian ini digunakan dataset berupa citra las yang dikumpulkan dari beberapa bengkel las yang dianotasikan dengan bantuan pakar las.
Dari penelitian ini dihasilkan sebuah sistem yang mampu mendeteksi kuali- tas las dari input citra RGB dengan output apakah kualitas las yang ada di dalam citra baik tau buruk menggunakan model UNet, ResNet50, dan ResNet101. Sistem yang dibuat memiliki nilai performa 84.8% untuk UNet, 67,7% untuk ResNet50, dan 63,9% untuk ResNet101.","Welding is a technique of joining two or more metal materials using high temperatures to melt them. Welding technology today has a variety of tools and techniques. Even different materials or parts have special techniques to give good results. There are also many welding processes that use robotic machines whose results are more precise than humans. The world of factories that produce heavy goods does not escape welding technology, every factory that produces heavy goods will of course carry out the welding process in bulk accompanied by quality control.
This research focuses on developing a system that uses semantic segmentation to detect the quality of a weld based on RGB camera images in real time. This research uses a dataset in the form of welding images collected from several welding workshops which are annotated with the help of welding experts.
This research resulted in a system capable of detecting the welding quality from the input RGB image with the output whether the weld quality in the image is good or bad using the UNet, ResNet50, and ResNet101 models. The system created has a performance value of 84.8% for UNet, 67.7% for ResNet50, and 63.9% for ResNet101.
",,,162555,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212210,,FADHILA AUFA F,Headline Generation for Bahasa Indonesia using Encoder-Decoder,,,"Encoder-Decoder, Headline Generation, Bahasa Indonesia","Mhd. Reza M. I. Pulungan, S.Si., M.Sc., Dr-Ing.; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2022,1,"Judul adalah kesan pertama tentang bagaimana pembaca akan membaca sebuah artikel. Ini mempengaruhi bagaimana artikel tersebut akan dianggap dan apa yang diharapkan pembaca dalam artikel tersebut. Saat ini, masalah dengan headline adalah bahwa beberapa headline tidak benar-benar mewakili artikel yang sebenarnya, ini dikenal sebagai headline clickbait. Selama bertahun-tahun dengan meningkatnya jumlah jurnalisme online, banyak penerbit menggunakan headline clickbait untuk menarik lebih banyak pembaca daripada menyampaikan informasi. Studi tentang pembuatan headline telah dilakukan untuk mencoba dan memberikan headline yang mewakili arti sebenarnya dari artikelnya. Namun, sebagian besar studi tersebut terfokus pada bahasa Inggris dan masih kurang pada bahasa lain.
Penelitian ini mempelajari kasus pembuatan headline dan mencoba mengimplementasikan beberapa model sebelumnya khususnya model encoder-decoder. Model tersebut akan dilatih menggunakan dataset artikel berita berbahasa Indonesia. Dataset &quot;CLICK-ID&quot; (William &amp; Sari, 2020) merupakan kumpulan headline berita Indonesia yang terdiri dari 46.517 headline yang dikumpulkan dari 12 penerbit berita lokal Indonesia. Berdasarkan percobaan dalam penelitian ini, hasilnya bervariasi berdasarkan jumlah maksimum konten yang dapat digunakan. Dalam penelitian ini, panjang konten maksimum 100 kata digunakan dan menghasilkan akurasi model sebesar 80% dan skor ROUGE 0,50.","Headlines are the first impression on how a reader will read an article. It effects on how the article will be regarded and what the reader will expect in the article. Nowadays, the problem with headlines is that some headlines don't truly represent the true article, this is known as clickbait headlines. Over the years with the increasing numbers of online journalism, lots of publishers use clickbait headlines to attract more readers rather than delivering the information. Studies on headline generation has been made to try and deliver a headline which represents the true meaning of its article. However, the majority of these studies are focused on English and still lacks in other languages.
This research studies the case of headline generation and tries to implement some previous models especially the encoder-decoder model. The model will be trained using a dataset of news articles in Bahasa Indonesia. The &quot;CLICK-ID&quot; (William &amp; Sari, 2020) dataset is a collection of Indonesian news headlines which consists of 46,517 collected headlines from 12 local Indonesian news publishers. Based on the experiments in this research, the results vary based on the maximum amount of the content which can be used. In this research, a maximum content length of 100 words is used and results in a model accuracy of 80% and a ROUGE score of 0.50.",,,163281,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
208116,,M FIKRI HELDIANSYAH,DETEKSI EMOSI PADA TWEET DENGAN MENGGABUNGKAN CONTEXTUALIZED WORD EMBEDDING DAN CONVOLUTIONAL NEURAL NETWORK (CNN),,,"Deteksi emosi, tweet, Convolutional Neural Network, contextualized word embedding, BERT, ELMo, Word2vec.","Drs. Edi Winarko, M.Sc., Ph.D.",2,3,0,2022,1,"Twitter merupakan salah satu sosial media dimana menjadi tempat para penggunanya untuk saling berbagi informasi melalui teks yang memiliki batasan 280 karakter yang disebut tweet. Tweet biasanya juga memiliki emosi para penggunanya dan sudah banyak penelitian dalam mendeteksi emosi pada tweet seperti contohnya dengan menggunakan kombinasi deep learning seperti CNN dengan word embedding seperti Word2vec, namun penelitian terkait deteksi emosi yang menggunakan deep learning dan contextualized word embedding pada tweet berbahasa Indonesia masih sangat terbatas.
Pada penelitian ini dilakukan deteksi emosi pada tweet berbahasa Indonesia dengan mengombinasikan model Convolutional Neural Network(CNN) dan contextualized word embedding seperti BERT dan ELMo serta traditional word embedding Word2vec sebagai ekstraksi fitur. Terdapat lima emosi yang akan dideteksi yaitu emosi marah (anger), cinta (love), takut (fear), bahagia (happy), dan sedih (sadness). Pengujian deteksi emosi pada ketiga model yaitu; BERT-CNN, ELMo-CNN, dan Word2vec-CNN, memberikan hasil paling baik pada model BERT-CNN dengan nilai macro-averaged precision sebesar 75,40, macro-average recall sebesar 71,62, dan macro-averaged f1-score sebesar 72,83. Deteksi emosi yang dilakukan pada data dengan stemming menunjukkan bahwa model Word2vec-CNN dan BERT-CNN tidak lebih baik daripada model yang menggunakan data tanpa stemming, sedangkan pada ELMo-CNN menunjukkan macro-averaged f1-score yang lebih baik pada data dengan stemming.","Twitter is one of the social media which is a place for users to share information with each other through text that has a limit of 280 characters called tweets. Tweets usually also have the emotions of their users and there have been many studies in detecting emotions in tweets, for example by using a combination of deep learning such as CNN with word embedding such as Word2vec, but research related to emotion detection uses deep learning and contextualized word embedding in tweets with Indonesian language is still very limited.
In this study, emotion detection in Indonesian-language tweets be carried out by combining Convolutional Neural Network (CNN) models and contextualized word embedding such as BERT and ELMo as well as traditional word embedding Word2vec as feature extraction. There are five emotions that will be detected, namely anger, love, fear, happy, and sadness. Testing of emotion detection on the three models, namely; BERT-CNN, ELMo-CNN, and Word2vec-CNN, gave the best results on the BERT-CNN model with a macro-averaged precision value of 75.40, a macro-average recall of 71.62, and a macro-averaged f1-score of 72.83. The emotion detection performed on stemming data shows that the Word2vec-CNN and BERT-CNN models are not better than the model using data without stemming, while the ELMo-CNN shows a better macro-averaged f1-score on the stemming data.",,,159799,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
220155,,DHEAN ADJI W. P,Facial Expression Recognition in Low-Resolution Media / Pengenalan Ekspresi Wajah di Media Resolusi Rendah,,,"Facial expressions recognition, efficiency, performance, accuracy, Python, FER2013","Wayhono, S. Kom., Ph.D",2,3,0,2022,1,"Pengenalan wajah telah menjadi alat yang sangat penting dalam banyak aplikasi, khususnya untuk keamanan. Pengawasan, biometrik, dll telah menjadi cara pokok untuk mengamankan data. Memiliki kemampuan mendeteksi ekspresi wajah juga akan menjadi sangat berharga dalam studi perilaku prediktif, terutama dalam membantu kontraterorisme.


Makalah ini akan terdiri dari ulasan dari berbagai sumber dengan tetap menjaga relevansi judul. Berbagai metode dipandang dan dibandingkan. Penelitian ini juga akan mengimplementasikan tiga metode yang sudah ada sebelumnya dan membandingkannya dalam hal efisiensi, performa dan akurasi. Penelitian akan dilakukan dengan menggunakan Python sebagai bahasa pemrograman dasarnya serta berbagai pustaka, dengan menggunakan database FER2013 sebagai sumber daya utamanya.","Face recognition has become quite an essential tool in many applications, specifically for security. Surveillance, biometrics etc have become a staple way of securing data. Having the ability to detect facial expressions would also become very valuable in predictive behavioural studies, especially in helping with counterterrorism. 


This paper would consist of reviews from different sources while still keeping with the relevancy of the title. A variety of methods are looked upon and compared. This research will also implement three pre-existing methods and compare them in terms of efficiency, performance and accuracy. The research will be done using Python as its base programming language as well as a variety of libraries, using the FER2013 database as its main resource.
",,,171442,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
208381,,ALEXANDER PRASETYA,Error Minimized Extreme Learning Machine Pada Prediksi Harga Sewa Properti Airbnb,,,"machine learning, harga sewa properti Airbnb, extreme learning machine, extreme learning machine error minimized.","Dr. Agus Sihabuddin, S.Si., M.Kom",2,3,0,2022,1,"Prediksi dengan pembelajaran mesin merupakan kegiatan menggunakan data-data yang ada di masa lampau dan algoritma pembelajaran mesin tertentu untuk memprediksi suatu keadaan di masa yang akan datang. Salah satu algoritma tersebut adalah Extreme learning machine yang mampu melakukan pembelajaran dengan waktu yang sangat cepat jika dibandingkan algoritma pembelajaran mesin lainnya. Algoritma Extreme learning machine sendiri memiliki banyak varian yang dikembangkan untuk meningkatkan performanya. Kelemahan dari ELM vanilla adalah keharusannya untuk mencari jumlah hidden node yang optimal dengan proses trial &amp; error ,proses ini memakan waktu yang berkali-kali lipat lebih lama.
Pada penelitian ini, akan digunakan algoritma Extreme Learning Machine versi Error Minimized dan kemudian dibandingkan akurasinya dengan versi vanilanya. Data yang akan digunakan adalah data harga sewa properti Airbnb di area London, Inggris pada 16 Februari 2020. Perbandingan yang dilakukan melingkupi waktu pencarian hidden node yang optimal dari model dan akurasi model yang diukur dengan menggunakan metode mean squared error (MSE), root mean squared error (RMSE), dan mean absolute percentage error (MAPE).
Model ELM dibangun untuk mengolah dataset harga sewa property Airbnb di London yang diambil dari situs Insideairbnb.com dan memiliki 79671 jumlah data dengan 21 fitur yang digunakan sebagai data input. Hasil yang didapat menyatakan Error Minimized ELM memiliki performa yang tidak sebaik ELM versi biasa kecuali dalam kasus pelatihan dengan jumlah hidden neuron yang jauh lebih banyak. Selain itu, Error Minimized ELM memakan waktu yang secara signifikan lebih lama dibandingkan ELM Vanila dalam proses pengujian dan menggunakan banyak Random Access Memory.","Prediction with machine learning is an act of using datas that had been collected in the past with a particular machine learning algorithm to predict a future occurance. One of this machine learning algorithms is the Extreme learning machine, it learns within an extremely short time compared to other machine learning algorithms. The Extreme learning machine algorithm itself has many variants that had been developed to increase it's performance. The weakness of the vanilla version of the ELM is in it's need to find the optimal number of hidden node using a trial &amp; error process, this process takes up  time that's multiple times longer.
In this experiment, the Error Minimized version of Extreme Learning Machine algorithm will be used and then it's accuracy will be compared to it's vanilla version. Dataset that will be used in this experiment will be about Airbnb property rent prices in London, England, which is taken on 16 February 2020. Comparison that will be done would include the time it takes to find the optimal number of hidden node from the model and the model's accuracy which will be measured using mean squared error (MSE), root mean squared error (RMSE), dan mean absolute percentage error (MAPE).
An ELM Model is built to process the Airbnb property rent price in London dataset, the dataset is taken from Insideairbnb.com dataset and has 79671 data entry with 21 features that will be used as input data. The result that was obtained shows that Error Minimized ELM did not perform as good as the regular version of ELM  except in training cases where there is much more hidden neurons used.  Other than that, Error Minimized ELM also consumes a significantly longer training time than ELM Vanila during the training process and it also uses a huge amount of Random Access Memory.",,,159954,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
212221,,ARIEF PUJO ARIANTO,Klasifikasi Motif Batik Menggunakan Capsule Neural Network,,,"Batik, Citra, Augmentasi, ANN, CapsNet","Aina Musdholifah, S.Kom., M.Kom., Ph.D; Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2022,1,"Batik adalah salah satu warisan khas dari Indonesia yang telah diturunkan dari
generasi ke generasi selama lebih dari setengah millenia. Indonesia memiliki beragam
jenis motif batik yang tersebar pada semua provinsi, dan setiap motif memiliki pola
yang berbeda yang membuat batik menjadi unik antara motif satu dengan lainnya.
Berdasarkan jenis motif dasar dan sifat keteraturan yang diturunkan dari motif, kain
batik dapat diklasifikasikan berdasarkan motif dasar yang dimiliki.
Penelitian ini bermaksud untuk membuat model CapsNet yang dapat melakukan klasifikasi motif batik dengan lebih baik lagi. Langkah untuk mendapatkan
model yang dapat melakukan klasifikasi batik dengan baik, pengukuran terhadap metric dilakukan untuk mengetahui seberapa baik model CapsNet untuk melakukan klasifikasi dengan melihat nilai akurasi untuk mengetahui seberapa banyak motif batik
yang dapat diklasifikasikan dengan benar oleh model CapsNet. Metode augmentasi
diterapkan dengan tujuan untuk membuat model CapsNet dapat mempelajari lebih
banyak variasi data.
Hasil yang didapatkan dari penelitian ini adalah 90% nilai akurasi dengan
menggunakan model CapsNet. Model ini mendapatkan nilai yang lebih baik dibandingkan dengan model Fully Connected Layer konvensional dengan akurasi 88%.
Oleh karena itu, hasil yang didapatkan dari penelitian ini adalah model CapsNet memberikan hasil yang lebih baik dibandingkan dengan model Fully Connected Layer
yang biasa digunakan dalam melakukan klasifikasi batik.","Batik is one of the distinctive heritages of Indonesia that has been passed down
from generation to generation for more than half a millennium. Indonesia has various
types of batik motifs spread across all provinces, and each motif has a different pattern
that makes batik unique from one motif to another. Based on the type of basic motif
and the nature of the regularity derived from the motif, batik cloth can be classified
based on the basic motif it has.
This study intends to create a CapsNet model that can classify batik motifs
better. The step to get a model that can classify batik well, metric measurements are
made to find out how well the CapsNet model is for classifying by looking at the
accuracy value to find out how many batik motifs can be classified correctly by the
CapsNet model. The augmentation method is applied with the aim of making the
CapsNet model able to study more variations of the data.
The results obtained from this study are 90% accuracy value using the CapsNet model. This model gets a better score than the conventional Fully Connected
Layer model with an accuracy of 88%. Therefore, the results obtained from this
study are that the CapsNet model gives better results than the Fully Connected Layer
model which is commonly used in classifying batik.",,,163592,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201472,,ACHMAD RIYADI,"ANALISIS SENTIMEN KOMENTAR YOUTUBE MENGGUNAKAN ALGORITME NA&Atilde;VE BAYES, DECISION TREE, DAN K-NEAREST NEIGHBOR",,,"Analisis Sentimen, Algoritme Na&Atilde;&macr;ve Bayes, Algoritme K-Nearest Neighbors, Algoritme Decision Tree","Medi., Drs., M.Kom.",2,3,0,2021,1,"Analisis sentimen digunakan untuk menentukan sikap dari seseorang terhadap suatu produk, topik, ulasan, komentar pada sosial media. Untuk toko daring atau pasar daring, ulasan dari pengguna sebelumnya dapat memainkan peran penting dalam proses pengambilan keputusan pelanggan, yang menunjukkan bahwa prediksi sentimen yang akurat dari ulasan tertentu dapat meningkatkan potensi keuntungan.
Metode klasifikasi yang digunakan pada penelitian ini adalah algoritme na&Atilde;&macr;ve bayes, k-nearest neighbors, dan decision tree. Naive Bayes digunakan untuk menghitung nilai kemungkinan dari kelas - kelas yang ada dan ini merupakan isu besar tentang bagaimana mengklasifikasikan data mentah secara rasional untuk meminimalisir suatu resiko. K-Nearest Neighbors (KNN) merupakan algoritma pembelajaran supervised dimana hasil klasifikasi dari data baru yang masuk tergantung dengan nilai paling banyak dari para tetangganya. Decision Tree memiliki keuntungan antara lain mudah dan cepat, menggunakan memori yang sedikit, bisa menangani data yang memiliki noisy, memiliki akurasi yang tinggi, efektif terhadap data yang besar.
Hasil dari penelitian ini adalah algoritme yang memiliki nilai akurasi paling tinggi adalah decision tree dengan nilai 98.66%. Algoritme yang memiliki nilai presisi paling tinggi adalah decision tree dengan nilai 98.91% dimana berarti decision tree memiliki tingkat false positive yang rendah. Algoritme yang memiliki nilai recall paling tinggi adalah na&Atilde;&macr;ve bayes dengan nilai 99.36% dimana berarti na&Atilde;&macr;ve bayes memiliki tingkat false negative yang rendah. Algoritme decision tree lebih unggul diantara algoritme na&Atilde;&macr;ve bayes dan algoritme k-nearest neighbors dalam melakukan analisis sentimen.","Sentiment analysis is used to determine a person's attitude towards a product, topic, review, comment on social media. For online stores or online market, former reviews can play an important role in customer's decision making process, which
indicates that an accurate prediction of sentiment from a certain review can increase potential profit.
The classification method used in this study is the na&Atilde;&macr;ve bayes algorithm, knearest neighbor, and decision tree. Na&Atilde;&macr;ve Bayes is used to calculate the probability value of the existing classes and this is a big issue about how to classify raw data rationally to minimize a risk. K-Nearest Neighbors (KNN) is a supervised learning algorithm where the classification results of the new incoming data depend on the most values of its neighbors. Decision Tree have advantages such as being easy and fast, using little memory, being able to handle noisy data, having high accuracy, being effective with large data.
The result of this research is the algorithm that has the highest accuracy value is the decision tree with a value of 98.66%. The algorithm that has the highest precision value is the decision tree with a value of 98.91% which means that the decision tree has a low false positive rate. The algorithm that has the highest recall value is Naive Bayes with a value of 99.36%, which means that Naive Bayes has a low false negative rate. The decision tree algorithm is superior to the nave Bayes algorithm and the knearest neighbors algorithm in performing sentiment analysis.",,,152799,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
204033,,ALIYYAH NUR AZHARI,Automatic Detection of Helmets on Motorcyclists Using Faster Region Based Convolutional Neural Networks (Faster R - CNN),,,"Object Detection, Faster R - CNN, Machine Vision, Helmet Detection","Wahyono, S.Kom., Ph. D.",2,3,0,2021,1,"Sepeda motor merupakan salah satu alat transportasi sehari - hari karena dengan harga yang terjangkau, masyarakat dari golongan atas hingga bawah mampu memilikinya. Setiap pemilik sepeda motor wajib mempunyai helm agar pengendara terlindungi dari kecelakaan fatal dan juga diwajibkan oleh undang - undang. Namun, tidak banyak pemilik sepeda motor mengikuti peraturan lalu lintas dan cenderung tidak memakai helm, sehingga meremehkan penggunaan helm tersebut. Maka dari itu, sebuah sistem yang dapat mendeteksi pengendara yang memakai helm atau tidak perlu diterpakan agar mengurangi konsumsi waktu bagi pekerja yang memantau kamera pengawas, dan untuk lebih meningkatkan sistem deteksi untuk aplikasi Intelligent Transportation System (ITS) yang mendukung kasus seperti penilangan sepeda motor.
Tujuan dari penelitian ini adalah membuat sebuah model dari salah satu algoritme pendeteksi objek, yaitu Faster R - CNN untuk mendeteksi pengendara yang memakai dan tidak memakai helm. Kumpulan data yang digunakan adalah 2, yakni sebuah rekaman jalanan umum dari sisi samping (custom dataset) dan sebuah rekaman CCTV. Penelitian ini membandingkan akurasi dan mAP dengan menggunakan learning rate yang berbeda, 0.0001 (1e-4) dan 0.00001 (1e-5). Dari penelitian ini menunjukkan bahwa menggunakan learning rate 1e-5 mencapai hasil tertinggi dibandingkan 1e-4, akurasi yang didapat adalah 88.30% dan mAP 87% untuk custom dataset, sedangkan hasil untuk dataset rekaman CCTV mencapai akurasi 93.30% dan mAP 79.4%.","Motorcycle has been a popular choice for a go-to daily mean of transportation due to its low price, making it affordable for high to low-class citizens. Helmets are required for every motorcycle owner so that the rider's head is protected from accidents and is also required by law. However, not many people follow the rules and tend to not wear the helmets, thus underestimating the usage of helmets. Therefore, a system that can detect riders who wears the helmet or not is needed in order to reduce time consumption for workers who monitors the surveillance cameras, and to further improve detection systems in Intelligent Transportation System (ITS) in which they support cases such as motorcycle ticketing. 
In this research, the author has implemented an object detection model to detect the riders who wears and those who does not wear the helmet by using a recording of a street from the side (custom dataset) and a CCTV recording. This research compares the accuracy and the mean average precision (mAP) using different learning rates, that is 0.0001 (1e-4) and 0.00001(1e-5). From this research, it shows that using the learning rate of 1e-5 achieved the highest results compared to 1e-4. For the custom dataset, it achieved an accuracy of 88.30% and an mAP of 87%, while for the CCTV recording dataset, it achieved an accuracy of 93.30% and an mAP of 79.4%.",,,155398,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195329,,FAJAR ABDUL MALIK,PEMETAAN DAERAH RAWAN COVID-19 DENGAN METODE ANALYTIC HIERARCHY PROCESS (STUDI KASUS : KABUPATEN SEMARANG),,,"COVID-19, Analytic Hierarchy Process, Simple Additive Weighting, K-Means clustering, Android","Dr. Sigit Priyanta, S.Si., M.Kom.",2,3,0,2021,1,"WHO mendeklarasikan COVID-19 sebagai pandemi global pada Maret 2020. Hingga 11 Oktober 2020 tercatat sebanyak 333.449 kasus terkonfirmasi positif di Indonesia, dengan 11.844 orang meninggal dunia. Virus SARS-COV-2 yang menyebabkan COVID-19 merupakan virus yang sangat mudah menyebar. Terdapat banyak faktor yang mempengaruhi penyebaran tersebut. Mengingat faktor yang banyakdan dinamis tersebut, penentuan tingkat kerawanan suatu daerah terhadap COVID-19 menjadi sulit untuk dilakukan karena kompleksnya permasalahan.
Analytic Hierarchy Process adalah salah satu metode dalam SPK, metode ini membandingkan tingkat kepentingan satu kriteria terhadap kriteria yang lain. Sementara itu, Simple Additive Weighting adalah salah metode dalam SPK yang sederhana dan mudah untuk digunakan dalam pengambilan keputusan. Algoritme K-Means clustering adalah suatu metode kuantisasi vektor, dengan tujuan untuk membagi n observasi ke dalam k klaster dengan tiap observasi berada pada klaster dengan mean terdekat. Dalam penelitian ini akan dikembangkan suatu sistem berupa aplikasi android untuk mengetahui peringkat dan zona tingkat kerawanan tiap kecamatan di Kabupaten Semarang terhadap COVID-19 dengan menggunakan metode AHP dan SAW untuk pemeringkatan serta K-Means untuk zonasi. Data yang digunakan berasal dari Tim Gerak Cepat COVID-19 Kabupaten Semarang, Badan Pusat Statistik dan Weather API.
Sistem yang dibangun berhasil memberikan keluaran berupa perankingan kecamatan beserta skor dan zona risikonya terhadap COVID-19 dengan menggunakan metode AHP, SAW dan K-Means clustering. Keluaran ini dapat membantu pengambil keputusan untuk mengetahui daerah mana yang paling rawan dan paling tidak rawan terhadap COVID-19.
","WHO declared COVID-19 as a global pandemic on March 2020. Until October11th 2020, it is reported that there are 333.449 positive case in Indonesia, with11.844 dead. SARS-COV-2 virus that causes COVID-19 can spread very quickly and easily. There are multiple factors that affect its spread. Due to these factors being multiple and dynamic, determining the risk level of areas towards the spread ofCOVID-19 becomes difficult because of the complexity.
Analytic Hierarchy Process (AHP) is one of the methods used in Decision Support System (DSS), this method compares the importance of each criteria comparedto others. Meanwhile, Simple Additive Weighting (SAW) is a simple method that is easy to implement in decision making. K-Means clustering is a method of vector quantization that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. This research develops a system in the form of an android application to determine the risk level of each district in Semarang Regency using AHP and SAW for rankings and K-Means clustering for zone mapping. Data used in this research is acquired from Tim Gerak Cepat COVID-19 Kabupaten Semarang, Badan Pusat Statistik and Weather API.
The system has been built and is able to display an output in the form of ranks of each district including its score dan risk zone towards COVID-19 by using AHP, SAW and K-Means clustering. This output should be able to help decision makers to understand which area has the most risk and which one has the least risk of COVID-19.
",,,146815,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199682,,AHMAD ALWARID,Peramalan Nilai Tukar Mata Uang Menggunakan Metode Average-based Length K-means Clustering pada Model Fuzzy Time Series Markov Chain,,,"Peramalan, Nilai Tukar Mata Uang, Fuzzy Time Series, Markov Chain, K-means Clustering","Dr. Agus Sihabuddin, S.Si., M.Kom.",2,3,0,2021,1,"Peramalan merupakan kegiatan memprediksi suatu keadaan di masa yang akan
datang berdasarkan data-data pada masa lampau. Peramalan sangat berguna bagi banyak
kalangan, diantaranya bagi trader dan pemerintah. Berbagai macam metode peramalan
telah dicoba untuk mendapatkan hasil prediksi yang akurat. Salah satu metode tersebut
adalah fuzzy time series markov chain. Metode penentuan interval sebelumnya pada
fuzzy time series markov chain yaitu average-based length belum mendapatkan hasil
yang optimal, sehingga perlu dilakukan riset lebih jauh agar mendapatkan interval
dengan performa lebih baik.
Pada penelitian ini, akan digunakan metode average-based length dan metode
usulan average-based length k-means clustering sebagai metode penentu interval dan
kemudian dibandingkan performanya. Data yang akan digunakan adalah data nilai tukar
mata uang Dolar Amerika terhadap Rupiah Indonesia (USD-IDR) dalam periode 1
Januari 2018 sampai 31 Maret 2021. Untuk membandingkan performa dari model,
digunakan metode mean squared error (MSE) dan directional statistics (Dstat).
Penelitian ini menghasilkan nilai MSE sebesar 1483,75 dan Dstat sebesar 81,31%
untuk prediksi data uji dengan metode average-based length. Sementara prediksi data
uji dengan metode usulan menghasilkan nilai MSE sebesar 569,71 dan Dstat sebesar
84,06%. Dari nilai tersebut, metode usulan mendapat nilai MSE yang lebih baik 61,6%
dan nilai Dstat yang lebih baik 2,75% daripada metode average-based length.","Forecasting is an act to predict a condition in the future based on data in the past.
Forecasting is very useful for many people, including stock or forex traders and the
government. Various methods have been used to obtain accurate prediction. Fuzzy time
series markov chain is one of them. The previous method of determining intervals in the
fuzzy time series markov chain, namely average-based length, has not yet obtained
optimal results, so further research is needed to obtain intervals with better performance.
In this study, average-based length and average-based length k-means clustering
will be used as the methods to determine the interval of fuzzy time series markov chain.
The data to be used are currency exchange rates of US Dollar to Indonesia Rupiah in the
period of 1 January 2018 - 31 March 2021. Performance measurement of the models will
be calculate using mean squared error (MSE) and directional statistics (Dstat).
This study produces MSE value of 1483.75 and Dstat value of 81.31% for the test
data using the average-based length method. Meanwhile, the test data using the averagebased length k-means clustering method produces MSE value of 569.71 and a Dstat value
of 84.06%. From those value, the proposed method got a better MSE value of 61,6% and
a better Dstat value of 2,75% than the average-based length method.",,,151459,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
198405,,RAYHAN NAUFAL RAMADHAN,Author Obfuscation terhadap Artikel Opini Menggunakan Algoritma Genetika,,,"author obfuscation, authorship attribution, algoritma genetika","Aina Musdholifah, S.Kom., M.Kom. Ph.D.; Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2021,1,"Authorship attribution adalah metode untuk mengidentifikasi penulis suatu teks dari sekelompok penulis potensial. Model tersebut dapat digunakan untuk memecahkan anonimitas penulis yang tidak diketahui. Hal tersebut mengancam kebebasan berpendapat dan privasi seseorang, terutama orang yang ingin menulis secara anonim. Untuk melawan ancaman tersebut, metode author obfuscation diusulkan untuk memodifikasi suatu teks supaya penulisnya sulit diidentifikasi tanpa mengaburkan topik utamanya.
Pada penelitian ini, model author obfuscation berbasis algoritma genetika dibuat untuk memodifikasi artikel opini berbahasa Indonesia supaya penulisnya tidak teridentifikasi oleh model authorship attribution dengan tetap menjaga semantik artikel yang dimodifikasi sama dengan aslinya. Model tersebut awalnya membuat kumpulan artikel kandidat yang akan menggantikan artikel asli. Kemudian, kumpulan artikel tersebut secara iteratif diseleksi untuk diubah beberapa kata di dalamnya menggunakan teknik crossover dan mutasi yang dipandu oleh fungsi fitness yang melibatkan probabilitas identifikasi dan kemiripan dengan artikel asli. Proses seleksi hingga mutasi terus dilakukan sampai ada artikel kandidat yang penulisnya tidak teridentifikasi oleh model authorship attribution atau jumlah maksimal iterasi telah tercapai.
Model tersebut dievaluasi berdasarkan parameter safety, soundness, dan sensibleness. Model tersebut memiliki safety yang baik karena dapat menurunkan akurasi model authorship attribution yang diberikan sebesar 0,3694, tetapi turun menjadi 0,1097 ketika diuji pada model yang berbeda dari yang dilibatkan pada fungsi fitness. Soundness model tersebut juga cukup baik karena kemiripan artikel yang dimodifikasi dengan aslinya mencapai 0,9601. Sensibleness model dievaluasi secara manual dan diperoleh skor 2,164 dari skala 0 sampai 4 yang menunjukkan bahwa tata bahasa sebagian artikel dapat diterima, tetapi tak sedikit juga yang tidak masuk akal.
","Authorship attribution is a method for identifying the author of a text from a group of potential authors and can solve the anonymity of unknown authors. Such method threatens privacy, especially for those who wish to write anonymously. To address this issue, author obfuscation is proposed to modify a text so that its author is hard to be identified.
In this research, a genetic algorithm-based author obfuscation model was created to modify Indonesian opinion articles to avoid identification from authorship attribution while keeping the semantics of the modified articles the same as their originals. The model initially generates a collection of candidate articles that will replace the original articles. Then, the candidates are iteratively selected to convert some of the words in it using a crossover and mutation technique guided by a fitness function that involves the probability of identification and similarity to the original article. The selection until mutation proccess continues until there is a candidate article whose authors are not identified by the authorship attribution model or the maximum number of iterations has been reached.
The model is evaluated based on safety, soundness, and sensibleness parameter. The model has good safety since it can reduce the given authorship attribution model's accuracy by 0,3694 but drops to 0,1079 when tested on different models. Its soundness is quite good since the similarity of the modified to the original articles reaches 0,9601. The model obtained a score of 2,164 on a scale of 0 to 4 in terms of sensibleness which indicates that some articles are acceptable in terms of grammar, but there are some other articles that make no sense.
",,,150003,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195848,,ANDIKA CRISHNA PRAMOEDYA,Pembangkitan Komposisi dan Dinamika Musik Menggunakan Recurrent Neural Network,,,"Komposisi Musik, Recurrent Neural Network, Pembelajaran Mesin, Dinamika","Anifuddin Azis, S.Si., M.Kom.",2,3,0,2021,1,"Perkembangan pembelajaran mesin semakin menyentuh kehidupan sehari-hari, salah satunya adalah di bidang musik. Salah satu penggunaan pembelajaran mesin di bidang ini adalah untuk membuat komposisi musik dengan model Recurrent Neural Network (RNN). Pada komposisi musik menggunakan RNN, terdapat fitur-fitur yang dijadikan acuan utama untuk membangkitkan lagu, di antaranya adalah fitur nada, waktu, dan dinamika. Pada beberapa penelitian, fitur dinamika tidak diperhatikan sehingga membuat model yang dibangun tidak dapat memprediksi keras atau lembutnya nada yang dikomposisikan.


Pada penelitian ini dikembangkan model RNN yang mampu melakukan pembangkitan komposisi musik dengan memprediksi fitur nada, waktu, pedal, serta dinamika pada lagu dengan menggunakan dropout dan batch normalization pada arsitekturnya sebagai metode regularisasi pada model.

Setelah dilakukan proses pelatihan dan validasi, model mampu membangkitkan komposisi musik yang menyerupai buatan manusia. Hasil evaluasi subjektif menunjukkan bahwa dengan menggunakan model terbaik, 55,95% dari total lagu hasil pembangkitan dengan genre musik klasik diklasifikasikan oleh responden sebagai lagu buatan manusia dan 41,67% dari total lagu hasil pembangkitan dengan genre jazz diklasifikasikan oleh responden sebagai lagu buatan manusia.","The advancement of machine learning is growing rapidly in our everyday life, one of which is in the field of music. One of the uses of machine learning in such field is music composition using Recurrent Neural Network (RNN) model. In music composition using RNN, there are features that are used as the main reference for generating songs, which are the notes, time, and dynamics. In several studies, the feature of dynamics is not considered which makes the developed model cannot predict how soft or how loud the composed notes are supposed to be.

In this research, an RNN model that is capable of generating musical composition by predicting notes, time, pedal, and dynamics features was developed. Dropout and batch normalization are used in the model's architecture as a method to perform regularization.

After the training and validation process is carried out, the model is able to generate musical compositions that resemble man-made songs. Subjective evaluation result shows that by using the best model, 55,95% of the generated pieces in classical music genre are classified by the respondents as man-made songs and 41,67% of the generated pieces in jazz genre are classified by the respondents as man-made songs.",,,147173,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199177,,FAISAL RAMADHAN,SISTEM TEMU KEMBALI VIDEO PEMBELAJARAN  ONLINE BERDASARKAN NAMA MATA KULIAH DAN SILABUS MENGGUNAKAN METODE COSINE SIMILARITY,,,"Sistem Temu Kembali, Video Pembelajaran, Cosine Similarity","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2021,1,"Internet menjadi kebutuhan penting para pelajar kehususnya mahasiswa dalam mencari bahan pembelajaran untuk mendukung kegiatan pembelajarannya. Bahan pembelajaran tersedia secara melimpah dalam internet baik dalam bentuk teks, gambar, dan video. Pembelajaran menggunakan video menjadi salah satu alternatif cara belajar yang sering digunakan misalnya menonton video pembelajaran menggunakan platform video YouTube. Akan tetapi, video pembelajaran tersedia begitu melimpah sehingga mencari video yang kontennya tepat menjadi sulit dan memakan waktu. Oleh karena itu, pada penelitian ini dibangun sistem temu kembali yang dapat menampilkan video berdasarkan mata kuliah dan silabus untuk membantu mahasiswa mencari bahan pembelajarannya.

Penelitian ini membangun sisem temu kembali yang memanfaatkan data kurikulum 2016 S1 Ilmu Komputer UGM dan data video dari YouTube. Mahasiswa diminta untuk memilih mata kuliah dan silabus sesuai preferensinya. Sistem kemudian mencari data video secara real time berdasarkan pemilihan mata kuliah dan silabus dari YouTube menggunakan YouTube API. Metadata video seperti judul dan deskripsinya (anotasi video) akan disimpan sistem. Sistem akan menghitung nilai cosine similarity antara mata kuliah dan silabus dengan anotasi video kemudian menampilkan lima buah video dengan nilai cosine similarity terbesar.

Sistem yang telah dibangun diuji menggunakan metode kuisioner untuk mendapatkan penilaian kinerja sistem. Kuisioner dilakukan dengan melibatkan 40 mahasiswa Ilmu Komputer UGM dan dilaksanakan pada bulan Maret tahun 2021. Hasil pengujian menunjukkan persentase dalam pencapaian tujuan sistem temu kembali untuk relevance sebesar novelty sebesar 78%, serendipity sebesar 77%, dan 85.5% untuk diversity. Selain itu, sistem mendapat 80.5% dalam pengujian kesesuaian video.","The internet is an important need for students, especially college students in finding for learning materials to support their learning activities. Learning materials are abundantly available on the internet in the form of text, images and videos. Learning using video is an alternative method of learning that is often used, for example watching learning videos using the YouTube video platform. However, there are so many learning videos available that finding videos with the right content is difficult and time-consuming. Therefore, in this study a recommendation system was built that could recommend videos based on courses and syllabus to help students find learning materials.

This study built a information retrieval system that utilizes Computer Science UGM curriculum data and video metadata from YouTube. Students are asked to choose courses and syllabus according to their preferences. The system then searches for video data in real time based on course selection and syllabus from YouTube using the YouTube API. Video data such as title and description (video annotation) will be stored by the system. The system will calculate the value of cosine similarity between courses and syllabus with video annotations then display five video with the largest cosine similarity value.

The information retrieval system that has been built is tested using a questionnaire method to obtain an assessment of system performance. The questionnaire was conducted involving 40 UGM Computer Science students and was held in March 2021. The test results showed that the percentage in achieving the retrieval system goals for relevance was 78%, serendipity was 77%, and 85.5% for diversity. In addition, the system got 80.5% in video compliance testing.",,,150757,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201482,,MUHAMMAD FAIZ M,Prediksi Perubahan Halaman Web Berita Berdasarkan Sitemap Menggunakan Fast Fourier Transform,,,"Berita, Web Crawler, FFT, Sitemap, Signal Processing, Forecasting.","Janoe Hendarto, Drs., M.Kom.",2,3,0,2021,1,"Melihat akan kebutuhan dalam mendapatkan sumber informasi dari situs berita secara cepat dalam jumlah yang banyak, diperlukannya penjadwalan web crawler yang dapat mengurangi beban biaya dan sumber daya bagi pihak server.
Setiap situs web, termasuk situs berita, memiliki file berekstensi XML yang berguna sebagai navigasi web crawler dalam melakukan perayapan. File ini disebut sitemap. Sitemap berisi metadata-metadata yang merujuk pada informasi situs web itu sendiri. Dari metadata ini, jika dilihat dari segi waktu, akan membentuk seperti pola gelombang.
Untuk mengolah pola gelombang ini dan menggunakannya sebagai prediksi untuk penjadwalan web crawler digunakan algoritma Fast Fourier Transform (FFT). FFT menggunakan sampel diskrit dan melakukan ekstrapolasi untuk memprediksikan pola selanjutnya.
","With the needs on getting information from online news site in bulk and rapidly, a good web crawler scheduler is required to reduce the burden from cost and resource on the server side.
In every website, including news site, there is an XML file that is useful for web crawler to navigate the website. This file is called sitemap. Sitemap contains a lot of metadata which refer to the website&acirc;€™s information itself. From these metadata, if shown from a time domain, will shape like a waveform pattern.
To process this wave pattern and use it for web crawler scheduler forecasting, we can use Fast Fourier Transform algorithm. FFT uses discrete sample of a waveform and use it for extrapolation to forecast the next pattern.
",,,152907,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195082,,PATRICK AURA WIBAWA,MOFIKASI ALGORITME LOCAL SEARCH PADA FLOWER POLLINATION ALGORITHM DENGAN TEKNIK SAMPLING,,,"Flower Pollination Algorithm, Metode Newton, Sampling, optimisasi","Anny Kartika Sari, S.Si., M.Sc., Ph.D; Faizal Makhrus, S.Si., M.Sc., Ph.D",2,3,0,2021,1,"Flower Pollination Algorithm (FPA) merupakan algoritme yang mampu
meminimalisir permasalahan local optima, di mana algoritme mengalami
konvergensi dini dan nilai optimal yang didapat bersifat lokal. Algoritme ini
memiliki batasan berupa penentuan stepsize yang terbatas pada distribusi Levy.
Pada penelitian sebelumnya, modifikasi FPA yaitu Metode Newton Termodifikasi
Flower Pollination Algorithm (MNFPA) memiliki hasil iterasi yang terbaik namun
dengan waktu konvergesi yang belum optimal.
Penelitian ini memiliki fokus untuk mengoptimasi metode MNFPA dengan
cara menerapkan teknik sampling. Simple random sampling dan stratified random
sampling akan diterapkan untuk pencarian Mean Absolute Error (MAE) dalam
Metode Newton Termodifikasi dari MNFPA. Pada penelitian ini diusulkan 2
metode pengembangan dari MNFPA; yaitu MNFPAS1, Metode Newton
Termodifikasi Flower Pollination Algorithm dengan Simple Random Sampling dan
MNFPAS2, Metode Newton Termodifikasi Flower Pollination Algorithm dengan
Stratified Random Sampling
Metode ini diujikan untuk menyelesaikan permasalahan klasifikasi data
menggunakan jaringan saraf tiruan. Metode MNFPAS1 dan MNFPAS2 berhasil
mencapai waktu konvergensi yang lebih cepat dibandingkan dengan metode
MNFPA pada kedua dataset yang diujikan, yaitu dataset Wine dan Breast Cancer.
Metode MNFPAS1 dan MNFPAS2 memiliki waktu konvergensi rata-rata 50% dari
waktu konvergensi rata-rata metode MNFPA. Pada sample size terbaik, kedua
metode yang diusulkan waktu konvergensi 1/10 dibandingkan metode MNFPA.
Dari sisi akurasi, metode MNFPAS1 dan MNFPAS2 memiliki rata-rata akurasi
yang lebih baik daripada metode MNFPA dengan selisih akurasi rata-rata 0,022 dan
0,15 secara berurutan.","Flower Pollination Algorithm (FPA) is an algorithm that can minimize local
optima problem, where the algorithm suffers from early convergence and the
optimal value obtained is not global. This algorithm has limitation due to its stepsize
that is limited to Levy distribution. In previous research, modification on FPA that
is Metode Newton Termodifikasi Flower Pollination Algorithm (MNFPA) has the
best iteration result but with suboptimal convergence time.
This research is focused on optimizing MNFPA method using sampling.
Simple random sampling and stratified random sampling will be used on finding
Mean Absolute Error (MAE) in Modified Newton Method from MNFPA. This
research proposed 2 method developed from MNFPA; that is MNFPAS1, Metode
Newton Termodifikasi Flower Pollination Algorithm dengan Simple Random
Sampling and MNFPAS2, Metode Newton Termodifikasi Flower Pollination
Algorithm dengan Stratified Random Sampling.
This method was tested to solve data classification problems using artificial
neural networks. The MNFPAS1 and MNFPAS2 methods succeeded in achieving
a faster convergence time than the MNFPA method on the two tested datasets,
namely the Wine and Breast Cancer dataset. The MNFPAS1 and MNFPAS2
methods have a mean convergence time of 50% of the mean convergence time of
the MNFPA method. In the best sample size, the two methods proposed a
convergence time of 1/10 compared to the MNFPA method. In terms of accuracy,
the MNFPAS1 and MNFPAS2 methods have an average accuracy that is better than
the MNFPA method with an average accuracy difference of 0.022 and 0.15,
respectively.",,,146390,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195340,,HANS SATRIA KUSUMA,Sistem Rekomendasi Topik Skripsi Menggunakan Content-Based Filtering,,,"sistem rekomendasi, topik skripsi, content-based filtering, euclidean distance","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2021,1,"Menurut Kamus Besar Bahasa Indonesia (KBBI) online, skripsi adalah karangan ilmiah yang wajib ditulis oleh mahasiswa sebagai bagian dari persyaratan akhir pendidikan akademisnya. Sebelum mengerjakan skripsi, mahasiswa biasanya menentukan topik skripsi terlebih dahulu. Akan tetapi, pada program studi Ilmu Komputer UGM, belum terdapat sistem rekomendasi yang dapat merekomendasikan topik skripsi berdasarkan minat serta kemampuan mahasiswanya. Oleh karena itu, pada penelitian ini akan dibangun suatu sistem rekomendasi yang dapat mengelompokkan topik skripsi berdasarkan minat dan kemampuan mahasiswa tersebut.
Content-based filtering merupakan salah satu metode sistem rekomendasi. Metode ini mengacu pada atribut yang dipilih oleh pengguna. Pada penelitian ini, mahasiswa diminta untuk memilih mata kuliah yang diminati beserta dengan nilainya. Sistem kemudian akan menghitung euclidean distance antara silabus mata kuliah dan abstrak publikasi kemudian menampilkan 5 publikasi dengan jarak terdekat. Dataset yang digunakan ada penelitian ada 2, yaitu dataset publikasi dosen dalam kurun waktu 3 tahun terakhir dan silabus mata kuliah Ilmu Komputer UGM. 
Setelah menjalankan penelitian ini, didapatkan bahwa sistem rekomendasi dengan menggunakan content-based filtering memiliki running time rata - rata sebesar 7.46 detik. Didapatkan pula bahwa sistem ini mendapatkan persentase rata - rata pencapaian tujuan sistem rekomendasi yaitu relevance, novelty, serendipity dan increasing recommendation diversity sebesar 83%.
","According to Kamus Besar Bahasa Indonesia (KBBI) Online, thesis is a scientific paper that must be written by each student as a part of their study. Currently, in Computer Science UGM, there is no recommendation system for deciding thesis topics. Therefore, a recommendation system will be built based on students' interest and abilities. This study developed a recommendation system using content-based filtering. 
Content based filtering is one of the method that is used to built a recommendation system. This method focusing on the attribute that is chosen by the users. In this study, students will be asked to choose the course that they interested in along with their grades. Then the system will calculate the euclidean distance between course syllabus and publication, and recommending 5 publications with the smallest euclidean distance. In this research, there are 2 datasets that is used, there are lecturer publications within 3 years and syllabus of Computer Science UGM course. 
After running this research, it's found that the recommendation system that is built has an average 7.46 seconds running time. It's also found that the recommendations system got an average 83% of the recommendation system objectives consists of relevance, novelty, serendipity, and increasing recommendation diversity.
",,,146611,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202254,,SOUMINDAR QOLBY,Pengestimasian Jarak Kendaraan di Malam Hari Berbasis Monocular Vision,,,"Estimasi Jarak Kendaraan, Deteksi Kendaraan, Monocular Vision","Wahyono, Ph.D.",2,3,0,2021,1,"Pengestimasian jarak kendaraan merupakan komponen penting dalam Advanced Driver-Assistance System (ADAS) untuk mengurangi resiko kecelakaan. Pengestimasian jarak kendaraan juga menjadi komponen penting dalam teknologi Autonomous Vehicle (AV) untuk mengatur posisi kendaraan relatif terhadap kendaraan lain di sekitarnya. Kamera monocular vision adalah salah satu sensor yang baik untuk mengestimasi jarak kendaran. Di malam hari, fitur kendaraan yang paling terlihat jelas adalah lampu belakang kendaraan yang menyala merah. Fitur ini dapat digunakan untuk mendeteksi dan mengestimasi jarak kendaraan.
Pada penelitian ini, proses deteksi kendaraan dilakukan melalui segmentasi lampu belakang kendaraan yang menyala merah sehinga menghasilkan bounding box di sekitar lampu belakang kendaraan. Lampu belakang kendaraan terletak di samping kanan dan kiri kendaraan, sehingga lebar kendaraan dalam citra dapat dihitung berdasarkan lebar bounding box lampu belakang kendaraan. Proses estimasi jarak kendaraan dilakukan melalui metode berdasarkan lebar kendaraan. Estimasi jarak kendaraan dihitung berdasarkan lebar asli kendaraan, panjang fokal kamera dan lebar kendaraan dalam citra.
Pengujian metode menggunakan 120 data citra digital dengan jarak, pencahayaan dan tipe kendaraan yang bervariasi menghasilkan nilai recall sebesar 89,5% dan precision sebesar 100% untuk proses deteksi kendaraan, serta nilai MSE sebesar 3,41 meter dan nilai error absolut maksimal sebesar 5,725 meter untuk proses pengestimasian jarak kendaraan.","Vehicle distance estimation is an important component in Advanced Driver-Assistance System (ADAS) to reduce the risk of accidents. It is also an important component in Autonomous Vehicle (AV) technology to positioning the vehicle relative to the surrounding vehicles. Monocular vision camera is a good sensor for estimating vehicle distance. At night, the most clearly visible feature of the vehicle is the red tail lights of the vehicle. This feature can be used to detect and estimate vehicle distance.
In this research, the vehicle detection process uses segmentation method for the vehicle tail lights that glow red to produce a bounding box around the vehicle tail lights. The vehicle tail lights are located on the right and left sides of the vehicle, so the width of the vehicle in the image can be calculated based on the width of the bounding box of the vehicle tail lights. The vehicle distance estimation process uses a method based on the width of the vehicle. The vehicle distance estimation is calculated based on the actual width of the vehicle, the camera focal length and the width of the vehicle in the image.
Testing using 120 digital image data with varying distances, lightings and vehicle types resulted in a recall value of 89,5% and a precision value of 100% for the vehicle detection process, as well as an MSE value of 3,41 meters and a maximum absolute error of 5,725 meters for the vehicle distance estimation process.",,,153479,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199697,,IMAM SYA'BULLAH M,Klasifikasi Penyakit Daun Jagung menggunakan Convolutional Neural Network,,,"Convolutional Neural Network, Penyakit Daun Jagung, Klasifikasi, Deep Learning, Citra HSV, K-Fold Cross Validation ","Drs. Sri Mulyana, M.Kom.",2,3,0,2021,1,"Penelitian mengenai klasifikasi penyakit daun menggunakan pembelajaran mesin di bidang pertanian terus berkembang saat ini. Sebagian besar input yang digunakan untuk proses pelatihan adalah citra RGB yang memiliki channel warna tiga dimensi. Seperti telah diketahui bahwa ketiga channel pada citra RGB hanya merepresentasikan tiga warna dasar dan terbatas hanya pada informasi Red, Green, dan Blue. Citra HSV (Hue, Saturation, Value) dapat digunakan karena memiliki informasi yang lebih spesifik pada tiap channel-nya, namun belum diketahui arsitektur dari model CNN yang tepat untuk digunakan pada citra HSV yang dapat memberikan akurasi tinggi. Pada penelitian ini, diimplementasikan klasifikasi penyakit daun jagung menggunakan Convolutional Neural Network (CNN) dengan masukan berupa input citra HSV yang memiliki channel warna untuk merepresentasikan corak warna, intensitas warna, dan kecerahan warna. Skema tuning hyperparameter dilakukan dengan melakukan variasi terhadap nilai learning rate dan nilai batch size setiap kali pelatihan selama 100 epoch, dengan total pelatihan sebanyak 12 kali pelatihan. Dataset yang digunakan sebanyak 8000 citra, dibagi menjadi data latih sebanyak 6400 dan data uji sebanyak 1600. Pengujian dilakukan untuk memperoleh dua arsitektur model CNN yang menghasilkan akurasi terbaik. Selanjutnya dilakukan K-Fold Cross Validation pada kedua model tersebut serta dilakukan pengujian kembali menggunakan data uji sebanyak 1600 data untuk memperoleh model terbaik dari kedua model tersebut. Model CNN yang menghasilkan akurasi terbaik pada saat pengujian dan setelah dilakukan K-Fold Cross Validation adalah model dengan hyperparameter berupa learning rate = 0,0001 dan batch size 32. Pengujian  model tersebut dilakukan menggunakan data uji sebanyak 1600 citra dan menghasilkan perolehan rerata akurasi mencapai 0,9569 atau 95,69% dan perolehan rerata loss sebesar 0,3982 pada skema pengujian K-Fold Cross Validation dengan jumlah fold sebanyak lima.","Research on the classification of leaf diseases using machine learning in agriculture continues to grow today. Most of the inputs used for the training process are RGB images which have three-dimensional color channels. As is well known that the three channels in the RGB image only represent the three basic colors and are limited to Red, Green, and Blue information. HSV (Hue, Saturation, Value) images can be used because they have more specific information on each channel, but it is not yet known the exact architecture of the CNN model to be used on HSV images which can provide high accuracy. In this study, the classification of corn leaf disease was implemented using a Convolutional Neural Network (CNN) with input in the form of HSV image input which has a color channel to represent color hue, color intensity, and color brightness. The hyperparameter tuning scheme is carried out by varying the learning rate value and the batch size value for each training for 100 epochs, with a total of 12 training sessions. Tests are carried out to obtain two CNN model architectures that produce the best accuracy. Furthermore, K-Fold Cross Validation was carried out on the two models and re-tested using 1600 test data to obtain the best model of the two models. The CNN model that produces the best accuracy during testing and after K-Fold Cross Validation is carried out is a model with hyperparameters in the form of learning rate = 0.0001 and batch size 32.The model testing is carried out using 1600 images of test data and results in an average accuracy of 0,9569 or 95.69% and the acquisition of an average loss of 0,3982 on the K-Fold Cross Validation test scheme with five folds.",,,151245,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202514,,AISYA NADYA DAMARCHA,CONTRAST STRETCHING TO ENHANCE PARKING LOT DETECTION AT NIGHT BASED ON CONVOLUTIONAL NEURAL NETWORK (CNN) AND SUPPORT VECTOR MACHINES (SVM),,,"CNN, Parking Lot Detection, Contrast Stretching, HOG, SVM, Machine Learning, Image Enhancement","Khabib Mustofa, S.Si., M.Kom., Dr.techn",2,3,0,2021,1,"Sistem pendeteksian tempat parkir adalah sistem pemandu parkir mobil yang efisien dengan memperbaiki sistem tempat parkir pada citra malam hari, untuk mencegah deteksi palsu karena distribusi cahaya yang minim pada citra. Fokus utama dari penelitian ini adalah bagaimana kontras peregangan digunakan untuk meningkatkan kualitas gambar dengan meregangkan nilai intensitas piksel yang dapat signifikan pada gambar ruang parkir malam hari. Untuk mensukseskan pendeteksian tempat parkir, diusulkan sebuah pembelajaran mesin yang membantu mendeteksi mobil dan tempat parkir kosong pada citra malam hari dengan algoritma CNN dan SVM. Mereka digunakan untuk melakukan tugas klasifikasi, kemudian hasil kinerjanya dibandingkan dan dievaluasi untuk mendapatkan hasil yang lebih baik. Setelah dilakukan evaluasi classifier, selanjutnya adalah pendeteksian mobil di tempat parkir dengan menggunakan sliding window search dan pendeteksian tempat parkir kosong menggunakan edge detection untuk membuat bounding box di sekitar objek. Hasil penelitian menggambarkan akurasi algoritma CNN dan SVM dalam peningkatan citra yang berbeda. Citra kontras yang diregangkan memiliki visualisasi yang lebih baik pada citra ruang parkir malam hari, dan hasil perbandingan akurasi dengan metode CNN dan SVM pada citra yang menggunakan regangan kontras dan citra tanpa regangan kontras dengan ekstraksi fitur HOG. Hasil ini juga menunjukkan bahwa metode tersebut dapat meningkatkan akurasi pendeteksian mobil dan tempat parkir, dengan akurasi tertinggi didapatkan dari kontras gambar yang diregangkan menggunakan model CNN dengan 99%. ","The detection system for parking space is efficient car park guidance systems by improving the system of empty parking lots and the vacant ones at night-time images, to prevent the false detection because of the minimum light distribution in images. The main focus of this research is on how contrast stretching is used to improve image quality by stretching the pixel intensity value that can be significant in night-time parking space images. To succeed the detection of parking space, a machine learning is proposed that help detecting the cars and empty parking space in night-time images by the CNN and SVM algorithm. They are used to do the classification task, then its performance results are compared and evaluated to get better result. After the evaluation of classifier, next was the detection of cars in parking space by using the sliding window search and empty parking space detection using the edge detection to make the bounding box around the object. The result of the research illustrates the accuracy of CNN and SVM algorithm in different image enhancement. The contrast stretched images had better visualization in the night-time parking space images, and the comparison of accuracy resulted with the method of CNN and SVM in images using contrast stretching and images without contrast stretching by features extraction of HOG. This result also shows that these methods can improve the detection accuracy of cars and parking space, with the highest accuracy was got from the contrast stretched images using CNN model with 99%. ",,,153977,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203538,,RAYHAN SAMTSAQIF N,DYNAMIC PRICING USING NEURAL NETWORK DEMAND MODEL AND GENETIC ALGORITHM OPTIMIZATION AND ITS POTENTIAL IMPLEMENTATION ON INTERNET SERVICE PRICE PREDICTION,,,"Monthly internet services price, Dynamic pricing, Demand model, Neural network, Genetic algorithm","Afiahayati, S.Kom., M.Cs, Ph.D",2,3,0,2021,1,"Layanan Internet Broadband mendapat lebih banyak permintaan dalam menanggapi pandemi COVID-19. Peningkatan permintaan ini terjadi karena peraturan kerja oleh pemerintah yang melarang orang bekerja di kantor daripada bekerja dari rumah. Ada juga masalah bagi orang-orang yang menuntut layanan internet, yaitu harga untuk layanan tersebut. Ketika menentukan kebijakan harga untuk memenuhi permintaan harga pelanggan, perusahaan juga perlu diuntungkan. Penelitian ini bertujuan untuk mengimplementasikan Single Layer Perceptron Neural Network Demand Model dan Genetic Algorithm Optimizer untuk data penjualan Internet Service dari Telkom Indonesia cabang Purwokerto. Hasil penelitian adalah model yang berpotensi untuk digunakan sebagai model Dynamic Pricing untuk dataset penjualan Internet Services menggunakan algoritma genetika sebagai model optimasi meskipun RMSE menunjukkan bahwa model tersebut underfitted.","Internet Broadband service has got more demand in response of COVID-19
pandemic. This increases demand happens because of the work regulations by
government that prohibited people to work on office rather they work from home.
There is also a problem for people who demand the internet services, namely the
price for the services. When defining a price policy to satisfy customer price
demand, company needs to be also profited too. This research has an objective to
implement Single Layer Perceptron Neural Network Demand Model and Genetic
Algorithm Optimizer for Internet Service sales data from Telkom Indonesia
Purwokerto branch. The result of the research is the model that have the potential
to be used as Dynamic Pricing model for Internet Services sales dataset with genetic
algorithm optimizers although the RMSE shows that the model underfitted.",,,154901,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207132,,MUHAMMAD BINTANG BAHY,Metode Non-dominated Sorting pada Algoritma Genetika Multi Objektif dan Multi Solusi untuk Bin Packing Problem,,,"Algoritma genetika, bin packing problem, multi objektif, multi solusi, non-dominated sorting","Aina Musdholifah, S.Kom, M.Kom, Ph.D.",2,3,0,2021,1,"Permasalahan bin packing adalah sebuah permasalahan di mana barang dengan volume dan dimensi berbeda dimasukkan ke dalam sebuah wadah sehingga volume barang yang dimasukkan maksimal. Permasalahan bin packing multi objektif adalah permasalahan yang lebih umum ditemukan di kehidupan sehari-hari, karena yang diperhatikan dalam pengepakan biasanya tidak hanya volume.
Pada penelitian ini diajukan sebuah algoritma genetika multi objektif untuk menyelesaikan permasalahan bin packing multi objektif. Algoritma genetika yang diajukan menggunakan metode non-dominated sorting dan crowding distance untuk mendapatkan solusi yang terbaik untuk tiap objektifnya dan menghindari adanya bias. Algoritma kemudian diuji dengan beberapa kelas uji yang menyatakan kombinasi ukuran barang dan wadah yang berbeda.
Dari hasil pengujian yang dilakukan didapatkan bahwa algoritma yang diajukan dapat menemukan beberapa solusi yang merupakan kandidat solusi terbaik untuk tiap objektif. Didapatkan juga bagaimana korelasi tiap objektif pada populasi.","	The bin packing problem is a problem where goods with different volumes and dimensions are put into a container so that the volume of goods inserted is maximized. The problem of multi-objective bin packing is a problem that is more commonly found in everyday life, because what is considered in packing is usually not only volume.
In this research, a multi-objective genetic algorithm is proposed to solve the multi-objective bin packing problem. The proposed genetic algorithm uses non-dominated sorting and crowding distance methods to get the best solution for each objective and to avoid bias. The algorithm is then tested with several test classes that represent different combinations of item and container sizes.
From the results of the tests carried out, it was found that the proposed algorithm can find several solutions which are the best candidate solutions for each objective. Also found how the correlation of each objective in the population.",,,158462,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
206877,,DIKA PERMANA PUTRA,Sistem Pendukung Keputusan Pemilihan Objek Pariwisata Tegal Menggunakan Logika Fuzzy,,,"Sistem Pendukung Keputusan, Fuzzy Tahani, Wisata, Objek Wisata di Tegal/ Decision Support System, Fuzzy Tahani, Tourism, Attractions in Tegal","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2021,1,"Sudah banyak instansi yang memiliki database tetapi dibiarkan tanpa pengelolaan yang baik dan bermanfaat. Misalnya di objek wisata di Kota &amp; Kab. Tegal dimana pengelolaan data pariwisatanya kurang tepat. Sehingga sulit ketika ingin mendapatkan informasi dan rekomendasi untuk memilih objek wisata secara efektif. Seiring pesatnya perkembangan teknologi pariwisata telah mengharuskan industri pariwisata untuk menerapkan teknologi informasi sehingga memberikan kemudahan bagi wisatawan untuk mengetahui daerah wisata sesuai dengan biaya, waktu dan jarak tempat wisata yang diinputkan. Penyediaan informasi pariwisata membantu wisatawan untuk mempertimbangkan dan mengambil keputusan untuk berwisata.
Oleh karena itu, di Kota &amp; Kab. Tegal, diperlukan suatu sistem pendukung keputusan untuk pemilihan objek wisata dengan menggunakan metode Logika Fuzzy Tahani untuk menghasilkan keputusan tentang objek wisata yang sesuai dengan kriteria pemilihan objek wisata. Logika Fuzzy Tahani dipilih karena Logika Fuzzy mampu memodelkan fungsi-fungsi keanggotaan dengan kriteria-kriteria yang diambil mampu memilih wisata yang tepat, dengan rentang nilai keanggotaan 0 sampai 1. Elemen dengan nilai keanggotaan yang paling mendekati 1 adalah pilihan keputusan yang layak dipertimbangkan untuk diambil. Sistem ini akan diimplementasikan dengan menggunakan web programming dan database MySQL, dimana variabel yang menjadi pertimbangan adalah Jenis Wisata, Jumlah Fasilitas, Harga Tiket Wisata, Jumlah Pengunjung Wisatawan, Jarak Perjalanan dari Pusat Kota.
Hasil dari penelitian yang telah dilakukan adalah sistem pendukung keputusan pemilihan objek wisata di Tegal berbasis web menggunakan metode Fuzzy Tahani yang mampu merekomendasikan objek wisata di Tegal sesuai dengan kriteria pengguna/wisatawan berdasarkan firestrength dari variabel yang dipilih. Hasil dari penelitian ini adalah sistem pendukung keputusan pemilihan objek wisata di Tegal menggunakan Fuzzy Tahani yang dapat merekomendasikan objek wisata di Tegal yang ditentukan oleh wisatawan tergantung pada kriteria wisatawan berdasarkan firestrength dari variabel yang di pilih.
","There are many agencies that have databases but are left without good and useful management. For example, in tourist attractions in the City &amp; District. Tegal where the management of tourism data is not appropriate. So it is difficult when you want to get information and recommendations to choose a tourist attraction effectively. Along with the rapid development of tourism technology, it has demanded the industry to apply information technology so as to provide convenience for tourists to find out tourist areas according to the cost, time, and distance of the tourist attractions entered. The provision of tourism information helps tourists to consider and make decisions to travel.
Therefore, in Kota &amp; Kab. Tegal, a decision support system is needed for the selection of tourist objects using the Fuzzy Tahani Logic method to produce decisions about tourist objects that match the criteria for selecting tourist objects. Holdi's Fuzzy Logic was chosen because the concept of Fuzzy logic is easy to understand, Fuzzy Logic is very flexible, and because the Holdi's logic method is a form of decision support where the main tool is functional with the main input criteria determined by the user/tourist. This system will be implemented using web programming and MySQL database, where the variables to be considered are Type of Tour, Number of Facilities, Price of Tour Tickets, Number of Tourist Visitors, Travel Distance from City Center.
The result of the research that has been done is a web-based decision support system for selecting tourist objects in Tegal using the Fuzzy Tahani method which is able to recommend tourist objects in Tegal according to the criteria of users/tourists based on the firestrength of the selected variables. The results of this study are a decision support system for selecting tourist objects in Tegal using Fuzzy Tahani which can recommend tourist attractions in Tegal which are determined by tourists depending on tourist criteria based on the firestrength of the selected variables.",,,159626,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
206366,,MUHAMMAD PRAMADIKA,METODE FIBONACCI RETRACEMENT DALAM PREDIKSI NILAI PASANGAN MATA UANG UNTUK ANALISIS TEKNIKAL PADA FOREX MARKET  MENGGUNAKAN EXPERT ADVISOR,,,"Forex, Fibonacci Retracement, Expert Advisor, Trading, Teknikal Analisis","Dr. Agus Sihabuddin, S.Si., M. Kom",2,3,0,2021,1,"Perdagangan mata uang asing memiliki likuiditas dan pergerakan harga yang tergolong tinggi. Oleh karena itu, banyak ahli berupaya untuk memprediksi pergerakan nilai tukar mata uang dan menentukan titik terendah dan tertinggi dalam pegerakan nilai mata uang tersebut. Berbagai macam metode dapat digunakan untuk memprediksi pergerakan harga pada perdagangan Forex, salah satu metode yang dapat digunakan adalah Fibonacci Retracement. Meski demikian, keputusan untuk melakukan transaksi perdagangan berada di tangan trader itu sendiri. Banyak faktor yang mempegaruhi para trader untuk mengambil keputusanm mulai dari psikologi hingga money management. Kesalahan dalam pengambilan keputusan oleh trader dapat dihindari apabila transaksi perdagangan dilakukan secara otomatis dengan bantuan Expert Advisor di platform MetaTrader. 
Expert Advisor pada peneltian ini dapat melakukan perdagangan secara otomatis dan melakukan penempatan transaksi jual atau beli dengan mempertimbangkan titik terendah dan tertinggi dari harga pasangan mata uang tersebut dengan melakukan perhitungan pada data histori menggunakan metode Fibonacci Retracement. Perbedaan metode Fibonacci Retracement dibanding metode lainnya seperti Moving Average, Moving Average Divergence Convergence (MACD), dan Average True Range (ATR) adalah Fibonacci Retracement dapat menentukan posisi open price dan close price yang akurat berdasarkan perhitungan Fibonacci Retracement dan data histori masa lalu di market untuk dijadikan acuan Take Profit (TP) maupun Stop Loss (SL), acuan itu memiliki beberapa lapis  untuk mempermudah trader menentukan TP1, TP2, TP3 dan seterusnya, sehingga trader dapat mengoptimalkan profit yang dihasilkan.  
Penelitian EA Fibonacci Retracement ini menghasilkan efisiensi dalam trading dan membantu money management trader dengan keuntungan laba bersih pada 3 jenis market yang berbeda yaitu market EUR/USD $289.35, market GBP/USD $508.08, dan market USD/ZAR $193.45.
","Foreign currency trading has high liquidity and price movements. Therefore, many experts attempt to predict the movement of currency exchange rates and determine the lowest and highest points in the movement of currency values. Various methods can be used to predict price movements in Forex trading, one method that can be used is Fibonacci Retracement. However, the decision to make a trade transaction is in the hands of the trader himself. Many factors influence traders to make decisions ranging from psychology to money management. Errors in decision-making by traders can be avoided if trading transactions are carried out automatically with the help of Expert Advisors on the MetaTrader platform.
This Expert Advisor can trade automatically and place buy or sell transactions by considering the lowest and highest points of the price of the currency pair by calculating historical data using the Fibonacci Retracement method. The difference between the Fibonacci Retracement method compared to other methods such as Moving Average, Moving Average Divergence Convergence (MACD), and Average True Range (ATR) is that Fibonacci Retracement can determine accurate open price and close price positions based on Fibonacci Retracement calculations and past historical data in the market. To be used as a reference for Take Profit (TP) and Stop Loss (SL), the reference has several layers to make it easier for traders to determine TP1, TP2, TP3 and so on, so that traders can optimize the profit generated.
This Fibonacci Retracement EA research results in efficiency in trading and helps money management traders with net profit gains in 3 different types of markets, namely the EUR/USD market of $289.35, the GBP/USD market of $508.08, and the USD/ZAR market of $193.45.
",,,157702,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
206112,,AZMI MUHAMMAD IHSAN,PERBANDINGAN PERFORMA METODE KLASIFIKASI UNTUK ANALISIS SENTIMEN PADA ULASAN MAHASISWA BERBAHASA INDONESIA,,,"Analisis Sentimen, Sentiment Analysis, ME, SVM, ANN","Suprapto, Drs., M.I.Kom., Dr.",2,3,0,2021,1,"Pada perguruan tinggi, penilaian terhadap mata kuliah dan dosen yang diberikan oleh mahasiswa di setiap akhir semester merupakan bagian penting dari kegiatan belajar mengajar. Terdapat dua cara untuk menilai kualitas pengajaran berdasarkan ulasan mahasiswa, yaitu dengan pertanyaan tertutup dan pertanyaan terbuka. Penggunaan pertanyaan terbuka memiliki kekurangan yaitu ukuran data ulasan dapat menjadi sangat besar dan rumit. Sementara itu, pembacaan data penilaian secara manual dapat memakan banyak waktu dan menimbulkan bias. Permasalahan tersebut dapat diselesaikan menggunakan analisis sentimen. Analisis sentimen adalah suatu cabang text mining yang bertujuan untuk mengenali sentimen manusia terkait suatu topik tertentu. Agar proses analisis memberikan hasil yang optimal, maka diperlukan metode klasifikasi sentimen yang terbaik.
Pada penelitian ini dilakukan perbandingan performa metode klasifikasi Maximum Entropy (ME), Support Vector Machine (SVM), dan Artificial Neural Network (ANN) untuk analisis sentimen dalam ulasan mahasiswa. Tahapan yang ditempuh adalah perancangan model, implementasi model, dan pengujian. Program dibuat dengan menggunakan bahasa pemrograman Python. Ulasan yang digunakan merupakan ulasan mahasiswa Universitas Pelita Bangsa. Perbandingan yang dilakukan adalah perbandingan presisi, recall, dan f1-score. Performa terbaik didapatkan dari metode SVM dengan nilai f1-score sama dengan 41%.","In a university, evaluations of courses and professors which are given by students at the end of every semester are important parts of teaching and learning activities. There are two ways to evaluate teaching quality based on student's reviews, namely by using closed-ended questions and open-ended questions. Open-ended questions usage has a disadvantage in which the size of review data may get very large and complicated. Meanwhile, understanding the evaluation data manually might take a lot of time and provoke bias. It could be dealt with using sentiment analysis. Sentiment analysis is a branch of text mining, which is used to recognize human sentiment about a particular topic. In order to obtain the best analysis result, the best sentiment classification method is needed.
In this research a performance comparison of classification methods Maximum Entropy (ME), Support Vector Machine (SVM), and Artificial Neural Network (ANN) for analyzing sentiment in students' reviews is carried out. The processes are model creation, model implementation, and evaluation. The program is created with Python programming language. The reviews used are of Pelita Bangsa University's students. The comparisons performed are precision, recall, and f1-score comparison. The best performance is obtained by using SVM, with f1-score equals to 41%.",,,157612,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205858,,PRAWIRO HADI TRIYONO,Pengaruh Tipe Fungsi Keanggotaan Fuzzy pada Metode LNF-FE untuk Diagnosis Awal Penyakit Jantung,,,"Diagnosis, Neuro-Fuzzy, LNF-FE, Penyakit Jantung, Logika Fuzzy","Drs. Retantyo Wardoyo, M.Sc, Ph.D",2,3,0,2021,1,"Penyakit tidak menular merupakan penyakit dengan tingkat kematian tertinggi di-Indonesia, salah satunya adalah penyakit jantung. Namun, tidak semua pasien atau orang yang mungkin terjangkit penyakit jantung dapat mengunjungi dokter atau bahkan tidak terdapat dokter spesialis jantung di daerah sekitarnya. Dalam menghadapi permasalahan tersebut, dapat dibuat sebuah sistem diagnosis yang menggunakan metode machine learning yang dapat mendiagnosis secara akurat. Penelitian ini mengimplementasikan metode Linguistic Neuro Fuzzy with Feature Extraction (LNF FE) untuk mendiagnosis awal penyakit jantung pada dataset Cardiovascular Disease dan membandingkan hasil dari setiap tipe fungsi keanggotaan fuzzy dan juga hasil tanpa menggunakan fuzzifikasi untuk mengetahui apakah adanya pengaruh fungsi keanggotaan fuzzy terhadap hasil diagnosis. Pengujian dilakukan dengan menggunakan rata-rata metriks evaluasi dari lima kali pelatihan. Metriks evaluasi yang digunakan adalah akurasi, presisi, recall, dan fscore. Berdasarkan pengujian yang telah dilakukan, diperoleh rata-rata akurasi dari sistem tanpa fuzzifikasi, dengan fungsi keanggotaan triangular, fungsi keanggotaan trapezoidal, fungsi keanggotaan Pi shaped, dan fungsi keanggotaan Gaussian secara berturut-turut adalah 67.45 persen, 71.30 persen, 71.19 persen, 71.01 persen, dan 71.77 persen.
","Non contagious diseases are diseases with the highest mortality rate in Indonesia, one of them is cardiovascular or heart disease. However, not every patient infected by cardiovascular disease can visit a doctor or maybe there is no cardiovascular specialist near their area. To deal with that problem, we can make a diagnosis system that use machine learning method to diagnosis accurately. This research implements Linguistic Neuro Fuzzy with Feature Extraction (LNF FE) method to diagnosis cardiovascular disease on Cardiovascular Disease Dataset and compare the result of every fuzzy membership function type also the result without using fuzzification step to find out is there an effect of fuzzy membership function to the result of diagnosis. The testing step use the average of evaluation metrics from five model training. Evaluation metrics that used are accuracy, precision, recall, and fscore. Based on the experiment, we got the average of accuracy from system without fuzzification, triangular membership function system, trapezoidal membership function system, Pi shaped membership function system, and Gaussian membership function system sequentially are 67.45 percent, 71.30 percent, 71.19 percent, 71.01 percent, and 71.77 percent.

",,,157385,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202537,,FANIA ARDELIA DEVIRA,Brand Logo Detection in User-Generated Content Using YOLO Object Detection,,,"User-Generated Content, Logo detection, Convolutional Neural Network, Deep Learning, You Only Look Once (YOLO)","Wahyono, Ph.D ",2,3,0,2021,1,"User-Generated Content merupakan salah satu dari banyak cara yang tersedia untuk membantu pengembangan strategi pemasaran. UGC mengacu pada konten yang disediakan oleh pengguna brand itu sendiri. Ini dapat berkisar dari teks, video, komentar, ulasan, dan gambar hingga posting media sosial. Mengingat semakin berkembangnya peran gambar di media sosial, salah satu cara untuk menganalisis gambar visual di UGC adalah dengan menggunakan deteksi logo. Deteksi logo menawarkan cara yang baik untuk mengetahui apa yang dibutuhkan pelanggan dan melihat bagaimana produk dikonsumsi dan dalam konteks apa. Dalam penelitian ini digunakan pendekatan deep learning berbasis Convolutional Neural Network (CNN). YOLO (You Only Look Once) adalah sistem deteksi objek real time yang memperlakukan deteksi gambar sebagai regresi yang mengambil gambar input dan mempelajari probabilitas kelas dan koordinat sebuah bounding box. YOLO akan melihat seluruh gambar menggunakan single convolutional network, membuat model berjalan pada kecepatan yang jauh lebih cepat. Setelah pelatihan pada total 2500 gambar dari 5 kelas logo yang berbeda, model YOLOv4 mencapai nilai rata-rata Presisi (mAP) sebesar 95,16% dan versi Tiny YOLOv4 dapat berjalan dengan kecepatan hingga 144,6 FPS."," User-Generated Content is one of the many tools available to aid in the development of a marketing strategy. UGC refers to the content provided by brand users. It can range from texts, videos, comments, reviews, and images to social media posts. Considering the growing role of images in social media, One of the ways to analyze visual images in UGC is by using brand logo detection. Logo detection offers a good way to figure out what customers need and seeing how products are consumed and in what context. In this research, deep learning approaches based on CNN are used. YOLO (You Only Look Once) is a real-time object detection system that treats the problem of image detection as a regression problem which takes an input image and learns the class probabilities and bounding box coordinates. YOLO would look at an entire image using a single convolutional network, making the model runs at a much faster speed. After training on a total of 2500 images of 5 different logo classes, YOLOv4 model achieved a mean Average Precision (mAP) value of 95.16% and Tiny YOLOv4 version could run up to 144.6 FPS.",,,153943,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200235,,ALVIN LAURENT E,"Komparasi Kinerja Protokol Tunnel GRE, EoIP, dan IPIP pada Jaringan Tunneling",,,"tunneling, GRE, EoIP, IPIP, throughput, latency, packet loss","Medi, Drs., M.Kom.",2,3,0,2021,1,"Bertambahnya tingkat penetrasi pengguna internet dan dampak dari pandemi global COVID-19 membuat tren penggunaan VPN dan teknologi access remote meningkat untuk beragam kegunaan seperti file sharing, VOIP, mengakses pekerjaan, hingga menghubungkan jaringan antara kantor yang terpisah jarak. VPN pihak ketiga dapat menjadi solusi, namun penggunaannya memakan biaya dan resiko dari segi kinerja maupun keamanan. Beberapa router SOHO memiliki fitur seperti tunnel GRE, EoIP, dan IPIP yang diimplementasikan oleh Mikrotik pada RouterBoardnya untuk menjembatani kebutuhan tersebut. Namun, belum ditemukan informasi perbandingan ketiga protokol tersebut dari segi performanya.
Penelitian ini membandingkan kinerja protokol tunnel GRE, EoIP, dan IPIP pada dua jaringan lokal tanpa menggunakan simulator jaringan. Parameter yang akan dilihat berdasarkan throughput, latency, dan packet loss yang direkam menggunakan bantuan perangkat lunak Wireshark, dihitung menggunakan script python, dan diuji secara statistik menggunakan bantuan SPSS.
Hasil penelitian menunjukkan bahwa seluruh protokol tunnel memiliki rata-rata throughput di atas 5.315kb/s, rata-rata latency di bawah 2ms, dan rata-rata packet loss yang tidak mencapai 0,1%. Pengujian statistik baik dari segi mean rank dan tingkat signifikansinya memperlihatkan bahwa GRE memiliki hasil yang lebih memuaskan dibandingkan EoIP dan IPIP.","The increasing penetration rate of internet users and the impact of global COVID-19 pandemic has made the trend of using VPNs and remote access technology to increase for various uses such as file sharing, VOIP, accessing works, to connect networks between offices that are far apart. A third-party VPN can be a solution, but its use costs and risks in terms of performance and security. Some SOHO routers have built-in features such as GRE, EoIP, and IPIP protocol tunnels which are implemented by Mikrotik on their Routerboards to bridge these needs. However, information on the comparison of the three protocols has not been found in terms of performance.
This study compares the performance of the GRE, EoIP, and IPIP tunnel protocols on two local networks without using network simulator. Parameter that will be seen are based on throughput, latency, and packet loss, recorded using Wireshark, calculated using a python script, and statistically tested using SPSS.
The result showed that all tunnel protocols had an average throughput above 5,315kb/s, an average latency below 2ms, and an average packet loss that did not reach 0.1%. Statistical testing both in terms of mean rank and level of significance shows that GRE has more satisfactory results than EoIP and IPIP.",,,151812,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202805,,MICHAEL VERELLINO,REVENUE PREDICTION USING CONVOLUTIONAL NEURAL NETWORK LONG SHORT-TERM MEMORY (CNN-LSTM),,,"Revenue, Time-series, Forecasting, CNN-LSTM, LSTM, ARIMA, Deep Learning, RMSE","Dr. Azhari SN, MT.",2,3,0,2021,1,"Forecasting time-series data is a common practice these days, and with various methods available, such as statistical methods or machine learning approaches. The main goal of this research is to propose a hybrid of two machine learning models which are Convolutional Neural Network (CNN) and
Long-Short Term Memory (LSTM) for sales revenue forecasting. Which then will be compared to a Vanilla LSTM and ARIMA model. 
This research will analyze the time-series dataset from a Kaggle Competition held in 2015. The dataset consists of Rossmann Store Sales split into an 80-20
train test data. The data will be preprocessed into a Min-Max Scaler then reshaped into a 4-dimensional input for the CNN model to extract features then fed into the LSTM model for predictions based on the feature extraction. 
The result of implementing these three methods gave different results in terms of Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) scoring for the evaluation method, with the
CNN-LSTM model giving the best score of the evaluation test in RMSE and MAE. Although the MAPE score is lower, due to the sensitivity of the model this number concludes that feature engineering is crucial for the model.","Forecasting time-series data is a common practice these days, and with various methods available, such as statistical methods or machine learning approaches. The main goal of this research is to propose a hybrid of two machine learning models which are Convolutional Neural Network (CNN) and
Long-Short Term Memory (LSTM) for sales revenue forecasting. Which then will be compared to a Vanilla LSTM and ARIMA model. 
This research will analyze the time-series dataset from a Kaggle Competition held in 2015. The dataset consists of Rossmann Store Sales split into an 80-20
train test data. The data will be preprocessed into a Min-Max Scaler then reshaped into a 4-dimensional input for the CNN model to extract features then fed into the LSTM model for predictions based on the feature extraction. 
The result of implementing these three methods gave different results in terms of Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) scoring for the evaluation method, with the
CNN-LSTM model giving the best score of the evaluation test in RMSE and MAE. Although the MAPE score is lower, due to the sensitivity of the model this number concludes that feature engineering is crucial for the model.",,,155232,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202294,,PRABU FARHAN N J,SISTEM PENGAMBILAN SAMPEL SISWA UNTUK ASESMEN NASIONAL DENGAN MENGGUNAKAN METODE STRATIFIED SAMPLING BERDASARKAN K-MEANS CLUSTERING,,,"Sampling Siswa, Stratified Random Sampling, Clustering, Algoritma K-Means, Elbow Method","Mardhani Riasetiawan, SE., MT., Dr",2,3,0,2021,1," Asesmen Nasional merupakan program penilaian terhadap mutu pendidikan setiap sekolah. Pada setiap sekolah akan dipilih beberapa siswa sebagai perwakilan untuk mengikuti kegiatan AN. Salah satu metode pemilihan sampel yang standar adalah Stratified Sampling. Dalam pengembangannya, terdapat Stratified Sampling berdasarkan clustering, dimana proses pengelompokkan yang standar digunakan pada Stratified Sampling diganti dengan metode clustering. K-Means merupakan salah satu algortima clustering yang yang sederhana dan populer. K-Means Clustering meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya.
	Hasil dari penelitian ini adalah memperlihatkan bahwa dengan membandingkan simpangan baku,  metode Stratified Sampling berdasarkan k-means Clustering memiliki keunggulan persebaran data berupa 22% jika dibandingkan dengan metode Stratified Sampling yang menggunakan pengelompokkan berdasarkan rombongan belajar.","The National Assessment is a program to assess the quality of education in each school. At each school, several students will be selected as representatives to participate in AN activities. One of the standard sample selection methods is Stratified Sampling. In its development, there is  Stratified Sampling based on clustering, where the standard clustering process used in Stratified Sampling is replaced by the clustering method. K-Means is a simple and popular clustering algorithm. K-Means Clustering minimizes the objective function set in the clustering process by minimizing variations between data in one cluster and maximizing variations with data in other clusters.
The results of this study show that by comparing the values of variance and standard deviation, the Stratified Sampling method based on k-means Clustering has the advantage of data distribution in the form of 22% when compared to the Stratified Sampling method which uses grouping based on study groups.",,,153653,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200253,,RIZKY ANANDA RADJAB,Sistem Pendukung Keputusan Klasifikasi Tingkat Kerawanan Banjir Wilayah Kabupaten Bantul Menggunakan Metode Promethee,,,"Banjir, Sistem Pendukung Keputusan Multikriteria, Promethee, Firebase, Website","Drs. Retantyo Wardoyo M.Sc., Ph.D.",2,3,0,2021,1,"	Banjir merupakan salah satu bencana yang paling sering terjadi di Indonesia. Banjir dapat disebabkan oleh beberapa faktor geografis, topografis, aktivitas manusia seperti tata guna lahan hingga  kurangnya prasarana pengendalian banjir. Salah satu cara pencegahan banjir adalah pemetaan daerah rawan banjir. Pemetaan menggunakan beberapa data dan metode pengambilan keputusan. Salah satu cara pengintegrasian datanya adalah menggunakan firebase dan metode yang dapat digunakan dalam pengambilan keputusan multikriteria adalah Promethee. Penelitian ini dilakukan dengan membangun Sistem Pendukung Keputusan untuk pengklasifikasian zona rawan banjir di Kabupaten Bantul dengan menggunakan metode Promethee. Data yang digunakan merupakan data yang didapat dari beberapa dinas terkait seperti Badan Pusat Statistik (BPS) dan Badan Penanggulangan Bencana Daerah (BPBD) Kabupaten Bantul yang beberapa diantaranya digolongkan menurut BMKG dan BPBD. Sistem dibangun pada aplikasi berbasis Website dengan harapan dapat memudahkan proses penilaian dan tampilan yang lebih baik. Keluaran dari sistem berupa kelas rawan banjir yang dibagi menjadi 3 kelas yaitu rendah, sedang, dan tinggi. Hasil akan membantu pengambil keputusan untuk mengetahui daerah mana yang paling rawan maupun tidak terkena banjir. Dalam penelitian yang digunakan digunakan 2 tipe bobot yaitu bobot menurut pakar yang mendapatkan kecocokan 53% dan bobot sama yang mendapatkan hasil 47%.
","Flood is one of the frequent disasters that happens in Indonesia. Flood can be happen caused  by some factor like geographical, topographical,  human activity like land used and lack of infrastructure of flood control. One way to prevent flood is flood mapping. Mapping used some data and decision support method. One of the way to integrate the data is by using firebase and the method that can be used for multicriteria decision making is Promethee.This research conducted  by building Decision Support System (DSS) to classified flood risk level in Bantul Regency using Prometheee method. Data used for research  obtained  from some related agencies like Badan Pusat Statistik (BPS) and Badan Penanggulangan Bencana Daerah (BPBD) Bantul Regency that some of the data classified according to BMKG and BPBD. System built on a Web application in the hope that in can be used to facilitate the assessment process and better interface.The output of the system is flood risk level that divided into 3 classess low, medium, and high. The result will help the decision makers to know which areas are most vulnerable or not flooded. In the research used 2 kinds of weight configuration that is according to expert from BPBD and get comptability  of 53% and equal weight configuration and gets the comptability of  47%.
",,,151892,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195648,,FATHREZZA FIRMANULL ARIEF,PENGGUNAAN SPARSE MATRIKS PADA PROSES STOKASTIK DISCRITE TIME MARKOV CHAIN,,,"Discrete-time Markov Chain, Sparse matriks, Proses Stokastik,Model Checking, Power Method, stationary-state","Dr.-Ing. Mhd. Reza M.I Pulungan, S.Si., M.Sc. ;Faizal Makhrus,S.Kom., M.Sc",2,3,0,2021,1,"Perkembangan teknologi dalam tataran global pada saat ini telah berkembangpesat, dan semakin tinggi pula teknologi perlu lebih andal.  Salah satu metode un-tuk mengecek keandalan sebuah teknologi adalah denganmodel checking.  Namundengan  perkembangan  teknologi,  maka  model  checking  mengalami  permasalahanstate-space  explosiondimana  komponen  kebutuhan  berupa  state  meningkat  secaraexponensial.  Salah satu metode untuk meningkatkan efisiensi dari state-space ada-lah dengan meningkatkan efisiensi penyimpannan seperti penyimpanan model sparsematriks. Banyak sekali metode untuk melakukan penyimpanan model sparse matriks,salah satu yang model penyimpanan tersebut merupakan Compressed-sparse row.Pada penelitian ini, akan dilakukan eksperimen bagaimana cara kerja penyim-panan sparse matriks model Compressed-sparse row pada salah satu langkah padastokastikmodel checkingyaitu berupa transient-state danstationary-stateprobability.Metodestationary-stateyang akan digunakan merupakan metode dasar berupaPowerMethod. Data yang digunakan pada penelitian ini berupa DTMC denganstate-spaceyang sangat besar.Dari hasil Penelitian, menunjukkan bahwa metodePower methodmempunyaiwaktu kompleksitas linier dalam menjalankan model. Dalam space complexity, stora-ge yang dibutuhkan merupakan konstan O(m+n) di manammerupakan jumlah statedannmerupakan jumlah vektor transisi yang tidak nol pada setiap iterasi.  Dilakuk-an juga test mengenasi konvergensi pada steady-state, diantaranya dalam penelitianini dilakukan dua metode konvergensi yaitu Konvergensi absolut dan relatif. Diantarakedua konvergensi, relatif melakukan proses running time lebih cepat dibanding Kon-vergensi Absolut tetapi mempunyai kemungkinan untuk gagal mencari konvergensi.","The development of technology globally over the recent decade has grownrapidly and with it, more demand of the technology to be more reliable. One methodto check the reliability of a technology is the use of model checking.  However, withthe development of technology it comes at cost which the model checking experi-enced an explosive state-space problem where the required component in the form ofa state increased exponentially.One of the method to increase the efficiency of state-space is by storing the states using sparse matrix model. There are many methods forstoring the sparse matrix model, one of which is the Compressed-sparse row.In this study, an experiment will be carried out on how the sparse matrix stor-age of the Compressed-sparse row model works in one of the methods in the stochas-tic model examination, namely in the form of transient-state and stationary-state prob-ability. The established method to be used for this experiment is the basic method inthe form of the Power Method. The data used in this study is DTMC with a very largestate space.From the experiment results, it shows that the Power method has a linear timecomplexity in running the model. In space complexity, the required storage is a con-stant O (m + n) where m is the amount of states on DTMC and N is the non-zeronumber of transition vectors in each iteration.  A test of convergence at steady-statewas also carried out, in this study we include two convergence methods, namely ab-solute and relative convergence.   Between the two convergences,  the running timeprocess is relatively faster than Absolute Convergence but has a chance to fail to getthe convergence.",,,146949,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205377,,SALMAN ALFARIZI H,Images Forgery Detection using Convolutional Neural Network (IUP),,,"Deepfake, Deep learning, CNN, Mesonet, K-Fold Cross Validation, Confusion Matrix.","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2021,1,"Deepfake adalah teknologi paling mutakhir yang dapat memberikan banyak keuntungan dan dapat dianggap sebagai suatu ancaman terhadap keberlangsungan hidup manusia. Semua orang dapat menggunakan aplikasi tertentu untuk membuat atau menciptakan suatu gambar dari orang lain yang seakan akan dapat menggambarkan suatu hal yang dilakukan ataupun di bicarakan seakan akan orang yang bersangkutan benar benar melakukan nya..
Tujuan dari eksperimen ini adalah untuk menciptakan suatu model program menggunakan deep learning yang dapat digunakan untuk memberi klasifikasi terhadap gambar yang dengan muka yang berbeda. Metode yang akan digunakan adalah MESONET yang merupakan bagian dari Convolutional Neural Network atau model CNN yang akan digunakan sebagai klasifier. Pada akhir penelitian ini, didapatkan hasil yang dapat membedakan gambar apabila gambar tersebut telah di implementasikan menggunakan deepfake atau belum.
Didalam eksperimen ini, telah menggunakan beberapa cara untuk menjalankan percobaan. Pertama akan dilakukan pre-processing, pelatihan data, dan Validation terhadap data setelah dilakukannya percobaan. Percobaan ini akan di evaluasi menggunakan K-Fold crossvalidation untuk melakukan cek terhadap model pelatihan terhadap dataset apabila stabil didalam persentasi tertentu.
Penyetelan hyperparameter digunakan dalam setiap pengujian dengan menggunakan Brute force dalam upaya untuk meningkatkan akurasidan stabilitas dalam validasi. Percobaan terbaik berhasil mendapatkan model dengan akurasi 99.50% dan akurasi rata rata K-Fold 70.50% dengan akurasi +/-15.49%.","Deepfake is the latest technology that can bring a lot of benefits and also can be considered as a threat to humanity. Everyone can use a certain application to create edited images of another individual to act as if the real person does or speak certain things as the user wanted them to do.
The objective of this research is to create a deep learning model that is able to do classification on those different images. The approach method that is going to be used is Mesonet that is a part of a CNN (Convolutional Neural Network) model that acts as the classifier. By the end of this research, it is expected to prove whether the images have been created using Deepfake or not.
There are several methods to do research, which are data pre-processing, training and cross validation. This research evaluation is conducted using K-Fold cross validation to verify if the training model is able to handle the dataset within a stable threshold
Hyperparameter tuning is used in each test by using brute force in attempt to increase the accuracy and the stability of the cross-validation. The best attempt able to produce a model that has an accuracy of 99.50% with Kfold average accuracy of 70.50% with +/- accuracy of 15.49%",,,156811,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
197702,,FAIZ MIFTAKHUR ROZAQI,SISTEM PENGUKURAN KETINGGIAN AIR SUNGAI MENGGUNAKAN METODE DETEKSI TEPI SOBEL,,,"Banjir, Deteksi Tepi Sobel, Pengolahan Citra Digital, Pengukuran Tinggi Air","Wahyono, Ph.D.",2,3,0,2021,1,"Banjir merupakan bencana alam yang sering terjadi di Indonesia. Meskipun telah menjadi rutinitas di beberapa daerah di Indonesia, bencana banjir selalu memberikan dampak kerugian bagi masyarakat. Selain diperlukan perhatian khusus dari pemerintah dan juga masyarakat untuk mencegah terjadinya banjir, diperlukan sebuah peringatan banjir sesegera mungkin untuk menekan angka kerugian akibat terjadinya banjir.
Deteksi tepi Sobel merupakan salah satu algoritma deteksi tepi yang ringan untuk dijalankan. Algoritma ini menggunakan nilai gradien vertikal dan horizontal untuk mengetahui tepian dari objek di dalam sebuah citra. Pada penelitian ini, algoritma deteksi tepi Sobel digunakan untuk menentukan tepian dari muka air yang kemudian diambil posisi piksel dan dihitung ketinggiannya dengan melakukan perbandingan citra dengan kondisi nyata.
Hasil pengujian sistem menunjukkan bahwa sistem ini sudah siap untuk diimplementasikan pada sungai nyata karena memiliki kesalahan yang dapat ditolerir dan dapat berjalan secara real time.
","Floods are natural disasters that often occur in Indonesia. Even though it has become routine in several regions in Indonesia, floods always have a detrimental impact on society. In addition to requiring special attention from the government and the public to prevent flooding, a flood warning system is needed as soon as possible to reduce the number of losses due to flooding.
Sobel edge detection is one of the fastest edge detection algorithms to run. This algorithm uses vertical and horizontal gradient values to find out the edges of objects in an image. In this research, the Sobel edge detection algorithm is used to determine the edge of the water level, then the pixel position is taken and the height is calculated by comparing the image with the real conditions.
The system test results show that this system is ready to be implemented on a real river because it has tolerable errors and can run in real time. 
",,,149109,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195913,,DENNISKIU FORTINO K,Semi-supervised Learning Method for Clickbait Detection in Bahasa Indonesia,,,"Semi-supervised learning, Self-training, Co-training, Clickbait Detection, Logistic Regression, Support Vector Machine","Sigit Priyanta, S.Si., M.Kom., Dr. ; Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2021,1,"Due to the growth of online media, online journalists are triggered to create something that can attract the attention of readers. One of the techniques used is clickbait. By using clickbait, content creators can attract more readers to read their content. As time progresses, the art of clickbait is developed, and it had become hard for humans to know which are clickbait and non-clickbait. As machine learning is then used to solve this problem, there is a need for huge amount of labeled data to give enough data for the machine to learn. To gather that much data, there are a few possible steps that can be followed. One of the steps to gather data manually and labeling the data manually as well and another way is to buy a labeled dataset. By gathering the data and labeling the data manually, it will take a lot of time. On the other hand, by buying labeled data, it is usually expensive to buy a huge, labeled dataset. Therefore, to gather huge amount of labeled dataset, it is usually time consuming and expensive. 
	This research aims to utilize a small size of labeled data to create a big enough dataset to detect clickbait from news articles in Bahasa Indonesia. The method used is semi-supervised learning, with self-training and co-training as its algorithm. Logistic Regression and SVM are used as its classifier.
	The best model for this research thus is achieved by self-training using Support Vector Machine. Using 150 seed data and producing 10885 labeled data. Achieving average accuracy of 78%, average precision of 78%, average recall of 78% and average F1-Score of 78%.
","Due to the growth of online media, online journalists are triggered to create something that can attract the attention of readers. One of the techniques used is clickbait. By using clickbait, content creators can attract more readers to read their content. As time progresses, the art of clickbait is developed, and it had become hard for humans to know which are clickbait and non-clickbait. As machine learning is then used to solve this problem, there is a need for huge amount of labeled data to give enough data for the machine to learn. To gather that much data, there are a few possible steps that can be followed. One of the steps to gather data manually and labeling the data manually as well and another way is to buy a labeled dataset. By gathering the data and labeling the data manually, it will take a lot of time. On the other hand, by buying labeled data, it is usually expensive to buy a huge, labeled dataset. Therefore, to gather huge amount of labeled dataset, it is usually time consuming and expensive. 
	This research aims to utilize a small size of labeled data to create a big enough dataset to detect clickbait from news articles in Bahasa Indonesia. The method used is semi-supervised learning, with self-training and co-training as its algorithm. Logistic Regression and SVM are used as its classifier.
	The best model for this research thus is achieved by self-training using Support Vector Machine. Using 150 seed data and producing 10885 labeled data. Achieving average accuracy of 78%, average precision of 78%, average recall of 78% and average F1-Score of 78%.
",,,147409,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200781,,M ZAKKI ALZIKRI,Identifikasi Dual Tone Multi Frequency pada Handphone berdasarkan Pengenalan Pola Suara dengan menggunakan Metode KNN Dan Ekstraksi Ciri MFCC,,,"identifikasi DTMF, Mel Frequency Cepstral Coefficient, ZCR, Frekuensi Dasar, K-Nearest Neighbor","Medi, Drs. M.Kom.",2,3,0,2021,1,"Dual-Tone Multi-Frequency merupakan suara dimana pada saat kita menekan keypad/tombol pada telepon akan mengeluarkan nada unik antara tombol satu dengan yang lainnya. Mengidentifikasi suara ini dapat diterapkan pada berbagai bidang diantaranya adalah sistem smart home.
Pada penelitian ini digunakan metode K-Nearest Neigbor (KNN) dengan tiga ekstraksi ciri yaitu Mel Frequency Cepstral Coefficient (MFCC), Zero Crossing Rate (ZCR), dan frekuensi dasar. Validasi data penelitian ini menggunakan 10-fold cross validation untuk mengetahui kinerja dari sistem. Sistem identifikasi DTMF ini mampu menghasilkan akurasi sebesar 72,5% menggunakan tiga ekstraksi ciri ketika diuji dengan 10-fold cross validation dan pengujian dengan sepuluh kali percobaan mampu menghasilkan tingkat keberhasilan sebesar 50,83%","Dual-Tone Multi-Frequency is a sound where when we press the keypad / button on the phone, it will produce a unique tone between one button and another. Identifying this sound can be applied to various fields including smart home systems.
This research uses the K-Nearest Neigbor (KNN) method with three feature extractions, namely Mel Frequency Cepstral Coefficient (MFCC), Zero Crossing Rate (ZCR), and fundamental frequency. Validation of this research data using 10-fold cross validation to determine the performance of the system. This DTMF identification system is able to produce an accuracy of 72.5% using three feature extractions when tested with 10-fold cross validation and testing with ten trials is able to produce a success rate of 50.83%. ",,,152406,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
198991,,NENDRA HARYO W,"Deteksi Serangan Distributed Denial Of Service Menggunakan Komparasi Metode LightGBM, XGBoost, Dan CatBoost",,,"Distributed Denial of Service, Machine Learning, Gradient Boosting Decision Tree, LightGBM, XGBoost, CatBoost","Mardhani Riasetiawan M.T., Dr.",2,3,0,2021,1,"DDoS adalah salah satu ancaman utama terhadap keamanan internet karena pola serangannya semakin sulit untuk dideteksi. Hal tersebut dikarenakan trafik serangannya sangat mirip dengan trafik normal pada umumnya di sebagian besar kasus. Banyaknya jenis serangan DDoS melalui berbagai port pun juga menambah kesulitan dalam membedakan antara trafik serangan atau trafik asli. 
Sudah ada banyak penelitian yang menyelidiki efek dari penggunaan algoritma klasifikasi untuk mendeteksi dan mencegah serangan DDoS. Namun, penelitian yang ada memiliki banyak kendala termasuk pencapaian tingkat kinerja dari sistem deteksi, keterlambatan deteksi, serta kemampuan untuk menangani dengan dataset dengan ukuran yang sangat besar. Metode - metode Gradient Boost Decision Tree (GBDT) merupakan metode yang dapat dilakukan untuk mempercepat proses klasifikasi pada dataset yang berukuran besar. Pada paper ini dipilih metode LightGBM, XGBoost, dan CatBoost untuk melakukan klasifikasi. Ketiga Algoritma ini akan melakukan pelatihan dengan menggunakan dataset dari CICIDS, kemudian hasilnya akan dibandingkan satu sama lain pada CPU dan GPU untuk mengetahui algoritma mana yang paling baik untuk mengklasifikasi trafik DDoS pada Intrusion Detection System (IDS).
Hasil penelitian menunjukkan bahwa ketiga meode menghasilkan model pendeteksian dengan akurasi lebih dari 95%, dan pelatihan menggunakan GPU lebih cepat daripada pelatihan menggunakan CPU.
","DDoS is one of the main threats to internet security as its attack patterns are increasingly difficult to detect. This is because the attack traffic is very similar to normal traffic in most cases. The wide variety of DDoS attacks via multiple ports also adds to the difficulty in distinguishing between attack traffic or genuine traffic.
There have been many studies investigating the effects of using classification algorithms to detect and prevent DDoS attacks. However, existing research has many constraints including attaining performance levels of detection systems, detection delays, and the ability to deal with datasets of very large sizes. Gradient Boost Decision Tree (GBDT) methods are methods that can be used to speed up the classification process on large datasets. In this paper, the LightGBM, XGBoost, and CatBoost methods are selected to perform classification. These three algorithms will conduct training using the dataset from CICIDS, then the results will be compared with each other on the CPU and GPU to find out which algorithm is best for classifying DDoS traffic on the Intrusion Detection System (IDS).
The results showed that the three methods produced a detection model with an accuracy of more than 95%, and training using the GPU was faster than training using the CPU.
",,,150601,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
197712,,HAEKAL RIZKY,Convolutional Neural Network for Elevator User's Speech Classification,,,"COVID-19, Speech Classification, Deep Learning, Convolutional Neural Network","Afiahayati, S.Kom., M.Cs, Ph.D.",2,3,0,2021,1,"Penggunaan deep learning untuk secara otomatis mengenali dan mengklasifikasikan nomor lantai dan perintah yang diucapkan oleh pengguna elevator dapat membantu mengurangi penularan COVID-19 melalui kontak fisik dengan tombol elevator. Untungnya, kemampuan Convolutional Neural Network (CNN) sebagai salah satu arsitektur deep learning untuk mengenali pola sudah dikenal dengan baik. Penelitian ini bertujuan untuk membuat data kata terisolasi dari nomor lantai dan perintah yang diucapkan kemudian membangun model pengklasifikasi untuk mengenali dan mengklasifikasikan nomor lantai dan perintah yang diucapkan oleh pengguna elevator. Dalam penelitian ini, data ucapan dikumpulkan dalam bahasa Indonesia dan diklasifikasikan menggunakan CNN dan Multilayer Perceptron (MLP). Pada akhir penelitian ini, ditemukan bahwa 94% akurasi klasifikasi diberikan oleh konfigurasi model CNN terbaik terhadap data uji. Hasil ini lebih baik daripada model MLP yang memberikan akurasi 80%.","The use of deep learning to automatically recognize and classify the floor numbers and commands spoken by elevator users could help reduce transmission of COVID-19 by physical contact with the elevator button. Fortunately, the ability of Convolutional Neural Network (CNN) as one of the deep learning architectures to recognize patterns is well-known. This research aims to create isolated-word data of spoken floor numbers and commands then build a classifier model to recognize and classify floor numbers and commands spoken by elevator users. In this research, speech data are gathered in Indonesian and classified using CNN and Multilayer Perceptron (MLP). At the end of this research, it is found that 94% of classification accuracy is provided by the best CNN model configuration towards test data. This outcome is better than the MLP model which provides 80% of accuracy.",,,149082,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201553,,MUHAMMAD AL FAJRI,PERBANDINGAN DETEKSI HOAX DENGAN METODE SUPPORT VECTOR MACHINE SEBELUM DAN SETELAH MENGGUNAKAN PARTICLE SWARM OPTIMIZATION,,,"Deteksi Hoax, Support Vector Machine,  Particle Swarm Optimization / Hoax Detection, Support Vector Machine, Particle Swarm Optimization","Suprapto, Drs., M.Kom., Dr.",2,3,0,2021,1,"Penyebaran berita pada media online semakin cepat, seiring dengan meningkatnya pengguna internet. Berita tersebut tidak sepenuhnya memuat informasi yang valid. Berita tidak valid atau hoax dapat menimbulkan dampak negatif, sehingga data sebaiknya diklasifikasikan sebagai hoax atau bukan. 
Pada penelitian ini dilakukan pendeteksian hoax menggunakan metode Support Vector Machine (SVM) dengan optimasi Particle Swarm Optimization (PSO).  SVM dapat menemukan hyperplane terbaik yang memisahkan dua buah kelas pada input space. Untuk mengatasi masalah optimasi digunakan PSO karena beberapa kelebihannya. Seperti mudah diimplementasikan, hanya membutuhkan sedikit parameter, lebih efisien dalam hal komputasi, dan lebih bersifat fleksibel. Selain itu, dilakukan juga perbandingan antara performa sebelum dan setelah penggunaan Particle Swarm Optimization (PSO). Evaluasinya menggunakan metode k-fold cross validation. 
Berdasarkan percobaan dalam penelitian didapat tingkat akurasi 77 % pada algoritma Support Vector Machine (SVM) tanpa menggunakan optimasi Particle Swarm Optimization (PSO), sedangkan dengan menggunakan optimasi Particle Swarm Optimization (PSO) didapat tingkat akurasi 82%.
","News is spreading more rapidly on online media along with the increasing number of internet users. However, not all of this news contains valid information. Invalid news or hoax could induce negative impacts, hence, data should be classified as a hoax or not. 
In this research, hoax detection was done using the Support Vector Machine (SVM) method optimized by Particle Swarm Optimization (PSO). SVM could find the best hyperplane which separates two classes on input space. PSO was used as the optimization method because of its advantages. It could be implemented easily, requires minimal parameters, is more efficient in computation, and is flexible. Furthermore, results from before and after optimization were compared. 
Based on the research experiments, 77% accuracy was achieved by the unoptimized SVM algorithm, whereas 82% accuracy was achieved by the PSO-optimized SVM.
",,,152899,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202836,,MUHAMMAD ILHAM I,SUBSISTEM KONTROL LAMPU LALU LINTAS: KLASIFIKASI BESAR ARUS KENDARAAN BERMOTOR MENGGUNAKAN ALGORITMA C4.5 DAN PREDIKSI TAMBAHAN DURASI LAMPU HIJAU,,,"arus kendaraan, decision tree, algoritma C4.5","Suprapto, Drs., M.I.Kom., Dr. ;Sri Mulyana, Drs., M.Kom",2,3,0,2021,1,"Peningkatan lalu lintas kendaraan bermotor dapat meningkatkan resiko terjadinya kemacetan terutama pada simpang. Peningkatan resiko terjadinya kemacetan pada simpang tersebut menyebabkan algoritma lampu lalu lintas yang digunakan saat ini semakin kurang efektif. Algoritma lampu lalu lintas yang banyak digunakan saat ini hanya mengubah rencana waktu sinyal pada waktu dan kondisi tertentu, bukan berdasarkan pada besar arus kendaraan pada simpang. Algoritma ini efektif untuk simpang dengan resiko kemacetan rendah, namun tidak untuk simpang yang memiliki resiko kemacetan tinggi. Simpang yang memiliki resiko kemacetan tinggi membutuhkan suatu algoritma yang dapat beradaptasi terhadap kondisi lalu lintas. Pengembangan algoritma yang dapat melakukan klasifikasi pada besar arus kendaraan dan memprekdisikan tambahan durasi untuk lampu hijau berdasarkan hasil klasifikasi tersebut dapat dijadikan salah satu komponen dalam pengembangan algoritma pengatur lalu lintas yang adaptif tersebut.
Algoritma yang digunakan pada penelitian ini adalah algoritma C4.5 untuk membangun decision tree yang digunakan untuk melakukan klasifikasi. Algoritma C4.5 membangun decision tree berdasarkan pada kriteria nilai Information Gain atau Gain Ratio yang didapatkan dari dataset training. Keputusan akhir yang merupakan hasil dari klasifikasi dataset testing menggunakan decision tree tersebut kemudian diterjemahkan menjadi prediksi tambahan durasi lampu hijau yang berkisar antara 0%-20% dari durasi lampu hijau sesuai dengan yang disarankan dalam Manual Kapasitas Jalan Indonesia.
Validasi dan evaluasi yang dilakukan pada decision tree dilakukan menggunakan metode 4-fold cross validation, dimana setiap iterasi menghasilkan 4 decision tree rule yang berbeda. Evaluasi performa dilakukan dengan mencari rata-rata nilai performa dari decision tree. Hasil evaluasi pada decision tree menunjukkan bahwa decision tree dalam classifier memiliki rata-rata nilai accuracy sebesar 92,559506%, precision sebesar 88,619450%, recall sebesar 84,670975%  dan nilai F-1 sebesar 83,003525%. Iterasi ke-4 untuk 4-fold cross validation merupakan iterasi terbaik dengan nilai performa rata-rata accuracy 93,650775%; precision 87,896825%; recall 86,817225%; dan F-1 score 86,8078%
","The increasing traffic of motorized vehicles at an intersection could increase the risk of congestion and traffic jams that could interfere with the smoothness of the traffic flow. The current algorithm used by most traffic light controller only change its signal timing plan at certain time and conditions. This kind of algorithm is effective on an intersection with a low risk of congestion and traffic jams, but not so for an intersection with a higher risk of congestion and traffic jams. Intersection with a high risk of congestion needs some sort of algorithm that could adapt to traffic conditions. Development of an algorithm that could classify the traffic flow at an intersection and predict the duration extension for the green signal based on the classification result could be used as a one of the component in the mentioned adaptive traffic light controller algorithm.
The algorithm that used in this research is C4.5 algorithm for constructing a decision tree. The C4.5 algorithm construct the decision tree based on the Information Gain or Gain Ratio criteria extracted from the training data. The final decision which is the result of classifying the testing dataset using the decision tree are translated into a prediction of green signal duration extension ranging from 0% to 20% of the green signal duration according to Indonesian Road Capacity Manual.
The validation and evaluation process on the decision tree classifier are done using 4-fold cross validation that produces 4 different decision tree rules for each iteration. Performance evaluation is carried out by calculating the average performance value of the decision tree. The evaluation results shows that the decision tree inside the classifier has an average accuracy value of 92,559506%, average precision value of 88,619450%, average recall values of 84,670975%, and average F-1 values of 83,003525%. The fourth iteration for 4-fold cross validation is the best performing iteration with average accuracy value of 93,650775%, average precision value of 87,896825%; average recall value of 86,817225%; and average F-1 score of 86,8078%.
",,,154229,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201558,,AISYAH MUHADDISI,Analisis Sentimen dengan Deteksi Sarkasme pada Komentar Instagram Politikus,,,"analisis sentimen, deteksi sarkasme, Random Forest, Naive Bayes.","Drs. Bambang Nurcahyo Prastowo, M. Sc.; Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2021,1,"Sarkasme merupakan salah satu tantangan yang mempengaruhi hasil dari analisis sentimen. Menurut Maynard dan Greenwood (2014), performa analisis sentimen dapat ditingkatkan ketika sarkasme dapat diidentifikasi. Beberapa penelitian menggunakan metode Naive Bayes dan Random Forest pada proses Dalam beberapa kasus analisis sentimen.  Menurut Salles, dkk (2018) dalam beberapa kasus Random Forest dapat mengungguli kinerja dari Support Vector Machine yang dikenal lebih superior. Pada penelitian ini dilakukan analisis sentimen pada kolom komentar akun Instagram politikus di Indonesia. Penelitian ini membandingkan akurasi dari metode analisis sentimen dengan deteksi sarkasme dan tanpa deteksi sarkasme, menggunakan metode Naive Bayes dan Random Forest untuk analisis sentimen lalu Random Forest untuk deteksi sarkasme. Penelitian ini berhasil meningkatkan akurasi pada model analisis sentimen dengan deteksi sarkasme pada model Random Forest dan Naive Bayes. Model analisis sentimen Random Forest dengan deteksi sarkasme meningkatkan rata-rata akurasi sebesar 0.7%,  dari akurasi awal 70.4% menjadi 71.1%. Model analisis sentimen Multinomial Naive Bayes dengan deteksi sarkasme meningkatkan rata-rata akurasi sebesar 0.6%,  dari akurasi awal 66.3% menjadi 66.9%.
","Sarcasm is one of the problems that affect the result of sentiment analysis. According to Maynard and Greenwood (2014), performance of sentiment analysis can be improved when sarcasm is also identified. Some research used Naive Bayes and Random Forest method on sentiment analysis process. On Salles, dkk (2018) research, in some cases Random Forest outperforms the performance by Support Vector Machine that is known as a superior method. In this research, we did sentiment analysis on the comment section on the Instagram account of Indonesian politician. This research compares the accuracy of sentiment analysis with sarcasm detection and analysis sentiment without sarcasm detection, sentiment analysis with Naive Bayes and Random Forest method then Random Forest for sarcasm detection. This research resulted in accuracy enhancement on sentiment analysis with sarcasm detection using Random Forest and Naive Bayes model. Sentiment analysis using Random Forest with sarcasm detection increased the accuracy value by 0.7% from initial accuracy value 70.4% to 71.1%.  Sentiment analysis using Naive Bayes with sarcasm detection increased the accuracy value by 0.6% from initial accuracy value 66.3% to 66.9%. 	
",,,153556,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203607,,HANIFAN AULIA,PERBANDINGAN METODE SELEKSI FITUR PADA KLASIFIKASI BERITA HOAKS TENTANG COVID-19 DENGAN NAIVE BAYES CLASSIFIER,,,"Hoaks, Informasi Palsu, Klasifikasi Teks, Naive Bayes, Seleksi Fitur, Chi-Square, Information Gain","Sri Mulyana, Drs., M.Kom.",2,3,0,2021,1,"Data yang berbentuk teks menyebabkan data memiliki dimensi yang tinggi dan fitur yang sangat banyak. Fitur yang banyak akan memakan waktu yang lama serta tidak efisien. Hal ini dapat diatasi dengan metode seleksi fitur. Pemanfaatan metode seleksi fitur sudah banyak dilakukan dalam klasifikasi teks, tetapi masih belum banyak diterapkan pada kasus klasifikasi berita hoaks dan valid. Penelitian dalam melakukan perbandingan berbagai macam metode seleksi fitur perlu dilakukan sebagai acuan untuk penelitian lain kedepannya. 
Pada penelitian ini dilakukan perbandingan metode seleksi fitur pada klasifikasi berita hoaks dengan NB (Naive Bayes Classifier). Perbandingan yang dilakukan yaitu antara chi-square dan information gain. Penelitian dilakukan menggunakan dataset berisi 9727 berita berlabel hoax dan 474 berita berlabel valid. 
Berdasarkan hasil stratified k-fold cross validation diperoleh nilai akurasi tertinggi yaitu 0,87198 untuk klasifikasi menggunakan seleksi fitur chi-square. Nilai akurasi terendah didapatkan pada klasifikasi menggunakan seleksi fitur information gain dengan nilai akurasi sebesar 0,83541. ","Data in the form of text causes the data to have high dimensions and a lot of features. Many features will take a long time and are inefficient. This can be overcome by the feature selection method. The use of feature selection methods has been widely used in text classification, but it has not been widely applied to the classification of hoax and valid news. Research in comparing various methods of feature selection needs to be done as a reference for other future studies. 
In this study, a comparison of the feature selection method in the classification of hoax news with NB (Naive Bayes Classifier) was carried out. The comparison is between chi-square and information gain. The study was conducted using a dataset containing 9727 news labeled hoax and 474 news labeled valid. 
Based on the results of stratified k-fold cross validation, the highest accuracy value is 0.87198 for classification using the chi-square feature selection. The lowest accuracy value is obtained in the classification using the information gain feature selection with an accuracy value of 0.83541.",,,154924,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207192,,MUHAMMAD DZAKI P,"Comparative Analysis of Sentiment Analysis Using Naive Bayes with TF-IDF, Bag of Word, and Word2Vec (Case Study: Lazada Electronic Product Reviews)",,,"Feature Extraction, Naive Bayes, Sentiment Analysis","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2021,1,"Electronic commerce (e-commerce) adalah platform online di mana penjual dan pembeli bertemu untuk bertukar produk atau layanan. Selain itu, perdagangan elektronik memainkan peran penting dalam pertumbuhan Usaha Kecil dan Menengah. Baru-baru ini, e-commerce menjadi populer yang mendorong bisnis untuk datang dengan strategi bisnis baru. Penjualan adalah metrik bisnis utama bagi para pemain e-commerce. Ulasan produk dalam e-commerce telah secara signifikan mengarahkan citra pedagang atau merek ke dalam penjualan. Menanggapi masalah ini, data pelanggan historis dalam e-commerce diharapkan dapat memberikan wawasan bisnis yang berharga melalui implementasi data mining, seperti text mining.

Penelitian ini bertujuan untuk membandingkan model ekstraksi ciri pada Naive Bayes untuk masalah analisis sentimen. Analisis sentimen bermanfaat untuk pengambilan keputusan e-commerce, terutama untuk merek dan pedagang. Pendekatan penelitian menggunakan konsep text mining mulai dari pengumpulan data, pra-pemrosesan (tokenization, stop word removal, dan stemming), ekstraksi fitur, analisis sentimen, dan evaluasi. Dataset tersebut merupakan data e-commerce yang di-crawl dari produk elektronik Lazada dengan output matriks kebingungan ekstraksi fitur. Ekstraksi yang digunakan untuk penelitian ini adalah TF-IDF, Bag of Word, dan Word2Vec.","Electronic commerce (e-commerce) is an online platform where sellers and buyers meet to exchange products or services. In addition, electronic commerce plays an important role in Small and Medium Enterprise growth. Recently, e-commerce became popular which drives businesses to come up with fresh business strategies. Sales are the main business metric for e-commerce players. Product reviews in e-commerce have significantly led the merchant or brand image into sales. Responding to this issue, historical customer data in e-commerce is expected to give valuable business insight through data mining implementation, such as text mining.

This research is aimed to compare feature extraction models on Naive Bayes for the sentiment analysis problem. The sentiment analysis is beneficial for e-commerce decision-making, especially for brands and merchants. The research approach is using text mining concepts starting from data gathering, pre-processing (tokenization, stop word removal, and stemming), feature extraction, sentiment analysis, and evaluation. The dataset is crawled e-commerce data from Lazada electronic products with the feature extraction confusion matrix as the output. The extractions used for this research are TF-IDF, Bag of Word, and Word2Vec.",,,158506,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
204891,,RAYHAN RIZQI BEBRYAN,Sistem Pendukung Keputusan Penentuan Lokasi Pembangunan Kafe Terbaik Menggunakan Metode Simple Additive Weight Dan Analytical Hierarchy Process,,,"Sistem Pendukung Keputusan, AHP, SAW, lokasi kafe.","Drs. Retantyo Wardoyo, M.Sc., Ph.D",2,3,0,2021,1,"Saat ini bisnis kedai kopi (kafe) berkembang pesat di Indonesia, terutama di kota-kota besar. Dalam pembangunannya, pemilihan lokasi perlu diperhatikan. Pemilihan lokasi usaha merupakan salah satu keputusan bisnis yang sebaiknya dibuat secara hati-hati. Berbagai aspek perlu diperhatikan dalam penentuannya karena pemilihan lokasi dengan tepat akan menentukan keberhasilan usaha, sedangkan kesalahan dapat menghambat keberhasilan. Pemilihan lokasi usaha saat ini masih belum didukung oleh sebuah sistem dan masih menggunakan metode konvensional.
Berangkat dari permasalahan tersebut, pada penelitian ini dibangun sebuah sistem pendukung keputusan yang dapat membantu mengatasi permasalahan tersebut. Sistem yang dibangun akan memberikan rekomendasi lokasi terbaik untuk pembangunan kafe dari beberapa titik alternatif yang diinputkan oleh pengguna. SPK diimplementasikan dalam bentuk web menggunakan metode AHP untuk pembobotan dan SAW untuk perangkingan. Kriteria yang digunakan antara lain aksesibilitas, visibilitas, lingkungan, kompetitor, parkir, ukuran lokasi, dan biaya.
Di akhir penelitian, SPK yang dibangun berhasil memberikan rekomendasi lokasi pembangunan kafe yang sesuai dengan preferensi pengguna dan mendapatkan rata-rata hasil evaluasi dengan nilai 19,33 dari nilai maksimal 20.","Currently the coffee shop (cafe) business is growing rapidly in Indonesia, especially in big cities. In its construction, site selection needs to be considered. Choosing a business location is a business decision that should be made carefully. Various aspects need to be considered in its determination because choosing the right location will determine the success of the business, while mistakes can hinder success. The choice of business location is currently not supported by a system and is still using conventional methods.
Departing from these problems, in this study a decision support system was built that can help overcome these problems. The system built will provide recommendations for the best location for cafe construction from several alternative points inputted by the user. SPK is implemented in a web form using the AHP method for weighting and SAW for ranking. The criteria used include accessibility, visibility, environment, competitors, parking, location size, and cost.
At the end of the study, the SPK that was built succeeded in providing recommendations for the location of cafe construction according to user preferences and getting an average evaluation result with a value of 19.33 from a maximum value of 20.",,,156384,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
196447,,JOTA FAUZAN HUTOMO,PERINGKAS TEKS OTOMATIS BERITA EKONOMI BERBAHASA INDONESIA DENGAN MENGGUNAKAN 5W+1H DALAM KONSIDERASI ,,,"Keyword: Natural Language Processing, Naive Bayes, TF-IDF, 5W+1H","Suprapto, Drs., M.Kom., Dr. Pembimbing",2,3,0,2021,1,"	Seiring perkembangan jaman, akses informasi menjadi lebih mudah. Dengan mudahnya akses informasi dan jumlah orang yang berbagi informasi lebih banyak, untuk seseorang bisa memahami semua informasi - informasi tersebut menjadi lebih susah. Untuk membantu masalah tersebut, rangkuman bisa dibentuk oleh manusia untuk pembaca bisa membaca terlebih dahulu dan membuat pilihan untuk lanjut membaca atau tidak. Namun rangkuman tidak bisa seterusnya dibentuk oleh manusia karena jumlah informasi untuk dirangkum juga sangat banyak. Jadi dibentuk peringkas teks otomatis untuk mempercepat proses pembuatan rangkuman.
	Akan diteliti perangkuman teks berita yang memiliki topik ekonomi. Digunakan konsep 5W+1H sebagai fitur klasifikasi. Pilihan untuk mengambil konsep 5W+1H dilakukan karena konsep tersebut adalah dasar dari bagimana membuat berita yang baik dan benar, serta merupakan elemen utama dari sebuah berita. Untuk klasifikasi berita yang memiliki aspek 5W+1H tersebut digunakan klasifier Naive Bayes. Serta untuk mengurangi jumlah kalimat lebih banyak, digunakan penilaian teks berbasis TF-IDF.
	Dalam penelitian ini, evaluasi dihitung dengan ROUGE-1 dan ROUGE-2. Kombinasi klasifier Naive Bayes dengan penilaian teks TF-IDF dapat menghasilkan rata - rata precision ROUGE-1 sebesar 0,65 dan rata - rata precision ROUGE-2 sebesar 0,56 pada rata - rata kompresi teks artikel sebesar 40% dari teks asli. 
","	Over time, access to information has become easier. With how easy it is to access new information, and more people have access to share their information, information has been abundant. Understanding every information someone gets has been more challenging than ever. To help the problem, summarization can be generated by other people for readers to be able to understand what the text is about and to continue reading or not. However, humans cannot make summarizations forever, as there is much information to summarize. 
	Automatic text summarization has been studied to accelerate the process of summarization generation. Automatic text generation will be researched for news text with the economy as its topic. The concept of 5W+1H will be used as a feature for classification. The choice for using the concept of 5W+1H is because that concept is the basis of how excellent and coherent news should be made and is the main element for news. For the classification of news that has the 5W+1H aspect, a Naive Bayes classifier will be used. Moreover, to further reduce the number of sentences generated, sentence scoring using TF-IDF will be used. 
	In this research, evaluation is calculated with ROUGE-1 and ROUGE-2. The combination of Naive Bayes classifier and TF-IDF sentence scoring produces a machine-generated summary with average ROUGE-1 precision of 0,65 and average ROUGE-2 precision of 0,56 with average compression of 40% from its original text.",,,147884,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
194657,,ADAM AFRIANSYAH B,Decision Support System for Folding Bike Selection Using Analytic Hierarchy Process and Simple Additive Weighting,,,"Folding bike, DSS, AHP, Normalization, SAW","Retantyo Wardoyo, Drs., M.Sc., Ph.D.",2,3,0,2021,1,"Sepeda lipat pada dasarnya didesain untuk memiliki bentuk yang ringkas, dengan tujuan agar dapat dibawa kemanapun, dan disimpan di ruangan yang kecil. Sepeda lipat cocok untuk digunakan bepergian ke suatu tempat, juga untuk berolahraga. Sepeda lipat dibuat dengan spesifikasi yang berbeda-beda, agar dapat memenuhi kebutuhan pengguna secara lebih spesifik. Namun, hal tersebut juga dapat membuat orang bingung ketika hendak membeli sepeda lipat yang terbaik untuk mereka.
	Untuk membantu menyelesaikan permasalahan tersebut, penulis melakukan penelitian mengenai Sistem Pendukung Keputusan (SPK) yang ditujukan untuk membantu mereka yang ingin membeli sepeda lipat dalam menentukan pilihan terbaik. SPK yang dibuat mengimplementasikan beberapa metode pengambilan keputusan, yaitu Analytic Hierarchy Process (AHP) untuk proses pembobotan, normalisasi untuk menghitung skor setiap kriteria untuk seluruh alternatif, dan Simple Additive Weighting (SAW) untuk menentukan nilai preferensi atau menentukan peringkat. Kriteria yang digunakan oleh SPK ini ditentukan dengan mengadakan survei. Survei tersebut menanyakan para responden mengenai spesifikasi apa saja yang dijadikan pertimbangan ketika hendak membeli sepeda lipat. Dari hasil survei tersebut, ditentukan bahwa kriteria yang digunakan untuk pengambilan keputusan adalah harga, desain, merk, berat, metode pelipatan, ukuran roda dan jumlah gigi. 
	Di akhir penelitian, SPK untuk pemilihan sepeda lipat telah berhasil dibuat. Berdasarkan pengujian hasil dari pengujian sistem, SPK untuk pemilihan sepeda lipat dapat bekerja dengan tepat, dan juga membantu pengguna dalam menentukan pilihannya. ","Folding bikes are basically designed to be compact, with purpose that the users can carry the bikes anywhere and put them in a tiny space. They are good options for commuting, as well as for sport. However, folding bikes come with different specifications to meet the needs of the users, therefore, people might potentially be confused on selecting the best one. 
	In order to help people solving that problem, author conducted a research about Decision Support System (DSS) which is specifically aimed to help people on selecting folding bike. The DSS itself implements several methods, which include Analytic Hierarchy Process (AHP) for weighting process, normalization for scoring, and Simple Additive Weighting (SAW) for calculating the preference values. The criteria are determined by conducting a survey that asks the respondents to choose the specifications to be considered on buying a folding bike. Based on the survey result, the criteria used for making the decision encompass price, design, brand, weight, folding method, speed and wheel size. 
	At the end of the research, the decision support system for folding bike selection which implements AHP and SAW is successfully built. Based on the result of system testing, it appears that the system works properly, and it helps users in determining the best folding bike to buy.  ",,,145982,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
194919,,HADI FAHRIZA,"Analisis Klasifikasi Gempa Seismik Tektonik dan Vulkanik dengan metode Support Vector Machine, Random Forest, dan Deep Neural Network",,,"Seismic, Machine Learning, Support Vector Machine, Random Forest, Deep Neural Network","Dr. Mardhani Riasetiawan, SE Ak, M.T.",2,3,0,2021,1,"Gempa Bumi dapat menyebabkan banyak kerusakan, korban jiwa, dan harta benda. Wilayah Indonesia memiliki kondisi geografis yang memungkinkan terjadi gempa vulkanik maupun gempa tektonik. Dibutuhkan adanya sistem klasifikasi gempa vulkanik dan gempa tektonik untuk dapat memberikan keputusan yang tepat jika terjadi gempa.
Support Vector Machine, Random Forest, dan Deep Neural Network merupakan algoritma yang bisa digunakan untuk mengklasifikasi gempa tektonik maupun gempa vulkanik berdasarkan sinyal seismik. Ketiga Algoritma ini akan diproses dengan menggunakan dataset dari IRIS Wilber 3, kemudian hasilnya akan analisis untuk mengetahui performa dari masing-masing algoritma dalam mengklasifikasi sinyal seismik.
Performa algoritma Random Forest mampu mencapai akurasi sampai dengan 73.1% untuk mengklasifikasi gempa tektonik maupun gempa vulkanik, sedangkan algoritma Support Vector Machine mampu mencapai akurasi sampai dengan 54.0% dan algoritma Deep Neural Network mampu mencapai akurasi sampai dengan 58.7%.
","Earthquakes may cause of major destruction, casualties, and property. Indonesia has geographical condition that allows volcano earthquake and tectonic earthquake to occur. The system to classify the characteristic of volcanic earthquakes and tectonic earthquake is needed to be able to make the right decisions when earthquake occur.
Support Vector Machine, Random Forest, and Deep Neural Network are algorithm that can be used to classify tectonic earthquakes and volcanic earthquakes based on seismic signal. These algorithms will be processed using the dataset from IRIS Wilber 3, then the results will be analyzed to determine the performance of each algorithm in classifying seismic signals.
The performance of the Random Forest algorithm is able to achieve an accuracy up to 73.1% for classifying tectonic earthquake and volcanic earthquake, the other side the Support Vector Machine algorithm is able to achieve an accuracy up to 54.0% and the Deep Neural Network algorithm is able to achieve an accuracy up to 58.7%. 
",,,146283,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205418,,RICKY SETIAWAN,AUTHORSHIP ATTRIBUTION UNTUK TEKS BERBAHASA INDONESIA DENGAN METODE MULTI-TASK LEARNING BERBASIS LSTM,,,"Authorship Attribution, Multi Task Learning, LSTM, Word Embedding","Lukman Heryawan, S.T., M.T. Ph.D.; Yunita Sari S. Kom., M. Kom., Ph. D.",2,3,0,2021,1,"Authorship attribution merupakan permasalahan klasifikasi yang bertujuan untuk menentukan penulis dari suatu teks berdasarkan kumpulan data yang terdiri dari penulis dan tulisannya. Penelitian terkait dengan topik ini masih terus dipelajari karena terkait dengan aplikasi forensik yang penting seperti identifikasi penulis dari pesan anonim, dll. Salah satu metode yang sering digunakan dalam permasalahan ini yaitu LSTM. LSTM mengekstraksi fitur pada data sekuensial untuk mendapatkan konteks informasi secara efektif sehingga dapat mengenali pola tulisan seseorang dengan baik. Performa dari LSTM dapat ditingkatkan dengan menggunakan model Multi Task Learning (MTL). MTL memanfaatkan korelasi diantara keterkaitan task dengan belajar task secara paralel sehingga membuat model dapat menggeneralisasi lebih baik pada task utama yang berdampak pada peningkatan akurasi klasifikasi.

Pada penelitian ini, model multi-task learning (MTL) berbasis LSTM diusulkan untuk memecahkan permasalahan authorship attribution sebagai task utama dan identifikasi gender sebagai task pembantu. Pada setiap jenis kelamin mempunyai karakteristik penulisan yang berbeda, berdasarkan hal tersebut diketahui bahwa terdapat potensi penggunaan MTL membantu meningkatkan performa model. Penelitian ini diimplementasikan pada dataset berbahasa Indonesia, menggunakan GloVe embedding dengan dimensi 300, 1 LSTM layer dengan hidden layer sebanyak 256 node, 4 dense layer yang secara berurutan mempunyai 256, 128, 64, dan 32 node. Hasil performa dari Model MTL kemudian dibandingkan dengan arsitektur single task.

Percobaan diterapkan pada dataset Twitter dan situs berita daring. Pada setiap dataset banyak data untuk masing-masing kelas terdistribusi secara merata. Dataset dibagi menjadi 72% train, 8% val, dan 20% test. Dari hasil pengujian akurasi terhadap data uji didapatkan peningkatan akurasi sebesar rata-rata 0.94% untuk seluruh dataset. Meskipun hasil peningkatan akurasi yang didapatkan tidak terlalu signifikan, namun dari penelitian ini dapat dibuktikan bahwa penggunaan authorship attribution dengan identifikasi gender dalam arsitektur MTL dapat meningkatkan performa.","Authorship attribution is a classification problem to identify the author from a text based on a dataset consisting of the author and his writing. Research related on this topic is still being studied because it is related to important forensic applications such as to identify the author of anonymous messages, etc. One method that is often used in this problem is LSTM. LSTM can extracts features on sequential data to get the context of information effectively so that it can recognize a person's writing pattern well. The performance of the LSTM can be improved by using the Multi Task Learning (MTL) model. MTL utilizes the correlation between related tasks and learning tasks in parallel so as to make the model generalize better on the main task which results in an increase in classification accuracy. 

In this study, an LSTM-based multi-task learning (MTL) model is proposed to solve the problem of authorship attribution as the main task and gender identification as an auxiliary task. Each gender has different writing characteristics, based on this it is known that there is a potential for using MTL to help improve model performance. This research is implemented in Indonesian language dataset, using GloVe embedding with dimension of 300, 1 LSTM layer with 256 nodes, and 4 dense layers which have 256, 128, 64, and 32 nodes respectively. The performance results of the MTL model are then compared with the single task architecture.

The experiments have been applied to the twitter and online news dataset. For each dataset all of the data for each class is evenly distributed. The dataset is divided into 72% train, 8% val, and 20% test set. From the results of model testing using test data, it was found that an increase in accuracy of an average of 0.94% for all datasets.
Although the results of the increase in accuracy obtained are not very significant, from this research it can be proven that the use of authorship attribution with gender identification in MTL architecture can improve the model performance.",,,157372,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
206444,,LUCAS S. AJI DHARMA,Classifying Natural Disaster Tweet using a Convolutional Neural Network and BERT Embedding,,,"Text Classification, Deep Learning, Natural Language Processing (NLP), Convolutional Neural Network (CNN), Bidirectional Encoder Representation from Transformers (BERT), Word Embedding.","Drs. Edi Winarko, M.Sc., PhD",2,3,0,2021,1,"Platform media sosial telah menjadi sarana untuk menemukan sumber informasi yang luas di seluruh internet. Twitter telah menjadi salah satu platform microblogging yang lebih populer di sekitar kita, dan semakin banyak pengguna di platform ini berarti semakin beragam jenis informasi yang dapat ditemukan dalam sehari. Di Twitter pengguna dapat mengekspresikan diri mereka melalui tweet, ini kemudian akan ditampilkan di timeline twitter dan pengguna lain dapat melihat tweet tersebut. Jika sebuah tweet tiba-tiba menjadi viral, Twitter akan menempatkan tweet pengguna ke halaman tren yang memungkinkan lebih banyak pengguna untuk melihat tweet tersebut. Saat terjadi bencana alam seringkali banyak tweet yang menyebutkan tentang bencana sehingga tweet tersebut menjadi trending topik
di Twitter. Dari sini, sejumlah besar tweet tentang bencana dapat dikumpulkan sebagai data, tetapi tidak selalu tweet tersebut berisikan informasi tentang bencana. Seringkali beberapa tweet menggunakan kata-kata bencana alam tetapi tidak berbicara tentang bencana itu sendiri, sehingga tidak informatif dan dapat diklasifikasikan sebagai tweet non-bencana. Makalah penelitian ini bertujuan untuk mengusulkan sebuah sistem untuk mengklasifikasikan tweet bencana dan tweet non-bencana pada saat terjadi bencana. Metode yang diusulkan didasarkan pada Convolutional Neural Network (CNN), menggunakan Bidirectional Encoder Representation from Transformers (BERT) sebagai
sebuah Embedding. Sebagai pembanding, akan digunakan metode embedding lain yang bernama Word2Vec.
Hasil Evaluasi setelah pelatihan dan pengujian CNN dengan embedding BERT memberikan hasil yang paling konsisten mencapai precision 0,963, recall 0,960, dan f1-score 0,961.","Social media platforms have become a medium to find a vast source of information throughout the internet. Twitter has become one of the more popular microblogging platforms out there, and the more users there are in these platforms means the more various types of information can be sent out in a day. On Twitter users can write their expression in the form of tweets, this will then create a post on twitter's timeline and other users can see these tweets. If a tweet suddenly gets viral, Twitter will put the user&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s tweets into the trending page allowing even more users to view the said tweet. During an event of a natural disaster often a lot of the tweets that are being posted, have mention of the disaster making it a trending topic on Twitter. From this, a vast amount of tweets about a disaster can be collected as data, but not always are the tweets containing information about the disaster. Often some tweets use natural disaster words but do not talk about the disaster itself, hence are not informative and can be classified as a non-disaster tweet. This research paper aims to propose a system to classify the disaster tweets and the non-disaster tweet during a disaster. The proposed method is based on Convolutional Neural Network (CNN), using a Bidirectional Encoder Representation from Transformers (BERT) as an Embedding. As a comparison, it will then be compared with another embedding method named Word2Vec.
The Evaluation result after training and testing of the CNN with BERT embeddings gave the most consistent results attaining a precision of 0.963, a recall of 0.960, and an f1-score of 0.961.",,,157917,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205169,,RAMA MAHATVA,HUMAN DETECTION FOR BOUNDING BOX OUTPUT USING NON MAXIMUM SUPRESSION ALGORITHMS,,,"Human Detection, HOG (Histogram of Oriented Gradients), SVM (Support Vector Machine), NMS (Non-Maximum Suppression), Machine Learning","Azhari, Drs., M.T., Dr",2,3,0,2021,1,"Manusia merupakan objek penting dari video surveillance sehingga penerapan teknologi pendeteksian manusia sangat penting dalam bidang kebutuhan video surveillance dimana peran detektor objek berguna untuk menganalisis gambar dan kemudian menunjukkan kepada kita objek tersebut, dan dalam hal ini adalah manusia. Sistem pendeteksian yang dilakukan pada penelitian ini adalah untuk pendeteksian manusia pada setting pejalan kaki di dalam sebuah citra. Fokus utama dari penelitian ini adalah bagaimana mengurangi output positif palsu yang terdeteksi, diusulkan pembelajaran mesin yang membantu mendeteksi manusia di dalam gambar menggunakan algoritma HOG, Linear SVM dan NMS untuk meningkatkan kualitas kotak pembatas yang terdeteksi pada gambar. dengan menghapus kotak pembatas yang tidak perlu yang terdeteksi dalam deteksi akhir. HOG digunakan untuk mengekstrak fitur dari citra untuk diteruskan ke Linear SVM untuk melakukan tugas klasifikasi, kemudian hasil performansinya dari beberapa percobaan yang menggunakan jumlah dataset yang berbeda dibandingkan dan dievaluasi untuk mendapatkan hasil terbaik. Setelah dilakukan evaluasi terhadap classifier, selanjutnya dilakukan pendeteksian manusia di dalam citra menggunakan sliding window search dan deteksi NMS untuk membuat bounding box akhir di sekitar objek yang terdeteksi.

Hasil penelitian menggambarkan akurasi, skor presisi, skor f-1, dan skor recall model SVM terlatih dari beberapa percobaan yang dilakukan. Dataset dengan citra terbanyak berisi 1200 citra untuk eksperimen yang akan diekstraksi fitur dengan HOG yang kemudian dimasukkan ke dalam model pelatihan SVM Linear yang dikombinasikan dengan algoritma NMS untuk menyaring kotak pembatas yang tidak perlu yang terdeteksi pada hasil deteksi mengembalikan akurasi terbaik sebesar 99,33%. Skor IoU dari kotak pembatas akhir di dalam gambar diekstraksi untuk menunjukkan stabilitas model yang digunakan dengan percobaan ketiga yang berisi 1200 dataset gambar sebagai model pelatihan, mengembalikan skor IoU terbaik masing-masing sebesar 0,9568 dan 0,9720 pada gambar uji 1 dan 2 untuk evaluasi ekstraksi skor IoU.","Humans are an important object of an image base video surveillance thus the application of human detection technology is very important in the field of video surveillance needs where the role of object detectors comes in handy for analyzing the images and then showing us the object, and in this case is the humans. The detection system conducted in this research is for human detection in a pedestrian setting inside an image. The main focus of this research is on how to reduce the false positive output detected, a machine learning is proposed that helps detecting the human inside an image using HOG, Linear SVM and NMS algorithms to improve the quality of the detected bounding box in the images by removing the unnecessary bounding box detected in the final detection. HOG is used to extract the features from the images to be passed on to the Linear SVM to do the classification task, then its performance results from multiple experiment which use a different amount of dataset conducted are compared and evaluated to get the best result. After the evaluation of the classifier, next was the detection of the human inside the image using the sliding window search and NMS detection to make the final bounding box around the detected object.
The result of the research illustrates the accuracy, precision score, f-1 score, and recall score of the trained SVM model from multiple experiments conducted. The dataset with the most image containing 1200 images for the experiment to be feature extracted with HOG which then fed into the Linear SVM training model combined with NMS algorithm to filter the unnecessary bounding box detected in the detection result returns the best accuracy at 99.33%. The IoU score of the final bounding box inside the image are extracted using the model trained from three different experiments to show the stability of the model used with the third experiment containing 1200 image dataset as the training model, return the best IoU score of 0.9568 and 0.9720 in test image 1 and 2 respectively for the IoU score extraction evaluation.",,,156537,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
198522,,Ainun Nafilatur Rusyda,Perbandingan Word Embedding untuk Analisis Sentimen pada Data Twitter,,,"Sentimen Analisis, Word Embedding, Word2Vec, Glove, SVM, LSTM, GRU","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2021,1,"Twitter merupakan platform yang menyediakan banyak data teks singkat mengenai opini mengenai berbagai macam hal. Data twit banyak digunakan untuk diolah menjadi informasi sentimen terhadap suatu hal dengan sentimen analisis. Jenis data ini memiliki kekurangan yaitu noise dan memiliki fitur semantik unik karena ukurannya yang pendek. Oleh karena itu pemilihan fitur dan model analisis sentimen menjadi hal yang penting untuk menentukan kualitas hasil sentimen pada data teks singkat.
Penelitian ini mengimplementasikan metode word embedding untuk pembuatan fitur analisis sentimen. Word embedding merupakan metode representasi kata berbentuk vektor bernilai riil dengan dimensi rendah yang disebut vektor embedding. Vektor embedding dibuat dengan pembelajaran dari data teks yang besar untuk dapat menangkap sintaks dan semantik kata. Terdapat beberapa penelitian yang menyediakan vektor embedding yang sudah melalui proses pembelajaran (pre-trained) yaitu Word2Vec dan Glove. Kedua vektor embedding akan digunakan dalam penelitian ini bersama dengan vektor embedding yang dibuat oleh Quanzi. Vektor embedding tersebut akan digunakan untuk pembuatan fitur. Setelah itu, fitur yang dihasilkan akan dibandingkan menggunakan algoritma SVM, LSTM dan GRU.
Pengujian dilakukan dengan menghitung nilai akurasi, precision, recall dan skor F-1. Berdasarkan penelitian yang telah dilakukan, ketiga model klasifikasi memberikan hasil akurasi tertinggi sebesar 79% terhadap dua dataset yang digunakan. Untuk vektor embedding buatan Quanzi dan Word2Vec memberikan peningkatan 1% akurasi dari vektor embedding Glove.
","Twitter is a platform providing a lot of short text data of opinions on many different things. Tweets data are often processed to be underlying sentiment information to a certain issue by using a sentiment analysis. However, this type of data also has its shortcomings as it contains noise and its semantic features are unique due to its short size. Therefore, selection on features and sentiment analysis model become important as to determine the quality of sentiment results in short data texts.
This research employs word embedding method to obtain features of sentiment analysis. Word embedding is a method of word representation in a form of real-valued vector with low dimension, which also known as embedding vector. This embedding vector is obtained through learning from large text corpus as to capture syntax and semantics of the word. Some researches actually provide pre-trained vector embeddings such as Word2Vec and Glove. These two embeddings will be used in this research along with vector embedding produced by Quanzi. These vector embeddings will be employed to obtain features. In what follows, the resulted features are compared to SVM, LSTM, and GRU algorithms.
The testing was done by calculating the accuracy value, precision, recall and F-1 score. Based on the conducted research, the highest accuracy resulted by three models of classifications to the two used datasets is on 79%. For vector embedding managed by Quanzi and Word2Vec, they give 1% accuracy increasing from Glove embedding vector.
",,,150142,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201086,,ANDREAS KEVIN RAHMAN,Periocular Region Based Masked Face Recognition using CNN,,," Facial Recognition, Deep learning, CNN, Masked Face Recognition ","Wahyono, Ph.D",2,3,0,2021,1,"COVID-19 adalah penyakit yang mengubah hidup kita. COVID-19 dapat menyebar melalui droplet, yang dihasilkan oleh tindakan umum seperti berbicara atau batuk. Cara penularannya di udara dan cukup mudah mengharuskan orang untuk memakai masker wajah saat berada di luar ruangan. Namun, memakai masker wajah dapat menghambat kinerja sistem pengenalan wajah. Masker wajah, bila dipakai dengan benar, menyembunyikan sebagian besar wajah, termasuk ujung hidung, hanya menyisakan area periokular, area di sekitar mata, yang terlihat. Deteksi ujung hidung penting dalam pengenalan wajah, karena membantu deteksi fitur wajah, koreksi pose, dan normalisasi wajah. Penelitian ini bertujuan untuk mengusulkan metode pengenalan wajah dengan Convolutional Neural Networks yaitu VGG16 dan EfficientNetB0. Penelitian ini menganalisis efektivitas dua jaringan saraf yang dilatih pada wajah yang tidak bermasker dalam memprediksi identitas subjek yang bertopeng. Efektivitas akan diukur dengan menggunakan akurasi, presisi, recall, dan confusion matrix. Metode lain untuk menggunakan gambar khusus, yang diambil dari aliran webcam, juga diperkenalkan. Ada beberapa tahapan dalam penelitian ini, yang pertama adalah akuisisi dataset dari github, preprocessing data, implementasi model dengan convolutional neural networks, baik VGG16 maupun EfficientNetB0, pengujian dengan masked face, evaluasi model, dan pengujian dengan video stream dari webcam. Hasil dari penelitian ini adalah EfficientNetB0 berperforma lebih baik dari VGG16, baik dalam akurasi train dan validasi, dan pengujian dengan masked face. ","COVID-19 is a disease that changes our lives. COVID-19 can be spread through droplets, produced by common actions such as talking or coughing. Its airborne, fairly easy way of transmission requires people to wear a facial mask when being outdoors. However, wearing facial masks can hamper the performance of facial recognition systems. Facial mask, when worn correctly, hides a large part of the face, including the nose tip, leaving only the periocular area, the areas around the eyes, visible. Nose tip detection is important in facial recognition, as it helps with facial feature detection, pose correction, and face normalization.   This research aims to propose a method to recognize faces with Convolutional Neural Networks, namely VGG16 and EfficientNetB0. This research analyzes the effectiveness of the two neural networks that is trained on unmasked faces in predicting the identities of a masked subject. The effectiveness will be measured by using accuracy, precision, recall, and confusion matrix. Another method to use custom images, taken from webcam stream, is also introduced.  There are several phases in this research, the first being dataset acquisition from github, data preprocessing, model implementation with convolutional neural networks, both VGG16 and EfficientNetB0, testing with masked faces, model evaluation, and testing with video stream from webcam.  Results of this research is that EfficientNetB0 performs better than VGG16, both in train and validation accuracy, and testing with masked faces.",,,152731,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205695,,Muammar Khadafi,Studi Perbandingan Metode Koefisien Korelasi sebagai Bobot Jarak Topologi Jaringan Minimum Spanning Tree Pasar Saham Indonesia,,,"saham, koefisien korelasi, minimum spanning tree, topologi jaringan, community detection","Dr. Andi Dharmawan, S.Si., M.Cs.",2,3,0,2021,1,"Dalam pasar saham, seringkali ditemukan harga suatu saham dengan harga saham lainnya mengalami kenaikan/penurunan secara bersamaan. Untuk dapat menganalisa hal tersebut, dibuatlah topologi jaringan saham. Metode topologi jaringan yang umum dipakai adalah Minimum Spanning Tree (MST) dengan metode Pearson Correlation Coefficient (PCC) sebagai bobot jaraknya. Tetapi PCC memiliki beberapa kelemahan, yaitu PCC hanya bisa mendeteksi hubungan linear antar saham, masih dipengaruhi oleh indeks harga saham yang dapat mengubah hasil analisis, dan hanya bekerja dengan baik dengan asumsi harga saham memiliki distribusi normal, sedangkan harga saham memiliki distribusi yang tidak normal.
	
	Pada penelitian ini dilakukan pembuatan topologi jaringan saham di Indonesia dengan metode MST menggunakan beberapa metode bobot jarak, yaitu metode Pearson Correlation Coefficient (PCC), Kendall's Tau Correlation Coefficient (KTCC), Partial Pearson Correlation Coefficient (PPCC), dan Partial Kendall's Tau Correlation Coefficient (PKTCC) untuk dibandingkan performanya berdasarkan kemiripan dengan Jakarta Stock Industrial Classification (JASICA). Data saham yang digunakan adalah data saham IHSG yang diambil dari situs Yahoo Finance dengan kriteria rentang waktu tanggal 18 Februari 2013 hingga 17 Februari 2021, sudah terdaftar di bursa selama delapan tahun atau lebih, dan harga sahamnya tidak pernah stagnan berturut-turut selama lebih dari 5% dari total data. Metode pembuatan komunitas saham yang digunakan adalah algoritme Leiden dan penghitungan nilai performa menggunakan Adjusted Mutual Information (AMI) dan Adjusted Rand Index (ARI).
	
	Hasil penelitian ini yaitu dari nilai AMI dan ARI, metode PKTCC memiliki kemiripan paling tinggi dengan JASICA daripada metode bobot jarak lainnya, sehingga metode PKTCC merupakan metode terbaik untuk digunakan dalam pembuatan topologi jaringan saham dibandingkan dengan metode PCC, KTCC, dan PPCC.","In the stock market, it is often found that the price of a stock with other stock increases/decreases simultaneously. To be able to analyze this, a stock network topology is created. The method commonly used for building stock network topology is Minimum Spanning Tree (MST) with the Pearson Correlation Coefficient (PCC) method as the distance weight. However, PCC has several weaknesses, such as PCC can only detect a linear relationship between stocks, PCC is still influenced by the stock price index which can change the analysis results, and PCC only works well assuming stock prices have a normal distribution, while stock prices have a skewed distribution.

    In this study, the stock network topology in Indonesia was made using MST method with several distance weight methods, which is Pearson Correlation Coefficient (PCC), Kendall's Tau Correlation Coefficient (KTCC), Partial Pearson Correlation Coefficient (PPCC), and Partial Kendall's Tau Correlation Coefficient (PKTCC) to compare their performance based on their similarities to Jakarta Stock Industrial Classification (JASICA). The stock data used is JCI stock data taken from the Yahoo Finance site with the criteria for a time span of 18 February 2013 to 17 February 2021, has been listed on the stock exchange for eight years or more, and the stock price has never been stagnant in a row for more than 5% of the total data. Leiden algorithm is used to create stock community, and Adjusted Mutual Information (AMI) and Adjusted Rand Index (ARI) is used to calculate performance.

    The results of this study are based on the AMI and ARI values, the PKTCC method has the highest similarity with JASICA than other distance weight methods, so the conclusion is the PKTCC method is the best method to be used in making stock network topology compared to the PCC, KTCC, and PPCC methods.",,,157218,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201088,,SULKHA MARFUAH,SISTEM PENDUKUNG KEPUTUSAN PENENTUAN PRIORITAS BANTUAN BEDAH RUMAH KABUPATEN KULON PROGO MENGGUNAKAN METODE AHP DAN PROMETHEE,,,"Bedah rumah, sistem pendukung keputusan, Analytic Hierarchy Process, Preference Ranking Organization Method for Enrichment Evaluation.","Retantyo Wardoyo, Drs., M.Sc., Ph.D.",2,3,0,2021,1,"Kemajuan suatu daerah dapat dilihat dari kesejahteraan warganya baik dari segi ekonomi, pendidikan, kesehatan dan lingkungan. Di Kabupaten Kulon Progo, angka kemiskinan masih tercatat 24,38% pada tahun 2015. Salah satu persoalan kemiskinan di Kabupaten Kulon Progo adalah persoalan rumah tidak layak huni(RLTH). Berdasarkan Pembaharuan Data Dinas Sosial Provinsi DIY 2011, tercatat 6.238 RLTH di Kabupaten Kulon Progo. Untuk mengurangi jumlah RLTH, Pemerintah Kabupaten Kulon Progo memberikan bantuan bedah rumah kepada masyarakat miskin.  Pada proses pemilihannya, masih dilakukan secara manual yaitu dimulai dari pemilihan di tingkat pemerintah desa,  lalu diakhiri dengan verifikasi oleh pemerintah Kabupaten. Penilaian tersebut dinilai kurang efisien sehingga diperlukan sebuah sistem untuk mempermudah penilaian untuk menentukan prioritas penerima bantuan bedah rumah di Kabupaten Kulon Progo. 
Sistem Pendukung Keputusan (SPK) merupakan sebuah sistem untuk membantu pengambilan keputusan yang memiliki keluaran berupa alternatif keputusan yang pada umumnya berbentuk prioritas pilihan. Analytic Hierarchy Process(AHP) adalah salah satu metode dalam SPK, metode ini membandingkan tingkat kepentingan satu kriteria terhadap kriteria yang lain. Preference Ranking Organization Method for Enrichment Evaluation(Promethee) adalah sebuah metode SPK yang berasal dari metode outranking dan menggunakan model MCDM yang mudah untuk digunakan dalam pengambilan keputusan.
 Dalam penelitian ini, akan dikembangkan Sistem Pendukung Keputusan berupa aplikasi berbasis web sebagai solusi untuk memperbaiki sistem manual di Kabupaten Kulon Progo dalam penentuan prioritas bantuan bedah rumah di Kabupaten Kulon Progo menggunakan metode AHP untuk pembobotan kriteria dan Promethee untuk perankingan. Data yang digunakan berasal dari Sekretariat Daerah Bagian Administrasi Kesra dan Kemasyarakatan Kabupaten Kulon Progo yang berupa calon penerima bantuan bedah rumah di Kabupaten Kulon Progo. Hasil berupa rekomendasi prioritas penentuan bedah rumah di Kabupaten Kulon Progo, Hasil kemudian akan dianalisis dan diambil kesimpulannya.

Kata Kunci : Bedah rumah, sistem pendukung keputusan, Analytic Hierarchy Process, Preference Ranking Organization Method for Enrichment Evaluation.","The development of a region can be seen from the welfare of its citizens in terms of economy, education, health, and environment. In Kulon Progo Regency, the poverty rate was recorded at 24.38% in 2015. One of the problems of poverty in Kulon Progo Regency is related to uninhabitable houses (Rumah Tidak Layak Huni/RLTH). Based on the Data Update of the Social Office of the Special Region of Yogyakarta Province in 2011, there were 6,238 uninhabitable houses in Kulon Progo Regency. To reduce the number of uninhabitable houses, the Government of Kulon Progo Regency provides house renovation assistance to the poor. The selection process is carried out manually, starting from selection within the village level to verification by the regency government. The assessment is considered inefficient so that a system is needed to facilitate the assessment to determine the priority of house renovation assistance in Kulon Progo Regency. 
Decision Support System (DSS) is a system to assist decision making that has an output in the form of alternative decisions which are generally in the form of priority choices. Analytic Hierarchy Process (AHP) is one of the methods in DSS, which compares the importance of one criterion to the others. Preference Ranking Organization Method for Enrichment Evaluation (Promethee) is a DSS method derived from outranking method using the MCDM model which is easy to use in decision making. 
  In this study, a Decision Support System in the form of a web-based application was developed as a solution to improve the manual system in Kulon Progo Regency in determining the priority of house renovation assistance using AHP for weighting the criteria and Promethee for ranking. The data used were obtained from the Regional Secretariat of Welfare and Community Administration Section of Kulon Progo Regency, which were in the form of lists of candidates for house renovation assistance in Kulon Progo Regency. The results were in the form of priority recommendations for determining house renovation in Kulon Progo Regency, which were subsequently analyzed and from which conclusions were drawn.


Keywords: House Renovation, Decision Support System, Analytic Hierarchy Process, Preference Ranking Organization Method for Enrichment Evaluation. ",,,152476,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
204929,,FAHMI ZUFARIAN,FORECASTING SUGARCANE YIELD USING  MACHINE LEARNING ALGORITHM,,,"Sugarcane crop yield, Neural network, Multi Layer Perceptron, Linear Regression, Support Vector Regression","Anny Kartika Sari, S. Si., M.Sc., Ph. D",2,3,0,2021,1,"Sugarcane is one of the main commodities that plays strategic role inside the economic role in Indonesia. East Java reaches 52% of total sugarcane production, which makes it the largest sugarcane producer in Indonesia. On the other hand, climate become the most crucial part of sugarcane growth. A new framework needs to be built to forecast sugarcane yield through the use of nearest precipitation point as well as temperature data since existing research focused on using other parameters to predict the crop yield.
The objective of this research is to develop a model that is able to predict sugarcane yield based on the monthly average precipitation, temperature, and previous year's yield. The machine learning algorithms to be used are Linear Regression, Multi Layer Perceptron, and Support Vector Regression. The dataset consists of the sugarcane yield data in East Java from 1976 to 2019.  The result of the models based on both algorithms are then compared and reviewed in order to see which one has better accuracy between the two methods.
Experiments have been done to compare the models using Linear Regression, Multi Layer Perceptron, and Support Vector Regression. The results were 3.	Multi Layer Perceptron performs 26.5% better compared to Multiple Linear Regression for MSE value in the test dataset. Multi Layer Perceptron also performs better with 8% and 3.4% gap compared to Support Vector Regression for both MSE train and test dataset. Support Vector Regression also perform 24% much better compared to Multiple Linear Regression on MSE value from using test data. Percentages within each values are calculated manually.","Sugarcane is one of the main commodities that plays strategic role inside the economic role in Indonesia. East Java reaches 52% of total sugarcane production, which makes it the largest sugarcane producer in Indonesia. On the other hand, climate become the most crucial part of sugarcane growth. A new framework needs to be built to forecast sugarcane yield through the use of nearest precipitation point as well as temperature data since existing research focused on using other parameters to predict the crop yield.
The objective of this research is to develop a model that is able to predict sugarcane yield based on the monthly average precipitation, temperature, and previous year's yield. The machine learning algorithms to be used are Linear Regression, Multi Layer Perceptron, and Support Vector Regression. The dataset consists of the sugarcane yield data in East Java from 1976 to 2019.  The result of the models based on both algorithms are then compared and reviewed in order to see which one has better accuracy between the two methods.
Experiments have been done to compare the models using Linear Regression, Multi Layer Perceptron, and Support Vector Regression. The results were 3.	Multi Layer Perceptron performs 26.5% better compared to Multiple Linear Regression for MSE value in the test dataset. Multi Layer Perceptron also performs better with 8% and 3.4% gap compared to Support Vector Regression for both MSE train and test dataset. Support Vector Regression also perform 24% much better compared to Multiple Linear Regression on MSE value from using test data. Percentages within each values are calculated manually.",,,156487,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205187,,MUFLIHANTO,KLASIFIKASI ALARM INTRUSION DETECTION SYSTEM BERDASARKAN TINGKAT POTENSI KERUSAKAN JARINGAN MENGGUNAKAN METODE MACHINE LEARNING,,,"machine learning, IDS, intrusion detection system, snort, decision tree, support vector machine, adaboost, xgboost","Mardhani Riasetiawan, M.T., Dr.",2,3,0,2021,1,"Snort adalah sebuah packet sniffer dan logger yang dapat digunakan sebagai sistem deteksi intrusi jaringan (NIDS). Snort dapat melakukan analisis protokol, pencarian/pencocokan konten, dan dapat digunakan untuk mendeteksi berbagai serangan dan probe. Mode NIDS Snort menghasilkan laporan log alarm yang berisi detail serangan serta label prioritas dari setiap serangan. Dalam pengujian menggunakan Metasploit framework pada jaringan, Snort menghasilkan laporan dengan tingkat alarm false positive sebesar 56,2%.
Machine Learning adalah sebuah metode yang dapat digunakan untuk menghasilkan ekspresi pengklasifikasian yang cukup sederhana untuk dipahami dengan mudah oleh manusia. Sejumlah penelitian menggunakan beberapa jenis model machine learning untuk meringkas dan mengategorikan alarm intrusi. Beberapa model yang digunakan adalah Decision Tree, Support Vector Machine (SVM), AdaBoost dan XGBoost.
Penelitian ini menjabarkan analisis komparasi performa beberapa model machine learning yaitu Decision Tree, Support Vector Machine (SVM), AdaBoost dan XGBoost dalam mengelompokkan alarm Snort saat mendeteksi serangan dalam dataset ISCXIDS2012. Pada percobaannya, dilakukan pengelompokan alarm Snort dengan dan tanpa alarm false positive.
Dari hasil penelitian yang diperoleh, didapatkan bahwa penerapan metode machine learning dapat digunakan untuk mengelompokkan alarm Snort dan mengurangi tingkat false positive. Alarm Snort dapat dikelompokkan ke dalam 3 kelas prioritas serangan dan 1 kelas alarm false positive. Akurasi model dalam mengelompokkan alarm ke dalam 3 kelas prioritas dan 1 kelas alarm false positive berkisar antara 90,6% hingga 98,4%. Pada dataset laporan Snort dengan tingkat false positive 26,77%, sebuah model XGBoost yang merupakan classifier dengan performa terbaik mampu menurunkan tingkat alarm false positive Snort hingga mencapai angka 0,39%.","Snort is a packet sniffer and logger that can be used as a network intrusion detection system (NIDS). Snort can perform protocol analysis, content searching/matching, and can be used to detect various attacks and probes. Snort NIDS mode generates alarm reports containing attack details as well as priority labels of each attack. In a test using the Metasploit framework on the network, Snort produced a report with a false positive alarm rate of 56.2%.
Machine learning is a method that can be used to produce classification expressions that are simple enough to be understood easily by humans. Several studies use several types of machine learning models to summarize and categorize intrusion alarms. Some of the models used are Decision Tree, Support Vector Machine (SVM), AdaBoost and XGBoost.
This study describes a comparative analysis of the performance of several machine learning models namely Decision Tree, Support Vector Machine (SVM), AdaBoost and XGBoost in grouping Snort alarms when detecting attacks in the ISCXIDS2012 dataset. In the experiment, Snort alarm grouping was done with and without false positive alarms.
From the results of the research obtained, it was found that the application of machine learning methods can be used to group Snort alarms and reduce false positive alarm. Snort alarms can be grouped into 3 attack priority classes and 1 false positive alarm class. The accuracy of the model in grouping alarms into 3 priority classes and 1 class of false positive alarms ranges from 90.6% to 98.4%. In the Snort report dataset with a false positive rate of 26.77%, XGBoost model that is the best performing classifier is able to lower the Snort false positive rate to 0.39%.",,,156551,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
197253,,AMIRA SALSABILA H,TWITTER POSTS EMOTION CLASSIFICATION USING SUPPORT VECTOR MACHINE AND NAIVE BAYES CLASSIFIERS FOR PRODUCTS' FEEDBACK ANALYSIS,,,"emotion classification, Naive Bayes, SVM, feedbacks, tweets","Edi Winarko, M. Sc., Ph. D",2,3,0,2021,1,"Penelitian tesis ini terinspirasi dari skema anotasi yang ada untuk emosi dalam teks dan akan mengoptimalkan teknik pembelajaran mesin, Naive Bayes Classifiers dan Support Vector Machines, untuk klasifikasi. Model emosi skema untuk anotasi data dalam penelitian ini mendefinisikan emosi dasar menjadi kemarahan, kebahagiaan, kesedihan, kejutan positif dan kejutan negatif. Penelitian ini akan memberikan kontribusi pengetahuan terhadap model komputasi emosi yang paling sesuai untuk digunakan dalam proses analisis umpan balik. Selain itu, tesis ini mengkaji konten emosi dari postingan Twitter dan menggabungkan konten tersebut dalam strategi analisis umpan balik produk.

Dataset yang dihasilkan dalam penelitian ini adalah dataset iPhone 11 Camera yang terdiri dari 4.046 tweet beranotasi, dataset Amazon Alexa yang terdiri dari 747 tweet beranotasi, dan dataset Kylie Lip Kit yang terdiri dari 293 tweet beranotasi. Semua dataset diimplementasikan pada dua Naive Bayes Classifiers dan tiga kernel Support Vector Machines.

Penelitian ini menyimpulkan bahwa untuk klasifikasi emosi umpan balik produk, SVM linear dan RBF kernel memiliki kinerja terbaik, dengan akurasi 95% dan 96% pada Alexa Dataset masing-masing dan 91% dan 92% pada dataset iPhone 11 Camera. Sedangkan untuk Naive Bayes Classifiers, Complement Naive berkinerja terbaik, meskipun masih belum sebanding dengan RBF dan Linear kernel SVM, ia mengungguli kernel polinomial SVM.","This thesis research is inspired by existing annotation schemes for emotion in text and will optimise machine learning techniques, Naive Bayes Classifiers and Support Vector Machines, for classification. The emotion model schemed to annotate the data in this research defines basic emotions into anger, happiness, sadness, positive surprise and negative surprise. This research will contribute knowledge towards a computational model of emotions that is most suitable for use in the process of analyzing feedback. Moreover, this thesis examines the emotion contents of Twitter posts on and joins these contents in products' feedback analysis strategies.

The datasets produced in the research are the iPhone 11 Camera dataset consisting of 4,046 annotated tweets, the Amazon Alexa dataset consisting of 747 annotated tweets and the Kylie Lip Kit dataset consisting of 293 annotated tweets. All datasets were implemented on two Naive Bayes Classifiers and three kernels of Support Vector Machines.

This research concluded that for emotion classification of products' feedback, SVM's linear and RBF kernel performed best, with accuracies of 95% and 96% on the Alexa Dataset respectively and of 91% and 92% on the iPhone 11 Camera dataset. As for the Naive Bayes Classifiers, Complement Naive performed best, even though it was still not up to par with SVM's RBF and Linear kernels, it outperformed SVM's polynomial kernels.",,,158601,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203141,,AULYA HABIBY,ANALISIS SENTIMEN MENGENAI PENERAPAN PEMBELAJARAN DARING DI MASA PANDEMI COVID-19 MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK,,,"Analisis Sentimen, Pembelajaran Daring, Convolutional Neural Network, Naive Bayes","Khabib Mustofa, S.Si., M.Kom., Dr.tech.",2,3,0,2021,1,"Pandemi COVID-19 mengharuskan pemerintah untuk menerapkan sistem Pembelajaran Jarak Jauh secara Daring sebagai alternatif. Pembelajaran Daring mendapat banyak opini dari masyarakat, baik dukungan ataupun kritikan. Opini-opini tersebut disampaikan melalui berbagai media, Twitter salah satunya. Penelitian ini melakukan analisis sentimen mengenai opini masyarakat di Twitter tentang Pembelajaran Daring. Analisis sentimen yang dilakukan menggunakan metode Convolutional Neural Network dan Naive Bayes. 
Pengujian mendapatkan kecenderungan sentimen negatif mengenai Pembelajaran Daring dengan persentase rata-rata 46% negatif, 30% netral dan 24% positif pada pengujian 3 kelas. Pengujian 2 kelas mendapatkan 82% sentimen negatif dan 18% positif. Hasil yang didapatkan pada pengujian 3 kelas menggunakan Convolutional Neural Network yaitu 61.47% akurasi dengan waktu uji 3.53 detik dan 60.14% akurasi dengan  waktu uji 0.00182 detik menggunakan Naive Bayes. Pada pengujian 2 kelas menggunakan Convolutional Neural Network didapatkan 87.99% akurasi dengan waktu uji 3.9 detik dan 85.32% akurasi dengan waktu uji 0.00122 detik menggunakan Naive Bayes. Tidak terdapat perbedaan signifikan pada nilai akurasi menggunakan Convolutional Neural Network dan Naive Bayes. Pada sisi waktu komputasi, Naive Bayes memiliki waktu komputasi yang jauh lebih cepat dibanding Convolutional Neural Network.
","The COVID-19 pandemic requires the government to implement an Online Distance Learning system as an alternative. Online Learning gets a lot of opinions from the public, both support and criticism. These opinions are conveyed through various media, Twitter is one of them. This study conducted a sentiment analysis of people's opinions on Twitter about Online Learning. Sentiment analysis conducted using convolutional Neural Network and Naive Bayes methods. 
The test found a tendency of negative sentiment regarding Online Learning with an average percentage of 46% negative, 30% neutral and 24% positive in the 3-class test. The 2-class test received 82% negative sentiment and 18% positive. The results obtained in the 3-class test using convolutional Neural Network were 61.47% accuracy with a testing time of 3.53 seconds and 60.14% accuracy with a testing time of 0.00182 seconds using Naive Bayes. In the 2-class test using Convolutional Neural Network, 87.99% accuracy was obtained with a testing time of 3.9 seconds and 85.32% accuracy with a testing time of 0.00122 seconds using Naive Bayes. There were no significant differences in accuracy values using the Convolutional Neural Network and Naive Bayes.  On testing time, Naive Bayes has much faster testing times than the Convolutional Neural Network.
",,,154556,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201358,,RANGGA DIKA DANA PERMANA RANGKUTI,DETEKSI OFFENSIVE TWEET BAHASA INDONESIA DENGAN METODE NAIVE BAYES CLASSIFICATION DAN SUPPORT VECTOR MACHINE,,,"twitter, support vector machine, svm, nbc, naive bayes classifier, oversampling, bahasa indonesia","Bambang Nurcahyo Prastowo, Drs., M.Sc; Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2021,1,"Jumlah pengguna internet di Indonesia menurut survei APJII (Asosiasi Penyelenggara Jasa Internet Indonesia) tahun 2018 adalah 171,17 juta jiwa atau sebesar 63,85 persen dari total penduduk Indonesia. Dari penggunaan tersebut, mayoritas pengguna mengakses sosial media, salah satu yang populer adalah Twitter. Media sosial Twitter mengedepankan teks sebagai fitur utama untuk berbagi dengan tweet yang sangatlah beragam dari tweet yang informatif, menghibur, sampai menyinggung orang lain.
Sampai saat ini dengan bertambahnya jumlah pengguna media sosial Twitter, maka semakin banyak juga tweet yang mengandung umpatan, bernada negatif atau offensive. Dari tweet yang mengandung umpatan tersebut dapat dianalisis dengan metode analisis sentimen, sehingga adanya penelitian ini dilakukan untuk mengklasifikasi unggahan bernada offensive dengan mengoptimalkan model Support Vector Machine dan Naive Bayes Classifier.
Kedua model diimplementasi dengan menggunakan dataset tweet bernada offensive yang berbahasa Indonesia, namun dengan label yang tidak seimbang. Untuk menyelesaikan permasalahan imbalanced data, digunakan metode oversampling sehingga label pada kedua kelas menjadi seimbang. Terjadi kenaikan nilai pada metrik penilaian untuk model Naive Bayes Classifier sebanyak 0,002 untuk nilai akurasi, recall, dan f-1 score. Sedangkan untuk nilai precision naik sebanyak 0,001. Kenaikan terjadi setelah model NBC ditambahkan parameter set hasil dari pencarian model terbaik menggunakan grid search yaitu alpha=1,0 dan fit prior bernilai false. Hasil akhir dari model NBC adalah 0,906484 untuk akurasi, 0, 906484 recall, 0,90836 precision, dan 0,906494 untuk f-1 score.
xi
Sedangkan hasil yang didapat oleh model Support Vector Machine adalah akurasi 0,966169, recall 0,966169, precision 0,967553, dan f-1 score yang bernilai 0,966104. Nilai-nilai tersebut tidak mengalami kenaikan setelah ditambahkan parameter set hasil dari model terbaik. Parameter set yang dimaksud adalah parameter c=1, kernel=rbf, degree=1, gamma=scale, coef=0, shrinking bernilai true, dan probability bernilai true. Sehingga dapat disimpulkan dalam penelitian ini model Support Vector Machine lebih efektif dalam mendeteksi tweet dalam bahasa Indonesia apakah bernada offensive atau tidak.","The number of internet users in Indonesia according to the APJII (Indonesian Internet Service Provider Association) in 2018 is 171,17 million people or as much as 68,35 percent of total of the Indonesian population. The majority of internet users are social media users, one of popular social media platform is Twitter. Twitter allows user to send and receive short text or short post which contains informative, entertaining, or offensive tweet.
The increase in the number of users on Twitter has had an impact on a growing number of offensive tweet. The offensive tweet can be analyzed using sentiment analysis, therefore this research used Support Vector Machine and Naive Bayes Classifier to optimize the offensive classification.
Both models has been implemented using dataset contains offensive tweet in Bahasa Indonesia. Dataset used in this research is imbalanced which has unequal class distribution, so that oversampling is implemented to handle this problem. Based on test performed, the value of accuracy, recall, and f-1 score of Naive Bayes Classifier model went up 0.002 while the precision only rose 0.001. An increase in these metrics can be caused by best parameter set which produced by hyperparameter tuning using grid search, they are 1,0 for alpha and false for fit prior. So that we have obtained 0.906484 of accuracy, 0.906484 of recall, 0.90836 of precision, and 0.906494 of f-1 score with Naive Bayes Classifier.
The trend was reversed in Support Vector Machine model, there is no increase in the metrics after best parameter set were implemented. They are
xiii
0.966169 of accuracy, 0.966169 of recall, 0.967553 of precision, and 0.966104 of f-1 score, and also the parameter set we have obtained c=1, kernel=rbf, degree=1, gamma=scale, coef=0, shrinking is true, and probability is true. From these results, Support Vector Machine is more effective than Naive Bayes Classifier to detect offensive tweet in Bahasa Indonesia.",,,152804,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200849,,SIDIQ TRI PRATIKTO,DETEKSI SIMILARITAS DENGAN METODE SEMANTIC DAN SYNTACTIC DALAM PENULISAN TUGAS AKHIR,,,"Similaritas Jaccard, semantic, syntactic, pustaka Gensim, preprocessing, text segmentation, stop word removal, stemming, suspected, sumber, word embeddings, vector cosine similarity, threshold value, precision, recall, F-measure","Suprapto, Drs., M.Kom., Dr.",2,3,0,2021,1,"Penelitian ini bertujuan untuk membangun program yang dapat mendeteksi tingkat similaritas dua dokumen tugas akhir yang berfokus kepada abstraknya. Terdapat banyak algoritma dalam mendeteksi tingkat similaritas dua dokumen di mana salah satunya menggunakan similaritas Jaccard (Jaccard, P., 1901). Proses perhitungan diawali dengan tahapan pre-processing abstrak-abstrak tugas akhir yang merupakan abstrak suspected dan yang merupakan abstrak sumber yang terdiri atas proses text segmentation, stop word removal dan stemming. Setelah masing-masing abstrak melalui tahapan pre-processing, selanjutkan setiap kalimat yang berada di abstrak suspected dipasangkan dengan setiap kalimat yang berada di abstrak sumber untuk dihitung nilai similaritasnya. Perhitungan dilakukan dengan menggunakan dua metode, yakni metode semantic dan metode syntactic yang kadarnya ditentukan suatu variabel alfa dalam rentang nilai 0-1. Dalam proses perhitungan nilai similaritas pasangan kalimat dari abstrak suspected dan abstrak sumber, terdapat proses perhitungan nilai similaritas setiap dua kata di mana proses ini menggunakan konsep word embeddings dengan data train berupa dokumen-dokumen Skripsi Program Studi Ilmu Komputer UGM yang diimplementasikan menggunakan word2vec dan vector cosine similarity. Pasangan kalimat yang memiliki nilai similaritas melewati batas threshold value dinyatakan similar dan menjadi kandidat pasangan kalimat yang similar dalam pemeriksaan dua abstrak. Program ditulis dalam bahasa Python dan menggunakan pustaka Gensim yang di dalamnya terdapat word2vec. Hasil akhir dari penelitian ini memperlihatkan bahwa metode yang diterapkan pada penelitian ini memiliki hasil yang baik, yakni memiliki performa terbaik dengan nilai Precision didapatkan di angka 1, nilai Recall didapatkan di angka 1 dan F-measure didapatkan di angka 1, setara dengan metode similaritas Jaccard yang memiliki performa terbaik serupa. Namun, dikarenakan dataset yang tidak banyak, metode yang diterapkan di penelitian ini memberikan nilai similaritas yang cukup tinggi terhadap pasangan kalimat yang tidak similar, sehingga threshold value ditetapkan dengan nilai yang tinggi untuk mendapatkan performa yang baik.","This research aims to build a program that can detect the level of similarity of two thesis that focused on the abstract. There are many algorithms in detecting the level of similarity of two documents, one of which uses Jaccard similarity (Jaccard, P., 1901). The calculation process begins with the pre-processing abstract thesis which is the suspected abstract and which is the source abstract consisting of text segmentation, stop word removal and stemming. After each abstract goes through the pre processing step, then each sentences in the abstract suspected is paired with every sentences in the source abstract to calculate the similarity value. The calculation is carried out using two methods, namely the semantic method and the syntactic method whose content is determined by a variable alpha in the value range of 0-1. In the process of calculating the similarity value of sentence pairs from the suspected abstract and the source abstract, there is a process of calculating the similarity value of each two words where this process uses the concept of word embeddings with data train in the form of Computer Science UGM thesis documents implemented using word2vec and vector cosine similarity. Sentence pairs that have similarity values exceeding the threshold value are declared similar and become candidate pairs of sentences that are similar in the examination of two abstracts. The program is written in Python and uses the Gensim library which includes word2vec. The final result of this study shows that the method applied in this study has good results, namely it has the best performance with the value of Precision obtained at number 1, the value of Recall obtained at number 1 and F-measure obtained at number 1, equivalent to the Jaccard similarity method which has the same best performance result. However, because there are not many dataset, the method applied in this study gives a fairly high similarity value to sentence pairs that are not similar, so threshold value is set with a high value to get performance the good one.",,,152464,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201105,,GREGORIUS ARIA NERUDA,DETEKSI PERISTIWA LALU LINTAS MENGGUNAKAN KOMBINASI CONVOLUTIONAL NEURAL NETWORK (CNN) DAN BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS (BERT) DARI DATA TWITTER,,," deteksi peristiwa lalu lintas, text classification, tweet classification, multi-label classification, Convolutional Neural Network, CNN, Bidirectional Encoder Representations from Transformers, BERT"," Drs. Edi Winarko, M.Sc., Ph.D.",2,3,0,2021,1,"Mengetahui kondisi lalu lintas secara real-time merupakan salah satu hal yang penting bagi masyarakat modern ini. Hal ini menjadi tantangan, karena sulitnya untuk mendeteksi peristiwa lalu lintas menggunakan sensor fisik konvensional.

Perkembangan sosial media dapat menjadi alternatif solusi untuk masalah ini. Twitter merupakan salah satu sosial media yang populer di kalangan masyarakat Indonesia. Mayoritas pengguna Indonesia menggunakan Twitter untuk
mengungkapkan keluh kesahnya di dalam sebuah tweet, di antaranya adalah keluh kesah tentang lalu lintas. Beberapa penelitian telah memanfaatkan data tweet untuk membangun sebuah model klasifikasi tweet lalu lintas berbahasa Indonesia. Namun, belum ada yang mengombinasikan model deep learning dan contextualized word embedding di dalam arsitekturnya.

Pada penelitian ini akan dilakukan deteksi peristiwa lalu lintas menggunakan data Twitter dengan mengombinasikan model Convolutional Neural Network (CNN) dan Bidirectional Encoder Representations from Transformers (BERT). CNN akan digunakan untuk klasifikasi dan BERT digunakan untuk ekstraksi fitur. Ada 4 macam peristiwa yang diklasifikasi: insiden, alam, intensitas lalu lintas, dan lainnya.","Knowing traffic condition in a real-time manner is one of an important feature for modern society. This can be a challenge due to how hard it is to detect a traffic event using a conventional physical sensor. The growth of social media can be an alternative solution for this problem.

Twitter is one of these social media popular in Indonesian society. The majority of Indonesian users use Twitter to talk about something in a tweet, one of them being complaint about traffic. A few researches has taken advantage of
tweet data to build a traffic tweet classification model in Bahasa Indonesia. However, non has combined a deep learning model and contextualized word embedding in its
architecture. 

This research proposes a model that combines Convolutional Neural Network (CNN) and Bidirectional Encoder Representations from Transformers (BERT). CNN
will be used to classify whilst BERT is used for feature extraction. 4 kinds of traffic events is classified: incident (insiden), nature (alam), traffic intensity (intensitas lalu
lintas), and others (lainnya).",,,152499,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
204690,,HUDA MUSTAKIM,ANALISIS SENTIMEN BERBASIS ASPEK PADA ULASAN PENGGUNA APLIKASI KAI ACCESS MENGGUNAKAN SUPPORT VECTOR MACHINE DAN RANDOM OVERSAMPLING,,,"aspect based sentiment analysis, support vector machine, imbalance data, oversampling, kai access","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2021,1,"Tingginya penggunaan internet telah mempengaruhi berbagai hal. Salah satunya adalah dengan diluncurkannya aplikasi KAI Access oleh PT Kereta Api Indonesia (PT KAI) sebagai sistem pemesanan tiket online. Sayangnya, banyak ditemukan ulasan negatif di Google Play Store yang harus diperhatikan pihak pengembang. Sudah ada penelitian pada ulasan tersebut menggunakan metode klasifikasi Support Vector Machine (SVM), namun hasil klasifikasi hanya mampu mengklasifikasi sentimen pada ulasan secara umum, sehingga sentimen dan informasi terkait aspek penting seperti aspek-aspek usability aplikasi belum diketahui. Selain itu, belum adanya penanganan imbalance data menyebabkan akurasi prediksi pada kelas minoritas menjadi rendah. Pada penelitian ini dilakukan analisis sentimen berbasis aspek untuk mengetahui sentimen pengguna aplikasi KAI Access terhadap aspek usability aplikasi yaitu learnability, efficiency, errors, dan satisfaction, menggunakan metode SVM, dengan penerapan Random Oversampling untuk mengatasi imbalance data, serta beberapa tahap preprocessing data dengan ekstraksi fitur menggunakan TF-IDF. Untuk mendapatkan hasil klasifikasi yang lebih baik, juga dilakukan hyperparameter tunning pada model SVM menggunakan GridSearch CV. Sementara itu, data penelitian ini diambil dari Google Play Store. Dari hasil penelitian, berhasil dibuat model klasifikasi dari metode SVM yang mampu mendeteksi sentimen ulasan di setiap aspek aplikasi, dengan performa yang cukup baik, yakni dengan rata-rata skor dari seluruh model klasifikasi dari hasil pengujian yang memiliki skor akurasi 91,24%, dan f1-score 71,42%. Penerapan teknik oversampling juga berhasil meningkatkan f1-score dan kemampuan model klasifikasi pada kelas minoritas. Sementara itu, hasil analisis menunjukkan bahwa mayoritas sentimen bernilai negatif di seluruh aspek aplikasi terutama pada aspek errors, yang menandakan tingginya tingkat kesalahan sistem.","The high use of the internet has affected various things. One of them is PT Kereta Api Indonesia (PT KAI) which has launched KAI Access application as online ticket booking system. However, since its launch, many negative reviews have been found in Google Play Store that developers must pay attention to. There has been research on the review using classification method Support Vector Machine (SVM), but the system created is only able to classify sentiment on review data in general, so that sentiment and information regarding importance aspects related to the application are not yet known. In addition, the absence of handling imbalance data causes prediction accuracy in the minority class is poor. In this study, an aspect-based sentiment analysis was carried out to determine the sentiments of users of the KAI Access application on the usability aspects of the application, namely learnability, efficiency, errors, and satisfaction, using the SVM method, with the application of Random Oversampling to overcome data imbalance, as well as several stages of preprocessing data with feature extraction. using TF-IDF. To get better classification results, hyperparameter tuning is also performed on the SVM model using GridSearch CV. Meanwhile, this research data was taken from the Google Play Store. From the results of the study, a classification model from the SVM method was successfully created and able to detect sentiment on each review for every application aspect, with a fairly good performance, with average score of all classification models from the test results has accuracy score of 91.24%, and f1- score 71.42%. The use of oversampling technique also successfully enhance the f1-score and the model's ability in classifying the minority class. Meanwhile, the results of the analysis show that the majority of sentiments are negative in all application aspects especially on errors aspect, which indicates a high level of system error.",,,156120,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195739,,RIVANDA IRAWAN,TRANSLITERASI AKSARA JAWA MENJADI ALFABET LATIN MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK,,,"pengenalan karakter, convolutional neural network, deep learning","Nur Rokhman, S.Si., M.Kom., Dr. ;Dyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2021,1,"Aksara Jawa dapat ditemukan dalam berbagai tulisan Jawa kuno, dan sampai sekarang masih digunakan untuk berbagai aplikasi di Jawa modern, tetapi dukungan untuk penggunaan Aksara Jawa dalam media digital serta pengetahuan mengenai Aksara Jawa untuk kebanyakan orang masih terbilang kurang, dikarenakan preferensi penggunaan alfabet Latin dibanding Aksara Jawa.

Penelitian ini dilakukan untuk merancang sebuah metode untuk melakukan transliterasi 20 Aksara Jawa Wyanjana menjadi alfabet Latin dengan menggunakan Convolutional Neural Network untuk melatih sebuah mesin pengenal karakter. Dataset yang digunakan adalah 5000 citra yang terdiri dari 1000 citra tulisan tangan Aksara Jawa dan 4000 data hasil augmentasi citra tulisan tangan tersebut.

Hasil penelitian menunjukkan bahwa konfigurasi paling optimal untuk perancangan mesin pengenal karakter Aksara Jawa menggunakan Convolutional Neural Network adalah dengan menggunakan algoritma optimisasi AdamW Optimizer dengan konfigurasi hyperparameter jumlah epoch sebanyak 50 dan minibatch size sebesar 32, dengan rata-rata akurasi sebesar 89.60%.","The Javanese script can be found in many old Javanese texts, and is still used to some extent in modern Java. However, there is a somewhat noticeable lack of support for the script digitally and knowledge about the script is limited for most people, which would prefer standard Latin alphabet rather than the script.

The purpose of this research was to devise a method to transliterate 20 Wyanjana Javanese scripts into Latin alphabet with the use of a Convolutional Neural Network to train an Optical Character Recognition. The dataset used was 5000 images which consists of 1000 Javanese Script handwriting images and 4000 augmentated image data of the handwritings.

Through the research, it was found that the most optimal configuration to build a Javanese Script Recognition program using Convolutional Neural Network is using the AdamW Optimizer optimisation algorithm, with hyperparameters of 50 epochs and minibatch size of 32, which returns an average accuracy of up to 89.60% for this research.",,,147065,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200604,,MUHAMMAD MUKHARIR,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN LAPTOP MENGGUNAKAN METODE AHP DAN PROFILE MATCHING,,,"SPK, Laptop, AHP, Profile Matching, Interpolasi Linear","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.",2,3,0,2021,1,"Laptop adalah personal computer (PC) desktop yang dimensinya diperkecil untuk meningkatkan fleksibilitas dalam penggunaannya. Akan tetapi, banyaknya produk akan membuat kesulitan oleh konsumen dalam menentukan pilihan laptop yang sesuai dengan kebutuhan konsumen yang ingin membelinya.

Tujuan penelitian ini adalah untuk membantu pembeli yang ingin membeli produk laptop sesuai kebutuhan pembeli dengan membuat Sistem Pendukung Keputusan (SPK). Kriteria yang dipertimbangkan pada penelitian ini ada 12 kriteria yaitu harga, prosesor, kapasitas RAM, kapasitas harddisk, kapasitas SSD, kapasitas V-RAM, kapasitas maksimal upgrade RAM, berat laptop, ukuran layar, jenis layar, refresh rate layar, dan resolusi layar. Dalam memilih produk laptop ada nilai kriteria dari produk laptop dan nilai kriteria preferensi dari pembeli sebagai pembuat keputusan. Juga nilai-nilai kriteria pada produk laptop memiliki kontribusi berbeda terhadap nilai keseluruhan produk laptop tersebut. Dengan demikian, metode yang dipakai adalah Analytical Hierarchy Process (AHP), Profile Matching (PM) dengan interpolasi linear, dan Simple Addictive Weighting (SAW) untuk menentukan rekomendasi pilihan. Pada akhir penelitian ini diharapkan SPK yang telah dibuat mampu memberikan rekomendasi alternatif pilihan terbaik dan paling sesuai dengan kebutuhan pembeli dalam proses pemilihan produk laptop.

Kata kunci: SPK, Laptop, AHP, Profile Matching, Interpolasi Linear","A laptop is a desktop personal computer (PC) whose dimensions are reduced to increase flexibility in its use. However, the large number of products will make it difficult for consumers to choose a laptop that suits the needs of consumers who want to buy it.

The purpose of this research is to help buyers who want to buy laptop products according to their needs by making a Decision Support System (SPK). There are 12 criteria considered in this study, namely price, processor, RAM capacity, hard disk capacity, SSD capacity, V-RAM capacity, maximum RAM upgrade capacity, laptop weight, screen size, screen type, screen refresh rate, and screen resolution. In choosing a laptop product there is a criterion value of a laptop product and a value of preference criteria from the buyer as a decision maker. Also the criteria values on laptop products have different contributions to the overall value of the laptop product. Thus, the methods used are Analytical Hierarchy Process (AHP), Profile Matching (PM) with linear interpolation, and Simple Addictive Weighting (SAW) to determine the recommended options. At the end of this research, it is hoped that the SPK that has been made will be able to provide recommendations for the best alternative choices and best suit the needs of buyers in the laptop product selection process.

Keywords: DSS, Laptop, AHP, Profile Matching, Linear Interpolation",,,152090,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201119,,MOH. MAHDI AMANULLAH,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN SUPLEMEN UNTUK KEBUTUHAN NUTRISI SETELAH GYM MENGGUNAKAN METODE PROFILE MATCHING DAN SIMPLE ADDITIVE WEIGHTING (SAW),,,"Suplemen, SPK, Profile Matching, SAW","Drs. Retantyo Wardoyo, M.Sc., Ph.D",2,3,0,2021,1,"        Suplemen gym merupakan suatu asupan tambahan yang diperuntukan bagi para penggiat aktivitas fisik berupa gym atau fitness. Suplemen gym diperlukan bagi para penggiat aktivitas gym yang ingin membentuk otot tubuh dan menjaga kebugaran secara serius. Hal ini dikarenakan suplemen gym mnegandung berbagai macam  makro-nutrient yang diperlukan dalam pembangunan otot dan pemulihan stamina. Kendala dalam memilih suplemen gym adalah banyaknya varians dari suplemen itu sendiri, dimana tiap-tiap varian diperuntukan bagi tiap-tiap individu sesuai dengan kondisi fisik dan kebutuhan masing-masing.

        Berkaitan dengan hal tersebut, peneliti melakukan penelitian mengenai sistem pendukung keputusan (SPK) yang bertujuan untuk membantu para penggiat aktivitas gym terutama untuk kalangan pemula dalam memilih suplemen yang sesuai dengan profil fisik dan kebutuhan masing-masing. Penelitian ini memanfaatkan metode profile matching dan metode simple additive weighting (SAW). profile matching akan digunakan sebagai penentu skor dari masing-masing kriteria alternatif. SAW digunakan untuk menentukan besaran nilai akhir dan urutan peringkat paling direkomendasikan dari alternatif-alternatif pilihan yang tersedia. 

        Pada akhir Penelitian ini, SPK pemilihan suplemen untuk kebutuhan nutrisi setelah gym dengan menggunakan metode profile matching dan simple additive weighting (SAW) berhasil dibuat. Berdasarkan pengujian hasil menggunakan 17 data yang didapatkan melalui survey dan pengujian sistem, SPK pemilihan suplemen untuk kebutuhan nutrisi setelah gym dengan menggunakan metode profile matching dan simple additive weighting (SAW) dapat bekerja dan menghasilkan informasi yang sesuai dengan kebutuhan (dibuktikan dengan sesuainya hasil keluaran dari sistem dengan keputusan langsung oleh pengguna) sehingga dapat membantu pengguna dalam menentukan pilihanya.


Kata Kunci: Suplemen, SPK, Profile Matching, SAW
","        Gym supplements are an additional intake that intended for physical activity activists in the form of a gym or fitness. Gym supplements are needed for gym activists who want to build muscle and maintain fitness seriously. This is because gym supplements contain a variety of macro-nutrients needed for muscle building and stamina recovery. The obstacle in choosing gym supplements is the large number of variants of the supplements themselves, where each variant is intended for each individual according to their physical conditions and needs.


Keywords: Supplements, DSS, Profile Matching, SAW
 	In this regard, researchers conducted research on decision support systems (DSS) which aims to help gym activity activists, especially for beginners, in choosing supplements that suit their physical profiles and individual needs. This research utilizes the profile matching method and the simple additive weighting (SAW) method. profile matching will be used as a determinant of the score of each alternative criteria. SAW is used to determine the final value and the most recommended ranking order of the available alternatives.

       At the end of this study, the DSS for the selection of supplements for nutritional needs after the gym using profile matching and simple additive weighting (SAW) methods was successfully made. Based on the test results using 17 data obtained through surveys and system testing, the DSS for selecting supplements for nutritional needs after the gym using the profile matching method and simple additive weighting (SAW) can work and produce information according to needs (as evidenced by the appropriate output from system with direct decisions by the user) so that it can assist users in making their choices.
",,,152523,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200879,,DANIEL SETYOHANDOKO,PERAMALAN POPULARITAS GENRE VIDEO GAME DI YOUTUBE MENGGUNAKAN MODEL AUTO-REGRESSIVE BERDASARKAN PERTAMBAHAN JUMLAH VIEW HARIAN,,,"peramalan, auto-regresi, YouTube, video game","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2021,1,"YouTube, situs penyedia video yang saat ini sudah menjadi salah satu media hiburan terbesar, menarik minat banyak orang baik untuk menjadikannya sarana hiburan atau ladang penghasilan, dengan topik-topik video yang ada meliputi berbagai macam hal, mulai dari berita, klip musik, tips-tips memasak, hingga tentang video game. Orang-orang yang menjadi pembuat video atau melakukan bisnis yang berkaitan dengan YouTube akan membutuhkan data tentang hal apa yang akan bisa menjadi populer dengan menggunakan berbagai metode, salah satunya peramalan.

Model peramalan Auto-Regressive (AR) sebagai salah satu model peramalan digunakan dalam penelitian ini untuk dilihat keakuratannya dalam meramal genre video game apa yang akan menjadi populer berikutnya di YouTube, dengan menggunakan data observasi dalam rentang waktu 10 September 2020 hingga 9 Desember 2020 dari beberapa video yang dijadikan perwakilan untuk masing-masing genre video game yang dijadikan sampel. Data uji diproses dengan menerapkan model yang sudah dilatih terhadap 20% dari keseluruhan data tiap genre dan dibandingkan dengan data observasi untuk dihitung akurasinya. 

Hasil peramalan yang didapat menunjukkan hasil yang cukup akurat, dimana rata-rata nilai MAE yang didapat adalah 59.2196 dan rata-rata nilai MAPE 6.52%, dengan genre yang menjadi paling populer berdasarkan pertambahan jumlah view hariannya adalah genre Sandbox.
","YouTube, a video-sharing platform that has become one of the biggest entertainment media, attracts the interest of many people to use it as a way of entertainment or a way of getting income, which includes many videos on multitudes of topics such as news, music clips, cooking tips and even video games. People who become a video creator or having business related to YouTube will need datas about what kind of things that will become popular by using many methods, one of which is prediction.

Auto-regressive (AR) prediction model, as one of the prediction models, is used in this research to observe its accuracy in predicting which video game genre will become popular in YouTube, by using observation data from 10 September 2020 to 9 December 2020 from several videos selected as samples for each of the selected video game genre samples. The test data is processed by applying the trained model to 20% of the overall data from each genre and then compared to the original observed data to calculate the prediction model&acirc;€™s accuracy.

The result of the forecast displays quite accurate results, where the average value of MAE is 59.2196 and the average value of MAPE 6.52%, with the genre becoming the most popular based on its daily view gain is the Sandbox genre.
",,,152302,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
198580,,MUHAMMAD RIZKI RAMADHANA,PERBANDINGAN ANTARA SOCCER GAME OPTIMIZATION DENGAN ALGORITME BACKPROPAGATION PADA FASE LATIH JARINGAN SYARAF TIRUAN,,,"optimisasi Neural networks, Backpropagation, Algoritme Soccer Game Optimization, meta-heuristik","Suprapto, Drs., M.I.Kom., Dr. ; Faizal Makhrus,S.Kom., M.Sc., Ph.D",2,3,0,2021,1,"Feedforward neural networks (FNN) merupakan model pembelajaran mesin yang telah digunakan di berbagai bidang industri. FNN secara umum menggunakan algoritme backpropagation (BP) berbasis gradien dalam melakukan optimasi pada fase trainingnya. Algoritme BP dapat terjebak pada titik lokal optima pada ruang pencarian karena sifat dari gradien. Untuk itu para peneliti mencoba menggunakan algoritme metaheuristik pada FNN. Algoritme metaheuristik bernama Soccer Game optimization diperkenalkan pada tahun 2015. SGO memberikan hasil yang baik dalam masalah optimasi dari fungsi diskrit dan kontinu dibanding algoritme metaheuristik lain. Walalupun begitu, SGO belum pernah digunakan pada optimasi FNN.
Pada penelitian ini, dilakukan perbandingan algoritme SGO dengan algoritme BP dalam melakukan optimasi FNN. Kedua algoritme diterapkan pada lima dataset klasifikasi untuk dibandingkan nilai akurasi dan kecepatan waktu komputasi pada saat training.
Hasil yang didapat Algoritme BP memperoleh nilai akurasi yang lebih baik dibandingkan algoritme SGO pada 4 dari 5 permasalahan klasifikasi. Waktu komputasi SGO dapat lebih cepat dari pada BP dengan menggunakan stopping criteria. Hal itu terjadi karena nilai Cross entropy loss pada SGO membaik dengan cepat di iterasi awal, namun ternyata lebih mudah terjebak di lokal optima dibandingkan BP.","Feedforward neural networks (FNN) is a machine learning model which has the capabilities of approximating any continuous function. FNN used backpropagation which is a gradient-based optimization technique for optimizing its objective function. However, because of the limitations of gradient-based algorithms which has tendency to fall in one local optima or plateaus, the necessity of metaheuristic-based optimization methods was recognized. In 2015 a new metaheuristics algorithm named Soccer Game optimization (SGO) were introduced. SGO outperformed others popular metaheuristics methods in optimizing descrete and continous problems. Nevertheless, SGO have never been used in optimizing FNN.
This research attempts to compare SGO and BP in solving FNN optimization. Both algorithm being compared by their prediction accuracy and computation time while training. Benchmark problems consist of 5 classification problems from UCI machine learning dataset.
The result is BP outperform SGO in accuracy and loss function for 4 out of 5 classification problems. Computation time of SGO can be better than BP if using stopping criteria. That happen because loss function in SGO gets better in earlier iteration but easiear to fall into local optima than BP.",,,150214,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195522,,NATHANAEL GAVIN HASIHOLAN SIREGAR,Peringkasan Ekstraktif Otomatis pada Berita Berbahasa Indonesia Menggunakan A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS),,,"Peringkasan Teks Otomatis, Ekstraktif, HSSAS","Sri Mulyana, Drs., M.Kom.",2,3,0,2021,1,"Dengan semakin banyaknya masyarakat yang berpindah dari media cetak ke media digital dan berita daring yang tersedia di internet, maka dibutuhkan suatu sistem peringkasan otomatis untuk artikel berbahasa
Indonesia. Sistem ini akan mempermudah masyarakat yang ingin membaca banyak berita untuk dapat mengetahui dan memahami intisari dari keseluruhan berita dengan cara yang mudah dan dalam waktu yang singkat tanpa harus membaca seluruh isinya. Penelitian ini mengimplementasikan metode peringkasan artikel otomatis A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS) pada dataset artikel berbahasa Indonesia dan mengevaluasi performa dari metode tersebut dengan menggunakan metode ROUGE untuk membandingkan kualitas ringkasan yang dibuat secara otomatis dengan ringkasan yang dibuat manual oleh manusia yang tersedia pada dataset IndoSum. Pengujian dilakukan dengan menghitung skor f-measure dari metriks evaluasi ROUGE-1, ROUGE-2, dan ROUGE-L. Berdasarkan pengujian yang telah dilakukan, metode HSSAS memperoleh skor f-measure ROUGE-1 sebesar 67.37%, ROUGE-2 sebesar 60.65%, dan ROUGE-L sebesar 66.61%.","As many people are beginning to migrate from printed newspaper to the online version, the necessity of an automatic summarization for Indonesian news articles is also increasing. This system will help everyone who wants to read a lot of information from news articles to know and understand the essence of every news article easily and in a relatively short time without having to read the whole news article. This research implements A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS) model to process a dataset that consists of Indonesian news articles. Then, this research evaluates the performance of the model using the ROUGE method to compare the quality of the summary that is generated automatically by the model with the one that is arranged manually by humans, which is available on the dataset. The evaluation is done by calculating the f-measure score of the metric evaluation ROUGE-1, ROUGE-2 and ROUGE-L. Based on the result of the evaluation, the HSSAS model get f-measure score 67.37% for ROUGE-1, 60.65% for ROUGE-2, and 66.61% for ROUGE-L.",,,146771,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195779,,MUHAMMAD NUGROHO FITRIANTO,Klasifikasi Gambar Karakter Anime Menggunakan Convolutional Neural Network,,,"Anime, Convolutional Neural Network","Dr. Sigit Priyanta, S.Si., M.Kom.",2,3,0,2021,1,"Penglihatan komputer (Computer Vision) adalah bidang studi yang membahas dan berupaya mengembangkan teknik tentang bagaimana komputer dapat melihat dan memahami gambar atau video digital. Hal ini memungkinkan komputer untuk menangkap informasi pada gambar atau video yang diberikan dan melakukan berbagai tugas terkait gambar dan video. Metode yang paling populer digunakan dalam tugas klasifikasi gambar adalah Convolutional Neural Network (CNN). Jaringan convolutional bekerja dengan cara menerima rangsangan visual (gambar atau video) kemudian melakukan operasi penjumlahan berat dari piksel ke piksel yang terdapat pada citra digital melalui bagian kecil yang disebut kernel. Penelitian ini bertujuan membangun model CNN untuk mengenali objek gambar ilustrasi khususnya untuk klasifikasi karakter animasi khas Jepang atau sering disebut karakter Anime. Melalui penelitian ini dibuktikan bahwa model CNN yang dikhususkan dalam melakukan tugas klasifikasi suatu objek mencapai akurasi yang lebih baik daripada suatu model terlatih yang telah banyak melihat berbagai gambar dari dunia nyata. Model CNN dengan 6 lapisan tersembunyi mencapai tingkat akurasi 85,34% dan model CNN dengan 5 lapisan tersembunyi mencapai tingkat akurasi 77,26%, dibandingkan model terlatih VGG-16 dalam melakukan klasifikasi karakter Anime yang mencapai tingkat akurasi 66,98%.","Computer vision is a field of study that discusses and seeks to develop techniques on how computer could &quot;see&quot; and understand on images or videos. This allows the computer to capture information on a given image or video and perform various image and video related tasks. The most popular method used in image classifitcation task is the Convolutional Neural Network (CNN). Convolution works by receiving visual object (images or videos) and then performing the operation of adding weight from pixels to pixels contained in the digital image through a small part called the kernel. This study aims to build a CNN model to recognize illustration image objects, especially for the classification of Japanese animation characters or often called Anime characters. Through this study, it is proven that the CNN model which is specialized in performing the task of classifying an object achieves better accuracy than a trained model that has seen many images from the real world. The CNN model with hidden 6 layers achieves an accuracy rate of 85,34% and the CNN model with hidden 5 layers reaches a 77,26% accuracy rate, compared to the VGG-16 trained model in classifying Anime characters which reaches a 66,98% accuracy rate.",,,147210,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195530,,ALMAS FAUZIA WIBAWA,Pengaruh Ensemble Features pada Support Vector Machine untuk Analisis Sentimen Publik Terhadap Pembatasan Sosial Berskala Besar Jilid II DKI Jakarta,,,"analisis sentimen, Bahasa Indonesia, Instagram, ensemble features, TF-IDF, Support Vector Machine, COVID-19, Pembatasan Sosial Berskala Besar, PSBB Jilid II, DKI Jakarta.","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2021,1,"Salah satu kebijakan yang dibuat pemerintah dalam mengatasi dan mencegah wabah COVID-19 adalah Pembatasan Sosial Berskala Besar (PSBB) di Provinsi DKI Jakarta. Berbagai respon diterima pemerintah. Sementara itu, pemimpin perlu memiliki sudut pandang lain sebagai pertimbangan pengambilan kebijakan dan mengetahui keadaan masyarakat sebelum mengumumkan kebijakan agar tidak terjadi kepanikan publik. Instagram merupakan media sosial yang cukup populer di Indonesia. Pemimpin daerah pun mulai menggunakannya untuk mengumpulkan aspirasi dari masyarakat melalui kolom komentar. Sementara itu, analisis sentimen kerap dijadikan sarana penyelesaian untuk merangkum opini publik di berbagai topik dengan data bersumber dari media sosial.

Penelitian ini melakukan analisis sentimen untuk mengetahui respon publik terhadap diberlakukannya kembali PSBB di DKI Jakarta dengan melihat pengaruh fitur tekstual, fitur parts of speech, dan fitur lexicon-based pada ensemble features terhadap metode ekstraksi fitur Term Frequency - Inverse Document Frequency (TF-IDF) serta metode klasifikasi Support Vector Machine dengan data komentar Instagram berbahasa Indonesia. Data yang digunakan bersumber dari komentar pada akun @aniesbaswedan dan @dkijakarta di Instagram yang mengumumkan akan diberlakukannya PSBB Jilid II.

Setelah dilakukannya penelitian ini, diperoleh kesimpulan bahwa mayoritas respon publik terhadap kebijakan PSBB Jilid II bersentimen negatif. Selain itu, ensemble features mendatangkan pengaruh yang baik di sisi akurasi, recall, dan F1-measure, namun tidak di sisi presisi. Begitu juga dengan fitur parts of speech dan lexicon-based secara individu, namun tidak dengan fitur tekstual yang tidak berpengaruh baik di sisi akurasi. Model terbaik yang ditemukan merupakan model yang dihasilkan oleh kombinasi fitur lexicon-based dan TF-IDF dengan akurasi dan F1-measure tertinggi, secara berturut-turut, 79,41 % dan 55,80 %.","One of government's policy to overcome and prevent further damage from COVID-19 pandemic is to enact the Large Scale Social Limitation (LSSL) in DKI Jakarta. The government received all kinds of responses. Meanwhile, a leader should have another perspective as a consideration before making a decision and as a knowledge about public's perceptions before announcing a new policy in order to control public's response. Instagram is one of the most popular social media in Indonesia. Regional governments are already using it to gain public's aspiration through the comment section. Meanwhile, sentiment analysis often used as a solution to summarize public's opinion in a particular topics with social media data.

In this research, public's response's sentiment about the re-enactment of PSBB in DKI Jakarta are analyzed. The effect of textual features, parts of speech features, and lexicon-based features, as a part of ensemble features, on Term Frequency - Inverse Document Frequency (TF-IDF) and Support Vector Machine (SVM) with comments data from Instagram in Bahasa Indonesia are also being researched. Data that are used in this research are gained from public's comments on @aniesbaswedan and @dkijakarta's post, that are also LSSL Volume II enactment announcement, in Instagram.

After the research is done, we can conclude that the majority of public's responses about LSSL Volume II have negative sentiments. Meanwhile, ensemble features make a good effect on the accuracy, recall, and F1-measure of classification model using TF-IDF, SVM, and the data, but bad effect on the precision, as well as parts of speech features and lexicon-based features individually. It is different with the textual features that also have bad effect on accuracy. The combination of lexicon-based features and TF-IDF forms the best model with the best accuracy and F1-measure score 79.41 % and 55.80 %.",,,146801,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
198861,,IRFAN KARUNIA FAJRI,SOCIAL MEDIA CONTENT SORTING BY USER PREFERENCE,,,"Social media, information, retrieval, relevance, message sorting","Faizal Makhrus, S.Kom., M.Sc., Ph.D.",2,3,0,2021,1,"Tidak diragukan lagi bahwa saat ini manusia hidup di era informasi. Dengan
bantuan internet, khususnya World Wide Web, informasi mengalir dengan bebas tanpa
memperhatikan waktu, lokasi, dan konstruksi lain, baik buatan manusia ataupun alam,
fisik maupun konseptual. Laju informasi ini mempengaruhi bagaimana dunia bekerja.
Dari tingkat pemerintahan tertinggi misalnya, dimana informasi dari semua bagian
suatu negara dapat diolah, dan berpotensi mengarah pada proses pembuatan serta hasil
kebijakan yang lebih baik. Hal ini juga memengaruhi bagaimana cara kita menjalani
kehidupan sehari-hari. Saat ini informasi tentang pilihan dan favorit kita sedang ditata
oleh pemasar, khususnya dalam menampilkan iklan yang relevan dengan mereka, yang
kini disebut sebagai pemasaran target.
Media sosial memainkan peran yang sangat besar dalam proses ini. Secara
umum, pengguna akan mencari-cari tentang pilihan mereka, dan bahkan kadangkadang ikut berpartisipasi dalam suatu forum atau diskusi. Namun, informasi di media
sosial sering kali acak dan rawan dari interferensi. Informasi yang tidak relevan dapat
muncul dari waktu ke waktu. Oleh karena itu, diperlukan sebuah sistem yang memiliki
kemampuan untuk mengambil informasi yang relevan berdasarkan pilihan yang
diinginkan. Sistem ini akan menggunakan teknologi web yang ada, serta juga
mengandalkan Sistem Manajemen Basis Data Relasi untuk membantu memproses dan
menyimpan informasi, serta mesin pencari untuk menyampaikan hasil.
Sistem yang dikembangkan mampu memberikan informasi yang akurat
mengenai pesan penting yang sesuai dengan pilihan pengguna, dengan tingkat akurasi
mencapai 77% pada metrik Precision @ K (asumsi K = 10 Teratas), dan 95% pada
metrik R-Precision juga membuktikan bahwa Metodologi pencarian informasi yang
diuraikan dapat digunakan dan diterapkan dalam suatu kasus, yaitu memberi peringkat
pesan di media sosial berdasarkan pilihan pengguna.","It is without a shade of doubt that we are now living in an era of information.
With the help of the Internet, especially the World Wide Web, information flows freely
without regard of time, location, and other constructs, both man-made or natural,
physical or conceptual. This flow of information influences how the world works, from
the highest levels of government for example, where information from all parts of a
country could be processed, and can potentially lead to better policymaking process
and policies. It also influences the way we live our daily lives. Information about our
preferences and favorites are now being mined by marketers, in order to specifically
display ads relevant to them, in what is now called as targeted marketing.
Social media plays a very big part of this process. Naturally, users would search
about their preferences, and sometimes even participate in a forum or discussion
regarding it. However, information in social media is often times random, and prone to
interference. Irrelevant information could come up from time to time. Therefore, a
system that has the ability to retrieve relevant information, based on a particular user&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s
preference is needed. The system will make use of current web technologies, and also
rely on an Relation Database Management System to help process and store
information, and a search engine to present the results.
The system was able to give accurate information regarding important
messages that are suitable with user&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s preferences, with accuracy levels reaching 77%
in the Precision @ K metric (assuming Top K = 10), and 95% in the R-Precision metric
also proven that the information retrieval methodologies outlined could be used and
implemented in this particular use case, which is ranking messages in a social media
based on user preferences.",,,150510,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
200914,,AMELIA DITA H,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN KEYBOARD MEKANIK MENGGUNAKAN METODE AHP DAN PROFILE MATCHING,,,"Sistem Pendukung Keputusan, AHP, Profile Matching, Keyboard Mekanik.","Drs. Retantyo Wardoyo M.Sc., Ph.D.",2,3,0,2021,1,"Keyboard mekanik dirancang dengan berbagai bentuk, variasi, dan spesifikasi yang berbeda dengan jenis keyboard lainnya. Keyboard mekanik sendiri memiliki fungsi estetik yang memungkinkan penggunanya untuk melakukan kustomisasi. Adanya berbagai spesifikasi pada keyboard mekanik, menyebabkan munculnya berbagai pertimbangan, yang dapat menyulitkan pengguna dalam memilih keyboard mekanik yang sesuai dengan kriteria yang diinginkan. Didukung dengan hasil pengamatan dalam Indonesia Mechanical Keyboard Group (IMKG), beberapa pengguna masih terbatas pengetahuannya mengenai produk keyboard mekanik yang tersedia di Indonesia, juga, saat ini belum terdapat solusi yang dapat menangani permasalahan tersebut.
Diangkat dari permasalahan tersebut, pada penelitian ini dibangun sebuah SPK yang dapat membantu mengatasi masalah tersebut, dengan memberikan rekomendasi keyboard mekanik yang sesuai dengan keinginan pengguna. SPK diimplementasikan dalam bentuk web menggunakan metode AHP untuk proses pembobotan dan Profile Matching untuk proses skoring. Kriteria yang digunakan ditentukan dengan melakukan survei mengenai spesifikasi yang menjadi prioritas pertimbangan dalam memilih keyboard mekanik. 
Di akhir penelitian, SPK yang berhasil dibangun mampu memberikan rekomendasi prioritas keyboard mekanik yang sesuai dengan preferensi pengguna dan mendapatkan rata-rata hasil evaluasi sebesar 36.17 dari total nilai maksimal 40.
","Mechanical keyboards are designed with various shapes, variations, and specifications that are different from other types of keyboards. The mechanical keyboard itself has an aesthetic function that allows users to customize it. There are various specifications on mechanical keyboards, causing various considerations, which can make it difficult for users to choose a mechanical keyboard that fits the desired criteria. Supported by observations in the Indonesia Mechanical Keyboard Group (IMKG), some users are still limited in their knowledge of mechanical keyboard products available in Indonesia, also, currently there is no solution that can handle this problem.
Based on these problems, in this research, an DSS is built that can help overcome these problems, by providing recommendations for a mechanical keyboard according to the wishes of the user. DSS is implemented in web form using the AHP method for the weighting process and Profile Matching for the scoring process. The criteria used are determined by conducting a survey regarding the specifications that are the priority considerations in choosing a mechanical keyboard.
At the end of the study, the DSS that was successfully built was able to provide mechanical keyboard priority recommendations according to user preferences and get an average evaluation result of 36.17 out of a total maximum value of 40.",,,152354,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
197332,,JEVON EDMUND NAHASON,"FORECASTING FASHION TREND USING THE LONG SHORT-TERM MEMORY, CONVOLUTIONAL NEURAL NETWORK, AND AUTOREGRESSIVE INTEGRATED MOVING AVERAGE	",,,"Google Trend, LSTM, CNN, ARIMA, Deep Learning, Data Preprocessing, RMSE","Drs. Edi Winarko, M.Sc., Ph.D.	",2,3,0,2021,1,"Google Trend adalah kumpulan data yang terdiri dari data kata pencarian deret waktu dari mesin pencari Google di seluruh internet. Kita dapat mencari kata apapun di Google dan sejarah  dari aktivitas browsing akan disimpan dan dapat didownload di Google Trend, yang dapat dimanfaatkan dan digunakan sebagai data untuk peramalan masalah, oleh karena itu penelitian ini mencoba menggunakan Long Short-term Memor, Convolutional Neural Network, dan Autoregressive Integrated Moving Average untuk memperkirakan tren dari Google Trend Data dalam hal tren mode.

Tesis ini bertujuan untuk mengusulkan pendekatan Long Short-term Memory, Convolutional Neural Network, dan Autoregressive Integrated Moving Average untuk meramalkan dataset time-series yang dilakukan oleh Google Trend. Penelitian ini akan menganalisis dataset time-series dari Google Trend yang terdiri dari istilah pencarian lima perusahaan fashion (Adidas, Puma, Nike, Off-White, dan Balenciaga) dari tahun 2015 hingga 2020 dan kemudian melakukan peramalan data untuk 52 langkah ke depan di luar kumpulan data aktual (di luar sampel).

Terdapat beberapa tahapan untuk melakukan penelitian yaitu pengumpulan data dari Google Trend, eksplorasi data preprocessing menggunakan Power Transform, Min-Max Normalization, implementasi pemodelan pendekatan Long Short-term Memory, dan evaluasi model menggunakan RMSE dan MAPE. Evaluasi penelitian ini dilakukan melalui analisis hasil kinerja dengan membandingkan Long Short-term Memory dengan dua metode lainnya yaitu Autoregressive Integrated Moving Average dan Convolutional Neural Network menggunakan Root Mean Square Error dan Mean Absolute Percentage Error untuk menentukan mana yang memiliki skor terkecil.

Hasil penerapan ketiga metode ini memberikan hasil yang berbeda dalam hal penilaian RMSE untuk metode evaluasi, dengan model Autoregressive Integrated Moving Average memberikan jumlah Root Mean Square Error yang paling kecil dibandingkan model Long Short-term Memory dan Autoregressive Integrated Moving Average.

Kata Kunci : Google Trend, LSTM, CNN, ARIMA, Deep Learning, Data Preprocessing, RMSE","Google Trend is a dataset that consists of time series search word data from Google search engines across the internet. We can search any words on the Google and the history of the browsing activities will be stored and can be downloaded on Google Trend, which can be exploited and used as data for forecasting problem, therefore this research tries to use the Long Short-term Memory, Convolutional Neural Network, and Autoregressive Integrated Moving Average models to forecast trends from Google Trend Data in terms of fashion trends.

This thesis aims to propose an Long Short-term Memory, Convolutional Neural Network, and Autoregressive Integrated Moving Average approaches to forecasting time-series datasets conducted by Google Trend. This research will analyze the time-series dataset from Google Trend consisting of the search term of five fashion companies (Adidas, Puma, Nike, Off-White, and Balenciaga) from the year 2015 until 2020 and then forecasting the data for 52 steps ahead the actual dataset ( out of sample ).

There are several phases to do the research, which are data collection from Google Trend, data exploration-preprocessing using Power Transform, Min-Max Normalization, modeling implementation of the Long Short-term Memory approach, and model evaluation using RMSE and MAPE. This research evaluation is conducted through the performance result analysis by comparing Long Short-term Memory to the other two methods, Autoregressive Integrated Moving Average and Convolutional Neural Network using Root Mean Square Error and Mean Absolute Percentage Error to define which has the smallest scoring.

The result of implementing these three methods gave different results in terms of RMSE scoring for the evaluation method, with Autoregressive Integrated Moving Average model giving the smallest number of Root Mean Square Error compared to Long Short-term Memory and Autoregressive Integrated Moving Average models.
",,,148702,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195540,,SATRIO WISESA P,COMPARISON OF QUALITY OF SERVICE (QOS) PERFORMANCE USING FIXED DAILY MEASUREMENT INTERVAL AND RANDOM SAMPLING METHOD,,,"Quality of Service, FDMI, DPP, Random Sampling, t-Test","Medi, Drs., M.Kom",2,3,0,2021,1,"Tujuan penelitian ini adalah untuk membandingkan performa metode Fixed Daily Measurement Interval dan Random Sampling dengan membandingkan Quality of Service (QoS) seperti latency, packet loss, dan throughput pada masing-masing jaringan. 
Perbandingan dilakukan dengan menggunakan Wireshark sebagai alat pengumpulan data. Pengambilan data dilakukan selama 5 hari, 4 hari kerja dan 1 akhir pekan di jaringan Indihome. Setiap hari ada tiga rentang waktu, pagi (06.00-10.00), sore (10.00-14.00), sore (14.00-18.00). Data diolah menggunakan metode uji-t dengan 9 hipotesis. 
Dalam penelitian ini penulis membandingkan kedua metode tersebut dengan menggunakan metode uji-t, dan ditemukan bahwa FDMI dan Random Sampling memiliki parameter QoS yang lebih buruk jika dibandingkan dengan DPP sebagai metode kontrol. Jika hasil semua periode FDMI dibandingkan dengan Random Sampling, throughput FDMI digolongkan baik menurut Standar Mutu ITU-T G.114 dengan nilai 15,62 Mbps, sedangkan hasil throughput Random Sampling digolongkan buruk oleh ITU-T Standar Kualitas G.114 dengan nilai 2,21 Mbps. Untuk hasil latency dan packet loss, tidak ada perbedaan dalam penggolongannya, karena kedua metode digolongkan baik dengan FDMI yang memiliki nilai 110 ms untuk latency, dan 1,19% untuk packet loss, sedangkan Random Sampling memiliki nilai 121 ms untuk latency, dan 1,25% untuk packet loss. Penelitian ini menemukan bahwa lebih baik menggunakan FDMI daripada Random Sampling untuk mendapatkan data rata-rata dan maksimum yang lebih akurat yang lebih mewakili pengalaman pengguna. 
","This research&acirc;€™s purpose is to compare the performance of Fixed Daily Measurement Intervals and Random Sampling methods by comparing their quality of service (QoS) parameters such as latency, packet loss, and throughput on individual networks. 
The comparison was done using Wireshark as a data gathering tool. The data was taken for 5 days, 4 weekdays and 1 weekend on the Indihome network. On each day, there are three time ranges, morning (06.00-10.00), afternoon (10.00-14.00), evening (14.00-18.00). The data was processsed using the t-test method with 9 hypotheses.
In this research, the author compared the two method by using the t-test method, and it was found that FDMI and Random Sampling had worse QoS parameters when compared to DPP, which is the control method. When all period result of FDMI was compared to Random Sampling, throughput of FDMI was classed as good by ITU-T G.114 Quality Standards with a value of 15,62 Mbps, while Random Sampling&acirc;€™s throughput result was classed as poor by ITU-T G.114 Quality Standards with a value of 2,21 Mbps. For latency and packet loss results however, there was no difference in their classes, as both methods were classed as good with FDMI having 110 ms for its latency, and 1,19% for its packet loss, and Random Sampling having 121 ms for its latency, and 1,25% for its packet loss. This research found that it is better to use FDMI rather than Random Sampling to get a more accurate average and maximum data that is more representative of the user&acirc;€™s experience.
",,,146868,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199639,,ZULFA DZAKIYYATUN N,Analisis Sentimen Berdasarkan Aspek Terhadap Review Toko Online Berbahasa Indonesia Menggunakan Machine Learning,,,"Analisis Sentimen Berdasarkan Aspek, Support Vector Machine, Multinomial Naive Bayes, n-gram, TFIDF","I Gede Mujiyatna S.Kom., M.Kom.",2,3,0,2021,1,"Analisis sentimen pada level dokumen dan kalimat sering dilakukan namun kurang memberikan informasi yang mendetail. Oleh karena itu, dibutuhkan analisis sentimen berdasarkan aspek. Analisis sentimen berdasarkan aspek dibagi menjadi dua subtask yaitu klasifikasi aspek dan klasifikasi sentimen. Data yang digunakan adalah data review produk kategori kosmetik bahasa Indonesia pada shopee. Ekstraksi fitur TFIDF dan n-gram digunakan pada penelitian ini. Support Vector Machine dengan kernel rbf, linear dan poly serta Multinomial Naive Bayes diimplementasikan pada kedua subtask.
 	Fitur unigram-bigram dapat meningkatkan nilai performa klasifikasi pada kedua subtask dengan selisih akurasi pada klasifikasi aspek sebesar 1% untuk SVM dan 4% untuk MNB sedangkan pada klasifikasi sentimen sebesar 5% pada kedua metode. Hasil penelitian SVM dengan fitur unigram-bigram memperoleh akurasi, presisi, recall dan f1-score sebesar 77%, 96%, 78% dan 86% sedangkan MNB sebesar 75%, 93%, 77% dan 84%. Klasifikasi sentimen setiap aspek, SVM memiliki nilai rata - rata akurasi sebesar 88% sedangkan MNB sebesar 85%. SVM mendapatkan akurasi tertinggi pada 5 dari 8 aspek yaitu aspek harga, tekstur, ketahanan, pelayanan dan keaslian sedangkan MNB memiliki akurasi tertinggi pada aspek pengemasan dan warna. Metode SVM memperoleh nilai performa klasifikasi yang lebih baik pada kedua subtask dibandingkan MNB.","Sentiment analysis at the document level is often carried out but does not provide detail information. Therefore, aspect based sentiment analysis is needed. Aspect based sentiment analysis is divided into two subtasks, aspect categorization and sentiment each aspect. The data used is the Indonesian cosmetic product review data on shopee. TFIDF and n-gram are used in this study. Support Vector Machine and Multinomial Naive Bayes are implemented in both subtasks. 
Unigram-bigram feature can increase accuracy in both subtask with difference of 1% for SVM and 4% for MNB on aspect classification, while on sentiment classification the difference is 5% in both methods. Result for SVM with unigrambigram feature has accuracy, precision, recall and f1-score of 77%, 96%, 78% and 86% while MNB has 75%, 93%, 77% and 84%. In sentiment classification each aspect, average accuracy of SVM is 88% while MNB is 85%. SVM has the highest accuracy in 5 of 8 aspects which are price, texture, durability, originality and service while MNB has better on packaging and color. SVM has better accuracy in both subtask than MNB.
",,,151195,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199131,,ALAMSYAH IMANUDIN,Sistem Rekomendasi Dosen Pembimbing Skripsi Menggunakan Metode Content-Based Filtering dan Density-Based Spatial Clustering of Applications with Noise (DBSCAN),,,"sistem rekomendasi, content-based filtering, DBSCAN, dosen pembimbing skripsi","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2021,1,"Penelitian ini fokus dalam membangun sistem rekomendasi yang dapat memberikan rekomendasi dosen pembimbing skripsi berdasarkan kata kunci maupun teks singkat terkait topik skripsi mahasiswa.
	Metode yang digunakan pada penelitian ini yaitu Content-based filtering. Metode ini digunakan karena penelitian ini berfokus pada pengolahan dan penggunaan konten pada dokumen. Abstrak skripsi mahasiswa dan publikasi dosen digunakan sebagai data pada penelitian ini. Data tersebut kemudian dikelompokkan berdasarkan topik dengan menggunakan metode DBSCAN. Sistem memberikan rekomendasi dengan cara mencari cluster terdekat dengan data yang diinputkan pada sistem. Dan terakhir, sistem rekomendasi memberikan output berupa nama-nama dosen pembimbing skripsi yang berada pada cluster terdekat dengan data yang diinputkan pada sistem.
	Percobaan pada penelitian ini menggunakan abstrak dari skripsi mahasiswa sejumlah 50 data. Berdasarkan hasil percobaan kinerja terbaik, didapatkan parameter clustering menggunakan DBSCAN yaitu epsilon dan minimum points bernilai 0.0027 dan 2 menghasilkan akurasi hingga 70% dengan nilai silhouette index 0.10016797028250428. Lebih lanjut sistem telah dilakukan pengujian perbandingan dengan metode K-Means dan hasil yang didapatkan dari metode tersebut adalah 48%. Berdasarkan hal tersebut, dapat diketahui bahwa DBSCAN menghasilkan rekomendasi yang lebih tepat daripada K-Means dalam kasus seperti yang ada pada penelitian ini.
","This study focuses on building a recommendation system that can provide recommendations for thesis supervisors based on keywords or texts related to the student's thesis topic.
	The method used in this research is content-based filtering. The method used because this research is based on processing and using the content in the document. Student thesis abstracts and publications were used as data in this study. The data then grouped by topic using the DBSCAN method. The system provides recommendations by finding the closest cluster to the data entered on the system. And finally, the recommendation system provides output in the form of the names of thesis supervisors who are in the closest cluster to the data inputted into the system.
	Experiments in this study using abstracts from student&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s thesis with a total of 50 data. Based on the results of the best performance experiment, the parameter grouping using DBSCAN was obtained namely, epsilon and the minimum points were 0.0027, and 2 produces an accuracy of up to 70% with a silhouette index value of 0.10016797028250428. Furthermore, the system has been tested for this comparison with the K-Means method and the results obtained from this method are 48%. Based on this, it can be seen that DBSCAN produces recommendations that are more appropriate than K-Means in cases such as those in this study.
",,,150714,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205790,,PERDO KURNIAWAN,ANALISIS KELEMAHAN ALGORITME ENKRIPSI ADVANCED ENCRYPTION STANDARD (AES) TERHADAP SERANGAN CHOSEN PLAINTEXT PADA MODE OPERASI ELECTRONIC CODE BOOK (ECB) PADA SISTEM AUTENTIKASI,,,"AES, ECB, serangan chosen plaintext, mode operasi","Anny Kartika Sari, S.Si., M.Sc., Ph.D.",2,3,0,2021,1,"Salah satu algoritme enkripsi simetris utama yang digunakan saat ini adalah Advanced Encryption Standard (AES). Meskipun algoritme ini masih digunakan sampai saat ini, beberapa peneltian sebelumnya telah membuktikan bahwa algoritme enkripsi AES masih memiliki kelemahan  terhadap jenis serangan tertentu.
Penelitian ini membahas tentang kelemahan dari penerapan algoritme enkripsi AES dengan menggunakan mode operasi ECB. Pada penelitian ini dibuat suatu sistem sederhana berbarsis web yang menjalankan algoritme enkripsi AES-ECB pada proses autentikasi. Serangan yang digunakan adalah chosen plaintext. Tujuan dari serangan ini adalah untuk mendapatkan pesan dari cookies yang terenkripsi  pada proses autentikasi serta melakukan perubahan terhadap pesan tersebut sehingga bisa mendapatkan hak akses admin.
Dari penelitian ini, didapatkan hasil bahwa implementasi algoritme enkripsi AES-ECB dengan memberikan akses kepada user  untuk menyisipkan plaintext pada proses autentikasi memiliki celah keamanan. Untuk mencegah celah keamanan ini, dibuat security advisory yang berisi saran perbaikan sistem. Saran perbaikan ini yaitu penyaringan input user, pembatasan jumlah input user serta melakukan pengecekan integritas data. Ketiga saran ini telah dicoba dan berhasil untuk mencegah serangan chosen plaintext.","One of the main symmetric encryption algorithms in use today is Advanced Encryption Standard (AES). However, several previous research have proven that the AES encryption algorithm still has weaknesses against certain types of attacks.
This research discusses the weaknesses of the application of the AES encryption algorithm using the ECB operating mode. In this research, a simple web-based system is created that runs the AES-ECB encryption algorithm in the authentication process. The attack used is the chosen plaintext. The purpose of this attack is to get messages from cookies that are encrypted in the authentication process and make changes to these messages so that they can get admin access.
From this research, it was found that the implementation of the AES-ECB encryption algorithm by giving access to the user to insert plaintext in the authentication process has a security hole. To prevent this security hole, a security advisory was made containing suggestions for system improvement. Suggestions for this improvement are filtering user input, limiting the number of user inputs and checking data integrity. These three suggestions have been implemented and succeeded to prevent chosen plaintext attack.",,,157262,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195296,,RYAN ADINUGRAHA S,Measuring Effectiveness of Bidirectional Encoders Represenatation from Transformers as Embedding Method for Story Cloze Test,,,"Natural Language Processing, Embedding, BERT, Story Cloze Test, Classification Model","Drs. Bambang Nurcahyo Prastowo, M.Sc.; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2021,1,"Story Cloze Test adalah kerangka evaluasi yang dikembangkan untuk mengevaluasi pemahaman cerita dan pembelajaran naskah. Sejak pembuatannya, banyak penelitian telah dilakukan untuk menyelesaikan pengujian tersebut, dengan model terbaik saat ini mampu mencapai akurasi 77,6% (Chaturvedi et al., 2017). Perkembangan terkini dalam bidang Natural Language Processing memiliki beberapa terobosan dalam pengembangan model bahasa seperti Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) sebagai teknik pembelajaran mesin. Penelitian ini mempersembahkan solusi alternatif untuk menyelesaikan Story Cloze Test dengan menggunakan model BERT pretrained sebagai metode embedding. Integrasi metode embedding dengan 2 model klasifikasi unik (kelas jamak dan kelas biner) dilakukan untuk membentuk model pembelajaran mesin. Hasil akhirnya menunjukkan bahwa model memiliki akurasi 65,3% dan 75,4% untuk kedua model, di ambang hasil model state-of-art.","Story Cloze Test is a developed evaluation framework for evaluating story comprehension and script learning. Since its creation, numerous researches have been conducted to solve the test, with the current best model capable of achieving 77.6% accuracy (Chaturvedi et al., 2017). The latest development in Natural Language Processing field saw some breakthrough in language models development such as Bidirectional Encoder Representations from Transformers(BERT) (Devlin et al., 2018) as machine learning techniques. This research presented alternative solution for solving the Story Cloze Test using pretrained BERT models as embedding method. Integration of the embedding method with 2 unique classification models (multi-class and binary-class) was done to established the machine learning models. The end result showed that the model have 65.3% and 75.4% accuracy for both models, on verge of the result of state-of-art model.",,,146698,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199137,,YUSFI ADILAKSA,Sistem Rekomendasi Mata Kuliah Pilihan Menggunakan Metode Content-based Filtering,,,"sistem rekomendasi, mata kuliah pilihan, content-based filtering, cosine similarity","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2021,1,"Menurut Kamus Besar Bahasa Indonesia (KBBI) daring, mata kuliah adalah satuan pelajaran yang diajarkan di tingkat perguruan tinggi. Setiap program studi mewajibkan mahasiswanya untuk mengambil beberapa mata kuliah pilihan. Kesesuaian mata kuliah pilihan yang diambil dengan kemampuan mahasiswa dapat menjadi salah satu faktor keberhasilan studi mahasiswa. Penelitian ini berfokus untuk membangun sebuah sistem rekomendasi yang dapat memberikan sejumlah rekomendasi mata kuliah pilihan yang sesuai dengan riwayat akademik mahasiswa.
	Content-based filtering merupakan salah satu metode sistem rekomendasi. Metode ini mengacu pada item yang menjadi dasar rekomendasi. Pada penelitian ini, hasil rekomendasi diambil dari profil pengguna yang didasarkan pada item kata hasil preprocessing dari mata kuliah yang telah diambil pengguna. Kemiripan dengan mata kuliah pilihan didasarkan pada silabus mata kuliah dan dihitung nilainya menggunakan persamaan cosine similarity. Percobaan menggunakan dataset silabus mata kuliah S1 Ilmu Komputer UGM kurikulum 2016.
	Sistem rekomendasi yang telah dibangun diuji menggunakan metode kuisioner untuk mendapatkan penilaian kinerja sistem, dan menggunakan metode validasi untuk mendapatkan rata-rata akurasi. Kuisioner dilakukan dengan melibatkan 30 mahasiswa S1 Ilmu Komputer UGM. Hasil menunjukkan bahwa sistem rekomendasi ini mendapatkan persentase rata-rata pencapaian tujuan sistem, yaitu relevance, novelty, serendipity dan increasing recommendation diversity sebesar 77.83% untuk hasil rekomendasi tanpa bobot, dan untuk hasil rekomendasi dengan bobot sebesar 81.67%. Kemudian untuk akurasi hasil rekomendasi untuk responden mahasiswa tahun ke-2 dan tahun ke-3 sama-sama memiliki rata-rata sebesar sebesar 33%.
","According to the online Big Indonesian Dictionary (KBBI), courses are units of lessons taught at the tertiary level. Each study program requires students to take several elective courses offered. The suitability of the elective courses taken with the students' abilities can be one of the factors for the success of student studies. This research focuses on building a recommendation system that can provide several elective course recommendations according to the student's academic history.
	Content-based filtering is a recommendation system method. This method refers to the items on which the recommendation is based. In this research, the results of recommendations are taken from user profiles based on preprocessed word items from courses taken by the user. The similarity with elective courses is based on the course syllabus and the value is calculated using the cosine similarity measure. The experiment used a dataset in the form of a syllabus of Computer Science UGM courses.
	The recommendation system that has been built is tested using a questionnaire method to obtain an assessment of system performance, and using the validation method to get the average accuracy. The questionnaire was conducted by involving 30 students of the Computer Science UGM undergraduate program. The results show that this recommendation system gets an average percentage of the achievement of system goals, consists of relevance, novelty, serendipity, and increasing recommendation diversity are 77.83% for the results of non-weighted recommendations, and for the results of weighted recommendations are 81.67%. Then for the accuracy of the recommendations for the second year and third-year student respondents both have an average of 33%.
",,,150708,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202725,,M. N. AJIPAWENANG,"TOPIC DETECTION FOR CULINARY MARKET ANALYSIS USING LDA, NAIVE BAYES ALGORITHM AND K NEAREST NEIGHBOR ALGORITHM",,,"Food sales and market, LDA, Sentiment analysis, KNN, Multinomial Naive Bayes.","Azhari, Drs., MT., Dr",2,3,0,2021,1,"Pada umumnya, penjualan dan pasar makanan adalah lingkup konsumsi yang penting dan dengan berbagai macam pilihan produk, variasi dan kesempatan yang ada di dalamnya cukup luas. Walaupun memiliki pertumbuhan dan jangkauan yang luas, ilmu mengenai pengertian pasar ini seringkali terbatasi dengan bias dan spekulasi, karena sebelumnya, ada kekurangan dokumentasi data yang bisa diproses dan pelajari. Di tahun-tahun belakangan ini, media social telah memberikan kesempatan untuk konsumer mengetahui dan mengukur kualitas dari produk yang ada dengan cara membaca dan memeriksa ulasan yang diunggah oleh pengguna platform belanja online. Ditambah lagi, platform online seperti Amazon.com menyajikan pilihan kepada pengguna untuk menandai sebuah ulasan sebagai &quot;berguna&quot; apabila ulasan tersebut dianggap berisi dan berharga. Hal ini sangat membantu konsumer, pedagang, dan pabrik untuk secara efisien mengevaluasi preferensi umum dengan cara memindai atribut yang mengindikasi sentiment pada ulasan.
Dalam penelitian ini, penulis menyuguhkan solusi untuk membantu pengguna, dengan menggunakan topic modeling yang memanfaatkan algoritma LDA dan nilai topic coherence. Latent Dirichlet Allocation (LDA) digunakan untuk mencari kata-kata yang berhubungan dengan produk yang heterogen, juga untuk mencari kata yang sering digunakan. Topic coherence digunakan untuk memeriksa nilai dari topik yang bisa dimengerti oleh manusia. Dengan menggunakan analisis sentiment, penulis bisa menggunakan data opini, sentiment, dan emosi yang diekspresikan didalam komunikasi online. K-Nearest Neighbor (KNN) dan Multinomial Naive Bayes digunakan untuk mengklasifikasi sentiment seluruh dataset. dengan menggunakan topic modeling dan analisis sentiment, penelitian ini berupaya memberikan cara untuk menggabungkan informasi dari media sosial untuk memahami lebih dalam segmentasi pasar.
Hasil dari penelitian ini menunjukkan bahwa topik model LDA mampu mengidentifikasi dan menggolongkan kata-kata utama dari dataset, lalu divisualisasikan. Hasil dari LDA telah diuji dengan nilai topic coherence untuk memeriksa apakah topik tersebut dapat dipahami manusia. Dengan nilai topic coherence tertinggi (dari 1-49) 6 topik dipilih dengan nilai coherence tertinggi. Dengan analisis sentiment, menggunakan KNN dan Multinomial Naive Bayes, dataset dipahami lebih jauh seputar perbedaan sentiment yang ada, dengan hasil yang menunjukkan kecenderungan nilai menjadi True Positive, yang mana memberikan pengertian lebih tentang dataset tersebut, dan mampu membantu pemilihan keputusan tentang produk di pasar tersebut.
","Food sales and market in general have become an important channel for consumption and with numerous and wide range of product selection, the variation and opportunity is quite vast. Despite its growth and general reach, understanding on this market are often limited to bias and speculation, the reason is that previously, there was a lack of documented data to be worked on. In recent years, social media has given quite a handful of opportunity to the consumer in terms of gauging the quality of the products by reading and examining the reviews posted by the users of online shopping platforms. Moreover, online platforms such as Amazon.com provides an option to the users to label a review as &quot;Helpful&quot; if they find the content of the review valuable. This helps both consumers, merchants, and manufacturers to evaluate a general preferences in an efficient manner by monitoring the attribute that could indicate the sentiments on the reviews.
In this research, we offer a solution to assist the user, by using topic modeling that consists of LDA and topic coherence. Latent Dirichlet Allocation (LDA) is utilized to find the interrelated words related to heterogeneous products, also to find the most mentioned of words. Topic coherence is used to checks the value of topics to be comprehensible to human. By utilizing sentiment analysis, we could automatically assess the data regarding opinions, sentiments, and emotions expressed in communication. K-Nearest Neighbor (KNN) and Multinomial Naive Bayes is used to classify the overall sentiment of the datasets. By using both topic modeling and sentiment analysis, this research will provide a way to incorporate information from social media to give further understanding of the market.
The result of this research shows that LDA topic modeling are able to identify and rank top words present in the dataset then the dataset could be visualized. The results of the LDA have been tested with coherence score to be semantically interpretable to human judgement. Coherence score are then calculated to the highest possible number of topics (1-49) 6 topics are chosen with the highest coherence score. While the sentiment analysis, in the form of KNN and Multinomial Naive Bayes is capable to further analyze the dataset of its sentiment polarity with the results tends to be True Positive, hence providing additional insight of the dataset, through which further decision such as will this product will continue in the market or not.
",,,154175,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205801,,NOVIAN DENY CAHYO AJI,PENERAPAN NTRU SEBAGAI ALGORITMA POST-QUANTUM CRYPTOGRAPHY PADA APLIKASI PESAN INSTAN BERBASIS ENKRIPSI SECARA END-TO-END,,,"NTRU,end-to-end,aplikasi,pesan instan","Anny Kartika Sari, S.Si., M.Sc., Ph.D.",2,3,0,2021,1,"	Kriptografi menjadi salah satu bidang yang sangat penting bagi kehidupan digital saat ini. Salah satu jenis kriptografi yang penting di era sekarang ini adalah kriptografi kunci publik. Terdapat banyak kegunaan algoritma kriptografi kunci publik, salah satunya untuk melakukan komunikasi secara end-to-end. Sayangnya, fitur keamanan ini terancam oleh perkembangan komputer kuantum yang dikhawatirkan dapat merusak mayoritas algoritma kriptografi kunci publik.
	Penelitian ini membahas tentang penggunaan algoritma kriptografi kunci publik yaitu NTRU yang aman dari serangan komputer kuantum sebagai sarana komunikasi yang aman secara end-to-end. Pada penelitian ini dibuat aplikasi pesan instan yang dapat menghubungkan dua pengguna untuk berkomunikasi secara langsung dan aman. NTRU digunakan sebagai algoritma utama penyusun skema key encapsulation mechanism yaitu skema yang digunakan untuk membangkitkan kunci simetris. Dilakukan juga analisis terkait efisiensi NTRU dan pemilihan parameter NTRU untuk mencegah adanya decryption failure.
	Dari evaluasi terhadap aplikasi yang dibangun, didapatkan hasil bahwa NTRU lebih efisien waktu jika dibandingkan dengan RSA maupun X25519. Diperoleh juga rekomendasi metode pembangkitan pasangan kunci serta rekomendasi parameter NTRU.","	Cryptography is an important field in current digital civilization. One of the most important type of cryptography is public key cryptosystem. There are many uses of public key cryptosystem, one of which is to perform end-to-end encrypted communication. Unfortunately, this security feature is threatened by the development of quantum computers which are feared to be able to break most public key cryptosystem algorithms. 
	This research discusses the use of one of the public key cryptosystem algorithms, NTRU, which is safe from quantum computer attacks. The NTRU is used to perform an end-to-end secure communication. In this research, an instant messaging application with the sole purpose of connecting two users to be able to communicate in real-time. NTRU is used as the main algorithm for the key encapsulation mechanism, which is the scheme used to generate a symmetric key for symmetric cryptosystem algorithm. Researcher also performed analysis related to the efficiency of NTRU and the selection of NTRU parameters to prevent decryption failure. 
	Evaluating the resulting application, it was found that NTRU is more efficient when compared to RSA and X25519. The research also produces recommendations for key pair generation methods and NTRU parameters. ",,,157269,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202475,,Annisa Rahmawati Artha,PERBANDINGAN PENGGUNAAN DETEKSI BUZZER PADA PERHITUNGAN PREDIKSI HASIL PILPRES 2019 DENGAN SENTIMEN ANALISIS,,,"Twitter, analisis sentimen, deteksi buzzer, support vector machine, prediksi pilpres","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2021,1,"Twitter merupakan platform media sosial yang digunakan penggunanya untuk menyuarakan pendapat mereka dalam bentuk tweet. Berbagai penelitian sebelumnya menggunakan tweet untuk mengetahui pandangan pengguna terhadap aktifitas politik atau tokoh politik. Dalam penelitan-penelitian tersebut, analisis sentimen menggunakan tweet dengan algoritma SVM dapat digunakan untuk memprediksi hasil pemilihan umum atau pemilihan presiden. Beberapa penelitian juga menggunakan deteksi buzzer untuk menghapus tweet yang dihasilkan oleh akun buzzer, sehingga menghasilkan hasil prediksi yang lebih akurat.
Dalam penelitian ini, dilakukan klasifikasi sentimen terdadap tweet yang berkaitan dengan pasangan kandidat calon presiden dan wakil presiden pada pilpres 2019. Data tweet dan data profil akun diambil dari Twitter dalam jangka tanggal 1 1 Januari 2019 sampai 16 April 2019 dengan batasan jumlah data tweet yang diambil sebanyak 1000 tweet, 500 tweet untuk masing-masing pasangan. Setelah partisi tweet dilakukan, menghasilkan sub-tweet sebanyak 1509 untuk pasangan Jokowi-Ma'ruf dan 1357 untuk pasangan Prabowo-Sandiaga. Klasifikasi buzzer dilakukan untuk menghapus tweet yang dihasilkan oleh akun buzzer. Kemudian klasifikasi sentimen dilakukan untuk mengetahui sentimen tweet. Setelah itu perhitungan prediksi dilakukan dengan menghitung persentase jumlah tweet bersentimen positif, sebelum dan sesudah deteksi buzzer.
Hasil dari penelitian ini menunjukkan hasil prediksi sesudah dilakukan deteksi buzzer menghasilkan nilai error yang lebih kecil, yaitu sebesar 1.82%, dibandingkan hasil prediksi sebelum deteksi buzzer, yaitu sebesar 10.13%. Ditemukan juga bahwa pengguna buzzer lebih banyak menghasilkan tweet positif pasangan pilihan mereka.
","Twitter is a social media platform being used by its users to convey theis opinion in the form of tweets. Various previous studies used tweets to find out users' views on political activities or political figures. In those studies, sentiment analysis based on tweets using SVM was used to predict the results of the general election or presidential election. Some studies also used buzzer detection to delete tweets generated by buzzer accounts, resulting in more accurate predictions. 
In this study, sentiment classification was carried out on tweets related to the presidential and vice presidential candidate pairs in the 2019 presidential election. Tweet data and account profile data were taken from Twitter in the period January 1st, 2019 to April 16th, 2019 with a limit on the number of tweet data taken as many as 1000 tweets, 500 tweets for each pair. After the tweet partition was carried out, it resulted in a total of 1509 sub-tweets for the Jokowi-Ma'ruf and 1357 for the Prabowo-Sandiaga. Buzzer classification was done to delete tweets generated by buzzer accounts. Sentiment classification was carried out afterwards to determine the tweet sentiment polarity. Finally, the prediction was calculated by finding the percentage of positive tweets, before and after buzzer detection.
The results of this study indicate that the prediction results after buzzer detection produces a smaller error value, which is 1.82%, compared to the prediction results before buzzer detection, which is 10.13%. It was also found that buzzer users tend to generate more positive tweets for their preferred partner.
",,,154173,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203759,,RACHMADIO ANANDA S.,"The Development of Chatbot and Webservice for Searching Lodging Place Information in According to User Preferences, ",,,"Chatbot, We Service, Dialogflow, API, Lodging Place, Telegram","Medi, Drs., M.Kom",2,3,0,2021,1,"Pemesanan tempat penginapan sudah menjadi hal yang umum. Namun, dalam proses pencarian ini selalu terdapat kesulitan. Dalam proses pencarian tempat penginapan, pengguna sering mengalami masalah dengan antramuka yang disediakan oleh layanan pemesanan penginapan daring. Banyak pengguna awam yang bingung dengan tampilan antarmuka karena banyaknya pilihan pilihan yang biasa digunakan pengguna untuk mencari tempat penginapan dan sering kali pengguna lupa untuk memilih preferensi terutama pada preferensi tamu dan harga. Sedangkan proses ini akan lebih mudah dengan bantuan menggunakan komputer. Oleh karena itu, dibuat sistem bernama chatbot yang akan dibangun untuk mengakomodir kebutuhan tersebut.

Chatbot dipilih untuk memudahkan pengguna berinteraksi, karena chatbot menyediakan cara interaksi yang lebih umum dengan membuat pengguna merasa seolah-olah percakapan terjadi dengan manusia lain. Chatbot ini mengerti bahasa indonesia dan dapat memberikan jawaban mengenai nama penginapan, jenis penginapan, jumlah kamar tidur, jumlah kamar mandi, fasilitas, penilaian tempat tinggal, harga dan juga link pemesanan untuk pengguna jika ingin melakukan pemesanan secara langsung. Setelah dilakukan beberapa pengujian, chatbot memiliki akurasi sebesar 73.33% tingkat respon yang berhasil, chatbot mampu mengurangi waktu untuk pencarian penginapan sebanyak 58.86%  dibandingkan dengan pencarian menggunakan website dan nilai SUS (System Usability Scale) yaitu sebesar 79.","A booking for a lodging place is already a common way. However, in this searching process, there are always difficulties. In the process of searching a lodging place users frequently have a problem with the interface that online booking services served. Many common users are confused with the interface because there are many selection options that users usually use to find a lodging place and frequently users forget to select preferences especially on the guest and the price preferences. Meanwhile, this process would be simplify done by using a computer. Therefore, there is a system called a chatbot that is built to accommodate the needs. 

A chatbot is chosen to make the user easily to interact because a chatbot provides a common way of interaction which makes the user feel as if the conversation is happening with another human. This chatbot understands bahasa indonesia and can give an answer about the name of the listings, the type, the number of bedrooms, the number of bathrooms, the facilities, the ratings of the listings, the price, and also the direct URL link for the user if want to make a direct booking. After several tests, the chatbot has a 73.33% of accuracy successful response rate, the chatbot can reduce the time it takes to find a lodging place by 58.86% compared to using a website and the SUS (System Usability Scale) value is 79.",,,155078,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
201972,,FAQIH ETHANA P,Author Verification Menggunakan Siamese Convolutional Neural Network,,,"author verification, siamese network, word embedding, convolutional neural network","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.;Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2021,1,"Author verification adalah metode untuk menentukan apakah dua dokumen berasal dari penulis yang sama atau bukan. Model tersebut dapat memecahkan masalah anonimitas plagiarisme. Tantangan dalam memecah permasalahan tersebut adalah gaya penulisan seseorang sangat beranekaragam serta jumlah data yang tersedia pada masalah nyata sangat sedikit.
	
Salah satu solusi dalam menyelesaikan masalah author verification adalah melalui pendekatan deep learning. Siamese network merupakan arsitektur yang digunakan untuk melakukan perbandingan dua hal. Dengan menggunakan 2 jaringan yang sama, dilakukan proses komputasi yang menghasilkan output berbeda yang kemudian dilakukan perhitungan jarak kedua output tersebut. Jarak yang dihasilkan akan dijadikan acuan sebagai prediksi dari model.
	
Model tersebut dievaluasi berdasarkan parameter akurasi, presisi, recall, dan F1 skor. Model tersebut memiliki rata-rata parameter evaluasi sebesar 68.25%. Penelitian ini juga melakukan perbandingan akurasi yang dihasilkan dari word embedding FastText dengan Glove. Jenis arsitektur dari layer yang digunakan juga dilakukan perbandingan antara banyaknya hidden layer yang digunakan serta penggunaan CNN, LSTM, serta GRU. Analisis juga dilakukan pada penggunaan data besar dan data kecil.","Author verification is a method to determine whether two documents are from the same author or not. The model can solve the problem of anonymity plagiarism. The challenge in solving these problems is that a person's writing style is very diverse and the amount of data available on real problems is very small.

One solution in solving the author verification problem is through a deep learning approach. Siamese network is an architecture that is used to compare two things. By using the same 2 networks, a computational process is carried out that produces different outputs which then calculates the distance between the two outputs. The resulting distance will be used as a reference as a prediction of the model. 

The model was evaluated based on the parameters of accuracy, precision, recall, and F1 score. The model has an average evaluation parameter of 68.25\%. This study also compares the accuracy resulting from word embedding FastText with Glove. The type of architecture of the layer used is also made a comparison between the number of hidden layers used and the use of CNN, LSTM, and GRU. Analysis was also carried out on the use of big data and small data.",,,153585,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
207094,,NIKOLAS ADHI P,"Decision Support System to Prioritize Ventilators for COVID-19 Patient using AHP, Interpolation, and SAW",,,"DSS, Healthcare, Analytical Hierarchy Process, Interpolation, SAW","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.",2,3,0,2021,1,"Dalam era pandemi COVID-19, banyak rumah sakit yang mengalami kekurangan ventilator. Tenaga kesehatan berhadapan dengan dilema karena kurangnya suplai peralatan kesehatan dibandingkan dengan banyaknya nyawa yang harus diselamatkan. Ketika suplai cukup, semua pasien mendapatkan perlakuan dan kesempatan yang sama namun bagaimana apabila terdapat perbedaan besar diantara angka suplai peralatan dan pasien? Dibutuhkan mekanisme pemrioritasan yang dapat secara objektif memutuskan alokasi peralatan yang sedang langka, dengan harapan mendapatkan hasil yang terbaik.

Sistem pendukung keputusan merupakan sistem yang dapat membantu manusia yang menggunakan data sebagai pembuat keputusan, untuk memutuskan permasalahan semi terstruktur/tidak terstruktur. Tujuan dari penelitian ini adalah membuat SPK untuk pemrioritasan pasien yang membutuhkan ventilator berdasarkan kriteria dan kebijakan yang sudah ditentukan, dengan menggabungkan tiga metode berbeda. SPK yang dibuat merupakan aplikasi berbasis web, yang dibuat menggunakan bahasa pemrograman Python.

Berdasarkan hasil eksperimen, SPK yang dibuat dapat memeringkatkan pasien COVID-19 untuk alokasi ventilator menggunakan SOFA sebagai kriteria AHP, Interpolasi, serta SAW sebagai metodenya. Hasil pemeringkatan SPK berbeda dengan raw SOFA Score, dengan perbedaan utamanya terletak pada SPK yang dapat menghindari kriteria tiebreaker, yang biasanya disebabkan oleh hasil prioritas yang sama.","During the COVID-19 pandemic era, many hospitals face ventilator shortages. Healthcare workers face a dilemma due to the lack of healthcare supplies with many lives to save. When there are enough resources available, every patient gets an equal
treatment and chance but what if there is a big difference in number between supplies and patients? There needs to be a prioritization mechanism that can objectively decide the allocation of these already scarce resources in the hopes of achieving the best
outcome.

A decision support system is a system that can support humans using data as decision makers to help them decide semi-structured/unstructured problems. The goal of this research is to create a DSS to prioritize patients who need a ventilator based on predetermined criterias and policy by incorporating three different methods. The DSS
is a web-based app, made using Python language.

Based on the experiment, the proposed DSS is able to rank COVID-19 patients for ventilator allocation using SOFA as criteria and using AHP, Interpolation, and SAW as the method. The proposed DSS ranked the patients differently than raw SOFA Score, with the main difference being that the proposed DSS can avoid tiebreaker criteria usage caused by similar priority results.",,,158420,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
197367,,FACHRY FIRDAUS,Evaluasi Sistem Simulasi Emosi dan Kepribadian pada Non-Playable Character dalam Skenario Percakapan Game Menggunakan Sistem Inferensi Fuzzy,,,"logika fuzzy, simulasi emosi, karakter visual, video game","Janoe Hendarto, Drs., M.Kom",2,3,0,2021,1,"Karakter virtual dalam video games atau  disebut NPC (non-playable character) merupakan salah satu komponen penting dalam video games untuk menciptakan dunia games yang lebih natural dan masuk akal. Pada umumnya tingkah laku NPC sangat terbatas ketika player melakukan interaksi percakapan dengan NPC. NPC dinilai kaku dan selalu mengikuti pola decision tree yang gamblang. Pada penelitian ini upaya penyimulasian emosi dan kepribadian kepada NPC dilakukan untuk meningkatkan kualitas interaksi antara player dan NPC agar menjadi lebih bervariasi dan masuk akal seperti layaknya interaksi  sosial antar manusia. Pengukuran player experience akan dilakukan untuk membandingkan video game dengan simulasi emosi dan video game tanpa simulasi emosi.

Simulasi emosi ini akan mengadaptasi model emosi OCC (Ortony, Clore \&amp; Collins) yang memiliki hubungan dengan model kepribadian big five OCEAN (Openness, Consciousness, Extraversion, Agreeableness \&amp; Neuroticisim). Intensitas emosi akan dikomputasi menggunakan sistem inferensi fuzzy yang menerima faktor kepribadian dan variabel penentu intensitas emosi lainnya sebagai input. Ada tiga jenis sistem inferensi yang dibuat yaitu sistem inferensi fuzzy Mamdani, Sugeno, dan Tsukamoto. Perbandingan video game ini akan diukur menggunakan survei player experience GEQ (\textit{Game Experience Questionnaire}) kepada player.

Hasil survei player experience video game dengan simulasi emosi dan kepribadian lebih baik dibanding video game tanpa simulasi emosi dan kepribadian (p &lt; 0.05 dan alfa &gt; 0.7) dalam komponen positive affect (19.394%), sensory (16.190%), flow (9.938%), empathy (2.740%), positive experience (5.425%), returning to reality (8.450%).","Virtual characters in video games or called NPCs (non-playable characters) are one of the important components in video games to create a game world that is more natural and makes sense. In general, the behavior of NPCs was very limited when players had conversation interactions with NPCs. NPC is considered rigid and always follows a decision tree pattern. In this study, efforts to simulate emotions and personalities to NPCs were carried out to improve the quality of interactions between players and NPCs so that they became more varied and reasonable, just like social interactions between humans. Player experience measurements will be carried out to compare video games with emotion simulation and video games without emotion simulation.

This emotion simulation will adapt the OCC emotion model  (Ortony, Clore \&amp; Collins) which has a relationship with the OCEAN big five personality model (Openness, Consciousness, Extraversion, Agreeableness \&amp; Neuroticisim). Emotional intensity will be computed using a fuzzy inference system that accepts personality factors and other determinants of emotional intensity as input. There are three types of fuzzy inference systems created, Mamdani Inference System, Sugeno Inference System, and Tsukamoto Inference System. The video game comparison will be measured by conducting a survey using a GEQ player experience survey (\ textit {Game Experience Questionnaire}) to the player.

The results of the survey on the player experience of video game with emotion and personality simulation is better than video game without emotion and personality simulation (p &lt; 0.05$ and alfa &gt; 0.7) in component listed here : positive affect (19.394%), sensory (16.190%), flow (9.938%), empathy (2.740%), positive experience (5.425%), returning to reality (8.450%).",,,148735,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203767,,SARAH FADHILA PUTRI,Comparison of Naive Bayes Classifier in Application of Flesch Kincaid on BSE-Kemendikbud as The Gold-Standard Data Set,,,"BSE-Kemendikbud, Flesch Kincaid, Gaussian Naive Bayes, Multinomial Naive Bayes, Text Classification.","Drs. Bambang Nurcahyo Prastowo, M. Sc. ; Diyah Utami Kusumaning Putri, S.Kom., M.Sc., M.Cs.",2,3,0,2021,1,"Buku sangat penting sebagai media pembelajaran antara siswa dan guru. Kementerian Pendidikan dan Kebudayaan (Kemendikbud) telah menyediakan buku-buku dalam bentuk elektronik yang dapat diunduh oleh setiap siswa dan dikenal dengan Buku Sekolah Elektronik Kemendikbud (BSE-kemendikbud). Buku-buku yang telah disediakan diklasifikasikan dan ditunjuk sesuai dengan kelas masing-masing dari kelas 1-12 dari SD hingga SMA.
Penelitian ini bertujuan untuk mengukur tingkat keterbacaan buku-buku yang disediakan oleh Kementerian Pendidikan dan Kebudayaan (Kemendikbud) dengan menggunakan rumus Flesch Kincaid dan dengan bantuan metode Machine Learning. Pada penelitian ini dilakukan klasifikasi kesulitan buku dan mengklasifikasikan buku mana yang mudah atau sulit dengan menggunakan rumus Flesch Kincaid untuk mencari skor keterbacaan dan akan dilakukan perbandingan dengan kedua model tersebut.
Model pertama menggunakan rumus Flesch Kincaid untuk mencari skor dan memberi label pada data menggunakan threshold yang mengikuti ketentuan dari rumus pada tabel Flesch Kincaid Reading Ease. Model kedua akan dimodifikasi untuk menemukan threshold baru untuk proses pelabelan data. Kedua model tersebut akan diklasifikasikan menggunakan Gaussian Naive Bayes dan Multinomial Naive Bayes yang akan dibandingkan. Hasil klasifikasi pada model pertama menggunakan Gaussian Naive Bayes menunjukkan akurasi 100% dan menggunakan Multinomial Naive Bayes sebesar 97.5%. Model kedua menggunakan Gaussian Naive Bayes dengan akurasi sebesar 74.8% dan menggunakan Multinomial Naive Bayes menunjukkan akurasi sebesar 75.6%.
Hyperparameter tuning digunakan pada kedua model dan kedua klasifikasi dalam upaya untuk mengoptimalkan akurasi yang diperoleh. Dengan hasil akurasi pada model pertama menggunakan Gaussian Naive Bayes mendapatkan akurasi sebesar 100% dan model pertama menggunakan Multinomial Naive Bayes mendapatkan akurasi sebesar 100%. Model kedua menggunakan Gaussian Naive Bayes mendapatkan akurasi 98.3% dan model kedua menggunakan Multinomial Naive Bayes mendapatkan akurasi 79.8%.","Books are very important as a learning medium between students and teachers. The ministry of education and culture (Kemendikbud) has provided books in electronic form that can be downloaded by every student and is well-known as Buku Sekolah Elektronik Kemendikbud (BSE-kemendikbud). The books that have been provided are classified and designated according to the respective classes from the grade 1-12 from Elementary to High School. 
This research purpose is to measure the readability levels of the books that are provided by The Ministry of Education and Culture (Kemendikbud) using the Flesch Kincaid formula and with the help of a machine learning method. In this research, a book difficulty classification is carried out and classifies which books are Easy or Hard by using the Flesch Kincaid formula to find a score of the readability and this will do a comparison with the two models.
The first model uses the Flesch Kincaid formula to find the score and to label the data using a threshold that follows the provisions of the Flesch Kincaid Reading Ease Table formula. The second model will be modified to find a new threshold for the data labeling process. Both models will be classified using Gaussian Naive Bayes and Multinomial Naive Bayes which will be compared. The result of classification on the first model using Gaussian Naive Bayes showed the accuracy of 100% and using the Multinomial Naive Bayes is at 97.5%. The second model using Gaussian Naive Bayes is at the accuracy of 74.8% and using Multinomial Naive Bayes showed the accuracy at 75.6%. 
Hyperparameter tuning is used in both models and both classifications in an attempt to optimize the accuracy obtained. With accuracy results on the first model using Gaussian Naive Bayes got the accuracy at 100% and first model using Multinomial Naive Bayes got the accuracy of 100%. The second model using Gaussian Naive Bayes got an accuracy of 98.3% and the second model using Multinomial Naive Bayes got an accuracy of 79.8%.

",,,155153,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
202489,,AKMAL ADI SULISTYO,OPTIMASI KOMBINASI PAKAN KONSENTRAT TERNAK SAPI DENGAN ALGORITMA EVOLUTION STRATEGIES,,,"evolution strategies, optimasi pakan ternak","Anny Kartikasari, S.Si., M.Sc., Ph.D",2,3,0,2021,1,"Menurut Direktorat Jenderal Peternakan dan Kesehatan Hewan Kementerian Pertanian Indonesia, pada tahun 2020 kebutuhan daging sapi di Indonesia mencapai 302.300 ton per tahun sedangkan ketersediaan daging lokal hanya sebesar 165.478 ton. Hal ini sangat memprihatinkan karena hampir 50% konsumsi daging sapi di Indonesia berasal dari impor. Salah satu penyebab terjadinya sedikitnya ketersediaan daging sapi ini adalah tingginya biaya yang perlu dikeluarkan oleh sebuah peternakan, khususnya dalam hal pakan sapi ternaknya. Sebagai salah satu biaya terbesar dalam peternakan, biaya pakan dapat diminimalisasi dengan cara mengoptimasikan kombinasi pakan ternak yang selama ini dilakukan peternak secara manual. Beberapa penelitian telah menawarkan beberapa metode untuk optimasi biaya pakan, seperti linear programming, metode numerik, dan algoritma genetika. 
Penelitian ini bertujuan untuk membangun model untuk melakukan optimasi biaya kombinasi kombinasi pakan konsentrat ternak sapi menggunakan algoritma evolution strategies (ES). Ada beberapa pola yang bisa dipilih dalam ES, namun penelitian ini menggunakan pola di mana ES tidak melakukan proses rekombinasi dan pada proses seleksi melibatkan seluruh individu parent beserta offspring. Pengimplementasian algoritma ES terdiri dari 2 proses, yaitu penentuan populasi awal lalu dilanjutkan dengan reproduksi dengan tujuan untuk menentukan solusi local optimum dan global optimum.
Penelitian ini menghasilkan sebuah model untuk masing-masing dari sepuluh jenis sapi yang diujikan. Hasil dari proses optimasi oleh algoritma ES dibandingkan dengan hasil serupa yang dilakukan oleh metode simpleks. Hasil perbandingannya adalah dari sepuluh jenis sapi yang diuji coba, enam di antaranya mendapatkan kombinasi pakan konsentrat dengan biaya yang lebih rendah dari algoritma ES. Jika dilihat lebih jauh dan menyeluruh, selisih total perbandingan biaya dari hasil antara algoritma ES dan metode simpleks dari kesepuluh jenis sapi yang diujikan mencapai Rp4.251,09498 per kilogram pakan dengan ES yang lebih unggul. Sehingga, dengan menggunakan optimasi kombinasi pakan konsetrat sapi dengan algoritma ES, peternak dapat menghemat sekitar Rp425,1 per kilogram pakan per ekor sapi.

Kata kunci: evolution strategies, optimasi pakan ternak","According to the Directorate General of Livestock and Animal Health of the Indonesian Ministry of Agriculture, in 2020 the demand for beef in Indonesia will reach 302,300 tons per year, while the availability of local meat is only 165,478 tons. This is very concerning because almost 50% of beef consumption in Indonesia comes from imports. One of the causes of the low availability of beef is the high costs needed by a farm, especially in cattle feed. As one of the biggest costs in animal husbandry, feed costs can be minimized by optimizing the combination of animal feed that has been done manually by farmers. Several studies have offered several methods for optimization of feed costs, such as linear programming, numerical methods, and genetic algorithms.
This study aims to build a model to optimize the cost of the combination of cattle concentrate feed combination using the evolution strategies (ES) algorithm. There are some pattern in ES, but this study use the pattern where ES does not carry out the recombination process and the selection process involves all individual parents and their offspring. The implementation of the ES algorithm consists of 2 processes, namely determining the initial population and then proceeding with reproduction with the aim of determining the local optimum and global optimum solutions.
This study produced a model for each of the ten types of cattle tested. The results of the optimization process by the ES algorithm are compared with similar results carried out by the simplex method. The result of the comparison is that of the ten types of cattle tested, six of them received a combination of concentrate feed at a lower cost than the ES algorithm. If we look further and comprehensively, the difference in the total cost comparison of the results between the ES algorithm and the simplex method of the ten types of cattle tested reaches Rp. 4,251,09498 per kilogram of feed with ES being superior. Thus, by optimizing the combination of cow concentrate feed with the ES algorithm, farmers can save around IDR 425.1 per kilogram of feed per cow.

Keywords: evolution strategies, cattle feed optimization",,,153855,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
197883,,ACHMAD QURAISH HARIS,Order Picking Routing dan Path Planning Berbasis Algoritme Firefly dan Algoritme A* pada Kendaraan Warehouse,,,"Warehouse, order picking routing, path planning, firefly, A*","Anny Kartika Sari, S.Si., M.Sc., Ph.D.",2,3,0,2021,1,"Salah satu kegiatan utama di dalam warehouse adalah order picking atau pengambilan barang. Kecepatan dan efisiensi adalah kunci utama pada peningkatan kinerja warehouse, sehingga dibutuhkan metode order picking yang dapat memenuhi kebutuhan tersebut, salah satunya dengan order picking routing atau pencarian rute untuk menyelesaikan order picking dengan total jarak sependek mungkin. Di dalam order picking routing juga dibutuhkan path planning, yang merupakan metode pencarian jalur yang perlu ditempuh untuk melakukan perpindahan dari satu titik pengambilan barang ke titik pengambilan barang yang lain.
Penggabungan algoritme firefly untuk order picking routing dengan algoritme A* untuk path planning dilakukan pada penelitian ini. Hasil dari penerapan algoritme adalah rute order picking yang selanjutnya disimulasikan dengan animasi grafis visual menggunakan jalur yang didapatkan dari penerapan kedua algoritme.
Pengujian dilakukan dengan membandingkan algoritme firefly dengan algoritme ACO, yang merupakan salah satu algoritme yang paling banyak digunakan pada order picking routing, dengan menggunakan empat jenis layout warehouse yang berbeda. Hasil pengujian menunjukkan bahwa dengan meningkatnya jumlah titik pengambilan barang, algoritme firefly menghasilkan rute dengan rata-rata jarak yang lebih pendek dari algoritme ACO dengan rata-rata eror relatif -6,902%. Dari hasil pengujian tersebut dapat disimpulkan bahwa algoritme firefly dapat digunakan sebagai alternatif pencarian rute order picking.","One of warehouse's main activities is order picking. Speed and efficiency are main keys to warehouse's performance improvement, so order picking methods that could satisfy those are much needed, among them is order picking routing, which is a method to find routes to finish order pickings with minimum total distances. In order picking routing, path planning is also needed. Path planning would be the method to find paths that should be traversed to move from one order picking point to another.
Firefly algorithm is used in this research to perform order picking routing which then combines A* algorithm to perform path planning. The result of the application of the algorithms is an order picking route, including the full path, that would then be used in a simulation with visual graphic animation.
Tests are conducted by comparing firefly algorithm with ACO algorithm, which is one of the most popular algorithms to be used to perform order picking routing, using four different warehouse layouts. Tests results show that with the increase of order picking points, firefly algorithms on average could generate routes with shorter distances than ACO algorithm with the relative error of -6,902%. From those results it could be concluded that firefly algorithm could be used as an alternative to find order picking routes.",,,149317,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
199164,,MUHAMMAD MUKHTAR K,Sistem Rekomendasi Pemilihan Mata Kuliah Pilihan Menggunakan Algoritma Genetika,,,"recommendation system, elective courses, genetic algorithm","Aina Musdholifah, S.Kom., M.Kom. Ph.D.",2,3,0,2021,1,"Setiap awal semester, mahasiswa diwajibkan untuk memilih mata kuliah apa saja yang akan diikuti pada semester tersebut. Setiap program studi mempunyai banyak mata kuliah pilihan yang beragam. Mata kuliah pilihan biasanya didasarkan pada minat dan kemampuan dasar mahasiswa serta pengalaman mahasiswa yang lain.
Penelitian ini berfokus untuk membangun sistem rekomendasi mata kuliah pilihan menggunakan algoritma genetika. Algoritma genetika digunakan untuk memfilter sepuluh mata kuliah yang direpresentasikan sebagai individu berbentuk sebuah kromosom terdiri dari sepuluh gen representasi mata kuliah. Fungsi fitness untuk evaluasi individu adalah jumlah skor dari tiap parameter rekomendasi. Dataset yang digunakan yaitu mata kuliah pilihan S1 Ilmu Komputer UGM kurikulum 2016.
Pengujian dilakukan dengan mengujicobakan sistem terhadap 10 mahasiswa untuk menghitung akurasi pengambilan mata kuliah pilihan di semester 5 dan 6 mereka serta meminta penilaian dari responden yaitu sebanyak 30 mahasiswa. Sistem rekomendasi mengatur parameter algoritma genetika sesuai default, yang mana ukuran populasi sebanyak 100, ukuran generasi 500, probabilitas crossover 0.7, dan probabilitas mutasi 0.1. Hasil pencapaian akurasi sistem rekomendasi ini mendapatkan nilai sebesar 74.08% sedangkan hasil penilaian sistem dari responden yaitu 93.33% untuk relevance, 78.67%, untuk novelty, 73.33% untuk serependity, dan 84% untuk increasing recommendation diversity.","Every semester begining, students are required to choose which courses to take in that semester. Each study program has many different elective courses. Elective courses are usually based on the student's basic interests and abilities as well as the experiences of other students.
This research focuses on building a recommendation system for elective courses using genetic algorithms. The genetic algorithm is used to filter the ten elective courses which represented as an individual in the form of a chromosome consisting of ten gene representation of the course. The fitness function for individual evaluation is the sum of the scores for each recommendation parameter. The dataset used in this research is the elective course for S1 Computer Science UGM Curriculum 2016.
Testing was carried out by testing the system with 10 students to calculate the accuracy of taking elective courses in their 5th and 6th semesters and requesting a valuation from the respondents, namely 30 students. The recommendation system sets the genetic algorithm parameters by default, which are the population size of 100, the generation size of 500, the crossover probability 0.7, and the mutation probability 0.1. The results of achieving the accuracy of this recommendation system is 74.08% while the results of the system valuation test from respondents 93.33% for relevance, 78.67% for novelty, 73.33% for serependity, and 84% for increasing recommendation diversity.",,,150738,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203772,,WAFFIQ MAAROJA,Klasifikasi Spesies Ikan Menggunakan Deep Convolutional Neural Network Dengan Augmentasi Data Berbasis Generative Adversarial Network (GAN),,,"Klasifikasi Spesies Ikan, CNN, DCGAN, WGAN-GP, FID","Sri Mulyana, Drs., M.Kom.; Dzikri Rahadian Fudholi, S.Kom., M.Comp.",2,3,0,2021,1,"Proses deteksi maupun klasifikasi objek pada citra yang diambil pada perairan dengan objek ikan memiliki kualitas yang kurang baik sehingga sulit untuk dilakukan. Metode Deep Learning (DL) mampu mengekstraksi ciri atau karakteristik data sekaligus melakukan deteksi dan klasifikasi objek, namun distribusi data yang kurang merata atau ketidakseimbangan jumlah data pada setiap kelasnya dapat membuat performa model memburuk. Augmentasi data secara tradisional mampu menghasilkan data sintetis, namun objek pada data baru yang dihasilkan tidak berubah secara signifikan.

Pada penelitian ini dilakukan eksperimen terhadap metode dalam mengklasifikasi data citra dengan objek ikan menggunakan Convolutional Neural Network (CNN) dengan teknik transfer learning pada model ResNet-50V2 serta Generative Adversarial Networks (GAN) untuk membangkitkan data sintetis, di mana data tersebut akan digunakan sebagai tambahan data yang digunakan dalam proses pelatihan model pengklasifikasi. Pembangkit data sintetis menggunakan model DCGAN dan WGAN-GP. Hasil penelitian menunjukkan bahwa model pengklasifikasi memiliki performa yang lebih baik pada dataset yang menggunakan tambahan data sintetis yang diperoleh dari GAN dengan nilai akurasi, precision, serta f1-score berturut-turut sebesar 96,51%, 90,41%, dan 93,47% daripada data sintetis yang diperoleh dari metode augmentasi tradisional maupun dataset dengan tanpa adanya tambahan data sintetis. Sedangkan aspek pengujian terhadap GAN menggunakan nilai FID memberikan hasil terbaik pada model DCGAN di 3 kelas dengan nilai FID sebesar 1438,33; 1853,14; serta 1872,78 dan pada model WGAN-GP di 2 kelas dengan nilai FID sebesar 3028,58 dan 1653,01.","The process of detection and classification of objects in images taken in waters with fish objects has poor quality so it is difficult to do. The Deep Learning (DL) method can extract data features or characteristics while simultaneously detecting and classifying objects, but the uneven distribution of data or an imbalance in the amount of data in each class can make model performance worse. Traditional data augmentation can generate synthetic data, but the objects in the new data generated do not change significantly.

In this study, experiments were carried out on methods for classifying image data with fish objects using Convolutional Neural Network (CNN) with transfer learning techniques on the ResNet-50V2 model and Generative Adversarial Networks (GAN) to generate synthetic data, where the data will be used as an additional data used in the classifier model training process. Synthetic data generator using the DCGAN and WGAN-GP models. The results showed that the classifier model had better performance on datasets that used additional synthetic data obtained from GAN with accuracy, precision, and f1-score values of 96.51%, 90.41%, and 93.47%, respectively than synthetic data obtained from traditional augmentation methods or datasets without additional synthetic data. While the aspect of testing the GAN using the FID value gives the best results on the DCGAN model in 3 classes with an FID value of 1438.33; 1853.14; and 1872.78 and on the WGAN-GP model in the other 2 classes with FID values of 3028.58 and 1653.01.",,,155129,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
203518,,PUTRI RIZKI AMALIA,Analisis Sentimen Berdasarkan Aspek Pada Ulasan Restoran Berbahasa Indonesia Menggunakan Kombinasi Convolutional Neural Network (CNN) dan Contextualized Word Embedding,,,"analisis sentimen berdasarkan aspek, ulasan restoran, convolutional neural network, contextualized word embedding, ELMo, BERT, Word2vec","Drs. Edi Winarko, M.Sc., Ph.D.",2,3,0,2021,1,"Opini seseorang terhadap suatu produk atau layanan yang dituangkan melalui sebuah ulasan merupakan suatu hal yang cukup penting bagi pemilik ataupun calon pelanggan produk atau layanan tersebut. Namun, banyaknya jumlah ulasan menyulitkan para calon pelanggan untuk menganalisis informasi yang terdapat pada ulasan-ulasan tersebut. Analisis sentimen berdasarkan aspek merupakan suatu cara untuk mengetahui polaritas sentimen suatu kalimat opini berdasarkan aspek yang telah ditentukan. Penelitian terkait sudah banyak dilakukan menggunakan kombinasi deep learning seperti CNN, GRU, dan LSTM dengan traditional word embedding seperti Word2vec, GloVe, dan FastText. Namun, penelitian analisis sentimen berdasarkan aspek dengan kombinasi deep learning dengan contextualized word embedding masih sangat terbatas pada ulasan berbahasa Indonesia.

Model yang digunakan pada penelitian ini adalah kombinasi CNN dengan contextualized word embedding, BERT dan ELMo, lalu akan dibandingkan dengan model kombinasi CNN dengan traditional word embedding, Word2vec. Pengujian klasifikasi aspek terhadap tiga model; BERT-CNN, ELMo-CNN, dan Word2vec-CNN, memberikan hasil terbaik pada model ELMo-CNN dengan nilai nilai micro-average precision sebesar 0.86, micro-average recall sebesar 0.90, dan micro-average f1-score sebesar 0.88. Sedangkan untuk klasifikasi sentimen memberikan hasil terbaik pada model BERT-CNN dengan rata-rata nilai precision sebesar 0.88, recall sebesar 0.89, dan f1-score sebesar 0.91. Klasifikasi yang dilakukan dengan data tanpa stemming pada ketiga model memiliki hasil yang hampir mirip bahkan lebih baik dibandingan menggunakan data dengan stemming.
","Someone's opinion on a product or service that is poured through a review is something that is quite important for the owner or potential customer of the product or service. However, the large number of reviews makes it difficult for them to analyze the information contained in the reviews. Aspect-based sentiment analysis is the process of determining the sentiment polarity of a sentence based on predetermined aspects. Many related studies have been using a combination of deep learning such as CNN, GRU, and LSTM with traditional word embedding such as Word2vec, GloVe, and FastText. However, aspect-based sentiment analysis with a combination of deep learning with contextualized word embedding is still very limited to reviews of Indonesian-language.

The model used in this research is a combination of CNN with contextualized word embedding; BERT and ELMo, then it will be compared with the CNN combined with traditional word embedding; Word2vec. The result of aspect-classification on three models; BERT-CNN, ELMo-CNN, and Word2vec-CNN give the best results on the ELMo-CNN model with micro-average precision of 0.86, micro-average recall of 0.90, and micro-average f1-score of 0.88. Meanwhile, the sentiment-classification gives the best results on the BERT-CNN model with a precision value of 0.88, a recall of 0.89, and an f1-score of 0.91. Classification using data without stemming on the three models have almost similar results, even better than using data with stemming.",,,154835,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
205823,,FAKHRI SETO D,ANALYSIS COMPARATION OF RSA AND ELLIPTIC CURVE DIGITAL SIGNATURE ALGORITHM (ECDSA),,,"Digital Signature, Elliptic Curve, ECDSA, RSA","Medi, Drs., M,Kom",2,3,0,2021,1,"Penulisan ini akan membahas cara membandingkan kinerja algoritma tanda tangan digital kurva eliptik dan RSA.  Skema tanda tangan digital adalah skema yang merupakan salah satu aplikasi asimetri kriptografi yang digunakan untuk mensimulasikan sifat keamanan dari sebuah tanda tangan, hanya diterapkan secara digital, dan RSA merupakan kriptosistem kunci publik yang banyak digunakan untuk transmisi data yang aman.
 Tujuan dari makalah ini adalah untuk membuktikan seperti apa perbedaan kinerja antara Elliptic Curve Digital Signature Algorithm (ECDSA) dan RSA, mana yang lebih efektif dengan jumlah data dan waktu yang lebih sedikit.  Pendekatannya menggunakan tanda tangan digital yang dikombinasikan dengan kriptografi kurva eliptik kemudian akan dibandingkan dengan RSA.  Skema yang diusulkan memiliki waktu pemrosesan dan overhead penggunaan data yang rendah dibandingkan dengan RSA yang ada.
 Ada beberapa tahapan untuk melakukan algoritma ECDSA dan RSA.  Dalam ECDSA, tahapannya adalah pembuatan kunci, penandatanganan, dan verifikasi.  Dan untuk RSA fasenya adalah, pembuatan kunci, enkripsi dan penandatanganan, dekripsi dan verifikasi.  Evaluasi penelitian ini dilakukan melalui analisis hasil dengan membandingkan jumlah perhitungan parameter yang menghasilkan key time, waktu penandatanganan, dan waktu verifikasi menggunakan dataset dari dataset kompetisi Shopee.  Peneliti membuktikan bahwa ECDSA lebih efisien berdasarkan penggunaan data yang lebih kecil dan proses yang memakan waktu.","This writing will be discussing a way to compare the performance elliptic curve digital signature algorithm and RSA. Digital signature scheme is a scheme that is one of the applications of cryptographic asymmetry used to simulate the security properties of a signature, only applied digitally, and RSA is a public key cryptosystem that is widely used for secure data transmission.
The objectives of this paper are to prove what kind of performance differences between Elliptic Curve Digital Signature Algorithm (ECDSA) and RSA, which one is more effective with smaller amount of data and time. The approach is to use digital signature combined with elliptic curve cryptography then it will be compared with the RSA. The proposed scheme has low time processing and data usage overhead as compared to existing RSA.
There are several phases to do the ECDSA and RSA algorithm. In ECDSA, the phases are key generation, signing, and verification. And for the RSA the phases are, key generation, encryption and signing, decryption and verification. This research evaluation is conducted through the result analysis by comparing the number of parameters computation generating key time, signing time, and verifying time using dataset from Shopee competition dataset. Researcher proved that ECDSA has more efficiency based on smaller data usage and time consuming process.",,,157351,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
191233,,NADIA ELAESIANA P,Pengembangan Word Embedding untuk Domain Spesifik Ulasan Hotel Berbahasa Indonesia,,,"word embedding, Word2Vec, FastText, analisis sentimen","Yunita Sari, S.Kom., M.Sc., Ph.D. ; Aina Musdholifah, S.Kom., M.Kom. Ph.D",2,3,0,2020,1,"Word embedding merupakan salah satu metode representasi kata. Word embedding
mempunyai kelebihan dibanding representasi kata lain, yaitu dapat menangkap hubungan semantik pada kata. Pre-trained word embedding merupakan word embedding yang dilatih pada korpus yang besar dan domain yang umum. Pre-trained word embedding menghasilkan performa yang terbatas dalam menyelesaikan masalah NLP pada domain spesifik karena tidak menangkap informasi semantik pada domain spesifik.

Pada penelitian ini akan dikembangkan domain spesifik word embedding untuk
ulasan hotel berbahasa Indonesia. Domain spesifik word embedding dievaluasi secara intrinsik dan ekstrinsik. Evaluasi intrinsik dilakukan dengan melihat kemampuan model domain spesifik word embedding dalam menangkap sinonim dan kata turunan. Evaluasi ekstrinsik yang dilakukan adalah analisis sentimen. Evaluasi juga dilakukan pada pre-trained word embedding dan kombinasi pre-trained dan domain spesifik word embedding. Model word embedding yang dibuat pada penelitian ini adalah Word2Vec dan FastText.

Pada evaluasi intrinsik, domain spesifik word embedding memperoleh akurasi lebih rendah dibanding pre-trained word embedding, yaitu akurasi domain spesifik word embedding untuk Word2Vec dan FastText sebesar 0.56 dan akurasi pre-trained word embedding sebesar 0.65 untukWord2Vec dan 0.62 untuk FastText. Pada evaluasi ekstrinsik, domain spesifik word embedding memiliki performa paling tinggi, yaitu memperoleh akurasi 0.76, recall 0.76, dan F1-Score 0.79. Sedangkan pre-trained word embedding memiliki performa lebih rendah dibanding domain spesifik word embedding, terutama model pre-trained word embedding FastText yang memiliki performa paling rendah diantara semua model, yaitu dengan akurasi 0.71, recall 0.71, dan F1-Score 0.75.
","Word embedding is one of the word representation methods. One of the advantages
of word embedding compared to other word representation methods is that it can capture semantic relation between the words. Pre-trained word embedding is word embedding trained on large-scale generic corpora. The performance of pretrained word embedding for solving NLP tasks in domain specific is limited, since pre-trained word embedding do not capture domain specific semantics/knowledge.

In this study, domain specific word embedding will be developed with Indonesian
hotel reviews as the domain. Intrinsic and extrinsic evaluation will be conducted to evaluate domain specific word embedding. Intrinsic evaluation will be done by measuring the ability of domain specific word embedding in capturing synonyms and alternative forms. Extrinsic evaluation will be done with sentiment analysis. Evaluations are also carried out on pre-trained word embedding and combination of pretrained and domain-specific word embedding. Word embedding models that will be built in this study are Word2Vec and FastText.

In intrinsic evaluation, domain specific word embedding obtained lower accuracy
than pre-trained word embedding, with accuracy of 0.56 for Word2Vec and FastText domain specific word embedding, and accuracy of 0.65 for Word2Vec pretrained word embedding and 0.62 for FastText pre-trained word embedding. In extrinsic evaluation, domain specific word embedding has the highest performance, with an accuracy of 0.76, recall 0.76, and F1-Score 0.79. Whereas pre-trained word embedding has lower performance than domain specific word embedding, especially FastText pre-trained word embedding which has the lowest performance among all models, with an accuracy of 0.71, recall 0.71, and F1-Score 0.75.
",,,142592,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186370,,M QODIR ABIDIN,SISTEM PENILAI KUALITAS INTERNET PADA WIRELESS LOCAL AREA NETWORK,,,"AP, ICMP, internet, QoS, WLAN","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2020,1,"Aplikasi monitoring kualitas internet berguna untuk mengontrol dan
mengetahui tingkat kualitas internet suatu jaringan dalam menjalankan suatu
layanan. Aplikasi-aplikasi monitoring kualitas internet yang sudah ada masih
memiliki kekurangan, yakni pengguna harus membuat kesimpulan sendiri
mengenai kualitas internet berdasarkan nilai parameter Quality of Service (QoS)
yang diperoleh. Dalam hal ini, pengguna tentunya harus terlebih dahulu memahami
apa saja karakteristik dalam melakukan analisis kinerja suatu jaringan.
Dalam penelitian ini, dibangun sebuah sistem penilai kualitas internet yang
dapat memberikan kesimpulan tingkat kualitas internet dalam suatu jaringan.
Terlebih dalam Wireless Local Area Network (WLAN), dimana sistem seperti ini
dapat dimanfaatkan pengguna untuk rekomendasi pemilihan konektivitas Access
Point (AP) yang memiliki kinerja terbaik sesuai dengan layanan yang akan
dijalankan. Sistem yang dibangun pada penelitian ini memanfaatkan Internet
Control Message Protocol (ICMP) dalam mendapatkan data traffic internet dengan
karakteristik data rate maupun jumlah datanya mengikuti standar dari ITU-T
G.1010 dan penilaian kualitas parameter QoS mengikuti standar TIPHON.
Proses pengujian dilakukan dengan menguji fungsionalitas dari sistem,
yakni dengan membandingkan hasil nilai parameter QoS yang dihasilkan oleh
sistem yang dibangun dalam penelitian ini dengan hasil nilai parameter QoS yang
dihasilkan oleh aplikasi monitoring jaringan lainnya.","Internet quality monitoring application is useful for controlling and knowing
the level of internet quality of a network in running a service. Existing internet
quality monitoring applications still have shortcomings, that is, users must make
their own conclusions about internet quality based on the Quality of Service (QoS)
parameters obtained. In this case, the user must first understand what are the
characteristics in performing a network performance analysis.
In this study, an internet quality assessment system was built that can
provide conclusions about the level of internet quality in a network. Especially in
Wireless Local Area Networks (WLAN), where such a system can be used by users
to recommend the selection of Access Point (AP) connectivity that has the best
performance according to the service to be run. The system built in this study
utilizes the Internet Control Message Protocol (ICMP) in obtaining internet traffic
data with data rate characteristics and the amount of data following the standards of
ITU-T G.1010 and quality assessment of QoS parameters following the TIPHON
standard.
The testing process is done by testing the functionality of the system, namely
by comparing the results of the QoS parameter values generated by the system built
in this study with the results of the QoS parameter values generated by other
network monitoring applications",,,137396,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186628,,ANNISHA FIRDAUSY R,KLASIFIKASI TENDENSI HASIL UJIAN AKHIR MAHASISWA BERDASARKAN DATA KEBIASAAN DALAM PENGGUNAAN MOOC (STUDI KASUS: ELOK UGM),,,"Klasifikasi, SVM, Random Forest, Tendensi, Hasil Ujian Akhir.","Isna Alfi Bustoni, S. T., M. Eng.",2,3,0,2020,1,"Peningkatan ketertarikan penggunaan e-learning telah dilakukan salah satunya dengan implementasi gamifikasi. Gamifikasi memanfaatkan elemen pada gim untuk mengarahkan pengguna menuju tujuan dan perilaku yang ingin dicapai. Namun, im- plementasi gamifikasi belum sepenuhnya memberikan dampak yang baik. Terdapat peluang peningkatan hasil ujian akhir dengan mendeteksi hasil ujian akhir melalui klasifikasi dan pengolahan data kebiasaan pengguna.
Penelitian ini melakukan klasifikasi pada data kebiasaan penggunaan eLOK sebagai e-learning yang dipilih menggunakan algoritma Support Vector Machine dan Random Forest. Tujuannya unutk mengklasifikasikan hasil ujian akhir mahasiswa. Data kebisaan merupakan ekstraksi data log eLOK pembelajaran Pemrograman I se- mester gasal tahun ajaran 2017/2018 yang didukung oleh nilai harian, ujian tengah semester dan ujian akhir semster masing-masing pengguna
Akurasi yang dihasilkan oleh klasifikasi SVM tertinggi dihasilkan pada ming- gu ke-9 dan ke-17 adalah 0.51 tanpa dilakukannya seleksi pada fitur. Sedangkan akurasi pada klasifikasi Random Forest mengalami kenaikan pada pertambahan jum- lah data dengan akurasi tertinggi pada minggu ke-17 dengan nilai 0.68. Minggu yang dianjurkan untuk melakukan klasifikasi dimulai pada minggu ke-9 dengan menggu- nakan algoritma klasifikasi Random Forest","One of the ways to improve the use of e-learning is by implementing gamification. Gamification utilizes elements in the game that is capable of directing users towards the goals and behaviors to be achieved. However, the implementation of gamification has not fully had a good impact. There is an opportunity to improve the results of the final exam by detecting the final exam results through the classification and processing of user data habits.
The data used for the research are collected from eLOK. Classification is done to find the tendency of final exam scores. The behavior data was the extraction of eLOK log data in the Programming I learning odd semester of the academic year 2017/2018 which is supported by daily score, midterm, and final examinations for each user.
The accuracy produced by the SVM classification at 9th and 17th weeks is 0.51 without selection on features. While the accuracy of the Random Forest classification has increased in increasing the amount of data with the highest accuracy in the 17th week with a value of 0.68. The recommended week for classification starts at 9th week using the Random Forest classification algorithm",,,137652,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186630,,KEVIN GOLDWIN,PENGGUNAAN COMPLEMENT NAIVE BAYES UNTUK MEMECAHKANMASALAHIMBALANCE DATASETPADA TWITTER DENGAN STUDIKASUS JASA TRANSPORTASI ONLINE,,,"Twitter, Analisis Sentimen, Imbalance Dataset, Complementary Naive Bayes, Multinomial Naive Bayes","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2020,1,"Twitter adalah sosial media dimana penggunanya dapat menulis berbagai topik dan berdiskusi masalah yang terjadi. Masalah yang muncul saat melakukan analisis sentimen menggunakan data twitter adalah imbalance dataset. Imbalance dataset dapat menimbulkan risiko kesalahan klasifikasi  sehingga algoritma pengklasifikasi tidak bekerja dengan optimal. Penelitian ini menggunakan algoritma  klasifikasiComplement Naive Bayes(CNB) untuk mengatasi imbalance dataset pada data twitter. Penggunaan Multinomial Naive Bayes(MNB) akan dilakukan sebagai klasifikasi pembanding. Pengujian dilakukan pada dataset berjumlah 3268 tweet (24% positif dan 76% negatif serta netral) menggunakan seleksi fitur chi square 30% dan 50% fitur terbaik.Hasil Penelitian CNB dengan seleksi fitur chi square 30% berhasil mendapatkan performa lebih baik dibandingkan dengan seleksi fitur chi square 50% denganakurasi sebesar 88% presisi sebesar 61%, recall sebesar 72% dan F1-Score sebesar74%. CNB memiliki nilai recall dan F1-Score yang lebih baik dibandingkan MNB pada seleksi fitur chi square 30% dan 50% fitur terbaik.  Hasil Pengujian pada MNB dengan chi square 50% mendapatkan akurasi dan presisi yang lebih baik tetapi mendapatkan nilai recall dan F1-score yang lebih rendah dibandingkan dengan CNB.","Twitter is a social media platform where users can discuss and write about various topics. Sentiment analysis using Twitter dataset usually suffers from imbalanced dataset problem.  Imbalanced dataset can increase the risk of wrong classifications, causing the classifier model to perform less optimal. This research uses Complement Naive Bayes (CNB) classification algorithm for resolving imbalanced dataset in Twitter data. Multinomial Naive Bayes is also used for classification comparison. The test is done on the dataset of 3268 tweets (24% positive and 76% negative and neutral) using feature selection of chi-square30% and 50% for selecting best features. Result for CNB and chi-square 30% have the better performance compared to chi-square 50% with accuracy of 88%, precision of 61%, recall of 72%, and F1-scoreof 74%. CNB has better recall and F1-score using feature selection of chi-square 30%and 50% for selecting best features.  Result for MNB and chi-square 50% have the better performance in accuracy and precision but worse in recall and F1-score valuethan CNB.",,,140730,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186888,,VINCENT MICHAEL S,PREDIKSI STRUKTUR SEKUNDER PROTEIN MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK DAN SUPPORT VECTOR MACHINE ,,,"Protein Secondary Structure Prediction, Convolutional Neural Network, Support Vector Machine","Afiahayati, S.Kom., M.Kom., Ph.D.",2,3,0,2020,1,"Prediksi struktur sekunder protein adalah salah satu permasalahan dalam disiplin ilmu Bioinformatika. Prediksi struktur sekunder protein dilakukan untuk mengetahui fungsi dari suatu protein. Prediksi struktur sekunder protein dilakukan dengan mengklasifikasi setiap sekuens struktur primer protein ke dalam bentuk sekuens struktur sekunder protein. Permasalahan ini termasuk ke dalam kategori Sequence Labelling yang mana bisa diselesaikan dengan pendekatan Pembelajaran Mesin.

Convolutional Neural Network dan Support Vector Machine adalah 2 metode Pembelajaran Mesin yang sering digunakan dalam masalah klasifikasi. Pada penelitian ini Convolutional Neural Network dipilih karena kemampuanya untuk mengambil dan memperkaya pola hubungan dari sekuens struktur primer protein. Support Vector Machine juga dipilih untuk digunakan untuk memprediksi struktur sekunder protein berdasarkan data masukan berupa feature maps dari arsitektur Convolutional Neural Network.

Pada penelitian ini kombinasi arsitektur Convolutional Neural Network dan Support Vector Machine mampu menghasilkan tingkat akurasi yang lebih tinggi dibandingkan dengan arsitektur Convolutional Neural Network sederhana. Arsitektur kombinasi menghasilkan tingkat akurasi data uji CullPDB Q3 sebesar 80.75% dan Q8 sebesar 68.84% (Q3 meningkat 1.4% dan Q8 meningkat 0.71%), sementara itu pada data uji CB513 menghasilkan tingkat akurasi Q3 sebesar 78.51% dan Q8 sebesar 64.76% (Q3 meningkat 1.37% dan Q8 meningkat 0.67%).","Protein secondary structure prediction is one of the problems in the Bioinformatics discipline. Protein secondary structure prediction is conducted in order to find the function of proteins. Protein secondary structure prediction is done by classifying each sequence of protein primary structure into the sequence of protein secondary structure. This problem is included in the Sequence Labeling category which can be solved with the Machine Learning approach.

Convolutional Neural Network and Support Vector Machine are 2 methods of Machine Learning that are often used in classification problems. In this study, the Convolutional Neural Network was chosen because of its ability to extract and enrich the relationship patterns of the protein primary structures sequences. Support Vector Machine was also chosen to be used to predict the protein secondary structures based on feature maps from the Convolutional Neural Network architecture.

In this study, the combination of Convolutional Neural Network and Support Vector Machine is able to produce a higher level of accuracy compared to simple Convolutional Neural Network architecture. The hybrid architecture produces accuracy as high as 80.75% (Q3) and 68.84% (Q8) for CullPDB test data (Q3 increased by 1.4% and Q8 increased by 0.71%), while in CB513 test data resulted in an accuracy of 78.51% (Q3) and 64.76% (Q8) (Q3  increased by 1.37% and Q8 increased by 0.67%).",,,137912,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
185101,,MOHAMMAD GALIH A,Fake and Truthful News Classification on Website Articles Using Naive-Bayes Classifier,,,"Naive Bayes, Term Frequency-Inverse Document Frequency, Classification","Bambang Nurcahyo Prastowo, Drs., M.Sc",2,3,0,2020,1,"KLASIFIKASI BERITA PALSU DAN ASLI PADA ARTIKEL WEBSITE MENGGUNAKAN KLASIFIKASI NAIVE-BAYES
 
Mohammad Galih A.
15/380920 / PA / 16728
 
Artikel situs web yang dibuat oleh penulis konten mempunyai kredibilitas yang beragam. Artikel website disusun dengan sejumlah besar karakter. Artikel dapat berisi apa saja, mulai dari informasi hingga pendapat. Beberapa penelitian serupa telah dilakukan untuk melihat apakah artikel yang berisi berita ini dapat dianalisis untuk tujuan klasifikasi. Pengklasifikasi Bayesian memungkinkan algoritma pembobotan seperti TF-IDF diimplementasikan di dalamnya untuk meningkatkan kinerja.
Dalam penelitian ini, tujuannya adalah untuk menerapkan pembobotan TF-IDF ke pengklasifikasi Naive Bayes untuk menghasilkan skor kinerja tinggi ketika digunakan untuk tujuan mengklasifikasikan berita palsu dan berita yang benar. Penelitian ini dilakukan menggunakan kumpulan data artikel yang diperoleh dari Snopes.com. Data artikel yang dikumpulkan terdiri dari 99 berita benar dan 50 berita palsu
Hasil yang diperoleh dalam penelitian ini memberikan akurasi tertinggi 81,6%, presisi 100%, recall 78%, dan F1-Score 88%, untuk kemampuan sistem membedakan antara berita palsu dan berita yang benar.

Kata Kunci: Naive Bayes, Istilah Frekuensi-Inverse Dokumen Frekuensi, Klasifikasi","ABSTRACT
 
FAKE AND TRUTHFUL NEWS CLASSIFICATION ON WEBSITE ARTICLES USING NAIVE-BAYES CLASSIFIER
 
Mohammad Galih A. 
15/380920/PA/16728
 
A website article made by a content writer with varied credibility. It composed with a huge number of characters. An article can contain anything from information to opinions. Several similar researches have been done to see whether these articles containing news can be analyzed for the purpose of classification. A Bayesian classifier allows a weighting algorithm such as TF-IDF to be implemented within it to improve performance.
In this research, the goal is to implement TF-IDF weighting to a Naive Bayes classifier in order to produce a high performance score when used for the purpose of classifying fake news and truthful news. It is done with a collection of articles data obtained from Snopes.com. The article data collected consist of 99 true news and 50 fake news
The results obtained in this research gave the highest accuracy of 81.6%, precision of 100%, recall of 78%, and F1-Score of 88%, for the system&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s ability of distinguishing between fake news and truthful news.

Key Words: Naive Bayes, Term Frequency-Inverse Document Frequency, Classification
",,,136030,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190222,,ARNES RESPATI PUTRI,ATTENDANCE AUTHENTICATION SYSTEM USING BLUETOOTH LOW ENERGY AND FACE RECOGNITION IN ANDROID,,,"Attendance Management System, Android, Bluetooth Low Energy, Face Recognition, Convolutional Neural Network, Face Embedding Vector, Support Vector Machine","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2020,1,"Kehadiran dianggap sebagai aspek penting dalam menentukan performa mahasiswa. Di Universitas Gadjah Mada, perkuliahan dilakukan setiap hari, sementara rekaman kehadiran masih dilakukan secara manual. Sebagai solusi, sebuah sistem baru diusulkan yang memungkinkan otomatisasi dokumen, meminimalkan ketidakjujuran mahasiswa, hemat biaya, dan mendukung fleksibilitas kuliah, mis. seminar dan lokakarya.

Kombinasi \textit{Bluetooth Low Energy} (BLE) dan pengenalan wajah dalam sistem autentikasi kehadiran yang diusulkan mengatasi kelemahan sistem yang sudah ada. BLE digunakan untuk mengirimkan TOTP yang memungkinkan perekaman kehadiran yang cepat dan simultan, sementara algoritma pengenalan wajah yang canggih memastikan kejujuran siswa. Algoritma ini mencakup CNN untuk deteksi wajah dan pembuatan \textit{face embedding}. Selain itu, SVM digunakan dalam klasifikasi 200 identitas dalam pengenalan wajah di sistem ini.

Dalam evaluasinya, mekanisme BLE-TOTP berhasil mempertahankan efisiensi dalam waktu verifikasi 10 detik. Hal ini juga mendorong siswa untuk berada di dalam ruang kuliah karena kemampuan pendeteksian BLE menjadi sulit bersama bertambahnya jarak. Dengan menggunakan CNN bertingkat dengan input multi-dimensi dan penindasan non-maksimum untuk menghilangkan jendela deteksi yang salah pada tahap deteksi wajah, sistem dapat memverifikasi permintaan pengenalan wajah dalam waktu 30 detik dengan tetap menjaga akurasi deteksi wajah sebesar 80,8%. Selain itu, pengenalan wajah memberikan hasil FAR dan FRR 0,0059 dan 0,168 yang menjaga keamanan sistem dan menghilangkan kemungkinan proksi. Sistem ini dikembangkan sebagai aplikasi Android, menghasilkan pendekatan rekaman kehadiran yang terjangkau, cerdas, dan aman.","Attendance is considered as an important aspect in determining a student's performance. In Universitas Gadjah Mada, a considerable number of lecture events are conducted every day, while the attendance recording is still done manually. As a solution, a new system is proposed that allows the automation of paperwork, minimizes frauds, is cost-effective, and supports the flexibility of lectures, i.e. seminars and workshops.

The combination of Bluetooth Low Energy (BLE) and face recognition in the proposed attendance authentication system tackle the drawbacks of the existing system. BLE is used to transmit TOTP that allows fast and simultaneous attendance, while state-of-the-art face recognition algorithm ensures student honesty. The algorithm includes CNN for face detection and face embedding generation. Moreover, SVM is used in the classification of 200 identities in face recognition.

In the evaluation, the BLE-TOTP mechanism successfully maintains efficiency with the verification time of 10 s. It also encourages students to be inside the lecture room since BLE detection ability becomes challenging as the distance is added. By using cascaded CNNs with multi-dimensional input and non-maximum suppression to extremely eliminate false detection windows in the face detection stage, the system can verify a face recognition request within 30 s while keeping the face detection accuracy to be 80.8%. Additionally, the face recognition gives FAR and FRR result of 0.0059 and 0.168 which maintains the security of the system and eliminate proxy attempts. The system is developed as an Android application for good measure and hence provides an affordable, smart, and secure way for attendance taking.",,,141460,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184081,,Muhammad Hanif Satria Pratapa,PENGARUH PEMBAGIAN RESOURCE TERHADAP KINERJA MAPREDUCE,,,"Hadoop, MapReduce, YARN, pembagian resource, container per node","Dr. Mardhani Riasetiawan, SE Ak, M.T.",2,3,0,2020,1,"Hadoop Mapreduce merupakan framework yang populer digunakan untuk mengolah Big Data. Namun terdapat persoalan yang ditemui pengguna maupun pengembang aplikasi Hadoop MapReduce, di antaranya adalah optimasi performa melalui konfigurasi Hadoop dan mengelola resource secara efektif. Untuk mencapai performa yang optimal dan pengelolaan resource yang efektif, diperlukan pendalaman mengenai konfigurasi Hadoop, di mana terdapat lebih dari 200 parameter konfigurasi yang dapat memengaruhi performa Hadoop MapReduce.

Berbagai penelitian telah dilakukan untuk mengatasi permasalahan tersebut seperti pengujian pengaruh konfigurasi-konfigurasi tertentu terhadap kinerja MapReduce atau melakukan otomasi terhadap konfigurasi sesuai kebutuhan pengguna, namun penelitian-penelitian tersebut belum meneliti pengaruh konfigurasi pembagian resource terhadap kinerja Hadoop MapReduce. Pada penelitian ini dilakukan analisis pengaruh pembagian resource berupa jumlah container per node terhadap kinerja Hadoop MapReduce dengan indikator waktu, CPU, dan memori beserta menganalisis keterkaitan antara metrik-metrik yang lebih rinci antar indikator-indikator tersebut. Penelitian ini dilakukan dengan mengeksekusi Job MapReduce dengan konfigurasi container per node yang berbeda-beda dan juga diteliti pada lingkungan alokasi RAM yang berbeda-beda.

Ditemukan bahwa tidak terdapat pola tertentu pada waktu eksekusi Job MapReduce seiring naiknya jumlah container per node, akan tetapi ditemukan keterkaitan antara indikator-indikator kinerja dan ditemukan angka jumlah container per node yang relatif optimum yaitu 4 container per node.","Hadoop MapReduce is a populer framework used to process BiG Data. Yet there are numbers of problem that users and Hadoop MapReduce application developers face, mainly performance optimization through Hadoop configuration and effectively managing resource. To achieve optimized performance and effective resource management, in depth understanding on Hadoop configuration is needed, as there are more than 200 configuration parameters which can impact Hadoop MapReduce's performance.

A number of research have been done to resolve the problem mentioned such as testing the effect of certain configuration on MapReduce's performance or automating configuration based on the user's need, yet these research have not investigate the effect of resource partition configuration on Hadoop MapReduce. In this research, analysis is done on the effect of resource partitioning using container per node number on Hadoop MapReduce with time, CPU, and memory used as indicator, and how their subindicators interact with each other. This research is done by excuting MapReduce job with different numbers of contaner per node and done in different RAM allocation environment.

It is found that there is no pattern of MapReduce Job execution time as the number of container per node increase, but it is also found that there are relations between it's performance's indicator and there is a relatively optimum number of container per node, which is 4 container per node.",,,134975,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
191509,,MAULISA DEWI MAHENDA,ANALISIS SENTIMEN BERDASARKAN ASPEK PADA ULASAN HOTEL BERBAHASA INDONESIA,,,"Fuzzy C-Means clustering, analisis sentimen berdasarkan aspek, naive bayes","Aina Musdholifah, S.Kom, M.Kom, Ph.D ; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2020,1,"Ulasan dapat digunakan sebagai rekomendasi sebelum konsumen menentukan hotel yang akan dipilih. Pemesan diminta untuk memberikan penilaian per aspek dalam bentuk bintang yang disediakan oleh TripAdvisor.com dan menyebabkan timbulnya bias diantara ulasan yang sudah ditulis dan penilaian per aspek. Oleh karena itu, dibutuhkan analisis sentimen berdasarkan aspek dari ulasan yang sudah dituliskan oleh konsumen. Selain itu, dibutuhkan pula klastering aspect term untuk menentukan kata yang termasuk pada suatu aspek. Aspek yang digunakan pada penelitian ini adalah aspek lokasi, layanan, value, dan kebersihan.
Proses klastering aspect term menggunakan Fuzzy C-Means dan proses klasifikasi sentimen menggunakan algoritma Bernoulli Naive Bayes dan Multinomial Naive Bayes. Data yang digunakan merupakan data yang berasal dari TripAdvisor.com dan menggunakan 129,256 ulasan dari 580 hotel. 
Proses klastering aspect term menggunakan Fuzzy C-Means dan menghasilkan nilai koefisien fuzzy partition tertinggi sebesar 0.81. Proses klasifikasi sentimen menggunakan representasi fitur CountVectorizer dan TfIdfVectorizer. Berdasarkan hasil pengujian klasifikasi sentimen setiap klaster aspek, rata-rata akurasi tertinggi sebesar 74.48% dan rata-rata f-measure tertinggi sebesar 84.58% dihasilkan oleh algoritma Multinomial Naive Bayes menggunakan TfidfVectorizer. Hasil akurasi yang diperoleh dari perhitungan rating sebesar 79.78%
","Reviews can be used as recommendations before choosing hotel. The customer is asked to provide a per-aspect rating in the form of a star provided by TripAdvisor.com and causes a bias between the reviews that have been written and the ratings per aspect. Therefore, aspect based sentiment analysis is needed from the reviews written by customer. In addition, it also requires clustering of aspect terms to determine the words included in which aspect. Aspect that used in this experiment is location, service, value, and cleanliness. 
The clustering of aspect terms process uses Fuzzy C-Means and the sentiment classification process uses Bernoulli Naive Bayes algorithm and Multinomial Naive Bayes algorithm. The data used is data from TripAdvisor.com and uses reviews from 580 hotels that contains 129,256 reviews. 
The clustering aspects terms process uses Fuzzy C-Means and produces the highest fuzzy partition coefficient value of 0.81. The sentiment classification process uses the CountVectorizer and TfIdfVectorizer feature representations. Based on the results of the sentiment classification test for each aspect cluster, the highest average accuracy of 74.48% and highest average f-measure of 84.58% is generated by the Multinomial Naive Bayes algorithm using a TfIdfVectorizer. Accuracy result obtained from the calculation of rating prediction is 79.78%
",,,142640,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190235,,ADITHYA IRAWAN,DETECTION OF MAC SPOOFING IN MOBILE HOTSPOT USING RANDOM FOREST,,,"Mac addresses, RSS, Random forest, feature importance","Drs. Bambang Nurcahyo Prastowo, M. Sc.",2,3,0,2020,1,"In computer network, the authentication of each user in the network is essential to the business corporation (Bourgeois and Bourgeois, 2014). One of those authentication problem is mac spoofing. Previous research by Alotaibi and Elleithy (2016) have attempted to formulate a detection technique using RSS data with random forest based on their accuracy. This research attempted to use that technique with the help of feature importance technique as a way of detecting the mac spoofing in mobile hotspot. The implementation used RSS data, random forest, python library and feature importance technique. The RSS that already process by the feature importance shows that the top 3 most important feature of the benchmark model device that had been created cannot be mimic by other device with mac address spoofing technique.","In computer network, the authentication of each user in the network is essential to the business corporation (Bourgeois and Bourgeois, 2014). One of those authentication problem is mac spoofing. Previous research by Alotaibi and Elleithy (2016) have attempted to formulate a detection technique using RSS data with random forest based on their accuracy. This research attempted to use that technique with the help of feature importance technique as a way of detecting the mac spoofing in mobile hotspot. The implementation used RSS data, random forest, python library and feature importance technique. The RSS that already process by the feature importance shows that the top 3 most important feature of the benchmark model device that had been created cannot be mimic by other device with mac address spoofing technique.",,,141608,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190749,,MUHAMMAD ANINDITO ADHA,LONG SHORT TERM MEMORY UNTUK MEMPREDIKSI HARGA BITCOIN BERDASARKAN DATA TWITTER SENTIMEN DAN DATA RIWAYAT HARGA BITCOIN,,,"Forcasting, LSTM, Bitcoin, Cryptocurrency","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2020,1,"Bitcoin merupakan alternatif investasi yang lumayan banyak diminati oleh banyak
orang karena kemudahannya bahkan bisa digunakan sebagai alat pembayaran. Namun nilai
dari sebuah Bitcoin sangat fluktuatif sehingga banyak dilakukan penelitian untuk
meprediksi harga bitcoin salah satunya dengan bantuan LSTM yang berdasar data historis
harga bitcoin namun hasilnya sangat buruk, kemudian penelitian juga dilakukan dengan
bantuan analisis sentimen dari sosial media twitter.
Tujuan dari penelitian ini adalah untuk memprediksi harga Bitcoin dengan bantuan
LSTM namun data yang digunakan tidak hanya data historis harga bitcoin namun sekaligus
menggunakan data hasil analisis sentimen twitter. Pemilihan fiture yang akan digunakan
dari masing-masing data berdasarkan tingkat korelasi tertinggi antar fiture dengan bantuan
heatmap.
Hasil penelitian ini menunjukan dapat di gunkananya data historis harga bitcoin
dan hasil analisis sentiment twitter sebagai acuan untuk memprediksi harga Bitcoin dimana
setelah menggabungkan fiture yang memiliki tingkat korelasi paling tinggi pada masingmasing data dapat menghasilkan nilai simpangan Mean Asolute Eror sebesar 31.198 dan
untuk nilai Root Mean Square Eror sebesar 45.189. Sedangkan akurasi Dstat 82,288%.","Bitcoin is an investment alternative that is pretty much in demand by many because
of its convenience and can even be used as a means of payment. But the value of a Bitcoin
is very volatile so that a lot of research is done to predict the price of bitcoin one of them
with the help of LSTM based on historical data of bitcoin prices but the results are very
bad, then research is also carried out with the help of sentiment analysis from Twitter social
media. However,
The purpose of this study is to predict the price of Bitcoin with the help of
LSTM, but the data used are not only historical data of bitcoin prices, but at the same
time use data from the results of Twitter sentiment analysis. The selection of features to
be used from each data is based on the highest level of correlation between features with
the help of a heatmap.
The results of this study indicate the use of historical bitcoin price data and the
results of Twitter sentiment analysis as a reference to predict Bitcoin prices where after
combining features that have the highest level of correlation on each data can produce a
Mean Absolute Error value of 31.189 and for the Root Mean Square Error of 45,198.
While for the destat accuracy is 82,288%.",,,141902,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
193821,,MUHAMMAD ARDI PUTRA,Quran Recitation Style Classification using Convolutional Neural Network,,,"Quran recitation, Deep learning, CNN, MFCC","Wahyono, Ph.D.",2,3,0,2020,1,"Membaca kitab suci Al-Quran bisa dikatakan sebagai salah satu seni dalam agama Islam. Pada dasarnya semua orang diperkenankan untuk membaca Quran menggunakan iramanya masing-masing. Meskipun demikian, ada beberapa irama yang paling terkenal, yaitu Ajam, Kurdi, dan Nahawand.
Tujuan dari penelitian ini adalah untuk membuat model CNN yang dapat digunakan untuk membuat model CNN yang dapat digunakan untuk mengklasifikasikan irama-irama tersebut. Eksperimen akan dilakukan dengan melakukan beberapa metode ekstraksi fitur pada data audio suara rekaman bacaan Quran, yaitu MFCC, Log Mel Filter Bank, dan SSC. Selain itu, eksperimen yang dilakukan terhadap hyperparameter dari CNN meliputi ukuran filter, stride filter, jumlah filter, dan jumlah epoch.
Hasil dari penelitian ini menunjukkan bahwa model CNN mendapat akurasi terhadap data tes sebesar 88,4%. Hasil ini lebih baik dibandingkan model MLP dan LSTM yang menghasilkan akurasi 86,6% dan 55,5% secara berturut-turut.
","Reciting the holy book of Quran may be considered as an art in the religion of Islam. Basically, everyone is able to recite the book with their own style. However, there are several styles which are considered as the most famous, namely Ajam, Kurdi and Nahawand.
The objective of this research is to create a CNN-based model which is able to perform classification on those different recitation styles. Several different audio features and CNN hyperparameters are experimented in order to figure out the best model configuration for this classification task. The tested audio features in this research are MFCC, Log Mel Filter Bank and SSC. On the other hand, the tuned CNN hyperparameters involve filter size, filter stride, number of filters and number of epochs.
By the end of this research, it is found that the best CNN model gives 88.4% of classification accuracy towards data in the test set. This result is better compared to that of MLP and LSTM model which obtains the accuracy of 86.6% and 55.5%, respectively.",,,145426,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
188190,,ACHMAD YOHNI WAHYU F,Analisis Perbandingan Kinerja Algoritma Similarity Measure sebagai Tahapan Data Preprocessing: Text Normalization Bahasa Indonesia Untuk Analisa Sentimen,,,"Text Mining, Text Preprocessing, Text Normalization, Spell Correction, Classifcation, Sentiment Analysis","Afiahayati, S.Kom. M.Cs., Ph.D.",2,3,0,2020,1,"Pada masa pandemi COVID-19, kegiatan belajar mengajar dilakukan secara daring. Untuk meningkatkan standar belajar mengajar secara daring, perlu dilakukan analisa sentimen terhadap perangkat lunak belajar daring. Penggalian data pada analisa sentimen sendiri, memiliki banyak permasalahan seperti data noisy. Sehingga untuk meningkatkan performa data preprocessing diperlukan algoritma normalisasi kata yang paling baik untuk dataset Bahasa Indonesia. Dalam penelitian ini dilakukan perbandingan normalisasi kata untuk spell correction dengan algoritma Levenshtein Distance, Jaro-Winkler Distance, Bigram, dan Smith-Waterman dengan variasi threshold 60%, 70%, 80%, 90%. Penelitian ini menggunakan dua dataset yang berbeda yaitu data kuisioner dan data twitter. Dan pada penelitian ini dilakukan pengujian validasi untuk melihat dampak normalisasi terhadap klasifikasi sentimen. Hasil yang didapatkan algoritma Levenshtein Distance pada threshold 60% secara konsisten memberi nilai akurasi normalisasi paling baik. Namun dari waktu normalisasi, pengurangan jumlah kata unik hasil terbaik dihasilkan Jaro-Winkler Distance. Sedangkan normalisasi dengan performa terbaik tidak selalu memberi nilai klasifikasi terbaik.","During the COVID-19 pandemic, the learning activities were carried out online. To improve the online learning standards, sentiment analysis needs to be done on online learning software. Data mining on sentiment analysis itself has many problems such as noisy data. So, to improve the performance of preprocessing data, it is needed the best word normalization algorithm in spell correction for Bahasa dataset. In this research, word normalization using Levenshtein Distance, Jaro-Winkler Distance, Bigram, and Smith-Waterman algorithm with threshold variations of 60%, 70%, 80%, 90%. This study uses two different datasets namely questionnaire data and twitter data. And in this research, validation testing is done to see the affects of the normalization on sentiment classification. The results obtained by the Levenshtein Distance algorithm at the fully approved 60% threshold give the best normalization accuration. But from the time of normalization, the number of unique word results the best results are normalization with Jaro-Winkler Distance. While normalization with the best performance does not always provide the best classification score.",,,139343,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
188705,,Muhammad Nadhif Aswan,Deteksi Ulasan Deceptive Dalam Bahasa Inggris Menggunakan Gated Recurrent Unit,,,"ulasan truthful, ulasan deceptive, Gated Recurrent Unit, Long Short-Term Memory, word embedding","Yunita Sari, S.Kom., M.Sc., Ph.D.; Afiahayati, S.Kom., M.Cs., Ph.D",2,3,0,2020,1,"Ulasan adalah pendapat yang ditulis oleh pengguna untuk suatu produk maupun layanan berdasarkan pengalaman dari produk yang ditinjau. Keputusan pembelian semakin dipengaruhi oleh ulasan produk dan layanan, maka dari itu dibutuhkan pendeteksian ulasan deceptive untuk mengurangi kerugian yang ditimbulkan oleh ulasan deceptive.
Pada penelitian ini digunakan model Gated Recurrent Unit untuk mendeteksi ulasan deceptive. Model dilatih dengan menggunakan data ulasan hotel yang berjumlah 1600 ulasan dengan komposisi yang seimbang. Nilai akurasi, presisi, recall dan f1-score digunakan sebagai evaluasi performa dari model.
Hasil penelitian ini menunjukan model GRU memiliki performa akurasi, presisi, recall, dan f1-score yang masing-masing sebesar 82.19%, 79.88%, 84.52%, dan 82.13%. Model LSTM memiliki akurasi, presisi, recall, dan f1-score masing-masing sebesar 81.56%, 80.38%, 81.94%, dan 81.15%","Reviews are opinions written by users for a product or service based on the experience of the product being reviewed. Purchasing decisions are increasingly influenced by product and service reviews, therefore detection of deceptive review is needed to minimize the losses incurred by deceptive reviews.
In this study the Gated Recurrent Unit model was used to detect truthful and deceptive reviews. The model was trained using 1600 hotel review data with a balanced composition. Accuracy, precision, recall and f1-score values will be used as an evaluation of the performance of the model.
The results of this study shows that the GRU model has accuracy, precision, recall, and f1-score performance, each of which is 82.19%, 79.88%, 84.52%, and 82.13%. The LSTM model has an accuracy, precision, recall, and f1-score of 78.75%, 77.02%, 80.00%, and 78.48%.",,,139839,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
194850,,KEVIN JONATHAN,ETHEREUM BLOCKCHAIN BASED E-VOTING SYSTEM FOR DECENTRALIZED AND SECURE ELECTIONS ,,,"e-voting, blockchain, ethereum, bitcoin, decentralized applications","Anny Kartika Sari, S.si., M.sc., Ph.D.",2,3,0,2020,1,"Di saat kita terus bergerak terhadap sebuah dunia yang lebih canggih, salah satu hal yang tetap kita lakukan secara tradisional adalah voting. Melihat bagaimana tersebarnya literasi komputer, terasa lebih mengejutkan mengapa kita belum mengimplementasikan e-voting secara luas. Voting adalah sesuatu yang sangat berkembang jika didigitalisasikan. Hal ini dapat menghemat banyak usaha, kertas, dan tenaga kerja. Dan untungnya bagi kita, teknologi blockchain yang sedang berkembang adalah teknologi yang tepat untuk merealisasikan hal ini. Belakangan ini, penggunaan teknologi blockchain telah menyaksikan peningkatan yang tajam, terutama karena keunggulan yang ditawarkan. Blockchain juga memiliki banyak potensial dalam masa depan yang terdesentralisir.
Dalam riset ini, sebuah blockchain Ethereum diintegrasikan kedalam sebuah sistem e-voting dengan harapan jika menggunakan sistem ini, pemilihan umum dapat dilakukan lebih efisien, efektif, terdesentralisir, transparan, dan bebas penipuan. Ganache digunakan untuk mensimulasikan jaringan Ethereum, dan Metamask digunakan sebagai wallet untuk menangani transaksi yang dilakukan.
Aplikasi yang dikembangkan mensimulasikan Ethereum Virtual Network secara local dan smart contracts nya dapat dijalankan di blockchain. Tampilan aplikasinya kemudian dapat berkomunikasi dengan blockchain nya dengan bantuan Web3.js yang dapat mengambil dan menulis data secara permanen ke blockchain, sambil melakukan hot reload tampilannya untuk setiap transaksi yang ditambahkan kedalam perhitungan suara. Walau demikian, diperlukan riset dan pengembangan lanjut agar aplikasi ini dapat diadopsi kedalam penggunaan dunia nyata.","As we keep on moving forward towards a more sophisticated world, one of the things remained being done mostly traditionally is voting. Considering how widespread computer literacy has been, it is more surprising that we have not implement e-voting widely yet. Voting is one of the things that would improve greatly should it be digitalized. It could save a lot of work, paper, and manual labour. And luckily for us, the emerging blockchain is just the right technology to make that a reality. Lately, blockchain technologies have seen a significant boom, mostly due to the many advantages it offers that could be applied in just about every aspects. Blockchain is also seeing huge potentials in a future of decentralization.
In this research, an Ethereum Blockchain is integrated into an e-voting system in hopes that upon using this system, election could be done more efficiently, effectively, decentralized, transparent, and fraud-proof. Ganache is used to simulate the Ethereum network, while Metamask is used as the wallet to handle the transactions performed.
The developed application simulates the Ethereum Virtual Machine locally and the smart contracts can be successfully deployed to the blockchain. The application interface then can communicate with the blockchain with the help of Web3.js that it could fetch and write data immutably to the blockchain, successfully hot reloading the interface with every transaction added to keep track of the vote count. In spite of that, further research and development is needed for this application to be adopted into real world use.
",,,146478,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184359,,M YUSUF MANSHUR,IMPLEMENTASI BAG OF VISUAL FEATURE MENGGUNAKAN ALGORITMA PENGELOMPOKAN DBSCAN UNTUK PERSON RE-IDENTIFICATION,,,"person re-identification, bag of visual feature, algoritma pengelompokan, DBSCAN","Wahyono, Ph.D",2,3,0,2020,1,"Person re-identification merupakan topik yang cukup populer pada bidang computer vision saat ini. Person re-identification adalah proses untuk mengidentifikasi apakah dua citra atau lebih yang berada di sudut pandang kamera CCTV berbeda, adalah orang yang sama atau tidak. Berbagai penelitian telah dilakukan untuk membangun model descriptor yang cukup handal dan efisien untuk digunakan pada sistem person re-identification. Telah digunakan berbagai metode untuk membentuk model citra manusia salah satunya adalah model Bag of Visual Feature.
Bag of Visual Feature (BOVF) adalah salah model yang paling banyak digunakan untuk merepresentasikan data sebagai kumpulan fitur lokal yang digunakan untuk melakukan retrieval image maupun recognition. Fitur-fitur lokal ini dilibatkan dalam pembentukkan descriptor atau fungsi identitas yang diperlukan pada sistem person re-identification dengan mekanisme pengelompokan fitur. Salah satu algoritma pengelompokan dengan kemampuan mengelompokan data dengan jumlah data yang banyak dan berdimensi tinggi secara efisien adalah Density Based Spatial Clustering of Applications with Noise (DBSCAN). Dibandingkan dengan algoritma K-Means, DBSCAN mengelompokan data menjadi sejumlah klaster berdasarkan karakteristik (kerapatan) data, sehingga akan mampu menghasilkan jumlah klaster yang optimal. 
Pada penelitian ini digunakan model BOVF untuk proses person re-identification. Implementasi BOVF menggunakan algoritma DBSCAN pada proses pelatihan model citra dengan mempelajari fitur HOG (Histogram of Gradient Oriented). Hasil dari penelitian ini diperoleh akurasi terbaik pada rank-1=13,67% dan rank-5=43,33% menggunakan K-Means, kemudian pada rank-10=68,33%, rank-15=80,67%, dan rank-20=88% menggunakan DBSCAN. Selain itu K-Means masih lebih cepat dari pada DBSCAN, dengan kecepatan rata-rata sebesar 1,64 detik, sedangkan DBSCAN 1,85 detik.
","Person re-identification is one of the popular topic in the field of computer vision. Person re-identification is the process of identifying whether two or more images that are in the viewpoint of different CCTV cameras are the same person or not. Various studies have been carried out to build descriptor models that are reliable and efficient enough to be used in person re-identification systems. Various methods have been used to represent the full body human image, one of which is the Bag of Visual Feature model.
Bag of Visual Features (BOVF) is one of the most widely used models to represent data as a collection of local features used to perform image retrieval and recognition. These local features are involved in the formation of descriptors or identity functions required on the person re-identification system with a feature grouping mechanism. One of the grouping algorithms that have ability to group data with large amounts of data and high dimensions efficiently is Density Based Spatial Clustering of Applications with Noise (DBSCAN). Compared to the K-Means algorithm, DBSCAN groups data into a number of clusters based on the characteristics (density) of the data, so that it will be able to produce an optimal number of clusters.
In this research, BOVF model is used for the person re-identification process. BOVF implementation uses the DBSCAN algorithm in the process of training the image model by studying the HOG (Histogram of Gradient Oriented) features. The results of this research obtained the best accuracy at rank-1=13.67% and rank-5=43.33% using K-Means, then at rank-10=68.33%, rank-15=80.67%, and rank-20=88% using DBSCAN. In addition, K-Means is still faster than DBSCAN, with an average speed of 1.64 seconds, while DBSCAN is 1.85 seconds.
",,,135359,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
185127,,DEVNI KARMELITA P,LANGUAGE RELATEDNESS IN MULTI-SOURCE NEURAL MACHINE TRANSLATION WITH MISSING DATA,,,"machine translation, neural networks, multi-source NMT, language relatedness","Dr. Mardhani Riasetiawan, S.E., M.T.",2,3,0,2020,1,"Mesin translasi memiliki peran untuk menerjemahkan dari sumber bahasa ke target bahasa. Di dalam mesin translasi ada 3 pendekatan yang sering digunakan, Mesin Translasi Statistik, Mesin Translasi Berbasis Aturan, dan Mesin Translasi Syaraf Tiruan. Dalam beberapa waktu terakhir, mesin translasi syaraf tiruanlah yang paling sering digunakan, namun pendekatan ini membutuhkan jumlah data yang sangat besar, dan tidak semua bahasa memiliki corpus multi-sumber dalam jumlah yang besar.
Penelitian ini mencoba untuk memaksimalkan kombinasi bahasa dalam masukan dari multi-sumber MTS. Multi-sumber MTS memiliki peran untuk menerjemahkan dari satu sumber bahasa ke target bahasa dengan bantuan dari bahasa lain. Secara umum, mesin translasi membutuhkan jumlah corpus multibahasa lengkap yang sangat besar sebagai masukan., tapi MTS ini bisa memproses  corpus multibahasa yang tidak lengkap. Data yang digunakan dalam penelitian ini adalah  corpus multibahasa yang tidak lengkap yang bersumber dari TED Talks.
Hasil dari penelitian ini menunjukan bahwa multi-sumber MTS meraih skor BLEU yang lebih bagus dibandingkan satu-sumber MTS secara keseluruhan. Pada bagian sumber bahasa, jumlah kalimat yang hilang akan memberikan pengaruh lebih daripada relasi antara sumber bahasa dan bahasa pembantu dalam akurasi translasi. Penambahan BLEU terbaik dari pengaturan two-to-one NMT adalah 3.2 dari {En, Nl}-to-De, disini dapat dilihat bahwa Bahasa Jerman memperoleh hasil terbaik dengan bantuan dari Bahasa Belanda yang memiliki relasi yang sangat dekat. Sedangkan pada setelan three-to-one NMT, penambahan BLEU terbaik dicapai oleh {En, Uk, Be}-to-Ru dan {En, Es, Uk}-to-Ru dengan penambahan score 5.6, kedua model tersebut memiliki target Bahasa Rusia dan di bantu dengan Bahasa Ukraina yang dekat relasinya dengan target bahasa.
","Machine translation is tasked to translate from source language to target language. There are three major approaches for this task, Rule-based Machine Translation, Statistical Machine Translation, and Neural Machine Translation. The recent machine translation is using the neural networks approaches, but these approaches need a large amount of data to deliver a decent translation result, and not all languages have a large amount of data complete multilingual corpus.
This research tries to maximize the language combination in input of multi-source NMT in low resource settings. The multi-source NMT system task is to translate one source language to target language with the help of another language. Generally, machine translation needs a complete multilingual corpus for input, but this NMT can process incomplete multilingual corpus as the input. The data used is an actual incomplete multilingual corpus from TED Talks transcription.
The results show that multi-source NMT is generally performing better than single-source NMT with average BLEU gains is 1.2. On the source side the number of missing sentences in the helper language give more impact compared to the relatedness between source language and helper language in translation quality. The best BLEU gains in two-to-one NMT setting is 3.2 from {En, Nl}-to-De, it showed German perform best with Dutch, which is very closely related to German, as a helper language. In three-to-one NMT setting, the best BLEU gaines is from {En, Uk, Be}-to-Ru and {En, Es, Uk}-to-Ru task with 5.6 BLEU gaines, both models have Russian as target language and Ukrainian as helper language, which is very closely related to Russian.
",,,136047,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184619,,Randi Aulia Ramadhan,STUDI KOMPARASI PERFORMA KLUSTER SERVER SCALLING UP DENGAN NON-SCALLING UP MENGGUNAKAN TEKNOLOGI DOCKE,,,"Kluster Server, Scaling,  Throughput, Response Time","Dr.techn, Ahmad Ashari, M.Kom.",2,3,0,2020,1,"Web server merupakan bagian penting pada situs web, namun kerap kali terjadi sebuah kegagalan pada web server berupa server down. Server down disebabkan oleh request terhadap situs web terlalu banyak pada satu waktu sehingga server tidak mampu menerima request tersebut. Untuk mengatasi hal tersebut dapat dilakukan clustering pada server dengan menggunakan docker, selain itu scaling up dapat juga diterapkan pada kluster server sehingga server dapat memberikan performa yang maksimal. 
Pada penelitian ini dilakukan pengujian perbandingan performa nilai throughput dan response time pada kluster server yang diterapkan scaling up dan akan dibandingkan dengan kluster server tanpa menggunakan scaling up. Setelah nilai throughput dan response time didapat, dilakukan komparasi kluster server yang menerapkan scaling up dengan kluster server tanpa scaling up. 
Hasil dari penelitian ini memperlihatkan bahwa kluster server yang menerapkan scaling up memiliki waktu throughput dan response time yang lebih baik dibandingkan dengan kluster server yang tidak menerapkan scaling up. Perbandingan performa menunjukkan untuk semua skenario pengujian yang dilakukan, kluster server yang menerapkan scaling up memiliki nilai rata-rata performa yang lebih baik dari kluster server tanpa scaling up.
","Web server is an important part from a website, but often a failure occurs on a web server in the form of a server down. Server down caused the web server can&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;t handle number of requests at one time, server is unable to accept the request. To overcome the problem, clustering can be done on the server by using a docker, also scaling up can be applied to the server cluster and make the server provide maximum performance.  
In this research, a comparison test of the performance of the throughput and response time value on the server clusters applied by scaling will be compared with server clusters without the use of scaling up. After the throughput and response time value is obtained, statistical processing is performed to see differences in performance. 
 The result of this research, showing the cluster server that implement scaling up have a better throughput and response time compared cluster server that do not implement scaling up. Performance comparation showing for all testing scenario, cluster server that implement scaling up have an better average value performance compared cluster server that do not implement scaling up.
",,,137969,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
188971,,MUHAMMAD RIZKI HAKIM,SISTEM DETEKSI PENYALAHGUNAAN DANA MENGGUNAKAN METODE RANDOM FOREST,,,"Random Forest, Klasifikasi, Pohon Keputusan, Bank Dunia, Deteksi Penipuan, Classification, Decision Trees, World Bank, Fraud Detection","Azhari SN, Drs., MT., Dr",2,3,0,2020,1,"Praktek korupsi sudah menjadi masalah utama di Indonesia dan belum ada tindakan signifkan yang dilakukan untuk menghentikan korupsi di Indonesia. Menurut laporan Tahunan KPK pada tahun 2019, Indeks Persepsi Korupsi
Indonesia naik dari tahun 2015 hingga tahun 2019 (KPK, 2019). Tak hanya di Indonesia, praktek korupsi juga terjadi di tingkat Internasional, salah satunya di Bank Dunia. Bank Dunia menyediakan pinjaman uang dengan bunga yang rendah untuk membangun infrastruktur negara-negara berkembang.
Proses deteksi korupsi yang dilakukan oleh Bank Dunia memakan waktu yang sangat lama karena sistemnya yang bergantung pada pengajuan komplain. Jika, tidak ada komplain yang diajukan, maka Bank Dunia tidak akan melakukan penyelidikan. Penelitian ini mengimplementasi metode klasifikasi Random Forest untuk penyelidikan lebih cepat dilakukan tanpa menunggu ada laporan pada pihak
terkait dan mempercepat proses penyelidikan yang akan dilakukan Bank Dunia. Fitur yang digunakan ada tipe kategori dan integer. Untuk tipe kategori akan diubah
menggunakan metode dummy variable.
Setelah model random forest berhasil dibangun, hasil Mean Squared Error terendah dicapai untuk jumlah dataset 50 baris kontrak. Metode dummy variable membuat jumlah fitur pada model sama dengan jumlah label pada fitur suatu
dataset, sehingga label baru yang ada pada dataset baru yang akan diprediksi artinya tambahan fitur asing yang tidak dikenali oleh model yang telah dibangun. Artinya, model yang telah dibangun tidak bisa memprediksi label baru yang belum ada pada dataset pada saat proses fitting.","Corruption practices have become a major problem in Indonesia and no significant action has been taken to stop corruption in Indonesia. According to the KPK Annual report in 2019, Indonesia's Corruption Perception Index rose from 2015 to 2019 (KPK, 2019). Not only in Indonesia, corrupt practices also occur at the International level, one of them at the World Bank. The World Bank provides low interest money loans to develop the infrastructure of developing countries.
The fraud detection process carried out by the World Bank takes a very long time because the system is dependent on filing complaints. If no complaint is submitted, the World Bank will not conduct an investigation. This research
implements the Random Forest classification method for faster investigation without waiting for reports from relevant parties and accelerates the investigation process to be carried out by the World Bank. The features used are of category type and integer type. For category type features will be changed by using the dummy variable method.
After the random forest model was successfully built, the lowest MSE results were achieved for the 50 contract rows dataset. The dummy variable method makes the number of features in the model the same as the number of labels in the features of a dataset, so that new labels that exist in the new dataset that will be predicted means the addition of foreign features that are not recognized by the model that has been built. This means that the model that has been built cannot predict new labels that do not yet exist in the dataset during the fitting process.",,,140111,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183856,,M BAGAS DEWANTORO,License Plate Character Recognition Using Convolutional Neural Netwokr,,,"Convolutional Neural Network, pre-processing methods","Drs. Agus Harjoko, M.Sc., Ph.D.",2,3,0,2020,1,"Convolutional Neural Network juga dikenal sebagai CNN adalah sebuah algoritma yang mampu mengenali karakter dari plat nomor kendaraan dengan bantuan metode pra-pemrosesan dan mendapatkan hasil yang akurat. Hal ini menjadikan CNN dengan metode pra-pemrosesan yang mampu mengenali plat nomor di area outdoor dengan kecerahan tinggi atau kecerahan rendah selama karakternya dapat dilihat.
Penelitian ini bertujuan untuk membandingkan metode pra-pemrosesan untuk pengenalan plat nomor di area luar ruangan menggunakan CNN. Menyesuaikan model CNN, dengan mengubah layer tersembunyi dan menggunakan ukuran batch yang berbeda.","Convolutional Neural Network also known as CNN is an algorithm that is able to recognize characters from the vehicle's license plate with the help of pre-processing methods and obtain accurate results. This makes the CNN with pre-processing methods capable to recognize the license plate in the outdoor area with high brightness or low brightness as long as the character can be seen.
In this research, the goal is to compare the pre-processing method for license plate recognition in the outdoor area using CNN. Adjusting the CNN model, by changing the hidden layer and use different batch sizes.",,,134800,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184625,,EMA NUR AFIFAH,ANALISIS SENTIMEN KOMENTAR PENGGUNA TERHADAP THREAD PADA FORUM BERITA DAN POLITIK DI KASKUS,,,"Kaskus, Analisis Sentimen, TF-IDF, Support Vectore Macine, Multinominal Naive Bayes","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2020,1,"Kaskus merupakan forum daring terbesar di Indonesia. Salah satu forum yang paling banyak dikunjungi adalah forum Berita dan Politik. Pengguna dapat bebas berkomentar mengenai isu politik terkini melalui situs Kaskus. Situs Kaskus digunakan dalam penelitian ini sebagai sumber data untuk menganalisis sentimen komentar thread pada forum Berita dan Politik di Kaskus. Analisis yang digunakan menggunakan metode Multinominal Naive Bayes (MNB) dan Support Vector Machine (SVM), kemudian dibandingkan metode manakah yang paling sesuai.  Pertama, data komentar dikumpulkan dengan metode scraping data dari situs Kaskus. Kedua, data yang diperoleh dilabeli secara manual untuk dijadikan data training. Kemudian dilakukan preprocessing dengan kamus dan tanpa kamus untuk memudahkan melakukan analisis sekaligus menambahkan daftar kata slang untuk penggantian dengan kata baku. Setelah data tersebut melewati preprocessing, kemudian dianalisis untuk mengetahui metode manakah yang memilliki akurasi paling baik dan dibandingkan hasilnya antara penggunaan kamus dan tanpa kamus.

Pada percobaan ini, digunakan 1000 komentar thread yang telah dilabeli yaitu 400 positif, 400 negatif, dan 200 netral. Pada preprocessing menggunakan kamus dihasilkan rata-rata akurasi metode MNB sebesar 50,4%, precision score 47,1%, recall score 42,6%, f1 score 38,9%, dan run time 7532s. Sedangkan tanpa kamus dihasilkan akurasi sebesar 50,8%, precision score 47,1%, recall score 43%, f1 score 40%, dan run time 6,67s. Pada metode SVM preprocessing menggunakan kamus dihasilkan rata-rata akurasi sebesar 48,2%, precision score 43,3%, recall score 42,5%, f1 score 41,5% dan run time 7531s. Sedangkan tanpa kamus dihasilkan akurasi sebesar 48,1%, precision score 42,9%, recall score 42,3%,  f1 score 41,1%, dan run time 6,69s. Dari hasil ini, diketahui bahwa tanpa penggunaan kamus, hasil akurasi dan evaluation metrics kedua metode menjadi lebih baik, serta memiliki run time yang lebih tinggi. Metode MNB mampu menganalisis komentar thread di Kaskus lebih baik daripada SVM baik menggunakan kamus maupun tanpa kamus.
","Kaskus is the largest online forum in Indonesia. One of the most visited forums is the News and Politics forum. Users can freely comment on current political issues through the Kaskus website. The Kaskus website is used in this study as a source of data for analyzing sentiment comment threads on the News and Politics forum at Kaskus. The analysis used using the Multinominal Naive Bayes (MNB) method and Support Vector Machine (SVM), then compare which method is most suitable. First, comment data is collected by scraping data from the Kaskus website. Second, the data obtained is labeled manually to be used as training data. Then preprocessing is done with a dictionary and without a dictionary to make it easy to do the analysis while adding a list of slang words to replace with standard words. After the data has passed through preprocessing, it is then analyzed to find out which method has the best accuracy and compare the results between using a dictionary and without a dictionary.

In this experiment, 1000 thread comments that have been labeled are 400 positive, 400 negative, and 200 neutral. In preprocessing using a dictionary, the average accuracy of MNB method is 50.4%, 47.1% precision score, 42.6% recall score, f1 score 38,9%, and run time 7532s. Whereas without a dictionary the accuracy is 50.8%, the precision score is 47.1%, the recall score is 43%, the f1 score is 40%, and the run time is 6.67s. In the SVM preprocessing method using a dictionary produced an average accuracy of 48.2%, 43.3% precision score, 42.5% recall score, 41.5% f1 score and 7531s run time. Meanwhile, without a dictionary, the accuracy is 48.1%, the precision score is 42.9%, the recall score is 42.3%, the f1 score is 41.1%, and the run time is 6.69s. From these results, it is known that without the use of dictionaries, the results of the accuracy and evaluation metrics of the two methods are better, and have a higher run time. The MNB method is able to analyze thread comments on Kaskus better than SVM using both dictionaries and without dictionaries.
",,,135612,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183863,,Axel,I PENGATUR TINGKAT KESULITAN DINAMIS PADA PERMAINAN RETRO MENEMBAK MENGGUNAKAN NEURAL NETWORK DAN ALGORITMA GENETIKA,,,"kercerdasan buatan, algoritma genetika, permainan retro, game","Dr. Andi Dharmawan, S.Si., M.Cs.",2,3,0,2020,1,"Permainan Retro atau Retro Game merupakan permainan video yang dikembangkan pada masa kebangkitan industri permainan video atau video games yang dimana terjadi pada tahun 1972 hingga awal 1980. Di dalam permainan Retro terdapat beberapa elemen yang membuat permainan menarik untuk dimainkan. Permainan Retro jenis tembak-menembak atau Shooting merupakan permainan retro dimana pemain mengendalikan sebuah karakter di dalam permainan. Kemudian dengan karakter tersebut, pemain menembaki musuh sambil menghindari tembakan atau serangan dari lawan. Penelitian yang sebelumnya lebih berfokus pada perubahan tingkat kesulitan permainan secara dinamis daripada tingkah laku dari dalam permainan sehingga dilakukan penelitian ini.
Pada penelitian ini dilakukan perekaman permainan yang dilakukan oleh manusia melawan manusia. Kemudian dari data tersebut di proses dan dilakukan pembelajaran mesin untuk memperoleh bobot dan bias yang digunakan ke dalam permainan. Metode pembelajaran yang digunakan menggunakan back propagation dengan optimasi ADAM. Hasil dari pembelajaran mesin kemudian dioptimasi menggunakan algoritma genetika.
Hasil dari penelitian ini adalah performa AI dengan akurasi rata-rata paling tinggi sebesar 30 %, tingkat kemenangan rata-rata paling tinggi AI melawan NPC yang dikontrol secara random yaitu sebesar 85%, dan tingkat kemenangan rata-rata AI melawan melawan manusia paling tinggi yaitu sebesar 49%.

","Retro Games or Retro Games are video games that were developed during the revival of the video game industry or video games which occurred in 1972 to early 1980. In Retro games there are several elements that make the game interesting to play. Retro type shootout or Shooting is a retro game where the player controls a character in the game. Then with that character, the player shoots at the enemy while avoiding shots or attacks from the opponent. Previous research focused more on dynamically changing the level of difficulty of the game rather than the behavior of the game so this research was conducted.
In this study recording games made by humans against humans. Then the data is processed and machine learning is carried out to obtain the weights and biases used in the game. The learning method uses back propagation with ADAM optimization. The results of machine learning are then optimized using genetic algorithms.
The results of this study are the performance of AI with the highest average accuracy of 30%, the highest average AI win rate against randomly controlled NPC is 85%, and the highest AI win rate against humans is by 49%.",,,134760,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
185154,,HARY SUSENO,COMPARISON OF WIRELESS SENSOR NETWORK FOR ENERGY MONITORING SYSTEM BETWEEN MQTT AND WEBSOCKET PROTOCOL,,,"Internet of Things, Energy Monitor, Arduino Uno, ESP8266, ACS712","Muhammad Idham Ananta Timur ST., M.Kom",2,3,0,2020,1,"Kebutuhan akan listrik sangat besar dan berbanding lurus dengan penggunaan listrik maka dari itu kebutuhan akan sistem yang dapat mengontrol aliran listrik, seperti memberikan batasan penggunaan saat ini, sehingga penggunaan listrik dapat digunakan sesuai dengan kebutuhan. Sistem ini dibuat untuk memantau konsumsi listrik dari peralatan rumah tangga dengan menggunakan modul NodeMCU EPS8266 dan sensor arus ACS712 yang diproses oleh Arduino uno. Skripsi ini mengimplementasikan teknologi IoT pada outlet terminal untuk memudahkan pengguna memilih metode mana yang lebih baik antara protokol MQTT dan Webscoket dalam mengelola penggunaan peralatan listrik di rumah tangga. Sementara perangkat lunak akan ditampilkan sebagai situs web. Perbandingan dilakukan dengan menguji nilai throughput, dan paket delay. Sistem pemantauan energi dapat digunakan sebagai sumber data untuk pengujian kinerja protokol MQTT dan Websocket.","The need for electricity is very large and directly proportional to the use of electricity then the need for a system that can control the flow of electiricity, as providing limits of current usage, so the use of electricity can be used in accordance with the usage. This system is created to monitor electricity consumption from household appliances by using NodeMCU EPS8266 module and ACS712 current sensor processed by an Arduino uno. This final project implements IoT technology on a terminal outlet to facilitate the user which method is better between MQTT and Webscoket protocol in managing the use of electrical appliances in the household. While the software will be shown as a website. Comparison is performed by testing the value of throughput, and packet delay. The energy monitoring system can be used as data source for performance testing of MQTT and Websocket protocol.",,,136058,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186692,,AFLAH NADHIF HAMMAM,DECEPTIVE OPINION SPAM DETECTION USING CONVOLUTIONAL NEURAL NETWORK (CNN) ENSEMBLE,,,"Convolutional Neural Network Ensemble, Convolutional Neural Network, Deceptive Opinion Spam Detection, Outliers","Mardhani Riasetiawan, SE (Accounting), MT (IT), Dr (Computer Science)",2,3,0,2020,1,"User generated content such as reviews have a big impact in influencing con- sumer decision on the internet. Several parties are interested in using this platform to achieve their agenda such as profit or many other malicious intention by manu- facturing deceptive opinion spam (Ott et al., 2013). Deceptive opinion spam is the fictitious reviews that have been deliberately written to sound authentic and aims to deceive the reader (Ott et al., 2013). Human detection of deceptive opinion spam is proven to be as low as 57.33% (Ren and Ji, 2017), But research done by Ott et al. (2013) shows the possibility of performing detection using machine learning meth- ods. This research aims to create CNN Ensemble models that implement the concept of using a collection of neural networks by using CNN as the base model to further increase the reliability of the detection model due to the existence of outliers in the base model. The implementation of the CNN Ensemble model consists of 30 CNN model where their individual results is combined to get the CNN Ensemble final re- sult. From the 30 CNN models, 15 of them is using GloVe word embedding while the other 15 models is using FastText word embedding. The individual models start by embedding the reviews to get the numerical representations of the reviews, followed by the convolutional, pooling, and activation layer to get the vector representations of each sentences, and followed by these same layers again to get the vector representa- tions of the whole review. Lastly from these review vector, it is used to get the final prediction of the individual models. The CNN Ensemble model achieve an accuracy of 82%. Finally the CNN Ensemble model experiments using 30 CNNs and 10 CNNs shows that the increasing number of individual model used in the ensemble, without using more individual model variation, would yield insignificant improvements.","User generated content such as reviews have a big impact in influencing con- sumer decision on the internet. Several parties are interested in using this platform to achieve their agenda such as profit or many other malicious intention by manu- facturing deceptive opinion spam (Ott et al., 2013). Deceptive opinion spam is the fictitious reviews that have been deliberately written to sound authentic and aims to deceive the reader (Ott et al., 2013). Human detection of deceptive opinion spam is proven to be as low as 57.33% (Ren and Ji, 2017), But research done by Ott et al. (2013) shows the possibility of performing detection using machine learning meth- ods. This research aims to create CNN Ensemble models that implement the concept of using a collection of neural networks by using CNN as the base model to further increase the reliability of the detection model due to the existence of outliers in the base model. The implementation of the CNN Ensemble model consists of 30 CNN model where their individual results is combined to get the CNN Ensemble final re- sult. From the 30 CNN models, 15 of them is using GloVe word embedding while the other 15 models is using FastText word embedding. The individual models start by embedding the reviews to get the numerical representations of the reviews, followed by the convolutional, pooling, and activation layer to get the vector representations of each sentences, and followed by these same layers again to get the vector representa- tions of the whole review. Lastly from these review vector, it is used to get the final prediction of the individual models. The CNN Ensemble model achieve an accuracy of 82%. Finally the CNN Ensemble model experiments using 30 CNNs and 10 CNNs shows that the increasing number of individual model used in the ensemble, without using more individual model variation, would yield insignificant improvements.",,,137848,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190276,,Andhika Satria Bagaskoro,DETEKSI PLAGIARISME DENGAN MENGGUNAKAN LEXICAL SCORE,,,"Deteksi Plagiarisme Intrinsik, Plagiarisme, Lexical Score, Gradient Boosting Regressor, PAN-2018","Edi Winarko, Drs., M.Sc., Ph.D; Yunita Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2020,1,"Plagiarisme sering terjadi dalam lingkup akademik, khususnya pada institusi
pendidikan tinggi, karena mudahnya mengakses dokumen orang lain secara bebas.
Sudah banyak metode untuk mendeteksi plagiarisme dan secara besar dibagi dua bagian, yaitu deteksi plagiarisme ekstrinsik dan deteksi plagiarisme intrinsik. Metode deteksi plagiarisme ekstrinsik merupakan sebuah metode yang menggunakan satu
dokumen yang dianggap plagiarisme lalu dibandingkan dengan dokumen lain untuk
mendeteksi plagiarisme. Metode ini membutuhkan banyak dokumen sebagai pembanding agar hasilnya akurat. Sedangkan pada metode deteksi plagiarisme intrinsik
dapat mendeteksi dengan hanya menggunakan beberapa dokumen.
Pada penelitian ini, digunakan metode deteksi plagiarisme intrinsik dengan
menggunakan lexical score untuk dilihat performanya dan dibandingkan dengan metode oleh Kuznetsov et al., 2016. Data yang digunakan berasal dari PAN-2018. Metode ini menggunakan frekuensi kata pada setiap kalimat, paragraf maupun dokumen,
lexical score, penggunaan kelas kata dan penggunaan tanda baca. Fitur-fitur tersebut
kemudian akan dilatih menggunakan Gradient Boosting Regressor yang menghasilkan nilai kalimat. Dengan menggunakan nilai threshold tertentu, setiap kalimat akan
dilabeli plagiarisme atau tidak.
Dengan menggunakan lexical score, sistem dapat mendeteksi kalimat plagiarisme dengan hasil F1 score tertinggi sebesar 42,67%. Hasil tersebut memiliki kenaikan yang signifikan, jika dibandingkan dengan tidak menggunakan lexical score.","Plagiarism often occurs in the academic sphere, especially in higher education institutions, because it is easy to access other people&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s documents. There are
many methods to detect plagiarism and are divided into two parts, namely extrinsic
plagiarism detection and intrinsic plagiarism detection. Extrinsic plagiarism detection method is a method that uses a document that is considered plagiarism and is
compared with other documents to detect plagiarism. This method requires a lot of
documents as a comparison for accurate results. Whereas intrinsic plagiarism detection method can detect using only a few documents.
In this study, the intrinsic plagiarism detection method is used by using a lexical score to see its performance and compared with the method by Kuznetsov et al.,
2016. The data used comes from PAN-2018. This method uses word frequency in
each sentence, paragraph or document, lexical score, use of word classes and use
of punctuation. These features will then be trained using the Gradient Boosting Regressor which generates sentence values. By using a certain threshold value, each
sentence will be labeled plagiarism or not.
By using a lexical score, the system can detect plagiarism sentences with the
highest F1 score of 42.67%. These results have a significant increase, when compared
to not using a lexical score.",,,141481,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190535,,ARIFIN BENNY P,Predicting Rice Crop Yield Production Through Climate Data with Neural Network,,,"Rice crop yield, Multi-layer Perceptron, Neural network","Dr. Agus Sihabuddin, S.Si., M.Kom ; Yunita Sari, S.Kom, M.ScPh.D",2,3,0,2020,1,"Rice is one of the primary agricultural resources in Indonesia. The highest rice production is in East Java. By nature of yearly rice yield data, only a limited quantity of the dataset could be acquired. Furthermore, there is only a limited study that uses weather data, especially precipitation and temperature, are essential in rice crop. There is yet to be a rice yield prediction using weather data in East Java Indonesia, with last year yield production and most of the past researcher in Indonesia using regression methods and not using a neural network

Predicting the rice yield of Indonesia&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;s East Java province from 1971 to 2018, with climate conditions that are precipitation and average temperature per month, and last year rice yield production as predictors was researched using Multi-layer Perceptron (MLP) and Linear Regression (LR) models. 

In the evaluation, last year yield emerged as a dominant predictor that affect Multilayer Perceptron model evaluation percentage error reduction in 90.22% in Mean Squared Error (MSE) evaluation and 68.77% Root Mean Squared Error (RMSE) than not using last year yield data. The result shows that Multi-layer Perceptron (MLP) model performance is better than Linear Regression model.

","Rice is one of the primary agricultural resources in Indonesia. The highest rice production is in East Java. By nature of yearly rice yield data, only a limited quantity of the dataset could be acquired. Furthermore, there is only a limited study that uses weather data, especially precipitation and temperature, are essential in rice crop. There is yet to be a rice yield prediction using weather data in East Java Indonesia, with last year yield production and most of the past researcher in Indonesia using regression methods and not using a neural network

Predicting the rice yield of Indonesia&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;s East Java province from 1971 to 2018, with climate conditions that are precipitation and average temperature per month, and last year rice yield production as predictors was researched using Multi-layer Perceptron (MLP) and Linear Regression (LR) models. 

In the evaluation, last year yield emerged as a dominant predictor that affect Multilayer Perceptron model evaluation percentage error reduction in 90.22% in Mean Squared Error (MSE) evaluation and 68.77% Root Mean Squared Error (RMSE) than not using last year yield data. The result shows that Multi-layer Perceptron (MLP) model performance is better than Linear Regression model.

",,,141703,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
189770,,ALIFIA LISTU SAMATHA,Clustering Patients With Personality Disorders Using C-Means Clustering Algorithm,,,"C-Means Clustering, Clustering Personality Disorder, Purity, Entropy","Afiahayati, S.Kom., M.Kom., Ph.D",2,3,0,2020,1,"Pada dasarnya, setiap individu memiliki potensi untuk memiliki gangguan kepribadian, tetapi masih dalam batas yang tepat. Namun ada orang-orang yang memiliki gangguan yang tak terkendali yang bisa membahayakan seseorang dari gangguan tersebut. Sangat penting untuk memetakan komorbiditas atau terjadinya lebih dari satu gangguan pada individu yang sama pada gangguan kepribadian dengan kecenderungannya. Jika profesional gagal memetakan salah satu diagnosis, pasien harus menderita Dissociative Identity Disorder (DID), yang merupakan jenis disosiasi serius yang menyebabkan hilangnya identitas.
Penelitian ini bertujuan untuk mengelompokkan kasus-kasus gangguan kepribadian ke dalam beberapa kluster. Dalam penelitian ini, algoritma C-Means clustering akan digunakan untuk melakukan tugas clustering, kemudian kinerjanya akan divalidasi.
Hasil penelitian menunjukkan bahwa C-Means clustering terbaik untuk melakukan pengelompokan pada dataset gangguan kepribadian dengan menggunakan kombinasi parameter fuzzy m = 3 untuk menghitung centroid dan m = 2 untuk menghitung fungsi keanggotaan. Skor validasi adalah 0,94 untuk kemurnian klaster dan 0,2 untuk entropi klaster yang mengindikasikan klaster yang baik.
","Essentially, every individual has potential to have a personality disorder but still within an appropriate limit. But there are those who have an uncontrollable trigger which harms an individual from the disorder. It is very important to map comorbidity or the occurrence of more than one disorder in the same individual on personality disorders with its tendencies. At worst, if professionals fail to map any of the diagnosis, the patient will have to suffer from Dissociative Identity Disorder (DID), which is a serious type of dissociation that causes a loss of sense of identity.
This research aims to cluster personality disorder cases into multiple clusters. In this research, the C-Means clustering algorithm is going to be used to do the clustering task, then its performance will be validated.
The result shows that the C-Means clustering was best to perform clustering on the personality disorder dataset with the combination of parameter m = 3 for calculating centroid and m = 2 for calculating membership function. The validation score was 0.94 for cluster purity and 0.2 for cluster entropy which indicate a good clustering.
",,,140940,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183628,,ALVIN FARKHAN R,Perbandingan Metode Non-Negative Matrix Factorization dan Latent Dirichlet Allocation pada Permasalahan Question Retrieval untuk Studi Kasus Forum Kesehatan,,,"Pemodelan Topik, Latent Dirichlet Allocation, Non-negative Matrix Factorization, Rekomendasi Pertanyaan, Pengambilan Pertanyaan, Sistem Tanya Jawab","Drs. Bambang Nurcahyo Prastowo, M.Sc",2,3,0,2020,1,"Forum tanya jawab kesehatan Alodokter merupakan salah satu media informasi kesehatan yang populer digunakan oleh masyarakat Indonesia. Sistem tanya jawab yang digunakan memiliki kelemahan pada kecepatan dan kesempatan untuk terjawabnya suatu pertanyaan. Untuk menyelesaikan permasalahan tersebut, salah satu solusi yang dapat dilakukan adalah dengan melakukan pencarian dan perbandingan pada pertanyaan yang telah dijawab untuk menjadi rekomendasi bagi pertanyaan baru yang serupa. Besarnya dataset dan terbatasnya komputasi membuat perlu dilakukan metode yang tidak membandingkan pertanyaan baru dengan keseluruhan
data yang ada pada dataset. Pada penelitian ini digunakan metode pemodelan topik untuk memperkecil lingkup data yang akan dilakukan pencarian. Penelitian ini menggunakan
model Latent Dirichlet Allocation (LDA) dan Non-negative Matrix Factorization (NMF) untuk mengelompokkan 148.867 data pertanyaan kesehatan ke dalam 280 topik. Jensen-Shannon Divergence digunakan untuk menghitung jarak relevansi antara suatu pertanyaan baru dan 100 data yang memiliki relevansi tertinggi pada topik dominan dari pertanyaan baru tersebut. lima data yang memiliki jarak relevansi terendah diberikan sebagai rekomendasi pertanyaan serupa. Metode tersebut diimplementasikan
pada 50 data tes yang kemudian dilakukan evaluasi oleh dokter dan pengguna non-medis untuk dihitung tingkat akurasinya menggunakan Mean Average Precision dan Mean Reciprocal Rank. Penelitian ini menghasilkan nilai akurasi
MAP sebesar 0,488 untuk model LDA dan evaluator dokter, 0,588 untuk model LDA dan evaluator non-medis, 0,556 untuk model NMF dan evaluator dokter, serta 0,584
untuk model NMF dan evaluator non-medis. Sedangkan untuk tipe evaluasi MRR, penelitian ini menghasilkan nilai akurasi sebesar 0,677 untuk model LDA dan evaluator
dokter, 0,746 untuk model LDA dan evaluator non-medis, 0,793 untuk model NMF dan evaluator dokter, serta 0,747 untuk model NMF dan evaluator non-medis.","Alodokter health question and answering forum is one of the popular health information media used in Indonesia. The question and answer system used has a weakness in term of speed and chance for a question to be answered. To solve this problem, one of the possible solution is to search and compare the whole dataset to get the similar question recommendations for new question. The size of the dataset
and the limited computational resources makes it required to use method that does not compare new question with entire data contained in the dataset. In this research,
topic modelling method is used to minimize the scope of searching and comparing. This research used Latent Dirichlet Allocation (LDA) model and Non-negative Matrix
Factorization (NMF) model to cluster 148,867 health question data into 280 topics. Jensen-Shannon Divergence is used to calculate the difference between new question
and 100 data that have the highest relevance to the dominant topic of the new question. FIve data that have the highest relevance is used as the recommendation. The method is implemented on 50 test data which are then evaluated by doctors and non-medical users to calculate their accuracy using Mean Average Precision and Mean Reciprocal Rank. This study produced an MAP accuracy value of 0.488 for the LDA model and the doctor evaluator, 0.588 for the LDA model and non-medical evaluator, 0.556 for
the NMF model and the doctor evaluator, and 0.584 for the NMF model and nonmedical evaluator. As for the MRR accuracy, this study produced an accuracy value
of 0.677 for the LDA model and the doctor evaluator, 0.746 for the LDA model and non-medical evaluator, 0.793 for the NMF model and the doctor evaluator, and 0.747 for the NMF model and non-medical evaluator.",,,134525,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
187736,,AHNAF MUHAMMAD A,ASPECT-LEVEL SENTIMENT ANALYSIS ON  SMARTPHONE CUSTOMER REVIEW,,,"Aspect Level Sentiment Analysis, Customer Review","Retantyo Wardoyo, Drs., M.Sc., Ph.D; Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2020,1,"Meningkatnya jumlah teks yang dihasilkan oleh pengguna di internet telah menjadikan analisis sentimen sebagai pendekatan yang populer untuk mengekstraksi informasi. Namun, sebagian besar analisis sentimen saat ini masih menganalisis teks di tingkat dokumen. Dengan menggunakan pendekatan ini, kita hanya akan memiliki keseluruhan analisis sentimen daripada analisis pada target tertentu. Dalam analisis sentimen tingkat aspek, analisis mengacu pada pendapat itu sendiri. Ini berdasar pada gagasan yang menyebutkan bahwa opini terdiri dari sentimen (positif atau negatif) dan target (pendapat). Penelitian ini bertujuan untuk mengidentifikasi aspek entitas dan sentimen yang diungkapkan untuk setiap aspek. Oleh karena itu, penelitian ini berfokus pada analisis sentimen pada tingkat aspek, yang terdiri dari: ekstraksi aspek, klasifikasi aspek, dan klasifikasi sentimen. Data yang digunakan dalam penelitian ini adalah review pengguna dari situs web GSMArena yang terdiri dari beberapa produk. Conditional Random Field digunakan untuk mengekstraksi aspek sementara Maximum Entropy digunakan untuk mengklasifikasikan kategori aspek dan polaritas sentimen. Hasil penelitian ini adalah: ekstraksi aspek mendapat skor rata-rata f1 0,8 (80%). Untuk klasifikasi aspek, model tertinggi dicapai dengan kombinasi fitur unigram dengan rata-rata skor-F1 adalah 0,75 (75%). Terakhir dalam analisis sentimen, model kami mencapai rata-rata F1-Score 0,746 (74,6%)","The increasing number of user-generated text on the internet has made sentiment analysis a popular approach to extract the information. However, most of the sentiment analysis nowadays is still analyze the text in the document level. Using this approach, we will only have the overall analysis of sentiment rather than analysis on the specific target. In aspect-level sentiment analysis, it directly looks at the opinion itself. It is based on the idea that an opinion is consists of a sentiment (positive or negative) and a target (of opinion). Also it aims is to identify the aspects of entities and the sentiment expressed for each aspect. Therefore, this research focuses on sentiment analysis on the aspect level, representing three main steps: aspect extraction, aspect classification, and sentiment classification. The data that used in this research are the user opinion from GSMArena website consists of several products. Conditional random fields are used for extracting the aspect while maximum entropy is for classifying the aspect category and the sentiment polarity. The aspect extraction task is able to achieve average f1 score 0.8 (80%). For aspect classification, the highest model was achieved by combination of unigram features with the average F1-Score is 0.75 (75%). Lastly in the sentiment analysis task, our model was achieved the average F1-Score 0.746 (74.6%)
",,,139167,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
189016,,BREVI ALFARISY,IMPLEMENTASI ALGORITMA MULTILEVEL FEEDBACK QUEUE UNTUK PEMBUATAN APLIKASI PEMESANAN BAJU PADA BUTIK DENGAN PLATFORM ANDROID,,,"Android, Butik, Multilevel Feedback Queue Scheduling (MFQ), Penjahit ","Suprapto, Drs., M.Kom., Dr",2,3,0,2020,1,"Perkembangan usaha yang sangat pesat mendorong setiap pengusaha untuk membuat suatu bentuk yang berbeda dari usaha yang lainnya, salah satunya adalah usaha Butik pada Butik Yasmin Collection, masalah umum yang sering dialami oleh Butik Yasmin Collection adalah penumpukan pemesananan pakaian yang mengakibatkan beberapa pesanan pelanggan tertunda pengerjaannya sehingga membuat pesanan tersebut telat untuk diambil. Sistem antrian pesanan pakaian di Butik Yasmin Collection dibuat menggunakan metode Multilevel Feedback Queue Scheduling (MFQ) yang berfungsi untuk membuat antrian pemesanan pakaian pada penjahit, sehingga waktu yang digunakan untuk pembuatan pakaian lebih terjadwal dan meningkatkan kepuasan pelanggan di Butik Yasmin Collection. Dalam membangun aplikasi pemesanan pakaian menggunakan algoritma Multilevel Feedback Queue Scheduling (MFQ) berbasis Android dengan menggunakan UML (Unified Modelling Language) untuk membuat perancangan sistemnya dan menggunakan bahasa pemrograman PHP dan Ionic serta menggunakan MySQL sebagai basis datanya.","The rapid development of the business pushes Lot of the company to create a different form of business, one of which is a business in the Boutique Yasmin collection, general literature that is often experienced by Butik Yasmin Collection is the buildup of order, which resulted in several order got postponed that makes the order late for the date was promised. The clothing order queuing system at the Yasmin Collection Boutique was created using the Multilevel Feedback Queue Scheduling (MFQ) method which serves to make a queue of ordering clothes for tailors, so that the time spent making clothes is more scheduled and increases customer satisfaction at the Yasmin Collection Boutique. Developing Android-based Multilevel feedback queue scheduling (MFQ) algorithm using UML (Unified modeling Language) to create its system design and use PHP and Ionic programming languages and using MySQL as its data base. ",,,140149,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183899,,FIRZAN IRFANDI F,AUTOMATED DATA ACQUISITION OF NTFS AND FAT FILE SYSTEM FOR DIGITAL FORENSICS,,,"Digital Forensics, NTFS, FAT, Cybercrime","Ahmad Ashari, M.I.Kom, Dr.Techn",2,3,0,2020,1,"Penggunaan komputer memiliki banyak keuntungan di berbagai sektor; seperti pendidikan, perusahaan, hiburan, keteknikan, dan banyak lagi. Akan tetapi, teknologi ibarat pedang bermata dua. Keuntungannya bisa berguna bagi para penjahat yang ingin melakukan tindakan keji mereka. Hal itu dikenal dengan istilah kejahatan siber. Dengan demikian untuk memerangi kejahatan dunia maya, mencari fakta, serta mencari pelaku di belakangnya, forensik digital diperlukan.
Penelitian ini menekankan pada akuisisi data otomatis pada dua jenis sistem berkas, yaitu NTFS dan FAT, yang mencakup beberapa proses yang didasarkan pada prosedur forensik digital. Program akan berjalan di Linux berbasis Debian. Program akuisisi data otomatis ditulis dalam bahasa pemrograman Python dengan mengimpor perpustakaan dan utilitas yang diperlukan. Studi kasus dibuat untuk menyimulasikan program.
Hasil penelitian menunjukkan bahwa program mampu melakukan akuisisi data. Program dapat memperoleh data dari penyimpanan. Program juga dapat melihat berkas, direktori, dan file tersembunyi yang tersedia dalam penyimpanan.
","The usage of computers has a big advantage in many sectors; such as education,
enterprise, entertainment, engineering, and many more. Technology is like a double
edged sword as well. The advantages can be handy for criminals who want to do their
heinous acts of crimes. Hence the term cybercrime. Thus to combat cybercrime, search
and facts and perpetrators behind, digital forensics is needed.
This research emphasizes on an automated data acquisition on two types of file
systems, namely NTFS and FAT, which includes a handful of processes that was based
on digital forensic procedures. The program will run on Debian-based Linux. The
automated data acquisition program is written in Python programming language by
importing the necessary libraries and utilities. A case brief is simulated for this
program.
The results show that the program was able to do data acquisition. The program
was able to acquire data from a storage. The program was also able to view files,
directories and hidden file available within a storage.",,,134796,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
191325,,AULIA RAHMAH,ANALISIS SENTIMEN BERDASARKAN ASPEK PADA ULASAN RESTORAN BERBAHASA INDONESIA MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK (CNN) DAN GATED RECURRENT UNIT (GRU),,,"Sentimen analisis, Convolutional Neural Network, Gated Recurrent Unit","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2020,1,"Analisis sentimen berdasarkan aspek merupakan proses penentuan polaritas sentimen suatu kalimat berdasarkan aspek yang telah ditentukan. Penelitian tentang analisis sentimen berdasarkan aspek pada kalimat berbahasa Indonesia pertama kali dilakukan pada tahun 2017 dengan menggunakan metode unsupervised learning. Lalu pada tahuan 2018 penelitian menggunakan metode deep learning CNN, LSTM, dan GRU sehingga metode yang digunakan pada analisis sentimen berdasarkan aspek untuk kalimat berbahasa Indonesia masih sangat terbatas. Oleh karena itu, dibutuhkan pengembangan metode lain untuk melakukan analisis sentimen berdasarkan aspek pada kalimat berbahasan Indonesia.
Penelitian ini akan menggunakan metode CNN dan GRU untuk melakukan analisis sentimen berdasarkan aspek pada kalimat review berbahasa Indonesia. Review yang digunakan adalah review yang berkaitan dengan ulasan suatu restoran yang didapatkan dari situs Trip Advisor. Aspek yang digunakan pada penelitian ini adalah aspek makanan, harga, tempat, dan pelayanan. Diperoleh hasil dari penelitian ini berupa precision score sebesar 0.67, recall score sebesar 0.73, dan f1 score sebesar 0.68 untuk klasifikasi aspek. Klasifikasi sentimen memperoleh hasil precision score sebesar 0.85, recall score sebesar 0.57, dan f1 score sebesar 0.66","Aspect based sentiment analysis is the process of determining the sentiment polarity of a sentence based on predetermined aspects. Research on aspect based sentiment analysis for Indonesian review was first conducted in 2017 using the unsupervised learning method. In 2018, the research used CNN, LSTM, and GRU deep learning methods. So, the method used in aspect based sentiment analysis for Indonesian reviews is very limited. Therefore, it is necessary to develop another method for analyzing sentences based on aspects of Indonesian reviews.
This study will use the CNN and GRU methods to conduct aspect based sentiment analysis for Indonesian reviews. The review is about review relating to a restaurant review obtained from the Trip Advisor site. The aspects used in this study are food, price, place, and service. The results of this research are performance in the form of precision score of 0.67, recall score of 0.73, and f1 score of 0.68 for aspect classification. Sentiment classification results in precision scores of 0.85, recall scores of 0.57, and f1 scores of 0.66.",,,142486,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190312,,ANTHONY JETHRO L,Semi Supervised Learning by Implementing Seeding Theory on Text Classification,,,"Multinomial Na&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;ve Bayes, Sentiment Analysis, Seeding, Semi-Supervise, Machine learning, Unlabelled data","Afiahayati, S.Kom., M.Kom., Ph.D. ; Yunisa Sari, S.Kom., M.Sc., Ph.D.",2,3,0,2020,1,"Jumlah data bertambah secara substansial dalam beberapa tahun, begitu pula dengan jumlah data teks. Sekarang, hampir semua teks data tersedia dan dapat di akses secara bebas oleh semua orang. Tetapi, untuk beberapa ada beberapa hal, tidak mungkin manusia dapat membaca dan menganalisis semua data teks. Karena keterbatasan manusia, mesin di gunakan untuk membaca dan menganalisis teks data tersebut, dengan cara supervised, unsupervised dan semi-supervised untuk melatih mesin. Metode supervised diketahui memiliki performa yang terbaik di antara yang lain. Tetapi membutuhkan semua data yang berlabel dan di kejadian sehari-hari data ada dalam bemtuk tidak berlabel. Metode semi-supervised dapat berkerja dengan membutuhlan sedikit jumlah data yang berlabel.
Dalam proses clustering, teori seeding dapat menaikkan performa dari metode pembelajaran Unsupervised. Di penelitian ini, kami meaplikasikan teori seeding pada Multinomial Na&Atilde;&macr;ve Bayes untuk mendeteksi sentiment. Dengan menggunakan jumlah data yang lebih sedikit dari supervised. Salah satu model mendapat akurasi 74.52% dengan menggunakan 10% data untuk melatih mesinnya. Ada juga penelitian sampingan yang di lakukan dengan mengganti jumlah data awal yang digunakan untuk melatih mesin
","The amount of data has increased substantially over the years, including text data. Today, most text data are abundantly available and can be accessed freely by everyone. However, for some tasks, it is impossible for humans to manually read and analyze this huge amount of text data. Because of this limitation, machines are trained and can be used to perform text analysis. In using machine learning to analyse text data, there are supervised, unsupervised learning, as well as semi-supervised learning methods. Supervised methods have been known to have the best overall performance. However, it requires wholly labelled data which are commonly scarce in real word scenarios. On the other hand, semi-supervised learning can work using a smaller set of data.
In clustering tasks, Seed theory have been shown to increase performance for unsupervised learning models. In this research, we attempt to apply seeding theory combined with Multinomial Na&Atilde;&macr;ve Bayes model on sentiment classification. By using much smaller data with semi-supervised learning, a model with relative performance to the supervised model can be generated. One of the model achieved 74.52% accuracy using 10% of data for initial seed compared with 84.1% supervised model. Other models were created in additional experiments by implementing different numbers of initial data for the seed. 
",,,141503,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
191084,,ICHSANUL AKBAR,Implementasi Enkripsi Homomorfik RSA Termodifikasi Untuk Sistem Electronic Voting,,,"e-voting, RSA, Modified RSA, Cryptography, Homomorphic, Encryption","Anny Kartika Sari, S.Si., M.Kom., Ph.D",2,3,0,2020,1,"Salah satu hal di Indonesia yang sampai sekarang masih belum sepenuhnya menggunakan teknologi dalam pelaksanaannya adalah proses pemilihan umum. Proses pemilihan dan perhitungan hasil masih dilakukan secara manual, sehingga membutuhkan waktu yang lama. Untuk mengatasi hal tersebut, diperlukan sistem pemilihan yang lebih memanfaatkan penggunaan teknologi dalam proses pemilihannya, yaitu e-voting. Akan tetapi masih ada pihak yang meragukan sistem e-voting dari keamanan data pemilihannya.

Pada penelitian ini akan dilakukan perancangan sebuah prototipe sistem e-voting dengan menggunakan enkripsi homomorfik berbasis RSA termodifikasi. Enkripsi yang bersifat homomorfik dirancang agar data pemilihan dapat diproses dalam keadaan terenkripsi, dan terlindungi dari pembocoran data. Sistem yang dirancang merupakan sistem yang berbasis website yang dibangun menggunakan bahasa pemrograman python dan basis data SQLite. Website dijalankan didalam server yang memiliki sistem operasi Linux distro Ubuntu 18.04 serta memiliki 4 vCPU dan 16GiB RAM. Setelah pembangunan prototipe sistem, dilakukan evaluasi berupa akurasi hasil akhir dan analisis security requirements milik sistem.

Dari hasil pengujian, didapatkan bahwa enkripsi homomorfik diimplementasikan dengan benar, terbukti dari kebenaran hasil voting setelah dilakukan dekripsi. Selain itu, sistem juga memenuhi requirement eligibility, unreusability, verifiability, tally correctness, uncoerability, auditability, fairness, efficiency dan integrity.","One of the things that Indonesia have not make use of the technology is an election event. The manual voting process and manual calculation which will take a long time still needs to be fixed. Therefore, a more technology based voting system is needed, which is an electronic voting system. However, there are multiple parties that still doubting the security of the voting data in e-voting system.

In this study, a prototype of e-voting system thats using a homomorphic property of Modified RSA encryption algorithm is designed. Homomorphic encryption is used so the system can process voting data while still in an encrypted state, and protects the data from security breach. The designed system is a website based application that was built using python programming language and SQLite database system. The website is running on a server with Linux Ubuntu 18.04 as the operating system, with 4 vCPUs and 16 GiB RAM. After the system prototype is built, an evaluation about the system's final result's accuracy and security requirements analysis is done.

From the results of the evaluation, this system has implemented homomorphic encryption correctly, proven by the accurate result after the voting data is decrypted and also fulfills security requirements such as eligibility, unreusability, verifiability, tally correctness, uncoerability, auditability, fairness, efficiency and integrity.

",,,142516,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190575,,DWI MOHAMAD AMIN,Perancangan Agen Cerdas Dalam Permainan Backgammon Menggunakan Reinforcement Learning,,,"Reinforcement Learning, SARSA Learning, Backgammon, Non Player Character, Intelligent Agents.","Isna Alfi Bustoni, S.T., M.Eng",2,3,0,2020,1,"Pengembangan Non Player Character (NPC) dalam industri permainan telah
mencapai babak baru. Keinginan pemain untuk menikmati permainan yang menarik
dan seimbang membuat para developer permainan berputar otak untuk mencari metode &Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12; metode yang bisa mengembangkan NPC menjadi lebih nyata dan beradaptasi
sesuai keadaan permainan. Beberapa penelitian terdahulu telah menghasilkan agen
cerdas untuk mengembangkan NPC menjadi lebih baik namun hasil yang didapatkan
masih kurang optimal. Penelitian ini menggunakan algoritme State-Action-RewardState-Action (SARSA) untuk menghasilkan agen cerdas pada permainan backgammon. Dengan menggunakan -greedy policy, agen SARSA memilih action yang terbaik ketika memindahkan checkers pada permainan backgammon. Hasil dari penelitian ini mencapai 93,6 % tingkat kemenangan dari 1000 kali permainan. Penerapan
agen cerdas pada permainan backgammon menjadikan permainan ini menjadi lebih
menantang dikarenakan NPC didalamnya sudah memiliki strategi permainan yang
sudah terlatih","The development of Non Player Character (NPCs) in the game industry has
reached a new phase. The desire of players to enjoy an interesting and balanced game
makes game developers thinks to look for methods that can develop NPCs to become
more real and adapt to the state of the game. Some previous studies have produced
intelligent agents to develop better NPCs but the results obtained are still not optimal. This study uses the State-Action-Reward-State-Action (SARSA) algorithm to
produce intelligent agents in backgammon games. By using -greedy policy, SARSA
agents choose the best action when moving checkers in backgammon games. The
results of this study reached 93.6 % win rate from 1000 games. The application of intelligent agents in the backgammon game makes this game more challenging because
the NPCs in it already have a trained game strategy",,,141763,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186480,,KHAZITA SEIYA S,Pendeteksi Umpan Klik untuk Artikel Berita dalam Bahasa Indonesia menggunakan Random Forests,,,"Random Forest, Clickbait Detection, News Headlines, N-gram, Uni-gram, Bi-gram, TF-IDF","Mhd. Reza M.I Pulungan, M.Sc., Dr.",2,3,0,2020,1,"Bersamaan dengan meningkatnya penggunakaan dan popularitas sebagai sumber informasi sekarang ini, media online mendorong para jurnalis untuk membuat konten yang menarik pembaca untuk dapat membaca artikel yang mereka buat. Oleh karena itu, mereka perlu untuk membuat judul berita yang menarik perhatian untuk artikel-artikel mereka, yang disebut juga sebagai clickbait atau (umpan klik). Tingginya jumlah penggunaan umpan klik menyebabkan beberapa masalah untuk para pembaca dan para pembuat konten itu sendiri. Salah satu hal yang dapat dilakukan adalah menggunakan teknologi machine learning yang mana dapat digunakan untuk mendeteksi umpan klik.
Penelitian ini bertujuan untuk mengklasifikasikan judul berita dalam Bahasa Indonesia ke dalam kategori umpan klik maupun tidak. Dalam penelitian ini, algoritma Random Forest akan digunakan untuk melakukan pengklasifikasian, kemudian hasil dari performanya akan dievaluasi.
Hasil dari penelitian menunjukkan bahwa Random Forest menunjukkan performa yang paling baik dalam mendeteksi umpan klik menggunakan fitur unigram dan nilai standar pada hyperparameter dibandingkan pada scenario lainnya, seperti saat menggunakan Naive Bayes, Random Forest menggunakan bigram dan nilai standar pada hyperparameter, serta Random Forest menggunakan bigram dan hyperparameter yang telah disesuaikan nilainya sebelumnya. Hasil skor evaluasi performa, yaitu 0,97 untuk nilai presisi, recall, f1-score dan akurasi dalam mendeteksi umpan klik.
","Along with the increasing use and popularity in present as information sources, online media platform triggers the journalist to create contents that are attractive for the readers to read their articles. Therefore, they need to make some catchy headlines for the articles, which are also known as clickbait. The high amount of clickbait causes some problems to the readers and to the content creators themselves. A necessary prerequisite is machine learning technology which is capable to reliably detect clickbait.
This research aims to classify the news headlines in Bahasa Indonesia to either clickbait or non-clickbait category. In this research, the Random Forest algorithm is going to be used to do the classification task, then its performance will be evaluated.
The result shows that the Random Forest was the best to perform the clickbait detection using unigram features and default value compared to the other cases, such as using Naive Bayes, Random Forest using bigram and default value, also Random Forest using bigram and the tuned hyperparameter. The evaluation score was 0.97 for precision, recall, f1-score and accuracy score in detecting the clickbait.
",,,137909,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
192880,,MUHAMMAD RAFI,PENGARUH METODE FEATURE EXTRACTION TERHADAP KINERJA RANDOM FOREST DALAM KLASIFIKASI BERITA HOAX BERBAHASA INDONESIA,,,"Hoax, Klasifikasi Teks, Random Forest, Ekstraksi Fitur, Word Embedding","Sigit Priyanta, S.Si., M.kom., Dr.",2,3,0,2020,1,"Hoax merupakan isu yang sangat problematik di Indonesia. Penyebaran hoax dianggap dapat mengakibatkan beragam konflik, perpecahan, dan pertikaian di beberapa bagian di Indonesia. Dalam mengantisipasi hal tersebut, beberapa penelitian terdahulu telah melakukan percobaan untuk melakukan klasifikasi hoax dalam bahasa Indonesia menggunakan pendekatan supervised learning, seperti SVM, random forest, dan deep learning, namun nilai akurasi yang dihasilkan kebanyakan masih di sekitar angka 70%.
Metode feature extraction dapat memberikan informasi tambahan dan mempermudah pembelajaran pada model supervised learning dengan mengubah data teks menjadi representasi vektor numerik. Penelitian ini menggunakan Word2Vec, TF-IDF, dan bag-of-words sebagai bentuk feature extraction untuk melakukan klasifikasi berita hoax berbahasa Indonesia dengan menggunakan model random forest. Pencarian hyper-parameters terbaik model random forest untuk setiap metode feature extraction dilakukan dengan menggunakan randomized cross-validation search.
Menggunakan dataset yang berisi 159 berita dengan label valid dan 91 berita dengan label hoax, hasil evaluasi dari 20% data testing menunjukan bahwa model random forest memiliki kinerja terbaik saat menggunakan fitur yang dihasilkan oleh Word2Vec skip-gram maupun CBOW dengan akurasi 90%, presisi  91%, recall 87%, dan f1-score 89%.
","Hoax is a really problematic issue in Indonesia. The spreading of hoax may create conflicts and riots in some regions. To prevent this, researches of Indonesian hoax classification using supervised learning are done in the past using supervised learning models such as SVM, random forest, and deep learning, but the resulting accuracy is still around 70%. 
Feature extraction method can give additional information. thus making it easier for classifier to learn the data by transforming textual data to numerical vector representation. This research uses Word2Vec, TF-IDF, dan bag-of-words to extract text features and random forest to classify Indonesian hoax news. Randomized cross-validation search is also used to find the best random forest hyper-parameters for each given feature extraction methods.
Using dataset consisting of 159 valid news and 91 hoax news, evaluation results on 20% testing data shows that random forest has the best performance when using features generated by either Word2Vec skip-gram or CBOW with accuracy of 90%, precision of  91%, recall of 87%, and f1-score of 89%.
",,,144104,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186739,,I GEDE INDRA WIDYANA,PREDIKSI PRODUKTIVITAS JAGUNG MELALUI DATA TEMPERATUR DAN CURAH HUJAN RATA-RATA MENGGUNAKAN JARINGAN SARAF TIRUAN,,,"Adam, SGD, Agriculture, Maize, Forecasting","Anifuddin Azis, S.Si., M.Kom",2,3,0,2020,1,Agriculture is a field with many uncertainties. One of the source of those uncertainties are temperature and precipitation which could vary across season and year. Previous research by Matsumura et al. (2015) have attempted to build a model that could predict yield through precipitation. This research attempts to build a model that could predict maize yield before harvesting month with Adam and SGD as optimizers. The implementation made use of precipitation and temperature data to produce a forecasted yield of the harvest. The best model produced RMSE error of 2.100530353.,Agriculture is a field with many uncertainties. One of the source of those uncertainties are temperature and precipitation which could vary across season and year. Previous research by Matsumura et al. (2015) have attempted to build a model that could predict yield through precipitation. This research attempts to build a model that could predict maize yield before harvesting month with Adam and SGD as optimizers. The implementation made use of precipitation and temperature data to produce a forecasted yield of the harvest. The best model produced RMSE error of 2.100530353.,,,137979,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186997,,Fahrul Rozi Meiduan,Author Obfuscation dengan Menggunakan Word2Vec dan Language Model,,,"natural language processing, word embedding, author obfuscation, language model","Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2020,1,"Author obfuscation merupakan suatu cara untuk melakukan parafrase dokumen dengan mengubah writing style dari dokumen. Author obfuscation merupakan salah satu cara untuk menjaga anonimitas author terhadap analisis stylometry. Stylometry sering digunakan untuk domain authorship verification, dimana pendekatan stylometry dapat menemukan fitur-fitur yang terdapat dalam sebuah dokumen, seperti writing style, maupun gender dari author. Proses keberhasilan author obfuscation bergantung pada pendekatan dari author verification. Semakin baik author verification, maka pendekatan yang sama dapat diaplikasikan untuk membuat author obfuscation yang baik.
Pada penelitian ini, digunakan Word2Vec dan language model sebagai metode dari author obfuscation yang dilakukan. Proses yang dilakukan adalah dengan melakukan modulizing text dengan memecah teks menjadi kalimat. Kalimat digunakan untuk menghitung posisi karakter dari setiap kalimat. Untuk mengubah data dari dokumen, dilakukan tokenisasi dan dilakukan penggantian sinonim yang didapatkan dari Word2Vec. Hasil dokumen yand didapatkan akan dilakukan pengujian dengan menggunakan tiga kondisi.
Hasil dari author obfuscation harus memenuhi tiga kondisi, yaitu safety dimana author verification software tidak dapat mengetahui author aslinya, soundness dimana dokumen yang di-obfuscate berhubungan secara tekstual dengan yang aslinya, dan sensibleness dimana dokumen tidak mencolok dari kata dan style yang digunakan. Hasil dari pengujian safety menggunakan GLAD dan mendapatkan penurunan akurasi sebesar 3.6%. Hasil pengujian dari sensibleness menggunakan pengecekan language model score dimana perplexity yang didapatkan masih mendekati nilai dokumen original. Hasil pengujian sensibleness menggunakan peer review dimana menunjukkan bahwa dokumen masih dapat diterima dan tidak terlihat seperti di-generate oleh mesin.","Author obfuscation is an act of paraphrasing document by changing the writing style of the document. Author obfuscation is a way to maintain author anonymity from stylometry analysis. Stylometry is a form of authorship identification that relies on linguistics information in a document. Stylometry has been a well-known approach in authorship verification, where documents attribute detected, such as writing style and gender of the author. The successful from author obfuscation and author verification depends on each approach used. The better the author verification, then the same approach can be applied to create a good author obfuscation. 
In this research, we used Word2Vec and language model as a method to make author obfuscation. The process is done by modifying text by breaking text into sentences. Sentences are used to calculate the character's position of each sentence. In order to change the data from dokumen, we used tokenization and synonym replacement that is obtained from Word2Vec. The results of this generated document will be evaluated using three conditions. 
Result of author obfuscation must satisfied three conditions, that is safety which author verification can not tell who is the original author, soundness which paraphrased document can be accepted in human knowledge and the content is preserved, and sensibleness where document is inconspicious. Safety evaluation is used by using GLAD and the accuracy dropped by this approach is 3.6%. Sensibleness evaluation is used by generating language model score and perplexity from each document, where this approach is still giving out results that obfuscated document is still near the original. Sensibleness evaluation is used by peer review and document generated is looks like not generated by machine.
",,,138085,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186749,,GALUH KIRANA M,Algoritma Genetika untuk Penyelesaian Permasalahan Alokasi Dosen Pembimbing Tesis,,,"Algoritma genetika, metaheuristik, permasalahan alokasi sumber daya","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2020,1,"Berdasarkan Dokumen Kurikulum 2017 Program Magister (S2), mahasiswa Departemen Ilmu Komputer dan Elektronika (DIKE) jenjang S2 Universitas Gadjah Mada untuk dapat lulus dan memperoleh gelar M.Cs. harus mampu menyelesaikan tesis sebagai salah satu mata kuliah wajib. 

Pada pendaftaran tesis periode 2018/2019 semester genap dan periode 2018/2019 semester genap, terdapat sekitar 54% dosen pembimbing tesis yang diusulkan mahasiswa melebihi kuota dosen yang tersedia. Dalam hasil keputusan rapat penugasan dosen pembimbing kedua periode tersebut, hanya 14% mahasiswa mendapatkan dosen pembimbing dengan bidang minat yang cocok dengan topik tesis yang diambil. Penelitian ini bertujuan untuk menyelesaikan permasalahan alokasi dosen pembimbing tesis yang dapat digunakan sebagai usulan dalam pengambilan keputusan rapat penugasan dosen pembimbing tesis menggunakan algoritma genetika.

Penelitian ini sudah mampu menghasilkan solusi yang valid dengan tingkat kecocokan bidang riset dosen dan mahasiswa mencapai 31% kecocokan untuk usulan penugasan dosen pembimbing pada data periode 2018/2019 semester genap dan mencapai 33% untuk periode 2019/2020 semester genap.","Based on the 2017's Curriculum Documents of the Masters Program (S2), students of the Department of Computer Science and Electronics of Universitas Gadjah Mada, to be able to graduate and obtain M.Cs. degree must be able to complete a thesis as one of the compulsory subjects. 

In the thesis registration 2018/2019 period in even semester and 2018/2019 period even semester, there are around 54% of the thesis supervisors proposed by students exceeding the available lecturer quota. In the results of the decision of the both period's lecturer supervisors assignment meeting, only 14% of students received a supervisor with a field of interest that matched the topic of the thesis taken. This study aims to resolve the issue of the allocation of the thesis supervisor which can be used as a suggested solution in the decisions making of the thesis supervisor assignment meeting by using genetic algorithm. 
This research has been able to produce a valid solution with a percentage of compatibility in the field of lecturer's research and students reach 31% for the proposed assignment of supervisors in the 2018/2019 period even semester and reach 33% for the 2019/2020 period even semester.",,,138002,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190592,,RANI RIZKIANI ILYAS,EVALUATION OF THE PERFORMANCE OF SVM AND NAIVE BAYES METHODS IMPLEMENTED WITH EMOTION CLASSIFICATION FOR SENTIMENT ANALYSIS ON TWITTER DATA,,,"Support Vector Machine, Multinomial Naive Bayes, Emotion classification, Text-Classification,  TF-IDF","Khabib Mustofa, S.Si., M.Kom., Dr. techn.",2,3,0,2020,1,"Sentiment Analysis is one of the most used methods in order to gather emotions through microblogging websites, particularly Twitter. However, the usage of Emotion Classification for Sentiment Analysis, particularly in the Indonesian language, is still rare, particularly with the usage of SVM and Multinomial Naive Bayes.","On popular microblogging websites such as Twitter where users are free to express their emotions, the concept of emotion as well as its classification is very apparent on texts. While the concept and study of emotion classification is quite crucial for the analysis of various topics in order to improve sales and general public opinion, research regarding the use of emotion classification for e-commerce websites is exceedingly rare. Moreover, research surrounding the development and analysis of emotion classification for tweets in other languages, particularly Indonesian, is still quite lacking.
This research attempts to study the use of emotion classification that is available on Indonesian tweets, as previous researches has only used Support Vector Machine. this research attempts to use other methods of classifying the text such as Multinomial Naive Bayes as well as Support Vector Machine to see which classification method is best used for emotion classification. This research also aims to increase the precision that was established by creating a new emotion classification dataset that is only specified around e-commerce related tweets. The dataset consists of a dataset with e-commerce related tweets from three Indonesian e-commerce applications such as Shopee, Tokopedia and Bukalapak. The extraction feature method that is used is TF-IDF and the performance measure that will be seen is the precision score.  With the highest accuracy score of 70% for Support Vector Machine, and 65% for the highest score for the Multinomial Naive Bayes. 
",,,141766,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
187009,,ANDIKA WILLIAM,Clickbait Detection for Bahasa Indonesia Using Bi-LSTM,,,"Bi-LSTM, Bahasa Indonesia, News, Clickbait, Text-Classification, LIME Explainer","Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2020,1,"Clickbait usage has been increasing rapidly over the years along with the
rise of online journalism. Since online publishers rely upon clicks in order to
generate revenue, there is a growing trend of writing headlines with the sole aim of
attracting clicks instead of delivering information. The problem of clickbait has
raised concerns in the international community in recent years, however, the
majority of studies are still focused on English and are still lacking in other
languages.
This research attempts to study the case of clickbait for Indonesian news, as
well as develop an automatic detection model using machine learning methods.
Following the lack of published datasets on Indonesian News Headlines, this
research attempts to contribute by constructing an Indonesian news dataset that was
extracted from 12 Indonesian publishers. The &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;CLICK-ID&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12; dataset is consisted of
46,517 articles data, along with a clickbait corpus of 15,000 annotated headlines.
Our labels shows that out of the 12 publishers, all publishers were detected of using
clickbait, indicating the use of clickbait is already common practice. By using this
corpus, a Bi-LSTM model was developed with its best model achieving 0.88
accuracy on testing. Finally, this research made use of the LIME explainer library
(Ribeiro et al., 2016) in order to analyze and gain insight on why our machine
learning model is able to classify clickbait and non-clickbait","Clickbait usage has been increasing rapidly over the years along with the
rise of online journalism. Since online publishers rely upon clicks in order to
generate revenue, there is a growing trend of writing headlines with the sole aim of
attracting clicks instead of delivering information. The problem of clickbait has
raised concerns in the international community in recent years, however, the
majority of studies are still focused on English and are still lacking in other
languages.
This research attempts to study the case of clickbait for Indonesian news, as
well as develop an automatic detection model using machine learning methods.
Following the lack of published datasets on Indonesian News Headlines, this
research attempts to contribute by constructing an Indonesian news dataset that was
extracted from 12 Indonesian publishers. The &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;CLICK-ID&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12; dataset is consisted of
46,517 articles data, along with a clickbait corpus of 15,000 annotated headlines.
Our labels shows that out of the 12 publishers, all publishers were detected of using
clickbait, indicating the use of clickbait is already common practice. By using this
corpus, a Bi-LSTM model was developed with its best model achieving 0.88
accuracy on testing. Finally, this research made use of the LIME explainer library
(Ribeiro et al., 2016) in order to analyze and gain insight on why our machine
learning model is able to classify clickbait and non-clickbait",,,138134,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
189320,,NUGRAHA ADI SANTOSO,Decision Support System For Determining Kartu Keluarga Sejahtera (KKS) Recipients Using Topsis Method,,,"DSS, Kartu Keluarga Sejatera (KKS), Technique for Order of Preference by Similarity to Ideal (TOPSIS)","Anifudin Azis, S.Si. M Kom. ",2,3,0,2020,1,"Kartu Keluarga Sejahtera (KKS) merupakan salah satu program pemerintah di bidang sosial ekonomi untuk mempercepat penanggulangan kemiskinan nasional. Salah satu pelaksana kebijakan program tersebut adalah pemerintahan kelurahan dan kecamatan, peran pemerintah kelurahan dan kecamatan dalam hal ini yaitu melakukan penyeleksian data penerima bantuan melalui musyawarah. Permasalahan yang dihadapi dalam penentuan penerima bantuan adalah proses penyeleksian yang kurang efektif dan efisien. Berdasarkan permasalahan tersebut maka diperlukan adanya alat bantu berupa sistem dengan metode yang tepat dalam menentukan penerima bantuan dan menghasilkan ranking dari hasil perhitungan bobot tiap kriteria.

Metode perhitungan yang digunakan ialah metode Technique for Order of Preference by Similarity to Ideal (TOPSIS). Metode pengumpulan data yang didapatkan adalah data oleh Badan Pusat Statistik (BPS) social economic survey. Sedangkan metode dan alat pengembangan sistem menggunakan metode terstruktur seperti Use Case, Activity Diagram, Entity Relationship Diargram (ERD) dengan bahasa pemrograman PHP didukung dengan HTML, CSS, Bootstrap, datatables, dan MySQL sebagai basis datanya.

Hasil yang diperoleh dari sistem pendukung keputusan dalam menentukan penerima bantuan KKS dari keluarga rumah tangga urutan 1 hingga urutan ke 20 mendapatkan hasil, bahwa keluarga rumah tangga urutan 2, 6, 8,  11, 17, 19 dan 20 dapat menerima bantuan program KKS dikarenakan nilai preferensi setiap alternatif melebihi atau sama dengan nilai 0.5. Data rangking tersebut dapat dijadikan sebagai alternatif pilihan untuk mengambil keputusan dalam musyawarah.
","Kartu Keluarga Sejahtera (KKS) is one of the government programs in the socio-economic field to accelerate national poverty reduction. One of the implementers of the program policies is the administration of urban village and sub-district, the role of the urban village and sub-district governments, in this case, it is to select recipient data through consultation. The problem faced in determining the recipient of assistance is the selection process that is less effective and efficient. Based on these problems, it is necessary to have a tool in the form of a system with the right method in determining the recipient of aid and produce a ranking of the results of the calculation of the weight of each criterion.

The calculation method used is the Technique for Order of Preference by Similarity to Ideal (TOPSIS) method. Data collection methods obtained are data by the Badan Pusat Statistik (BPS) social-economic survey. While system development methods and tools use structured methods such as Use Cases, Activity Diagrams, Entity Relationship Diagrams (ERD) with PHP programming languages supported by HTML, CSS, Bootstrap, datatables, and MySQL as the database.

The results obtained from the decision support system in determining the recipients of KKS assistance from households in order 1 to 20 get results, that households in order 2, 6, 8, 11, 17, 19 and 20 can receive KKS program assistance because of the value each alternative's preference exceeds or equals 0.5. The ranking data can be used as an alternative choice for making decisions in deliberations.
",,,140316,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
187787,,DANDI WIRATSANGKA S,Deteksi Pelat Nomor Kendaraan Berbasis Pengolahan Citra Digital dan Support Vector Machine,,,"Deteksi pelat nomor, support vector machine, pengolahan citra digital, sobel edge detection","Moh. Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2020,1,"Pelat nomor kendaraan merupakan identitas suatu kendaraan yang menyimpan informasi penting dari kendaraan tersebut. Dalam peranannya pelat nomor kendaraan sering digunakan dalam identifikasi sebuah kendaraan baik di dalam sistem lalu lintas, sistem masuk tol, sistem parkir, dan lain sebagainya. Oleh karena itu pendeteksian pelat nomor kendaraan pada citra merupakan hal yang sangat penting untuk dilakukan. Pada penelitian ini dilakukan pendeteksian pelat nomor kendaraan pada sebuah citra berbasis pengolahan citra digital dan support vector machine. Tulisan pada pelat nomor mengandung banyak tepi vertikal. Tepi tersebut akan digunakan sebagai ciri yang membedakan pelat nomor dari objek lainnya. Operasi morfologi opening dan dilation akan dilakukan untuk mendapatkan kandidat pelat nomor. Sedangkan untuk mengurangi salah deteksi (false positive) akan dilakukan connected component analysis dan verifikasi kandidat menggunakan support vector machine (SVM).  SVM akan mengklasifikasikan kandidat pelat nomor ke dalam kelas positif (pelat nomor) atau negatif (non pelat nomor). Model SVM terbaik dihasilkan dengan akurasi pengujian sebesar 99,61%. Sedangkan sistem deteksi paling baik menghasilkan nilai precision, recall, f1-score, dan waktu deteksi rata-rata masing-masing adalah 91,38%, 88,04%, 89,68%, dan 49 milidetik. Penggunaan SVM dalam sistem deteksi terbukti meningkatkan performa sistem deteksi.","The vehicle license plate is the identity of a vehicle that stores important information of the vehicle itself. In its role the vehicle license plate is often used in the identification of a vehicle such as in traffic systems, toll entry systems, parking systems, and so forth. Therefore, detection of vehicle license plates in the image is very important to do. In this study the detection of vehicle license plates on an image is performed based on digital image processing and support vector machines. The text on the license plate contains many vertical edges which will be used as a feature that distinguishes a license plate from other objects. Morphological opening and dilation will be carried out to obtain license plate candidates. Meanwhile, to reduce false detection (false positive), connected component analysis and candidate verification using support vector machine (SVM) will be performed. SVM will classify candidates into positive (license plate) or negative (non license plate) class. The best SVM model was produced with a testing accuracy of 99.61%. While the best detection system was produced with precision, recall, f1-score, and the average detection time is 91.38%, 88.04%, 89.68%, and 49 milliseconds respectively.  The use of SVM in detection systems has been proven to improve detection system performance.",,,138806,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
194192,,M SATRIA YUDA UTAMA,ASPECT BASED SENTIMENT ANALYSIS IN TOKOPEDIA REVIEW USING LONG SHORT TERM MEMORY (LSTM),,,"Aspect Based Sentiment Analysis, Sequence-to-Sequence, LSTM, Aspect Detection, Sentiment Analysis","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2020,1,"Nowadays, people are used to express their experience of buying things. People express their experience in the review section in marketplace where they bought their things. The large number of review in review section may helps prospective buyer to choose which seller they will choose to buy. However, the prospective buyer may confused the relevant information, since there are a large number of reviews. In this research, the author propose aspect based sentiment analysis that automatically give what aspect that people tell in a review sentence followed with the sentiment of aspect in there. In this research, the author explore the usage of Sequence-to-Sequence (Long Short Term Memory) LSTM to detect the aspect in review sentence, and explore the usage of LSTM to classify the sentiment of the sentence review with each aspect. The proposed model of aspect detection give accuracy of all model is 74.7%, recall value is 78.3%, precision value is 80.5% and the F1-Score 79.4%. and for the aspect sentiment classification model give 91.07%.","Nowadays, people are used to express their experience of buying things. People express their experience in the review section in marketplace where they bought their things. The large number of review in review section may helps prospective buyer to choose which seller they will choose to buy. However, the prospective buyer may confused the relevant information, since there are a large number of reviews. In this research, the author propose aspect based sentiment analysis that automatically give what aspect that people tell in a review sentence followed with the sentiment of aspect in there. In this research, the author explore the usage of Sequence-to-Sequence (Long Short Term Memory) LSTM to detect the aspect in review sentence, and explore the usage of LSTM to classify the sentiment of the sentence review with each aspect. The proposed model of aspect detection give accuracy of all model is 74.7%, recall value is 78.3%, precision value is 80.5% and the F1-Score 79.4%. and for the aspect sentiment classification model give 91.07%.",,,145540,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190354,,M NIZAR RAHMAN,IMPLEMENTASI ALGORITME DISCRETE WAVELET TRANSFORM DENGAN STATISTICAL THRESHOLD DAN DOUBLE CHAOTIC LOGISTIC MAP SEBAGAI TEKNIK KOMPRESI-ENKRIPSI PADA CITRA DIGITAL,,,"Citra digital, kompresi, DWT, Huffman, RLE, enkripsi, Chaos System","Anny Kartika Sari, S.Si., M.Kom., Ph.D",2,3,0,2020,1,"Pertukaran data pada era digital semakin sering terjadi, tidak terkecuali data citra digital. Kebutuhan untuk melakukan pengiriman data dalam ukuran yang lebih kecil menjadi tantangan. Selain itu, keamanan juga menjadi hal yang harus diperhatikan ketika melakukan pengiriman data melalui internet. Kombinasi antara kompresi dan enkripsi pada citra digital dapat diimplementasikan sebagai solusi atas permasalah tersebut. Penelitian ini bertujuan membangun model kompresi-dilanjutkan-enkripsi untuk citra digital dengan implementasi Discrete Wavelet Transform dan Chaos System. Citra grayscale ditransformasikan ke bentuk matriks DWT oleh Haar DWT. Tahap berikutnya adalah kuantisasi matriks yang dilanjutkan dengan kompresi huffman dan Run Length Encoding untuk menghasilkan matriks 1 dimensi. Barisan keluaran Logistic Chaotic Map dibangkitkan dan digunakan pada sisi enkripsi terhadap matriks 1 dimensi tersebut. Penelitian ini menghasilkan output dalam enam level dekomposisi DWT berbeda untuk dianalisis. Berdasarkan evaluasi, dekomposisi level 3 menghasilkan nilai yang optimal serta hasil yang baik secara visual dengan rata-rata rasio kompresi 74.03, rata-rata PSNR 30.91 dan rata-rata entropi 7.995.","Data exchange in the digital age is increasing, so do digital image. The need to transmit data in smaller sizes is a challenge. In addition, security is also a matter that must be considered when sending data via the internet. The combination of compression and encryption in digital images can be implemented as a solution to these problems. This study aims to build a compression-then-encryption model for digital images with the implementation of Discrete Wavelet Transform and Chaos System. The grayscale image is transformed into a DWT matrix by Haar DWT. The next step is matrix quantization followed by huffman compression and RLE to produce a 1- dimensional matrix. The Logistic Chaotic Map outputs are generated and used on the encryption side of the 1 dimensional matrix. This study produced outputs in six different DWT decomposition levels for analysis. Based on the evaluation, level 3 decomposition produces optimal values and visually good results with an average compression ratio of 74.03, an average PSNR of 30.91 and an average of entropy of 7,995.",,,141576,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
192403,,ZAHID NUR AL AZAMI,Sistem Pendeteksi Kebakaran Berbasis Internet of Things,,,"Sistem pendeteksi kebakaran, logika fuzzy mamdani, mean of maximum, Internet of Things, Raspberry Pi, bot telegram messenger","Azhari SN, Drs., MT., Dr.",2,3,0,2020,1,"Kebakaran merupakan salah satu bencana yang sering terjadi di Indonesia dan membutuhkan penanganan secara cepat dan tepat. Permasalahan ini disadari oleh banyak peneliti, sehingga banyak dirancang sistem untuk melakukan deteksi bencana kebakaran. Hasil terbaik dari penelitian sebelumnya dalam mendeteksi kondisi ruangan adalah ketika menggunakan metode defuzzifikasi centroid. Akan tetapi, metode centroid sebagai metode defuzzifikasi masih memiliki kompleksitas yang lebih besar dibandingkan metode Mean of Maximum, sehingga meskipun akurasinya tinggi, proses deteksinya lebih lambat. Selain itu, masih terdapat juga kendala-kendala untuk tindak lanjut pasca deteksi.

Penelitian ini akan menggunakan metode logika fuzzy mamdani dengan metode defuzzifikasi mean of maximum, dimana metode tersebut akan mengambil nilai rata-rata domain yang memiliki nilai keanggotaan maksimum. Selain itu, penelitian juga akan memanfaatkan teknologi Internet of Things, sehingga memungkinkan platform IoT untuk berkomunikasi dengan perangkat lain dan bot telegram messenger yang dibangun sebagai sistem notifikasi dan sistem kendali dari platform IoT.

Berdasarkan beberapa pengujian yang dilakukan, terbukti bahwa sistem pendeteksi kebakaran berhasil diimplementasikan dan dapat bekerja dengan baik karena kesesuaian antara hasil deteksi sistem dengan hasil perhitungan manual menggunakan formula fuzzy. Dalam proses deteksi, waktu kondisi deteksi tidak berbahaya paling
cepat dengan waktu eksekusi 0,0444 detik atau 44,4 milidetik, sedangkan waktu kondisi deteksi agak berbahaya dan berbahaya sedikit lebih lambat dengan waktu eksekusi yang sama yaitu 0,064 detik atau 64 milidetik. Rata-rata waktu eksekusi sistem dari ketiga kondisi deteksi tersebut adalah 0,057 detik atau 57 milidetik. Selain itu, bot pada telegram messenger dapat bekerja dengan baik sebagai sistem notifikasi pasca deteksi dan sebagai sistem kendali dari platform IoT.","Fire disasters is one of the most frequent disasters in Indonesia and requires fast and precise handling. Many researchers are aware of this problem, so that many systems are designed to detect fire disasters. The best results from previous research in detecting room conditions so far are when using the centroid defuzzification method. However, the centroid method as a defuzzification method still has greater complexity than the Mean of Maximum method, so that even though the accuracy is high, the detection process is slower. In addition, there are still problems about post-detection.

This research will use the mamdani fuzzy logic method with the mean of maximum defuzzification method, which the method will count the average value of the domain that has the maximum membership value. In addition, the research will also use Internet of Things technology, so that the IoT platform possible to communicate with other devices and telegram messenger bots which are built as notification systems and control systems of the IoT platform.

Based on the testing, it is proven that the fire detection system is successfully implemented and can work well because of the compatibility between the system detection results and the results of manual calculations using fuzzy formulas. In the detection process, the time for harmless detection conditions is the fastest with an execution time of 0.0444 seconds or 44.4 milliseconds, while the time for quite dangerous and dangerous detection conditions is slightly slower with the same execution time of 0.064 seconds or 64 milliseconds. The average system execution time from the three detection conditions is 0.057 seconds or 57 milliseconds. In addition, the bot on telegram messenger can work really well as a post-detection notification system
and as a control system for the IoT platform.",,,143626,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190616,,MOKH RIF'AN A,Klasifikasi Hukum Bacaan Tajwid Quran Menggunakan Segmentasi Berbasis Warna dan Convolutional Neural Network,,,"Segmentation, Classification, Detection, HSV, Mask R-CNN, Tajweed","Medi, Drs., M.Kom",2,3,0,2020,1,"Ilmu tajwid merupakan suatu ilmu pengetahuan mengenai tata cara membaca Al-Quran dengan baik dan benar sesuai dengan anjuran Rasullullah SAW. Ilmu tajwid menjadi sangat penting karena sebagai umat muslim diwajibkan untuk membaca Al-Quran dengan benar. Hukum bacaan tajwid dibentuk oleh beberapa huruf hijaiyah dan harakatnya. Perkembangan teknologi mengenai ilmu tajwid sendiri masih terus berkembang. Salah satunya adalah pengenalan ilmu tajwid pada data citra.

Klasifikasi hukum bacaan tajwid dengan menyertakan huruf huruf pembentuknya masih belum banyak dilakukan. Oleh karena itu, pada penelitian ini dilakukan klasifikasi hukum bacaan tajwid berdasarkan huruf-huruf pembentuknya yang dideteksi menggunakan pustaka Mask R-CNN pada citra yang sebelumnya dilakukan proses segmentasi menggunakan aturan warna HSV.","Tajweed is a set of rules of how to read the Quran properly and in accordance with the recommendations of Rasullullah SAW. Tajweed becomes very important because as Muslims are required to read the Quran correctly. Tajweed is formed from several hijaiyah letters. Researches and developments in technology about tajweed are still developed. One of them is about tajweed detection and classification in image data.

This thesis proposes a tajweed detection and classification and also provide the letters forming it using CNN library called Mask R-CNN and color based segmentation using HSV color space. The main purpose of this thesis is to provide an alternative way to detect and classify tajweed in image data.",,,141774,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190621,,JOKO TRIYANTO,Deteksi Comet Otomatis pada Citra Sel Buccal Mucosa,,,"comet assay, buccal mucosa, deteksi comet, 2d otsu method, pengolahan citra digital","Afiahayati, S.Kom., M.Kom., Ph.D",2,3,0,2020,1,"Comet assay merupakan metode untuk menentukan tingkat kerusakan pada
DNA yang menghasilkan citra comet. Sebelum dilakukan klasifikasi perlu
dilakukan deteksi comet terlebih dahulu. Pada penelitian ini dilakukan pendeteksian
comet pada sel buccal mucosa berbasis pengolahan citra digital dengan algoritma
deteksi berdasarkan pada jumlah objek dalam satu comet, instensitas comet, dan
luas comet. Pada proses preprocesing dilakukan segmentasi menggunakan
thresholding 2d Otsu method. Hasil pengujian sistem deteksi paling baik
menghasilkan nilai PPV, sensitivity, akurasi, secara berurutan adalah 19.71%,
51.85%, 84.16%.","Comet assay is a method for determining the level of DNA damage and the
results is a comet image. Before doing classification, comet detection needs to be
done first. In this research, detection of comets on buccal mucosa cells based on
digital image processing with detection algorithms is based on the number of
objects in one comet, comet instensity, and comet area. In the preprocessing process
segmentation is done using the 2d Otsu thresholding method. The best detection
system test results produce PPV values, sensitivity, accuracy, respectively 19.71%,
51.85%, 84.16%.",,,141781,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
193438,,M AFIF DALIANDA,Deteksi Ruang Parkir Otomatis Berbasis Pengolahan Citra Digital,,,"Deteksi Ruang Parkir, Pengolahan Citra Digital, Background Subtraction, Thresholding","Wahyono Ph,D.",2,3,0,2020,1,"Penelitian mengenai deteksi ruang parkir otomatis di berbagai negara masih
terus berjalan dan berkembang hingga saat ini demi terwujud nya konsep smart city.
Di Indonesia, penelitian mengenai deteksi ruang parkir masih belum banyak dilakukan dan belum banyak diimplementasikan dalam kehidupan sehari-hari. Sehingga diperlukan penelitian mengenai deteksi ruang parkir otomatis. Deteksi yang digunakan
di dalam penelitian ini adalah deteksi ruang parkir otomatis berbasis pengolahan citra
digital. Metode yang digunakan dalam penelitian ini adalah background subtraction,
sedangkan teknik yang penulis gunakan dalam penelitian ini meliputi thresholding,
resizing, dan smoothing.
Hasil penelitian ini menunjukkan bahwa metode background subtraction yang
digunakan dalam deteksi ruang parkir otomatis memperoleh hasil yang baik pada siang hari yakni dengan tingkat akurasi mencapai 100%, sedangkan untuk malam hari
memperoleh hasil akurasi yang tidak terlalu baik yakni dengan tingkat akurasi sebesar 91%. Sehingga dapat disimpulkan bahwa program deteksi ruang parkir otomatis
dengan metode background subtraction ini sangat bergantung pada kondisi cahaya
suatu citra.","Research on the detection of automatic parking spaces in various countries is
still ongoing and developing today in order to realize the concept of a smart city. In
Indonesia, this research has not been widely carried out and implemented in daily life,
therefore this research is requiered. Detector that is used in this research is automatic
parking space detection based on digital image processing. The method that is used in
this research is background subtraction, while the techniques writer use in this study
include thresholding, resizing, and smoothing.
The results of this research indicates that the background subtraction method
that is used in automatic parking space detection gets good result during the day with
100% accuracy rate, while at night the accuracy is not good enough, namely with 91%
accuracy rate. Thus we conclude that the automatic parking space detection program
with the background subtraction method is highly dependent on the light conditions
of an image.",,,144745,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184736,,MICHAEL EDRICK,CASE BASED REASONING FOR ORAL AND DENTAL DISEASE DIAGNOSIS,,,"expert system, case-based reasoning, k-nearest neighbor, weighted arithmetic mean.","Dr.Techn. Khabib Mustofa, S.Si., M.Kom.",2,3,0,2020,1,"Permintaan masyarakat akan layanan di bidang kesehatan gigi dan mulut meningkat setiap harinya. Meskipun dokter gigi merupakan pakar di dalam bidangnya, sebagai manusia, dokter gigi pada suatu waktu dapat membuat kesalahan pada hasil diagnosa yang dapat berlanjut ke pengambilan solusi yang salah. Ada sejumlah alasan terjadinya kesalahan diagnosis. Ini bisa karena kurangnya pengetahuan yang baik atau kurangnya pengalaman klinis dari dokter, hambatan bahasa antara pasien dan dokter, situasi di mana adanya kondisi yang jarang terjadi atau presentasi sangat tidak biasa, atau peralatan medis yang tidak berfungsi dengan baik. Untuk membantu mengurangi terjadinya masalah ini, diperlukan suatu sistem yang secara otomatis dapat mendiagnosis penyakit gigi dan mulut.
Penelitian ini mencoba menghasilkan model sistem pakar untuk melakukan penalaran berbasis kasus yang dapat digunakan untuk mendiagnosis penyakit mulut dan gigi. K-nearest neighbor merupakan metode yang akan digunakan dalam penelitian ini untuk menentukan peringkat skor kesamaan untuk model sistem pakar penalaran berbasis kasus. Parameter batasan nilai kesamaan yang digunakan dalam penelitian ini adalah 0.6, 0.7, 0.8, dan 0.9, dan data yang digunakan adalah catatan klinis dari &quot;The Dentist Dental Clinic&quot; dengan jumlah total 131 data. Data dibagi menjadi 3 jenis data, data representasi kasus, data validasi dan data testing. Data representasi kasus digunakan sebagai pengetahuan berbasis kasus yang digunakan oleh sistem. Data validasi dan testing digunakan sebagai masukan kepada sistem untuk menemukan nilai batasan yang paling cocok untuk sistem dengan membandingkan masing-masing akurasinya. 
Hasil penelitian menunjukkan bahwa dari 4 nilai batasan yang digunakan dalam penelitian ini, nilai batasan yang memberikan akurasi tertinggi pada validasi adalah 0.6 dan 0.7 dengan akurasi 96.25%. Akurasi yang diberikan menggunakan nilai batasan 0.8 adalah 92.5% dan akurasi yang diberikan menggunakan nilai batasan 0.9 adalah 83.75%.
","Public demand for services in the field of oral and dental health is increasing every day. Even though dentists are expert in their field, as human being, dentists can at one time make a mistake on the results of the diagnosis that can continue to the wrong solution taken. There are number of reasons for misdiagnosis to occur. It can be because of the lack of sound knowledge or lack of clinical experience on the part of the doctor, language barrier between patient and the doctor, a situation where condition is rare or presentation is extremely unusual, or malfunctioning medical equipment. To help reduce the occurrence of these problems, a system that can automatically diagnose oral and dental diseases is needed.
This research tries to generate case-based reasoning expert system model that can be used to diagnose oral and dental disease. K-nearest neighbor will be the one that are used in this research as a method to rank the similarity score for case-based reasoning expert system model. Similarity score threshold parameters used in this research are 0.6, 0.7, 0.8, and 0.9, and data used are clinical notes from &quot;The Dentist Dental Clinic&quot; with total amount of 131 data. The data is divided into 3 types of data, case representation data, validation data and testing data. The case representation data used as case-based knowledge used by the system. The validation and testing data used as input for the system to find most suitable threshold for the system by comparing their accuracy.
The result shows that from 4 threshold value used in this research, threshold value that gives the highest accuracy in validation is 0.6 and 0.7 with an accuracy of 96.25%. The accuracy given using a threshold value of 0.8 is 92.5% and the accuracy given using a threshold value of 0.9 is 83.75%.
",,,136051,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190368,,GHINAYA FAIRUZ ZAHRA,DETECTING DEPRESSIVE TWEETS USING GATED RECURRENT UNIT NETWORK (GRU) ,,,"Twitter, prediksi, Deep Learning, Word Embeddings, Recurrent Neural Network, Gated Recurrent Unit Network, AUC ROC","Drs. Edi Winarko, M.Sc., Ph.D.",2,3,0,2020,1,"Twitter adalah layanan microblogging dan jejaring sosial tempat pengguna dapat memposting dan berinteraksi dengan pesan yang dikenal sebagai &quot;tweet&quot;, memberikan kebebasan kepada pengguna untuk mengekspresikan diri. Banyak kasus penyakit mental tidak terdiagnosis atau tidak dikenali karena stigmatisasi oleh media yang membuat keinginan untuk membantu mereka menjadi terbatas. Oleh karena itu, penelitian ini bertujuan untuk membuat model pendeteksian pada Tweet menggunakan Deep Learning Model RNN dan GRU.
Penelitian ini memanfaatkan Word Embeddings yaitu Word2Vec dan GloVe untuk menghasilkan representasi kata yang lebih baik dibandingkan penelitian sebelumnya yang tidak menggunakan Word Embedding. Pengujian dilakukan pada dataset yang berisi total 10.314 tweet dengan label depresif (1) dan positif (0). Penelitian ini menggunakan train &amp; validation split 60/20 dan kurva AUC ROC sebagai variabel evaluasi tambahan.
Hasil setelah dilakukan pengujian pada 20% data menggunakan RNN &amp; GRU adalah seri. RNN memperoleh akurasi 99,6% dan GRU pada 99,5% dengan perolehan yang sama, presisi, dan skor f-1 pada 99,5%. Namun, GRU berhasil memperoleh skor kinerja keseluruhan yang lebih besar dalam pelatihan &amp; uji validasi dan skor kurva AUC ROC di mana ia memperoleh hasil yang lebih stabil, lebih tinggi, dan lebih sedikit berfluktuasi.
","Twitter is a microblogging and social networking service on which users can post and interact with messages known as &quot;tweets&quot;, giving users freedom to express themselves. Many cases of mental illness go undiagnosed or unrecognized due to stigmatization by the media making the desire to help them limited. Therefore, this research aims to create a detection model on Tweets using Deep Learning Model RNN and GRU.
This research makes use of Word Embeddings, namely Word2Vec and GloVe in order to create improved representations of words than previous researches that did not use Word Embedding. The test is done on a dataset containing a total of 10,314 tweets with depressive (1) and positive (0) as the label. This research uses train &amp; validation split of 60/20 and the AUC ROC curve as an additional evaluation variable.
The results after testing on 20% data using RNN &amp; GRU was a tie. RNN gained an accuracy of 99.6% and GRU at 99.5% with the same recall, precision and f-1 score at 99.5%. However, GRU managed to obtain a greater overall performance score in the training &amp; validation test and the AUC ROC curve score where it obtained steadier, higher and less fluctuating results.
",,,141639,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
192420,,HARDIANA,Normalisasi Teks Media Sosial Twitter dalam Bahasa Indonesia Menggunakan Word Embedding,,,"Twitter, normalisasi teks, text normalization, word embedding,Word2Vec, FastText","Sri Mulyana, Drs, M.Kom.",2,3,0,2020,1,"Twitter memiliki informasi dalam jumlah yang besar. Namun, informasi ini juga mengandung banyak noisy dan kata tidak standar yang menyulitkan untuk memilah informasi yang penting. Karena hal ini, perlu dilakukan normalisasi terhadap informasi yang diperoleh dari Twitter untuk mengubah kata menjadi bentuk standar.

Penelitian sebelumnya menunjukkan hasil yang baik dengan menggunakan word embedding dalam melakukan normalisasi teks. Word embedding dapat menangkap hubungan semantik dari kata. Fitur ini menjadi kelebihan dari word embedding yang akan dimanfaatkan untuk menangkap kesamaan semantik kata dari katakata pada Twitter. Pada penelitian ini, terdapat dua macam model word embedding yang dilatih . Model yang dilatih yaitu Word2Vec dan FastText. Penelitian ini ditujukan untuk mengetahui performa masing-masing dalam melakukan normalisasi dan membandingkan keduanya untuk untuk mengetahui yang lebih unggul. 

Berdasarkan hasil evaluasi, diketahui performaWord2Vec, performa FastText, dan model terbaik di antara keduanya. Model akhir Word2Vec yaitu model dengan learning rate 0.025, ukuran window 5, epoch 200, dan ukuran vektor 150. Hasil evaluasi menunjukkan model ini memperoleh akurasi sebesar 42.06% dan f1-score sebesar 41.87%. Model akhir FastText yaitu model dengan learning rate 0.025, ukuran window 7, jumlah epoch 300, dan ukuran vektor 300. Akurasi yang diperoleh yaitu 59.08% dan f1-score sebesar 68.14 % . Dari perbandingan model akhir keduanya diperoleh model FastText unggul dibandingkan model Word2Vec dalam melakukan normalisasi dengan menggunakan data di luar corpus. Untuk data pada corpus dengan pengambilan secara random Model FastText juga unggul dibandingkan model Word2Vec. Word2Vec memperoleh akurasi 59.41% dan f1-score 66.86% sedangkan FastText memperoleh akurasi 66.02% dan 75.68% pada f1-score. Untuk data pada corpus dengan frekuensi tertinggi, Model Word2Vec unggul dibandingkan model FastText. Word2Vec memperoleh akurasi 77.39% dan f1-score 84.38% sedangkan FastText memperoleh akurasi 69.89% dan f1-score 78.30%.
","Twitter has a large amount of information. This information contains noise and non-standard words that make it difficult to collect the essential information in it. Therefore, it is necessary to normalize the information obtained from Twitter in order to convert words into standard forms.

Previous studies have shown good results by using word embedding in normalizing text. Word embedding is a method of word representation that converts words into vector numbers. Word embedding can capture the semantic relationships of words. This feature is one of the advantages of word embedding that will be used to capture the semantic similarity from words on Twitter. In this study there are two types of word embedding models that are trained using Twitter data. The trained models are Word2Vec and FastText. This study aims to compare both models to see the performance of each in normalizing and to find out which model performs better.

Based on the evaluation results, we present the Word2Vec's performance, FastText's performance, and the best model of these two models. The final model of Word2Vec is a model with a learning rate of 0.025, window size 5, number of epoch is 200, and the vector size is 150. Evaluation results show that this model report 42.06% accuracy and 41.87% f1-score. The final model for FastText is a model with 0.025 learning rate , window size of 7, number of epoch is 300, and the vector size of 300. This model reached 59.08% accuracy and 68.14% f1-score. From the comparison between the best models, the FastText model is superior to the Word2Vec model in normalizing using data outside the corpus. For data on corpus with random retrieval, the FastText model is also superior to the Word2Vec model. Word2Vec obtained an accuracy of 59.41% and f1-score of 66.86% while FastText obtained an accuracy of 66.02 % and 75.68 % on the f1-score. For data on the corpus with the highest occurrence, theWord2Vec Model is superior to the FastText model. Word2Vec obtained an accuracy of 77.39 % and an f1-score of 84.38 %, while FastText obtained an accuracy of 69.89 % and an f1-score of 78.30 %.
",,,143655,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183976,,RIFQI DWI ARIAN,Mobile Ad-Hoc Network (MANET) Routing Protocols Performance Comparison under Blackhole Attack Using NS-3 Network Simulator,,,"MANET, AODV, DSDV, Blackhole Attack, NS-3","Dr.techn. Ahmad Ashari, M.I.Kom",2,3,0,2020,1,"  Mobile Ad Hoc Network (MANET) adalah jaringan nirkabel yang tidak menggunakan infrastruktur jaringan. Itu membuat jaringan MANET cocok untuk digunakan jika terjadi bencana alam yang menghancurkan infrastruktur telekomunikasi, atau daerah-daerah terpencil yang tidak memiliki infrastruktur telekomunikasi. MANET terdiri dari beberapa node yang saling berkoordinasi dan berkomunikasi.
  Studi ini akan membandingkan dua jenis MANET Routing Protocols, DSDV Routing Protocol dan AODV Routing Protocol. Keduanya akan disimulasikan di bawah Serangan Blackhole dan tidak. Perbandingan kinerja akan dilakukan dengan menjalankan simulasi dua Protokol Routing menggunakan NS-3 Network Simulator dan membandingkan data simulasi.
&Atilde;‚&Acirc;&nbsp;&Atilde;‚&Acirc;&nbsp;Hasil penelitian ini membuktikan bahwa ADOV Routing Protocol lebih baik daripada DSDV Routing Protocol, baik ketika disimulasikan dengan serangan blackhole atau tidak. Ini mungkin terjadi karena berbagai jenis dan prinsip kerja dari kedua protokol routing. Ini juga membuktikan bahwa Protokol Routing Reaktif memiliki kinerja yang lebih baik daripada Protokol Routing Proaktif.","  Mobile Ad Hoc Network (MANET) is a wireless network which not use network infrastructure. It makes MANET network is suitable for use in the event of a natural disaster which destroys telecommunications infrastructure, or remote areas that do not have telecommunications infrastructure. MANET consists of several nodes that coordinate and communicate with each other.
  This study will compare two types of MANET Routing Protocols, DSDV Routing Protocol and AODV Routing Protocol. Both will be simulated under Blackhole Attack and not. Performance comparison will be done by running the simulations of the two Routing Protocols using NS-3 Network Simulator and comparing the simulation data.
  The results of this study prove that the ADOV Routing Protocol is better than DSDV Routing Protocol, both when simulated with blackhole attacks or not. This might happen because of the different types and working principles of the two routing protocols. It also proves that Reactive Routing Protocol has better performance then Proactive Routing Protocol.",,,134889,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
188330,,YUES TADRIK HAFIYAN,Metode CNN-Extreme Learning Machine Untuk Klasifikasi Kerusakan DNA Menggunakan Comet Assay ,,,"Deep Learning, Convolutional Neural Network, Extreme Learning Machine, Transfer Learning, Comet Assay ","Afiahayati, S.Kom., M.Cs., Ph.D.",2,3,0,2020,1,"Dalam perkembangan teknologi di bidang medis, dihasilkan citra comet assay untuk menganalisis kerusakan DNA secara visual. Untuk mengklasifikasi kerusakan DNA tersebut dengan citra comet assay,  telah dilakukan berbagai penelitian untuk mengukur klasifikasi, yaitu menggunakan tool yang menghasilkan akurasi 11.5% dan menggunakan metode deep learning CNN dengan akurasi 60.5%. Pada penelitian ini, diimplementasikan metode  gabungan antara CNN dan ELM, dimana CNN sebagai feature extractor dan ELM sebagai classifier, bertujuan menghilangkan proses backpropagation sehingga menghilangkan terjadinya vanishing gradient dan mengurangi waktu training yang lama. Metode CNN-ELM ini dibagi menjadi 2 model, yaitu model CNN-ELM itu sendiri yang menghasilkan rata-rata akurasi terbaik pada 96.96% pada training speed 193.49 detik, serta model transfer learning-ELM yang menghasilkan akurasi 93.4% pada training speed 46.39 detik.","In the development of technology for medicine, comet assay images produced to analyze DNA damage by visual. To classify the DNA damage by comet assay image, there has been several research to measured classification, included using tool that resulted in accuracy 11.5% and using CNN deep learning method that resulted in accuracy 60.5%. In this research, the method that fusion between CNN and ELM has been implemented,  which is  CNN  as  feature extractor and  ELM  as a classifier,  that expect to remove the backpropagation process,  therefore also remove vanishing gradient and decreasing the long training duration.This CNN-ELM method divided into 2 models, including CNN-ELM model itself that resulted in accuracy rate at 96.6% and training speed at 193.49 seconds, also transfer learning-ELM model that resulted in accuracy rate at 93.4% and training speed at 64.39 seconds.",,,139483,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186803,,DELLA HERLI RAHMAN,ANALISIS DAN IMPLEMENTASI METODE WHITE SPACE REDUCTION PADA ALGORITMA TEXT DEWARPING MENGGUNAKAN INTERPOLASI LINIER,,,"Text dewarping, OCR, pengolahan citra, OpenCV, interpolasi linier.","Suprapto, Drs., M.Kom., Dr.;Faizal Makhrus, S.Kom., M.Sc., Ph.D.",2,3,0,2020,1,"Kebutuhan duplikasi citra dokumen semakin besar, namun penggunaan sistem scanning yang telah ada dapat merusak tatanan dan struktur dari dokumen itu sendiri. Metode Text Dewarping merupakan solusi yang efektif untuk memperbaiki kualitas citra dokumen. Penelitian ini membangun algoritma text dewarping dengan metode white space reduction menggunakan interpolasi linier. Data yang digunakan merupakan citra buku berbasis teks yang diambil dengan kamera kemudian disimpan dalam komputer.

Citra diolah dengan tahap prapemrosesan dari citra asli hingga menjadi citra biner. Tahap selanjutnya adalah melakukan penelusuran ketinggian ketinggian teks pada setiap kolom. Selanjutnya didefinisikan sebuah fungsi menggunakan interpolasi linier dengan harapan dapat menghasilkan nilai-nilai yang kontinu. Penelitian ini menghasilkan citra output untuk dianalisis kemudian dilakukan pengujian dengan OCR dan survei responden. Berdasarkan pengamatan secara visual, metode White
Space Reduction yang digunakan untuk perbaikan citra memperlihatkan hasil yang baik, meskipun total waktu komputasi yang diperlukan lebih besar dibanding dengan algoritma dewarping oleh Widardi (2018) yang menggunakan metode penyisipan piksel.","The need for document image duplication is increasing. However, the use of the existing scanning systems are accompanied with some image quality problems. Page dewarping algorithms is an effective solution to improve image quality. In this study, white space reduction using linear  interpolation is presented. The data used are text-based book image capture with a camera and stored in a computer.

The image is processed by the preprocessing methods from the original image to become a binary image. The next step is to trace the height of the text line in each column. Furthermore, a function is defined using linear interpolation in the hope that it can return continuous values. The output is an image to be analyzed and then tested by OCR and survey respondents. According to results the applied White Space Reduction method had shown the bigger improvement for the image visually, and so had for the execution time compare with one in Widardi (2018)'s pixel addition dewarping method.",,,137806,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186805,,IRVIN HABIBIE I,PERFORMANCE ANALYSIS OF FIFO AND FAIR JOB SCHEDULING ALGORITHM ON SPARK CLUSTER BY USING SPARK-BENCH BENCHMARK,,,"Big Data, Apache Spark, Cluster Computer, FIFO, FAIR","Dr. Mardhani Riasetiawan, SE Ak, M.T.",2,3,0,2020,1,"Dalam beberapa tahun terakhir, kebutuhan Big Data meningkat tajam untuk mempertahankan bisnis yang dibutuhkan. Big Data Framework yang memiliki tujuan untuk menganalisis data dan kemudian untuk memvisualisasikan data. Dengan menggunakan Apache Spark kita dapat mengatasi masalahnya bersama dengan komputer cluster. Spark sendiri memang memiliki dua algoritma penjadwalan, algoritma penjadwalan FAIR di mana setiap pekerjaan akan mendapatkan distribusi sumber daya yang sama untuk setiap pekerjaan, dan FIFO adalah algoritme penjadwalan pekerjaan yang setiap kali suatu pekerjaan mendekat, pekerjaan pertama yang mengirimnya terlebih dahulu akan mendapat prioritas dibandingkan dengan pekerjaan terakhir yang diajukan.
Dengan menggunakan benchmarking, Spark-Bench, kita dapat menganalisis kinerja penjadwalan pekerjaan di Apache Spark. Penelitian ini terdiri dari 3 skenario, skenario pertama membandingkan kinerja penjadwalan pekerjaan Fair dan FIFO, skenario 2 membandingkan masing-masing parameter konfigurasi kinerja bersama dengan korelasi dengan penjadwalan pekerjaan yang sama, FIFO dan Fair. Skenario 3 adalah tempat kombinasi beberapa parameter dalam algoritma penjadwalan pekerjaan dengan korelasi. Metrik evaluasi yang digunakan dalam penelitian ini adalah runtime Total dan runtime eksekusi.
Berdasarkan skenario 1, Fair mengungguli FIFO, masing-masing 1,6184 dan 1,6589 detik. Dalam skenario 2, secara keseluruhan, Adil mengungguli FIFO. Parameter konfigurasi tertinggi yang memiliki korelasi Pearson baik dalam penjadwalan pekerjaan yang adil dan penjadwalan pekerjaan FIFO adalah 0,1606. Dalam skenario terakhir, Wajar dilakukan lebih baik daripada FIFO. Hati-hati dengan parameter konfigurasi, beberapa di antaranya akan memperlambat penjadwalan pekerjaan.","In recent years, the needs of Big Data increase rapidly due to maintaining business data needed. Big Data framework that has goals to analyze the data and later for visualizing the data. By using Apache Spark we can overcome the solutions, along with cluster computer. Spark itself does have two job scheduling algorithms, FAIR scheduling algorithm where each job will get equal distribution of resources per each job, and FIFO is a job scheduling algorithm that every time a job is approaching, the first job who sends it first will have priority compared to latter jobs submitted.
By using benchmarking, Spark-Bench, we could analyze the performance of job scheduling in Apache Spark. This research consisted of 3 scenarios, the first scenario comparing the performance of Fair and FIFO job scheduling, scenario 2 were comparing each parameter configuration performance along with correlation with the same job scheduling, FIFO and Fair. Scenario 3 is where a combination of several parameters in job scheduling algorithms with correlation. The evaluation metrics used in this research are Total runtime and execution runtime.
Based on scenario 1, Fair outperform FIFO , 1.6184 and 1.6589 seconds respectively. In scenario 2, overall, Fair outperform FIFO. Parameter configurations that have highest Pearson correlations both in Fair job scheduling and FIFO job scheduling is 0.1606. In the last scenario, Fair is performed better than FIFO. Be careful with parameter configurations, some of them will slow down the job scheduling.
",,,138150,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183993,,NAUFAL IZZA MAHENDRA,IMPLEMENTATION AND ANALYSIS OF CEPH DISTRIBUTED FILE SYSTEM ON CONTAINER ORCHESTRATIONS,,,"Cloud, DFS, Orchestration, Ceph, Docker, Kubernetes","Ahmad Ashari, Dr-tech",2,3,0,2020,1,"Gebrakan cloud terbaru yang menghasilkan pertumbuhan luar biasa dalam virtualisasi dan sistem file terdistribusi (DFS) membuat klasterisasi komputer menjadi standar baru. Hampir setiap sistem terdistribusi diperlukan untuk mempertahankan tingkat keandalan dan ketersediaan tertinggi. Ini menciptakan permintaan baru dalam layanan penyimpanan yang konsisten untuk mendukung server.
DFS seperti Ceph, adalah teknik untuk memungkinkan set komputer terdistribusi untuk berbagi data dan sumber daya penyimpanan dengan menggunakan sistem file umum. Container orchestration seperti Swarm dan Kubernetes, adalah alat yang memungkinkan otomatisasi untuk menarik, menyebarkan, dan mengelola kontainer yang sedang berjalan. Kedua teknologi ini relatif baru dan menjadi tren di komunitas TI. Ceph, Docker dan Kubernetes menjadi kata yang semakin umum.
Penelitian ini melakukan implementasi dan analisis Ceph DFS pada container orchestration. Penelitian ini akan melakukan analisis komparatif antara kinerja Ceph di Swarm dengan Ceph di Kubernetes dalam empat skenario yang berbeda menghasilkan dua parameter yang berbeda yaitu throughput dan IO per detik. Setelah hasil yang diperoleh, hasilnya akan diproses lebih lanjut menggunakan metode statistik tertentu.
Hasil penelitian ini menunjukkan bahwa Ceph di Kubernetes menghasilkan hasil yang lebih baik terhadap Ceph di Swarm. Pengujian statistik memverifikasi ini dengan menunjukkan tingkat signifikansi statistik dan tren yang lebih cenderung ke arah Ceph di Kubernetes.","The recent cloud boom which resulted in tremendous growth in virtualization and distributed file system (DFS) made computer clustering become a new standard. Almost every distributed system is required to maintain the highest level of reliability and availability. This creates a new demand for consistent storage service to support servers.
DFS like Ceph is a technique to allow a set of distributed computers to share data and storage resources by using a common file system. Container orchestrator like Swarm and Kubernetes is a tool that enables automation to pull, deploy and manage running containers. Both technologies are relatively new and trending in the IT community. Ceph, Docker, and Kubernetes become more and more common buzzwords.
This research conducts an implementation and analysis of Ceph DFS on container orchestration. This research is going to conduct a comparative analysis between the performance of Ceph in Swarm with Ceph in Kubernetes in four different scenarios resulted in two different parameters which are throughput and IO per second. After the results obtained, the results are going to be further processed using certain statistical methods.
The results of this research show that Ceph in Kubernetes produces better results against Ceph in Swarm. The statistical testing verified this by showing statistical significance level and trends more inclined towards Ceph in Kubernetes.",,,134965,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
188857,,MUHAMMAD AZMI ARIS,Perbandingan Kinerja Metode Deep Learning dengan Metode Non-Deep Learning Pada Analisis Sentimen Tweet Berbahasa Indonesia,,,"Analisis Sentimen, Deep Learning","Khabib Mustofa, S.Si., M.Kom., Dr. techn.",2,3,0,2020,1,"Analisis sentimen merupakan salah satu proses atau teknik yang sangat populer digunakan untuk mengidentifikasi kepuasan pelanggan. Beberapa metode yang dapat digunakan untuk proses analisis sentimen adalah Naive Bayes, Support Vector Machine dan Deep Learning. Metode Naive Bayes dan metode Support Vector Machine yang merupakan metode non-Deep Learning memiliki kelemahan dalam memilih metode ekstraksi fitur dan menemukan fitur yang lebih baik (Su dkk., 2014). Di samping itu metode tersebut juga memiliki kelemahan pada data yang berjumlah besar yang mempengaruhi performa pada akurasi klasifikasi (Nasichuddin dkk., 2018). Deep Learning menunjukkan hasil yang bagus di bidang speech recognition, vision dan sebagainya sehingga banyak digunakan pada analisis sentimen untuk mendapatkan kinerja yang lebih baik (Ouyang dkk., 2015).

Pada penelitian ini dilakukan perbandingan metode Deep Learning yakni metode CNN dan metode LSTM dengan metode non-Deep Learning yakni Naive Bayes dari sisi akurasi dan waktu komputasi untuk mengetahui apakah dengan pengorbanan komputasi yang besar akan mendapatkan akurasi yang jauh lebih baik. Pengujian dilakukan dengan menggunakan K-Fold Cross Validation. Jumlah K-Fold yang digunakan adalah sebanyak 10. Data dibagi sebanyak K-1 bagian untuk data latih dan sisa
bagian lainnya untuk data uji.

Penelitian ini menghasilkan nilai akurasi sebesar 60.92% untuk metode LSTM, 59.29% untuk metode CNN, dan 57.59% untuk metode Naive Bayes. Waktu komputasi rata-rata dalam 1 fold yang didapat oleh ketiga metode tersebut adalah 57 milidetik untuk Naive Bayes, 110 detik 500 milidetik untuk metode CNN, dan 293 detik 730 milidetik untuk metode LSTM. Setelah dilakukan pengujian hipotesis dari sisi akurasi dan waktu komputasi dapat disimpulkan bahwa kinerja metode non-Deep Learning yakni metode Naive Bayes lebih baik daripada metode Deep Learning yakni LSTM dan CNN.","Sentiment analysis is one of the most popular processes or techniques used to identify customer satisfaction. Some methods that can be used for sentiment analysis are Naive Bayes, Support Vector Machine and Deep Learning. The Naive Bayes method and the Support Vector Machine method which are non-Deep Learning methods have weaknesses in choosing the feature extraction method and finding better features. In addition, these methods also have weaknesses in large amounts of data that affect the performance of the classification accuracy (Nasichuddin dkk., 2018). Deep Learning shows good results in the field of speech recognition, vision and so on so that it is widely used in sentiment analysis to get better performance (Ouyang dkk., 2015).

In this study a comparison of the Deep Learning method, the CNN method and the LSTM method with the non-Deep Learning method, namely Naive Bayes, in terms of accuracy and computational time to find out whether a large computational sacrifice will get much better accuracy. Testing is done using K-Fold Cross Validation. The number of K-Fold used was 10. The data is divided into K-1 sections for training data and the rest for the test data.

This research resulted in an accuracy value of 60.92% for the LSTM method, 59.29% for the CNN method, and 57.59% for the Naive Bayes method. The average computation time in 1 fold obtained by the three methods is 57 milliseconds for Naive Bayes, 110 seconds 500 milliseconds for the CNN method, and 293 seconds 730 milliseconds for the LSTM method. After testing the hypothesis in terms of accuracy and computational time it can be concluded that the performance of the non-Deep Learning method, the Naive Bayes method, is better than the Deep Learning method, namely LSTM and CNN.",,,139950,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
185022,,MOHAMAD INDRA W,PENDETEKSIAN API BERBASIS PENGOLAHAN CITRA DIGITAL MENGGUNAKAN METODE CONVOLUTIONAL NEURAL NETWORK,,,"Convolutional Neural Network, Pengolahan Citra Digital, Deteksi Api","Wahyono, Ph.D",2,3,0,2020,1,"Sistem alarm api konvensional bergantung pada penggunaan sensor suhu atau asap. Performa dari system ini sudah akurat naumn bukan berarti tidak mempunyai kelemahan. Akurasi dari system ini bergantung pada jarak antar titik api dengan posisi sensor ditempatkan. Penelitian ini mengusulkan system berbasis pengolahan citra digital menggunakan convolutional neural network untuk melengkapi system alarm api yang sudah ada.
Convolutional neural network sebagai metode penelitian berbasis pengolahan citra digital sudah umum diterapkan, Namun penelitian yang mengaplikasikan convolutional neural network dalam bentuk fully convolutional network masihlah sangat sedikit. Adaptasi Fully Convolutional Network 8 upsampled Prediction (FCN-8s) ciptaan Long et al.(2015) digunakan sebagai metode yang diusulkan pada penelitian ini. Performa model yang diusulkan kemudian akan dibandingkan dengan  pendeteksi api berbasis fully convolutional network yang diciptakan oleh Gonzales et al.(2017). Hasil menunjukkan bahwa performa FCN-8s mengalahkan performa SFEwAN-SD pada berbagai metrik baik dalam pengujian tingkat pixel dan tingkat citra.
","Conventional fire alarm system depended on application of heat sensor and smoke sensor. Performance of said system is accurate however it isn't without flaw. Accuracy of most system depends on proximity of sensor against fire hotspot. This research proposed a system based on digital image processing using convolutional neural network method to complement existing fire alarm system. 
Convolutional neural network as method in digital image processing research especially is already commonly used. However research that applies convolutional neural network in form of fully convolutional network still scarce. Adaptation of Fully Convolutional Network 8 upsampled Prediction (FCN-8s) by Long et al.(2015) is  used as proposed method. Performance of proposed method also then compared against existing fully convolutional network-based fired detection by Gonales et al.(2017). Results shows FCN-8s performance beats SFEwAN-SD in various metrics both pixel level and image level test.
",,,135979,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
187071,,ANDIKA RAHIM D,PENGARUH REDUKSI FITUR DENGAN PRINCIPAL COMPONENTS ANALYSIS (PCA) TERHADAP PERFORMA AUTOMATED ESSAY SCORING (AES) BAHASA INDONESIA,,,"PCA, Fitur Reduksi, Pembelejaran Mesin, Text Mining, Klasifikasi Teks","Drs. Sri Mulyana, M.Kom. ",2,3,0,2020,1,"Principal Components Analysis (PCA) merupakan salah satu algoritma bertujuan untuk mengurangi banyak fitur pada dataset. Pada teks klasifikasi, fitur terekstraksi sangat banyak sehingga teks menjadi sulit di klasifikasikan. PCA dapat mengurangi fitur yang ada dengan cara mengdekomposisi ulang Vector Space Model (VSM) yang ada menjadi VSM dengan fitur yang lebih sedikit.
Penelitian ini mengusulkan PCA digunakan sebagai reduksi fitur dalam pembuatan model classifier pada pengklasifikasian teks jawaban berbahasa Indonesia dari sebuah pertanyaan. Penelitian ini melihat bagaimana performa dari model classifier yang dibangun ketika menggunakan PCA untuk mengurangi fitur pada datasetnya. Classifier yang digunakan dalam penelitian ini adalah Decison Tree, Random Forest, Adaptive Boosting (AdaBoost), K-Nearest Neighbors (KNN), dan Support Vector Machine (SVM). Performa dilihat dari sejauhmana PCA dapat meningkatkan score dari model classifier yang dibangun dibandingkan tanpa menggunakan PCA serta seberapa jauh peningkatan performa waktu eksekusinya. 
Hasil Penelitan menunjukan bahwa PCA tidak selalu mampu meningkat performa akurasi maupun f1-score secara signifikan dari model classifier yang dibangun walau secara waktu eksekusi terdapat peningkatan. Penurunan terjadi pada classifier yaitu Decision Tree. Penurunan paling signifikan terjadi pada dataset Jaket. Pada dataset Machu Pichu, AdaBoost dan Random Forest mengalami peningkatan tapi tidak signifikan. Peningkatan performa f1-score dengan PCA signifikan terjadi pada KNN. Peningkatan terjadi untuk semua dataset yaitu sekitar 3%. Performa waktu eksekusi juga meningkat signifikan untuk KNN.","Principal Components Analysis (PCA) is one algorithm that aims to reduce many features in the dataset. In text classification, features are extracted so much that the text becomes difficult to classify. PCA can reduce existing features by decomposing existing Vector Space Model (VSM) into VSM with fewer features. This study proposes that PCA isused for feature reduction in making classifier models in classifying the answer text in Indonesian from a question.
This study looks at how the performance of the classifier model that is built when using PCA to reduce features in the dataset. The classifiers used in this study are Decison Tree, Random Forest, Adaptive Boosting (AdaBoost), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM). Performance can be seen from the extent to which PCA can increase the score of the classifier model that is built compared without using PCA and how far the increase in execution time performance. 
Research results show that PCA is not always able to significantly increase f1-score of the class model that was built even though the execution time has increased. The decrease occurred in the classifier namely Decision Tree. The most significant decrease occurred in the Jaket dataset. In the Machu Pichu dataset, AdaBoost and Random Forest have increased but not significantly. Accuracy and f1-scores increase significantly occurred in the KNN. The increase occurred for all datasets, arround 3%. The execution time performance also improved for KNN.
",,,138080,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
191949,,THOHA IKHWANUL HAQ,Iterated Function System untuk Memperkuat Advanced Encryption Standard pada Cryptosystem,,,"Kriptografi, AES, Iterated Function System ","Nur Rokhman, S.Si., M.Kom., Dr",2,3,0,2020,1,"Munculnya berbagai teknik kriptoanalisis modern dapat membahayakan kerahasiaan dan kemanan data. Teknik kriptoanalisis modern dapat digunakan oleh pihak ketiga untuk memecahkan enkripsi pada kriptosistem. Advanced Encryption Standard (AES) merupakan salah satu teknik enkripsi-dekripsi yang sering digunakan. AES 256-bit single core sudah berhasil dipecahkan dengan relatif cepat pada tahun 2017. 
Perkembangan teknik kriptoanalisis perlu diimbangi dengan pengembangan teknik kriptografi yang lebih kuat. AES sudah cukup kuat digunakan selama 20 tahun terakhir sehingga saatnya dikembangkan secara lebih lanjut. Pengembangan AES perlu didasari oleh teknik kriptoanalisis yang sudah ada seperti Kolmogorov-Smirnov Test dan Avalanche Effect. 
Penelitian ini bertujuan untuk menguji Iterated Function System(IFS) sebagai salah salah satu metode enkripsi-dekripsi pada kriptosistem. IFS merupakan fungsi-fungsi yang dapat diiterasikan secara tidak terbatas untuk mengenkripsi data, sehingga diharapkan Iterated Function System  dapat digunakan untuk memperkuat Advanced Encryption Standard pada kriptosistem. ","The development of various modern cryptoanalyses techniques can pose a threat to our privacy and data security. These modern cryptoanalyses methods could be used by a third-party to decrypt an encrypted message. The Advanced Encryption Standard, a cryptosystem which has been commonly used for years, has been successfully cracked to a certain degree. The AES 256-bit single has been successfully cracked relatively quickly in 2017 by a Dutch researcher.
The growth and development of various cryptonalysis techniques has proven that cryptography must be improved even further. The AES, which has been coomonly used for 20 years, should be further developed and should be used as a basis for any further development of any cryptography techniques.
This research aims to test the Iterated Function System (IFS) as an encryption-decryption method for a cryptosystem. IFS features a set of functions which can be iterated endlessly to encrypt a message. The IFS can be used in an AES by inserting IFS in between one of the steps in AES thus the IFS is expected to strengthen the AES of a cryptosystem. 
",,,143091,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186575,,MUHAMMAD IHSAN,Heuristik untuk Masalah Urutan Optimal Komposisi Fungsi-Fungsi,,,"heuristik, penjadwalan bergantung pada waktu, komposisi fungsi, fungsi monoton naik","Dr-Ing, MHD. Reza M.I. Pulungan, M.Sc.",2,3,0,2020,1,"Berawal dari masalah penjadwalan bergantung pada waktu yang secara umum adalah permasalahn NP dan telah banyak penelitian dengan heuristik untuk pencarian nilai optimalnya. Terdapat penelitian oleh Kawase et al. (2016) bahwa masalah penjadwalan bergantung pada waktu dapat dibawa ke permasalahan urutan optimal komposisi fungsi-fungsi. Beberapa kelas fungsi dapat diselesaikan dalam waktu polinomial, salah satunya fungsi linear naik.

Pada penelitian ini, dilakukan pendekatan heuristik untuk kelas fungsi linear naik dengan menambahkan sebuah fungsi monoton naik tak linear di mana kelas fungsi ini masuk pada permasalahan NP. Beberapa algoritma heuristik akan dilakukan, yaitu greedy, berbasis relasi urut, dan pemrograman dinamis.

Dibandingkan nilai optimal yang didapatkan dari masing-masing algoritma dan running time. Didapat bahwa heuristik berbasis relasi urut adalah yang terbaik dengan 96.25% kasus uji mendapatkan nilai terbaik. Runnig time dari heuristik berbasis relasi urut juga yang terbaik dengan kompleksitas waktu O(n^2).","Time-dependent scheduling are generally NP problem. There are many research uses heuristic to find the optimal value. Kawase et al. (2016) stated in their research that time-dependent scheduling problems are related to optimal ordering of composition problems. Some classes of functions can be solved in polynomial time, such as class of increasing linear functions.

In this research, a heuristic approach for the class of increasing linear functions is done by adding a nonlinear monotone increasing function, which is an NP problem. Some heuristic algorithms will be done, such as greedy algorithm, ordered relation-based algorithm, and dynamic programming.

The optimal values from those algorithms and their running time are compared.
It can be concluded that ordered relation-based heuristic algorithm is the best one where 96.25\% of the test cases get the best value. The running time of ordered relation-based heuristic algorithm is also the best one, whose time complexity is O(n^2).",,,137615,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186836,,RAFIE MUHAMMAD,PROOF OF CONCEPT SERANGAN  PADDING ORACLE PADA PENYANDIAN AES DENGAN MODE PCBC,,,"kriptografi, AES, PCBC, serangan kriptografi, padding oracle, exploit, proof of concept, security advisory","Anny Kartika Sari, S.Si., M.Sc., Ph.D.",2,3,0,2020,1,"Advanced Encryption Standard (AES) merupakan salah satu algoritme
kriptografi kunci simetris untuk mengamankan pengiriman data pada perangkat
lunak dan perangkat keras. AES sendiri memiliki beberapa mode aplikasi yang
dapat digunakan, salah satunya adalah mode Propagating Cipher Block Chaining
(PCBC). Meskipun bersifat standar, algoritme ini dengan beberapa penerapannya
masih rentan terhadap serangan kriptanalisis. Sejumlah penelitian pada penyandian
AES menggunakan berbagai macam teknik serangan untuk membuktikan celah
keamanan pada penerapan AES. Salah satu teknik yang digunakan adalah serangan
padding oracle yang memanfaatkan informasi pesan error pada validasi padding.
Penelitian ini menjabarkan sebuah proof of concept eksploitasi pada suatu
penerapan AES-PCBC yang menampilkan informasi terkait validasi padding dari
plaintext. Proof of concept diawali dengan usaha untuk mendapatkan pesan asli dari
suatu pesan yang terenkripsi dan melakukan modifikasi terhadap pesan yang
terenkripsi agar menjadi pesan asli yang diinginkan pada saat proses dekripsi. Pada
percobaannya, digunakan sebuah webservice sederhana sebagai target serangan
dengan menerapkan kustom enkripsi pada cookies autentikasi menggunakan
AES-PCBC.
Dari hasil penelitian yang diperoleh, didapatkan bahwa penerapan
AES-PCBC secara independen untuk proses autentikasi ditambah dengan
diberikannya informasi terkait validitas suatu padding dapat menimbulkan celah
keamanan padding oracle. Selain itu, dari hasil eksperimen didapatkan bahwa waktu
eksekusi untuk eksploitasi cukup singkat sehingga memungkinkan untuk terjadinya
serangan secara masif. Untuk mencegah celah kemanan yang telah diteliti, akan
diberikan beberapa security advisory.","Advanced Encryption Standard (AES) is one of the symmetric key
cryptographic algorithms to secure data transmission on software and hardware.
AES itself has several application modes that can be used, one of which is the
Propagating Cipher Block Chaining (PCBC) mode. Although standard, this
algorithm with some applications is still vulnerable to cryptanalysis. A number of
studies on AES encryption use a variety of attack techniques to prove security
loopholes in the application of AES. One of the techniques used is the padding
oracle attack that uses error message information on padding validation.
This study describes a proof of concept exploitation of an AES-PCBC
application that displays information related to validation padding from the text.
Proof of concept begins with an attempt to get the original message from an
encrypted message and modifies the encrypted message to make the original
message desired during the decryption process. In the experiment, a simple
webservice was used as an attack target by implementing custom encryption on
authentication cookies using AES-PCBC.
From the research results obtained, it was found that the application of
AES-PCBC independently for the authentication process coupled with the provision
of information related to the validity of a padding can lead to padding oracle
vulnerability. In addition, the experimental results show that the execution time for
exploitation is short enough to allow for massive attacks. To prevent security holes
that have been studied, several security advisory will be given.",,,137834,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186582,,Dyah Mawar Umbulsari,DETEKSI SPAM PADA SMS BAHASA INDONESIA MENGGUNAKAN SELEKSI FITUR BERBASIS PARTICLE SWARM OPTIMIZATION,,,"Deteksi spam, Klasifikasi, Particle Swarm Optimization, Gaussian Naive Bayes, Seleksi Fitur / Spam Detection, Classification, Particle Swarm Optimization, Gaussian Naive Bayes, Feature Selection","Anny Kartika Sari, S.Si., M.Kom.,Ph.D",2,3,0,2020,1,"Permasalahan spam pada Short Message Service (SMS) dapat diatasi salah satunya dengan cara mengklasifikasi atau memisahkan pesan spam dengan pesan bukan spam. SMS yang masuk idealnya dapat langsung dideteksi sebagai pesan spam atau bukan. Permasalahan ini menunjukan bahwa waktu sangat krusial, namun sistem klasifikasi teks memiliki permasalahan tersendiri yaitu besar dimensi data berbanding lurus dengan jumlah kata dalam teks. Besar dimensi data padahal akan mempengaruhi lama waktu yang diperlukan untuk mengenali data tersebut.
Penelitian ini bertujuan untuk mereduksi dimensi data tersebut dengan menerapkan algoritme Particle Swarm Optimization (PSO) pada tahap seleksi fitur dan melihat apakah performa klasifikasi dapat dipertahankan walaupun fitur datanya berkurang. PSO adalah algoritme random berbasis populasi untuk menemukan solusi optimal yang terinspirasi dari tingkah laku sekelompok burung. Model klasifikasi pada penelitian ini dibangun berdasarkan algoritme Gaussian Naive Bayes dengan beberapa tahapan dalam sistem yaitu pengumpulan data, preprocessing, seleksi fitur, klasifikasi dan evaluasi. Validasi yang dilakukan terhadap model menggunakan metode Stratified K-Fold Cross Validation dengan nilai k=10 dan paramaeter evaluasi berupa akurasi, presisi, recall dan f1 score.  
Hasil pengujian dan evaluasi model yang dibangun menunjukan terdapat peningkatan pada model dengan penambahan seleksi fitur PSO, yaitu sebesar 0,09 pada rata-rata nilai akurasi dari 0,84 menjadi 0,93 dan peningkatan sebesar 0,034 pada rata-rata nilai f1-score dari 0,919 menjadi 0,953 yang disertai pengurangan jumlah fitur sebanyak 324 fitur untuk dataset yang dikumpulkan secara manual dari 822 fitur menjadi 498 fitur. Model yang dibangun berdasarkan dataset publik menunjukkan peningkatan sebesar 0,0026 pada rata-rata nilai akurasi dari 0,908 menjadi 0,9106 dan peningkatan sebesar 0,003 untuk rata-rata nilai f1-score dari 0,908 menjadi 0,911 yang disertai pengurangan fitur sebanyak 1427 fitur dari 3382 fitur menjadi 1955 fitur.
","Spam problem in Short Message Service (SMS) can be resolved with which one of the solution is by classification or classifying text messages into spam or non spam. The classification system ideally should be able to detect spam messages as soon as the text arrives, thus time is very crucial in this problem. Classifying text itself meanwhile also have a problem and that is the dimension size of the data. The bigger size of the data dimension is, the longer it takes for the classifier to learn that data.
The research aims to reduces the data features by applying Particle Swarm Optimization (PSO) method in feature selection. The population based randomized algorithm PSO was inspired by the behaviour of a flock of birds in finding source of food or in this research represented by the optimal solution. The classifier that is used in this research is Gaussian Naive Bayes, the validation is done using the Stratified K-Fold Cross Validation method with the value of k=10. The evaluation of the model is measured using parameters such as accuracy, precision, recall and f1-score.
The results of system implementation and evaluation shows that by implementing PSO in feature selection the classification system could achieve 0,09 uplift average in accuracy from 0,84 to 0,93 and 0,034 uplift average in f1-score from 0,919 to 0,953 with 324 features reduced in manually collected dataset from 822 features to 498 features. The system achieved 0,0026 uplift average in accuracy from 0,908 to 0,9106 and 0,003 uplift average in f1-score from 0,908 to 0,911 with 1427 reduced features from 3382 features to 1955 features while using the public dataset.
",,,137614,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186838,,IQBAL YUDIAR S,IDENTIFICATION OF TRAFFIC FLOW IN YOGYAKARTA USING SUPPORT VECTOR MACHINE ALGORITHM WITH TWITTER DATASET,,,"Text mining, Support Vector Machine, Natural Language Processing","Anifuddin Azis, S. Si., M. Kom.",2,3,0,2020,1,"Kemacetan adalah hal yang sering terjadi di kota - kota besar. Yogyakarta adalah salah satu kota besar yang sering terjadi kemacetan. Karena terjadi macet biasanya orang menginformasikannya melalui Twitter. Namun Twitter memiliki keterbatasan tidak dapat mengidentifikasi lokasi dan waktu dari keadaan lalu lintas.
Pada penelitian ini akan dilakukan pengidentifikasian dan pengklasifikasian kondisi arus lalu lintas di Yogyakarta dengan menggunakan algoritma Support Vector Machine  dan data didapatkan dari Twitter. Data yang diperoleh berjumlah 1244 data. Kemudian data yang tidak relevan dihapus sehingga menyisakan 1022 data.  Kemudian data tersebut diolah untuk didapatkan hasil dari proses klasifikasi dan identifikasi.
Hasil dari proses klasifikasi berupa nilai akurasi 94%, nilai presisi 92%, nilai recall 91%, nilai f1-score 91%. Dan didapatkan juga hasil dari proses identifikasi berupa hari, waktu dan nama jalan  pada kondisi lalu lintas.Dan jalan yang paling sering macet adalah jalan Malioboro, jalan Bener, dan  jalan Magelang.","Congestion is a common thing in big cities. Yogyakarta is one of the big cities that is often congested. Because there is a traffic jam, people usually inform the condition via Twitter. But Twitter has the limitation of not being able to identify the location and the time of the traffic situation.
In this research, identification and classification of traffic flow conditions in Yogyakarta will be carried out using the Support Vector Machine algorithm and data obtained from Twitter. The data obtained are 1244 data. Then the irrelevant data is deleted, leaving 1022 data. Then the data is processed to obtain results from the classification and identification process.
The results of the classification process are an accuracy value of 94%, a precision value of 92%, a recall value of 91%, and an f1-score value of 91%. And the results obtained from the identification process in the form of days, time and street names traffic conditions. The identification can produce street names, day. and times when the flow of traffic is happening. And the most frequently jammed roads are Malioboro, Bener, and Magelang roads.
",,,137966,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186583,,Sheraton Pawestri,KOMPARASI PERFORMA MODEL ALGORITMA SUPPORT VECTOR MACHINE DAN SUPPORT VECTOR MACHINE PARTICLE SWARM OPTIMIZATION UNTUK KLASIFIKASI HOAX BAHASA INDONESIA,,,"hoax, particle swarm optimization, support vector machine, seleksi fitur /hoax, particle swarm optimization, support vector machine, feature selection","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2020,1,"Di era teknologi ini informasi mudah didapatkan baik dari internet, surat kabar, atau dari mulut ke mulut. Namun informasi juga bisa disalahgunakan seperti membuat berita atau informasi palsu atau hoax. Penelitian tentang klasifikasi hoax telah banyak dilakukan. Data yang berbentuk teks menyebabkan data memiliki dimensi yang tinggi dan fitur yang sangat banyak. Salah satu metode klasifikasi data berdimensi tinggi adalah Support Vector Machine (SVM). SVM saja tanpa seleksi fitur akan menghasilkan fitur yang banyak. Fitur yang banyak akan memakan waktu yang lama serta tidak efisien sehingga perlu adanya seleksi fitur. Salah satu metode seleksi fitur adalah Particle Swarm Opimization (PSO). 
	Pada penelitian ini dilakukan komparasi performa klasifikasi hoax dengan metode klasifikasi SVM dan SVM+PSO. Data yang diambil bertema pilpres 2019 dan politik. 200 data pilpres 2019 dengan rincian 100 hoax dan 100 non-hoax dan 20 tema politik dengan rincian 10 hoax dan 10 non-hoax. Evaluasi klasifikasi berupa akurasi, precision, recall dan lama waktu train test SVM dengan dan tanpa PSO.
	Evaluasi penelitian ini, rata-rata akurasi sebesar 0.89545 atau 89.545%, rata-rata precision sebesar 92.905% dan rata-rata recall sebesar 83.901% untuk SVM. Rata-rata akurasi tertinggi menjadi 90.454%, rata-rata precision yaitu 92.238% dan recall yaitu 86.238% untuk SVM+PSO. Pada penelitian ini, PSO mampu meningkatkan performa namun kenaikan tidak pesat yaitu sekitar 1%. ","In this technological era, information is easily obtained either from the internet, newspapers, or by word of mouth. But information can also be misused such as making hoaxes. Many studies about hoax classification. Text data can have many dimensions and features. One of text classification methods that have been used is Support Vector Machine (SVM). SVM without feature selection will produce many features that take a long time an inefficient for processing classification. So, feature selection is needed. One method of feature selection is Particle Swarm Optimization (PSO).
	In this study a comparison of the performance of the hoax classification with the SVM and SVM + PSO classification methods was performed. There are 200 data (100 hoaxes, 100 non-hoaxes) with presidential election 2019 data and 20 data with political theme (10 hoaxes, 10 non-hoaxes). Classification is done twice, SVM and SVM+PSO. Evaluations contain accuracy, precision, recall, and the length of time the SVM train test with and without PSO.
	Evaluations contain an average accuracy of 0.89545 or 89.545%, an average precision of 92.905%, and an average recall of 83.901% for SVM. Then the highest average accuracy is 90.454%, the highest average precision is 92.238% and the highest recall is 86.238% for SVM+PSO. In this study PSO was able to improve performance but the increase was not rapid, it was around 1%.
",,,137609,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
188119,,AGUNG DWI MUFTI,ALGORITMA GENETIKA UNTUK PREDIKSI NILAI INDEKS HARGA KONSUMEN,,,"prediksi, algoritme genetika, IHK, algoritme metaheuristik","Aina Musdholifah, S.Kom., M.Kom., ph.D.",2,3,0,2020,1,"Nilai IHK(Indeks Harga Konsumen) merupakan indikator yang digunakan untuk menentukan tingkat/laju inflasi. Kebijakan-kebijakan akan diambil guna mempertahankan laju inflasi pada tingkat tertentu ataupun menekan laju inflasi tersebut. Prediksi nilai IHK diharapkan mampu membantu pemerintah dalam membentuk kebijakan-kebijakan tersebut guna mengatur tingkat/laju inflasi. Ada beberapa cara dalam melakukan prediksi nilai IHK, salah satunya dengan menggunakan algoritme metaheuristik.

Algoritma genetika merupakan algoritme metaheuristik yang memiliki karakter selayaknya genetika pada makhluk hidup, karena kemampuannya untuk menyeleksi setiap solusi dan mempertahankan solusi terbaik. Penelitian ini bertujuan untuk menganalisa parameter algoritma genetika, semisal crossover, mutasi, dan seleksi dalam memprediksi nilai IHK dengan variabel pengaruhnya. Disamping itu, metode regresi linier sebagai jembatan variabel-variabel yang memengaruhi IHK tersebut guna menghitung nilai IHK. Variabel yang digunakan adalah nilai tukar rupiah-dollar AS, jumlah uang yang beredar di Indonesia, bi-rate, dan jumlah uang giral.

Pada penelitian ini prediksi nilai IHK menghasilkan nilai mean absolute error (MAE) sebesar 0.5895 dan nilai root mean squared error (RMSE) sebesar 0.7343 dengan beberapa kali percobaan untuk memastikan bahwa nilai telah mengarah pada konvergen.","The CPI (Consumer Price Index) is ones indicator that is used to determine the inflation rate. Policies will be taken to maintain the inflation rate at certain level or to reduce the inflation rate. Forecasting of CPI values is expected to be able to assist the government in creating the policies in order to regulate inflation rate. Forecasting of CPI values can be determined by several ways, one of them using metaheuristic
algorithm.

Genetic algorithm branch of metaheuristic algorithm that have genetically appropriate characteristics in living creature, that possible genetic algorithm maintain the best solution or purifing the all possibilites in solving problem like natural selection. This study aims to analyze the parameters of genetic algorithms, such as crossover, mutation, and selection in forecasting of CPI values with influence variables. In addition, the multiple linear regression method as a bridge to calculate among the CPI value with the influence variables. The variables that are used are the exchange rate of rupiah-US dollars, the amount of money circulating in Indonesia, bi-rate, and the amount of demand deposits.

This study, the forecasting of CPI values produces mean absolute error (MAE) value of 0.5895 and root mean squared error(RMSE) value of 0.7343 with several attempts to ensure that the value has led to convergence.",,,139118,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184536,,FERDINAND THEMA. W,"Comparison of Apriori, FP-Growth and ECLAT Algorithms to Determine Association Rules on Sales Data (Case Study: Toko Roti Ferisa, Bantul, DI Yogyakarta)",,,"Data mining, association rule, Apriori, FP-Growth, ECLAT","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2020,1,"Perkembangan teknologi yang telah mencapai industry 4.0 menjadikan setiap informasi sangat bernilai untuk diolah. Tak khayal seluruh aspek kehidupan seperti layanan kesehatan, instansi pendidikan, pemerintahan, perbankan, pabrik, perusahaan, retail dan lain-lain berlomba-lomba untuk menerapkan pengolahan data agar menjadi informasi yang tepat guna. Begitupun data transaksi dari sebuah laporan penjualan di Toko Roti pun dapat diolah menjadi informasi yang berguna mengenai pola belanja konsumennya. Data mining dapat dimanfaatkan dalam pencarian pola yang diinginkan dalam basis data. Menggunakan Association rule mining, teknnik ini digunakan untuk menemukan pola kombinasi dari data transaksi dengan menggunakan algoritma yang tersedia seperti Apriori, FP-Growth dan ECLAT.

Pada penelitian ini dilakukan perbandingan kinerja Apriori, FP-Growth dan ECLAT algorithms untuk association rule mining. Perbandingan dilhat dari segi jumlah rule yang dihasilkan dan lama waktu proses. Data yang digunakan untuk penelitian ini menggunakan data penjualan di Toko Roti Ferisa. Penelitian dilakukan dengan melakukan sepuluh kali percobaan menggunakan minimum support and minimum confidence values yang berbeda. Hasil percobaan kemudian dibandingkan dengan Apriori, FP-Growth and ECLAT algorithms.

Dari hasil tests yang telah dilakukan, jumlah association rules yang dihasilkan dari ketiga algoritma memiliki sedikit perbedaan. Apriori dan FP-Growth algorithms menghasilkan jumlah output yang sama sedangkan ECLAT algorithm memiliki sedikit lebih banyak jumlah output dihasilkan. Sedangkan dari proses waktu penyelesaian, ECLAT algorithm memberikan waktu yang lebih singkat, sehingga dalam penelitian ini, ECLAT algorithm dinilai memiliki algoritma yang lebih efisien.
","The development of technology that has reached industry 4.0 makes every information very valuable to be processed. Not imagined all aspects of life such as health services, educational institutions, government, banking, factories, companies, retails, and others competing to implement data processing in order to be appropriate information. Likewise, transaction data from a sales report at a bakery can also be processed into useful information about consumer patterns. Data mining can be used in finding the consumer pattern in the database. Using Association rule mining, this technique used to find patterns of transaction data using available algorithms such as Apriori, FP-Growth and ECLAT. 

In this research, performance comparison of Apriori, FP-Growth and ECLAT algorithms is performed for association rule mining. The comparison is seen in terms of the number of rules produced and the length of processing time. The data used for this study uses sales data at Toko Roti Ferisa. The study was conducted by conducting ten attempts using different minimum support and minimum confidence values. The experimental results were then compared with Apriori, FP-Growth and ECLAT algorithms.

From the results of tests that have been carried out, the number of association rules generated from the three algorithms has a slight difference. Apriori and FP-Growth algorithms produce the same amount of output, while the ECLAT algorithm has a slightly more amount of output produced. While from the completion time process, ECLAT algorithm gives a shorter time, so in this study, ECLAT algorithm is considered to have a more efficient algorithm.",,,135530,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
194520,,HILDA,Clickbait Detection for News Article in Bahasa Indonesia using Article Informality,,,"Clickbait Detection, Bahasa Indonesia, Support Vector Machine, Machine Learning","Sigit Priyanta, S.Si., M.Kom., Dr; Yunita Sari, S.Kom., M.Sc., Ph.D",2,3,0,2020,1,"Berkembang pesatnya online media membuat para pembuat konten terpacu untuk membuat konten yang menarik dan menggunakan beberapa teknik seperti penggunaan judul yang catchy untuk memikat pembaca membaca artikel beritanya. Namun, halaman ini mungkin berkualitas rendah dan cenderung kurang menayangkan konten yang dijanjikan pada judul. Sehingga dapat memanfaatkan beberapa jenis format pada tajuk utamanya serta struktur yang menarik untuk isinya sehingga tampak kurang formal daripada berita yang ditulis secara profesional. Halaman-halaman ini kemudian disebut clickbait. Tidak dapat dipungkiri bahwa clickbait biasanya terlihat melalui judul-nya, hal ini tidak menghentikan kemungkinan bahwa konten tersebut mungkin menunjukkan beberapa sinyal menuju klasifikasinya.

Penelitian ini bertujuan untuk memanfaatkan konten artikel dalam mendeteksi clickbait pada artikel berita berbahasa Indonesia. Fitur dari artikel beserta fitur dari headline akan diekstrak. Di mana ciri artikel mewakili beberapa ukuran informalitas dan keterbacaan. Pengelompokan fitur akan digunakan untuk mengukur performa kombinasi fitur yang berbeda di dua algoritma pembelajaran mesin, SVM dan Regresi Logistik. Pengelompokan fitur yang digunakan antara lain Headline + Artikel + Kesamaan, Headline + Artikel, dan Headline.

Model terbaik dalam penelitian ini dicapai dengan fitur Headline + Artikel + Kesamaan memanfaatkan SVM yang memperoleh kinerja tertinggi di seluruh metrik yang digunakan. Pencapaian rata-rata akurasi 92,26%, presisi rata-rata 92,73%, recall rata-rata 86,74%, dan rata-rata F-1 Score 89,49%.
","Due to the flourishing growth of online media, content creators are triggered to create content that is attractive and that they employ several techniques such as the usage of catchy headlines to lure the readers to read their news article. However, these pages might be of low-quality and tend to under-deliver the content promised in the headline.  Such that it may utilize some kind of formatting on its headline as well as engaging structures for its content that it appears less formal than professionally written news. These pages are then called clickbait.  It is undeniable that clickbait is usually seen through its headline, this does not cease the possibility that the content might show some signals towards its classification. 

This research aims to utilize the content of the article in detecting clickbait for news article in Bahasa Indonesia. Features from article along with features from headline will be extracted. In which feature from article represent several informality and readability measures. Feature groupings would be used to measure the performance of different feature combinations across two machine learning algorithm, SVM and Logistic Regression. Feature groupings that are used includes Headline + Article + Similarity, Headline + Article, and Headline. 

Best model for this research thus is achieved by Headline + Article + Similarity features utilizing SVM which obtained the highest performance across the metrics used. Achieving average accuracy of 92.26%, average precision of 92.73%, average recall of 86.74%, and average F-1 Score of 89.49%.  ",,,145829,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190426,,HANS PRAMA SETIAWAN,Perbandingan Metode Steganografi Bi-Directional Pixel-Value Differencing pada Citra Grayscale,,,"steganografi, citra grayscale, Pixel-Value Differencing, Bi-directional Pixel-Value Differencing, Vernam cipher, Gray code, kompresi Lempel-Ziv-Welch","Drs. Janoe Hendarto, M.I.Kom.",2,3,0,2020,1,"Steganografi merupakan metode untuk menyembunyikan keberadaan pesan pada berbagai media sehingga pihak ketiga tidak mengetahui adanya komunikasi yang sedang terjadi. Salah satu metode steganografi adalah Pixel-Value Differencing (PVD) Wu dan Tsai (2003) yang menggunakan citra grayscale sebagai media penyisipan pesan. Telah banyak penelitian dilakukan untuk mengembangkan metode steganografi ini. Metode Bi-directional Pixel-Value Differencing merupakan pengembangan metode PVD yang menggunakan enkripsi Vernam cipher dan kompresi Lempel-Ziv-Welch sebelum melakukan penyisipan pesan. Metode tersebut menggunakan metode PVD Wu dan Tsai (2003) yang merupakan metode awal PVD. Oleh karena itu, metode tersebut dapat dikembangkan lagi dengan memodifikasi metode PVD yang digunakan untuk meningkatkan kapasitas penyisipan pesan serta nilai PSNR.
Penelitian ini akan melakukan modifikasi terhadap metode steganografi Bidirectional Pixel-Value Differencing Himakshi dkk. (2014) dengan metode PVD Tseng dan Leng (2013) dan Chang dkk. (2007). Kapasitas penyisipan pesan maksimal serta nilai PSNR yang didapat dari penyisipan berbagai panjang pesan akan dibandingkan pada setiap metode.
Dari hasil penelitian yang diperoleh, kapasitas penyisipan tertinggi didapatkan dengan memodifikasi metode Bi-directional Pixel-Value Differencing dengan metode Chang dkk. (2007), namun PSNR yang dihasilakan lebih rendah. Pada penyisipan pesan dengan panjang yang bervariasi, metode Bi-directional Pixel-Value Differencing yang dimodifikasi dengan metode Tseng dan Leng (2013) menghasilkan nilai PSNR tertiggi.
","Steganography is a method to concealing the existence of a message on various media so that the third parties don't know about the communication that is happening. One of the steganography method is Wu and Tsai's (2003) Pixel Value Differencing, which uses grayscale image as the media to hide the message. Many research has been done to develop this steganography method. Bi-directional Pixel-Value Differencing method is a development of PVD method that uses Vernam cipher encryption and Lempel-Ziv-Welch compression before embedding the message. This method uses Wu and Tsai's (2003) PVD method which is the initial method of PVD. Therefore, this method can be further developed by modifying the PVD method to increase the embedding capacity and PSNR value.
This research will modify Himakshi et al.'s (2014) Bi-directional Pixel-Value Differencing steganography method with Tseng and Leng's (2013) and Chang et al.'s (2007) PVD methods. The maximum embedding capacity and PSNR values obtained from the embedding of various message lengths will be compared in each method.
From the research results, the highest embedding capacity obtained by modifying Bi-directional Pixel-Value Differencing method with Chang et al.'s (2007) method, but resulting in lower PSNR value. In the embedding of various messages lengths, Bi-directional Pixel-Value Differencing method modified with Tseng and Leng's (2013) method produces the highest PSNR value.
",,,141596,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
184029,,RULI SASTRA PUTRI,UNSCENTED KALMAN FILTER DAN RECURRENT NEURAL NETWORK PADA PERAMALAN NILAI TUKAR MATA UANG,,,"Peramalan, Nilai Tukar Mata Uang, Recurrent Neural Network, Unscented Kalman Filter, Gaussian.","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2020,1,"Dalam bidang ekonomi, penting untuk mengetahui seberapa kuat dan stabilnya mata uang suatu negara terhadap negara lain. Banyak upaya untuk mempredisi menggunakan data historis. Salah satu metode yang terkenal adalah Neural Network yang dapat memprediksi sistem non-linear seperti harga tukar mata uang. Selain Neural Network, Kalman Filter telah digunakan untuk training Neural Network.
Kalman Filter (KF) memiliki kekurangan karena ketidakstabilan dan besarnya cost dalam perhitungan matriks Jacobian. Oleh karenanya, telah banyak penelitian yang menyatakan bahwa Unscented Kalman Filter (UKF) lebih baik dari KF dan Extended Kalman Filter dalam kecepatan komputasi dan akurasi karena tidak perlu menghitung matriks Jacobian. Maka dari itu, penelitian ini melakukan prediksi menggunakan model RNN dengan UKF sebagai training bobot dalam prediksi harga tukar mata uang USD ke IDR dari dari 19 Februari 2015 sampai 6 Maret 2019 sebanyak 1055 data. Selain itu, dilakukan perbandingan antara Recurrent Neural Network (RNN) dan Multilayer Perceptron (MLP) yang juga menggunakan UKF untuk training. Sebagai upaya peningkatan akurasi, digunakan percobaan penggunaan parameter UKF dari berbagai penelitian. Selain itu, digunakan window size untuk input layer dan hidden nodes yang telah dianalisis hasilnya.
Hasil yang didapatkan dari UKF-RNN adalah Dstat sebesar 70%, MAE 78,8493, MSE 11984,8, MAPE 0,538269, RMSE 109,475 serta runtime sebesar 2612,12 ms. Runtime tersebut lebih lama dibandingkan dengan MLP-UKF sebesar 1589,22 ms dengan hasil uji Dstat sebesar 68%, RMSE 78,61, MSE 11283, MAPE 0,5379, RMSE 106,234 dan runtime 1589,22 ms.","In economics, it is important to know how strong and stable exchange rates between countries. There are so many methods to predict future rates using time-series data. One of the famous methods is Neural Network. Besides Neural Network, Kalman Filter has been used to train Neural Networks.
Kalman Filter (KF) has some issues such as its instability caused by initial condition to linearization and costly calculation of Jacobian matrices. Therefore, there has been research-proven that the Unscented Kalman Filter (UKF) is superior to KF or Extended Kalman Filter (EKF) variant in terms of speeds and accuracy because UKF has no need to calculate Jacobian. This research use Recurrent Neural Network (RNN) and UKF to train weights in order to predict USD/IDR exchange rates. The data obtained from 9 February 2015 until 6 March 2019 with 1055 values. Besides RNN, Multilayer Perceptron (MLP) was used to predict by choosing the best parameter for UKF's sigma points and analyzing loss by training the number of window size for input layer and the number of hidden nodes.
The results UKF-RNN obtained was the highest Dstat 70%, MAE 78.8493, MSE 11984.8, MAPE 0.538269, RMSE 109.475 and runtime 2612.12 ms by 5 window size and 7 hidden nodes with 100 epoch. Though MLP-UKF was superior only in the speed of computation by 1589.22 ms because MLP has no recurrent nodes for the hidden layer, its Dstat 68%, RMSE 78.61, MSE 11283, MAPE 0.5379, RMSE 106.234 and runtime 1589.22 ms.",,,134912,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190687,,HERY CIAPUTRA,Optimasi Arsitektur Convolutional Neural Network Secara Otomatis Menggunakan Algoritma Genetika Pada Studi Kasus Pengenalan Ucapan Penderita Dysarthria,,,"algoritma genetika, convolutional neural network, dysarthria","Afiahayati, S. Kom., M. Kom., Ph. D",2,3,0,2020,1,"Dysarthria merupakan gangguan berucap yang disebabkan oleh permasalahan motorik. Penderita dysarthria umumnya juga kesulitan dalam menggerakkan otot tubuh lainnya, sehingga dibutuhkan bantuan manusia atau teknologi untuk berkomunikasi dan beraktivitas. Sistem pengenalan ucapan lebih dipilih dibandingkan teknologi lain yang membutuhkan menggerakkan otot tubuh lainnya untuk berkomunikasi seperti penggunaan keyboard.
Sistem pengenalan ucapan dapat dibangun memanfaatkan algoritma-algoritma klasifikasi salah satunya adalah Convolutional Neural Network (CNN). CNN adalah salah satu algoritma pembelajaran mesin yang menunjukkan performa yang unggul untuk permasalahan visi komputer. CNN juga menunjukkan performa yang baik untuk pengenalan suara dengan menggunakan representasi suara dalam bentuk spectogram. Tetapi arsitektur CNN yang baik membutuhkan keahlian dalam bidang datanya dan juga CNN-nya. Permasalahan ini dapat diatasi dengan menggunakan algoritma metaheuristik yang mencari arsitektur CNN paling optimal dari suatu ruang pencarian. Pada penelitian ini, algoritma genetika digunakan untuk mencari arsitektur CNN yang paling mendekati optimal.
Hybrid antara CNN dan algoritma genetika ini menghasilkan akurasi klasifikasi yang lebih baik yaitu rata-rata 96,19% dibandingkan dengan penelitian sebelumnya yang merancang CNN secara manual yaitu dengan rata-rata 88,57%.","Dysarthria is a speech disorder caused by motoric imparment. Dysarthric speakers generally also suffer from difficulty of moving other body muscles, therefore human or technology assistance is needed to communicate and doing activities. Speech recognition is preferred over other assisting technologies that require moving other body muscles to communicate such as keyboard.
Automatic speech recognition can be build using classification algorithm, one of which is Convolutional Neural Network (CNN). CNN is one of machine learning techniques that shows superior performance in the computer vision field. CNN also shows a good performance in speech recognition using spectogram as the speech representation. However, a good CNN architecture requires experties in the data and CNN designing. This problem can be overcome by using metaheuristic algorithm that searches most optimal CNN architecture from a search space. In this research, genetic algorithm is used to search for CNN architecture that is closest to its optimal.
This hybrid between CNN and genetic algorithm shows a better classification accuracy averaging in 96,19% compared to prior research that manually design the CNN averaging in 88,57%.",,,141884,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186081,,FARDIAN SEFREZA A,QUALITY OF SERVICE (QoS) PERFORMANCE ANALYSIS ON DYNAMIC AND STATIC NETWORK FOR SENSITIVE APPLICATIONS,,,"Quality of Service, Hierarchical Token Bucket, Bandwidth Management ","Dr.techn. Ahmad Ashari, M.Kom.",2,3,0,2020,1,"Saat ini semua kegiatan terhubung ke internet. Dalam jaringan seperti itu banyak parameter dapat mempengaruhi penggunaan internet. Parameter ini adalah Throughput, Delat, Jitter, dan Packet Lost. Parameter ini termasuk dalam QOS (Kualitas Layanan). QOS berpengaruh sebagai pengalaman dalam menggunakan internet. Bucket Tok Hirarkis dapat membagi bandwidth tempat HTB dapat memengaruhi parameter dalam QOS.
Dalam penelitian ini, tujuannya adalah untuk mengimplementasikan HTB dan menganalisis kinerja QOS. Penelitian ini dilakukan dengan mengumpulkan data dengan menguji dan mengukur nilai-nilai parameter QOS. Data-data ini dikumpulkan menggunakan Mikrotik sebagai pembagi bandwidth.
Hasil dari penelitian ini adalah melihat dan menganalisis kinerja QOS. Hasil tes menunjukkan bahwa untuk Throughput, Delay, Jitter, Packet Lost sangat baik atau 4 berdasarkan pada indeks TIPHON.","Nowadays all activities are connected to the internet. In such a network many parameters can affect internet usage. These parameters are Throughput, Delat, Jitter, and Packet Lost. These parameters are included in the QOS (Quality of Service). The QOS affects as experience in using the internet. Hierarchical Token Buckets can divide the bandwidth where the HTB can affect the parameters in QOS.
	In this research, the goals are to implement HTB and analyze the performance of QOS. This study was conducted by collecting data by testing and measuring the values of the QOS parameters. These data are collected using Mikrotik as a bandwidth divider.
	The results of this study are looking at and analyzing the performance of QOS. The results of the test show that for Throughput, Delay, Jitter, Packet Lost are very good or 4 based on the TIPHON index.
",,,138121,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
192491,,BASKARA,Sistem Pendeteksi Halangan Pada Lintasan Kendaraan Menggunakan Semantic Segmentation dan Path Planning,,,"Semantic-segmentation, Path-planning, Approximate-cell-decomposition, BFS","Dr. Andi Dharmawan, S.Si., M.Cs.",2,3,0,2020,1,"Pada kondisi darurat yang mendesak kendaraan seperti ambulans ataupun mobil polisi harus memilih rute jalan yang optimal untuk meminimalkan waktu untuk menuju lokasi kejadian. Penggunaan drone memungkinkan pengambilan data mengenai kondisi lintasan secara real-time dari lokasi yang dinamis. Namun dalam penerapannya pengawasan jalan menggunakan drone masih dilakukan secara manual di mana diperlukan operator untuk melihat apakah ada halangan pada jalan.

Penelitian ini berfokus untuk mengembangkan suatu sistem yang menggunakan semantic segmentation berbasis CNN dan path planning berbasis approximate cell decomposition dan BFS untuk mendeteksi adanya halangan pada jalan secara otomatis pada citra yang diambil menggunakan drone. Pada penelitian digunakan dataset berupa citra jalan yang diambil dari berbagai lokasi di Yogyakarta menggunakan drone dari sudut pandang top-down.

Dari penelitian ini dihasilkan sebuah sistem yang dapat mendeteksi halangan pada jalan dengan menggunakan input berupa citra jalan dengan output berupa kondisi jalan apakah terhalang atau tidak. Sistem yang telah dibuat memiliki nilai performa accuracy sebesar 0,920; precision sebesar 0,989; recall sebesar 0,923; dan F1-score sebesar 0,954.","In an urgent condition, a vehicle such as an ambulance or police car must choose the optimal road route to minimize the time to get to the scene. The use of drones allows real-time data retrieval of road conditions from dynamic locations. However, in its application, road surveillance using drones is still done manually where the operator is required to see if there are any obstacles on the road.

This research focuses on developing a system that uses CNN-based semantic segmentation and path planning based on approximate cell decomposition and BFS to automatically detect roadblocks on images taken using drones. This research used a dataset in the form of road images taken from various locations in Yogyakarta using drones from a top-down perspective.

This research resulted in a system that can detect obstacles on the road by using an input in the form of a road image with an output in the form of road conditions whether blocked or not. The system that has been created has an accuracy performance value of 0.920, precision of 0.989, recall of 0.923, and F1-score of 0.954.",,,143719,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190444,,AHMAD FAUZI WIBOWO,Model Klasifikasi Hasil Terapi Antibiotik untuk Pasien Pneumonia menggunakan Algoritma CART Decision Tree,,,"Decision Tree, CART, Farmakokinetik, Antibiotik, Pembelajaran Mesin, Pneumonia","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2020,1,"Indonesia sebagai negara tropis sangat rentan terjadinya infeksi berat
yang mengancam jiwa pasien, salah satunya infeksi pneumonia. Terapi antibiotik
menjadi hal yang menentukan dalam terapi pneumonia mengingat kondisi
pasien yang biasanya memiliki penyakit penyerta lainnya yang akan mempengaruhi
keberhasilan terapi antibiotik. Terapi pneumonia sayangnya masih
mengalami kendala karena tidak adanya Therapeutic Drug Monitoring (TDM)
untuk memastikan kadar obat di dalam badan pasien.
TDM merupakan tahapan yang penting karena terdapat beberapa obat
yang memili- ki kisaran konsentrasi terapeutik yang sangat sempit sehingga sangat
memung- kinkan munculnya efek toksik ataupun kekurangan dosis yang bisa
mengaki- batkan pengobatan yang tidak efektif. Penelitian ini akan membuat
sebuah model dengan Classification and Regression Tree (CART) yang bertujuan
untuk memperkirakan hasil terapi pengobatan antibiotik khususnya golongan
Fluorokuinolon yaitu Siprofloksasin dan Levofloksasin pada pasien pneumonia
berdasarkan hasil perhitungan parameter-parameter farmakokinetik yang ada.
Penelitian ini juga akan mem- bandingkan antara model yang menggunakan
k-fold cross validation dan yang tidak menggunakannya.
Hasil penelitian menunjukkan bahwa model bekerja lebih baik ketika melalui
k-fold cross validation dengan memiliki nilai akurasi sebesar 78% dalam melakukan
prediksi. Sementara itu, model yang tidak menggunakan k-fold cross
validation hanya memperoleh nilai akurasi sebesar 72%. Penelitian ini juga
mengukur nilai presisi, recall, dan F1-Score dalam memprediksi kondisi pasien
yang membaik dan yang belum membaik, dimana rata-rata dari masing-masing
nilai tersebut masih lebih tinggi pada model yang menggunakan k-fold cross
validation.","Indonesia as a tropical country is very vulnerable to severe life-threatening
infections in patients, one of which is pneumonia infection. In the case of pneumonia
therapy, antibiotic therapy becomes a crucial factor considering the fact
that patients usually have other comorbidities that will affect the success of antibiotic
therapy. Unfortunately, pneumonia therapy is still experiencing plenty
of drawbacks due to the absence of Therapeutic Drug Monitoring (TDM) to
ensure drug levels in the patient&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s body.
In general, TDM is important because there are several drugs that have
a very narrow range of therapeutic concentrations which makes the emergence
of toxic effects or underdose that can result in ineffective treatment possible.
This study will attempt to create a model with Classification and Regression
Test (CART) that aims to estimate the results of antibiotic treatment therapy,
especially the Fluoroquinolone group, namely Ciprofloxacine and Levofloxacine
in pneumonia patients based on the results of the calculation of existing pharmacokinetics
parameters. A comparative study will also be carried out between
the model that goes through the process of k-fold cross validation and the one
that does not.
Based on the research that has been carried out, it can be concluded that
model works better with k-fold cross validation, which is proven by the derived
value of accuracy of 78 %. Meanwhile, the model that did not use k-fold cross
validation only obtained an accuracy value of 72 %. This study also measured
the value of precision, recall, and F1-Score in predicting the condition of those
patients who have improved and those who have not, in which the average score
of each value was higher in the model with k-fold cross-validation.",,,141663,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
194293,,Ilham Fathoni,Identifikasi dan Klasifikasi Argumen pada Twitter,,,"Klasifikasi Teks, Media Sosial, Argumen, Opini, Random Forest, Logistic Regression","Moh. Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2020,1,"Melalui media sosial seperti Twitter, semua pengguna dapat mengutarakan
pendapatnya. Fenomena ini dapat memantik adu argumen antar pengguna.
Pengklasifikasian argumen dapat dijadikan sebagai indikator diskusi yang
konstruktif dan pencegahan penyebaran disinformasi.
Pada penelitian ini dilakukan klasifikasi untuk membedakan argumen dari
non-argumen dan argumen faktual dari opini. Data yang digunakan berupa tweet
berbahasa Indonesia. Klasifikasi dilakukan menggunakan metode Random Forest
dan Logistic Regression. Ekstraksi fitur dilakukan dengan metode TF-IDF dengan
fitur unigram dan uni-bigram.
Hasil terbaik klasifikasi tweet argumen/non-argumen didapat dengan
menggunakan metode Random Forest dengan f 1 score sebesar 0,77. Sedangkan,
hasil terbaik klasifikasi tweet argumen faktual/opini didapat dengan menggunakan
metode Logistic Regression dengan f 1 score sebesar 0,83.","Through social media such as Twitter, everyone can express their opinion.
This phenomenon can trigger heated arguments between user. Arguments detection
can be used as an indicator of constructive discussions and to prevent the spread of
disinformation.
In this study, classifications were carried out to distinguish arguments from
non-arguments and factual arguments from opinions. Indonesian tweets are used as
data source. The classifications were done using Random Forest and Logistic
Regression method. Feature extraction were done using TF-IDF with unigram and
uni-bigram features.
The best result of arguments/non-arguments classification was obtained
using Random Forest method with f 1 score of 0,77. On the other hand, the best
result of factual arguments/opinions classification was obtained using Logistic
Regression method with f 1 score of 0,83.",,,145551,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190455,,FIKRI NURQAHHARI P,Extreme Learning Machine Prediction Model on Airbnb Base Price,,,"Airbnb, Extreme Learning Machine, Price Prediction","Dr. Agus Sihabuddin, S.Si., M.Kom.",2,3,0,2020,1,"This paper describes how Extreme Learning Machine is implemented as a prediction model to determine the base price of Airbnb properties. Extreme Learning Machine offers many advantages, such as fast learning speed, good generalization performance, and high prediction accuracy.
The steps in the extreme learning machine method, in general, are set hidden neuron number, randomly assign input weight &amp; hidden layer biases, calculate the output layer, the whole learning process completed through one mathematical change without iteration. The performance of the model will be measured using mean absolute percentage error (MAPE), mean squared error (MSE), and root mean squared error (RMSE). 
The ELM model will be tested with the Airbnb dataset in the London area taken from the Insideairbnb.com site with 79671 numbers of data with 21 features chosen. MAPE, MSE, RMSE are chosen for the evaluation method. From the experiments, the model show as the number of neurons increases, the link between the input and output layers would consequently increase. This leads to a better quality of learning produced from and the model generates MAPE value of 3.06% MSE value of 0. 038, and RMSE value of 0.239.","This paper describes how Extreme Learning Machine is implemented as a prediction model to determine the base price of Airbnb properties. Extreme Learning Machine offers many advantages, such as fast learning speed, good generalization performance, and high prediction accuracy.
The steps in the extreme learning machine method, in general, are set hidden neuron number, randomly assign input weight &amp; hidden layer biases, calculate the output layer, the whole learning process completed through one mathematical change without iteration. The performance of the model will be measured using mean absolute percentage error (MAPE), mean squared error (MSE), and root mean squared error (RMSE). 
The ELM model will be tested with the Airbnb dataset in the London area taken from the Insideairbnb.com site with 79671 numbers of data with 21 features chosen. MAPE, MSE, RMSE are chosen for the evaluation method. From the experiments, the model show as the number of neurons increases, the link between the input and output layers would consequently increase. This leads to a better quality of learning produced from and the model generates MAPE value of 3.06% MSE value of 0. 038, and RMSE value of 0.239.",,,141630,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190712,,JEHEZKIEL MARCIEL M,POPULAR MUSIC COMPOSITION USING LSTM NEURAL NETWORKS,,,"LSTM, RNN, Popular Music, Music Composition, MIDI, Machine Learning, evolution","Nur Rokhman, S.Si., M.Kom.,Dr",2,3,0,2020,1,"Komposisi Musik sudah berada selama ratusan tahun dan terus berevolusi. Begitu juga
dengan teknologi yang sudah sangat maju di zaman ini. Teknologi memberi kemampuan untuk
berinovasi, dan melalui inovasi ini pun komposisi atau pembuatan musik juga berevolusi.
Kecerdasan buatan atau &quot;Artificial Intelligence&quot; adalah sebuah batu loncatan baru di era ini,
dimana sebuah mesin atau teknologi pada dasarnya dapat mereplikasi kemampuan otak manusia,
yang dinamakan &quot;Neural Network&quot; atau Jaringan Syaraf.
Penelitian ini menggunakan Long Short Term Memory untuk mengkomposisi musik Pop,
dan LSTM sendiri adalah sebuah tipe jaringan syaraf buatan yang sesuai untuk mempelajari
dependensi temporal khususnya dalam pembuatan dan komposisi musik.
Dengan jaringan tidak terlalu dalam, dapat terbukti bahwa LSTM mampu memproduksi
musik yang sebanding dengan musik yang dikomposisi oleh manusia, dan dibuktikan juga oleh
survei yang dibuat dengan Turing Test sebagian.","Music composition has been around for a very long time, and so has the evolution of technology. It allows humans to create new exciting things, some even in the ways that the old timers would not be able to comprehend. Parallel to the evolution of music comes the evolution of technology. With technology, we are able to create and marvel at the capacity of what the human mind can do, and even simulating how the human mind works.  Right now, we are in the age of Artificial Intelligence, with Neural Networks being the centerpiece of the field. 
This research implements Long Short Term Memory, a type of Recurrent Neural Network that - simply put - is able to learn for a long time without forgetting its earliest memories on what it's learning. As it is a sequence-based algorithm, this type of network is very suitable for creating music. Despite the relatively shallow network used for this model, previous researches have proven that LSTM is able to perform very well, especially when creating sequences of music.
Using a vanilla LSTM and at the end, this research tries to create music based on Pop music data. With this network, the music created from this model was surprisingly good. In fact, during the Partial Turing Test conducted for the evaluation of the output music of this research's model, 70% of the surveyed participants deemed the produced music as man made. Thus, it proves that LSTM really does a great job in creating music.
",,,141922,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
190460,,GILANG BASKARA P A,SISTEM PENDUKUNG KEPUTUSAN UNTUK PENILAIAN PERFORMA RUTE BUS RAPID TRANSIT (BRT) MENGGUNAKAN METODE AHP-TOPSIS (STUDI KASUS: TRANSJOGJA),,,"TransJogja, performa rute, sistem pendukung keputusan, AHP, TOPSIS, desktop.","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.",2,3,0,2020,1,"TransJogja merupakan salah satu sistem Bus Rapid Transit (BRT) di DIY. Dalam operasionalnya pihak manejemen TransJogja dituntu harus berhati-hati dalam penentuan kebijakan karena tidak hanya Transjogja saja yang akan dirugikan jika terjadi kesalahan pada saat mengambil kebijakan tetapi juga masyarakat yang menggunakan TransJogja. Namun walaupun harus berhati-hati, pihak manajemen harus dapat menentukan kebijakan dengan cepat dan tepat untuk mencegah kerugian diantara kedua belah pihak. 
Penelitian ini dilakukan dengan membangun Sistem Pendukung Keputusan (SPK) untuk membantu penilaian performa rute TransJogja dengan menggunakan metode AHP dan TOPSIS. Data yang digunakan merupakan data laporan operasional TransJogja, kemudian diolah berdasarkan ketentuan-ketentuan yang sudah ada. Sistem dibangun pada aplikasi desktop dengan harapan dapat digunakan untuk memudahkan proses penilaian.
Keluaran dari sistem berupa nilai performa setiap rute TransJogja. Hasil pengujian didapatkan bahwa sistem mampu memberikan penilaian yang hampir mendekati analisis dari pakar. Sistem juga memiliki tingkat fleksibilitas pada bagian konfigurasi kriteria, dan pemilihan laporan yang akan dijadikan bahan penilaian. Berdasarkan hasil pengujian dapat disimpulkan bahwa SPK dengan metode AHP dan TOPSIS dapat digunakan untuk memberikan penilaian performa rute TransJogja.
","TransJogja is one of the Bus Rapid Transit (BRT) systems in DIY. In its operations, TransJogja&acirc;€™s management is obliged to be careful in determining the policy because not only Transjogja will be harmed if something goes wrong when making policies but also the people who use TransJogja. However, despite being careful, management must be able to determine policies quickly and precisely to prevent losses between the two parties. 
This research was conducted by building a Decision Support System (SPK) to help assess the performance of the TransJogja route using the AHP and TOPSIS methods. The data used is TransJogja&acirc;€™s operational report data, then processed based on existing provisions. The system is built on the desktop application in the hope that it can be used to facilitate the assessment process. 
The output of the system is the performance value of each TransJogja route. The test results found that the system is able to provide an assessment that is almost close to the analysis of experts. The system also has a degree of flexibility in the criteria configuration section, and the selection of reports will be used as evaluation material. Based on the test results it can be concluded that the SPK with AHP and TOPSIS methods can be used to provide an assessment of the performance of the TransJogja route.
",,,141832,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183550,,HIZKIA LAKSITO,SISTEM PENDUKUNG KEPUTUSAN DENGAN METODE SMARTER (SIMPLE MULTI-ATTRIBUTE RECOMMENDATION TECHNIQUE EXPLOITING RANK) UNTUK PENGISIAN JABATAN STRUKTURAL: STUDI KASUS DI BADAN KEPEGAWAIAN DAN PENGEMBANGAN SUMBER DAYA APARATUR KOTA BOGOR,,,"Pengisian jabatan struktural, sistem pendukung keputusan, SMARTER","Drs. Retantyo Wardoyo, M.Sc., Ph.D.",2,3,0,2020,1,"Berdasarkan Peraturan Pemerintah Nomor 100 Tahun 2000, Baperjakat beserta BKPSDA berwenang untuk menjamin kualitas dan obyektifitas dalam memberikan pertimbangan kepada Kepala Daerah dalam melakukan promosi dan mutasi pejabat. BKPSDA Kota Bogor telah mengembangkan Sistem Informasi Manajemen Kepegawaian yang mengelola data kepegawaian Pemerintah Kota Bogor, tetapi sistem ini memiliki kekurangan dalam mendukung keputusan yaitu pengguna perlu menelusuri data pegawai secara manual, sementara jumlah data pegawai bisa mencapai ribuan. Selain itu, dalam Peraturan Pemerintah Nomor 100 Tahun 2000 belum memiliki aturan secara khusus terkait bagaimana cara mutasi dan promosi pegawai secara objektif dan tidak ada kejelasan mengenai bagaimana menentukan prioritas suatu kriteria dengan kriteria lainnya untuk menentukan kelayakan calon pejabat dalam mengisi jabatan struktural.
Oleh karena itu, diperlukan suatu sistem yang dapat membantu Baperjakat dalam memberikan hasil berupa rekomendasi calon pejabat yang terbaik, salah satunya yaitu sistem pendukung keputusan (SPK). SPK yang dikembangkan pada penelitian ini menggunakan metode SMARTER. Pengambil keputusan dapat menentukan nilai bobot kriteria berdasarkan urutan kepentingan kriteria, lalu hasil pembobotan tersebut dapat digunakan untuk menghasilkan daftar pemeringkatan alternatif. Hasil penelitian ini berupa aplikasi yang dapat menyeleksi dan menilai calon pejabat dengan menggunakan enam kriteria. Hasil pengujian berupa urutan calon pejabat pada promosi dan mutasi jabatan eselon IIB, IIIA, dan IIIB yang dapat memberi pertimbangan bagi pembuat keputusan untuk mengisi jabatan sturktural.
","Based on Peraturan Pemerintah Nomor 100 Tahun 2000, Baperjakat and BKPSDA are authorized to guarantee the quality and objectivity in giving consideration to the Regional Head in conducting promotions and mutations of officials. BKPSDA of Bogor City has developed an Employment Management Information System (SIMPEG) that manages the employee data of Bogor City Government, but the system has weaknesses in supporting decisions that users need to manually search thousands of employee data. In addition, Peraturan Pemerintah Nomor 100 Tahun 2000 does not yet have rules specifically related to how to transfer and promote employee objectively and there is no clarity on how to determine the priority of a criterion with other criteria to determine the worthiness of prospective officials in filling structural positions.
Therefore, we need a system that can help Baperjakat in providing results in the form of recommendations from the best candidates, such as decision support system (DSS). In this study uses the SMARTER method to develop DSS. The decision maker can determine the weighting of criteria based on the order of importance of the criteria, then the results of the weighting can be used to produce an alternative ranking list. The results of this study are applications that can select and assess prospective officials using six criteria. The test results are in the form of candidates for officials in the promotion and transfer of positions of echelon IIB, IIIA, and IIIB that can give consideration to decision makers to fill structural positions.
",,,134434,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
186622,,AHMAD SYARIFUDDIN R,KLASIFIKASI KEPADATAN LALU LINTAS BERBASIS TEXTURE DESCRIPTOR,,,"klasifikasi citra, kepadatan lalu lintas, texture descriptor, convolutional neural network","Wahyono, Ph.D.",2,3,0,2020,1,"Penelitian mengenai klasifikasi kepadatan lalu lintas masih terus berjalan dan berkembang hingga saat ini. Metode yang paling sering digunakan adalah dengan melakukan image processing pada citra lalu lintas. Metode image processing yang umum dilakukan adalah dengan background subtraction dan edge detection.
Texture descriptor belum banyak digunakan dalam kasus klasifikasi kepadatan lalu lintas. Sehingga pada penelitian ini dilakukan klasifikasi kepadatan lalu lintas berbasis texture descriptor menggunakan arsitektur CNN dengan tujuan untuk membandingkan akurasi, waktu pemrosesan, dan ukuran citra setiap texture descriptor. Texture descriptor yang akan dibandingkan adalah RGB, grayscale, image gradient, dan local binary pattern.","Research about traffic density classification is still developing until now. Most common methods used in this field are image processing, which commonly used are background subtraction and edge detection.
This thesis proposes a traffic density classification based on texture descriptor by using CNN architecture. The purpose of this thesis is to compare the accuracy, processing time, and image size of each texture descriptor dataset. The texture descriptors that will be compared are RGB , grayscale, image gradient, and local binary pattern.",,,137732,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
195327,,SEPTIAN WIJAYA,Komposisi Potongan Musik Menggunakan Gated Recurrent Unit dan Notewise Encoding pada Berkas Musical Instrument Digital Interface ,,,"Komposisi musik, GRU, LSTM, notewise encoding, MIDI","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2020,1,"Seiring berkembangnya teknologi, manusia kini dapat menciptakan berbagai hal dengan bantuan teknologi digital, salah satunya hiburan berupa musik. Musik kini dapat diciptakan menggunakan proses pembelajaran mesin. Salah satu teknik yang umum digunakan adalah model model neural network dengan sel recurrent neural network (RNN) dan long short-term memory (LSTM) dengan input yang bervariasi. Terdapat penelitian baru yang mencoba merepresentasikan input dengan pendekatan natural language processing (NLP) yang diberi nama notewise encoding. Ditemukan juga penelitian yang menunjukkan performa varian RNN bernama gated recurrent unit (GRU) memiliki performa yang sebanding dengan LSTM yang umum digunakan namun dengan waktu latih yang lebih rendah.
Pada penelitian ini dibuat model menggunakan representasi input notewise encoding terhadap model neural network yang menggunakan sel GRU. Model dilatih menggunakan dataset berkas MIDI yang berisikan melodi piano ciptaan komposer klasik. Model yang telah dilatih dibandingkan antara varian GRU dengan LSTM dan musik ciptaan model terbaik diujikan pada manusia untuk menentukan seberapa baik model dalam menciptakan musik.
Berdasarkan penelitian yang dilakukan ditemukan bahwa model GRU merupakan model dengan evaluasi kuantitatif terbaik denganwaktu latih sebesar 267.9972s/epoch dan loss pada data tes sebesar 1.22235, dibandingkan dengan model LSTM memiliki waktu latih sebesar 313.9038s/epoch dan loss pada data tes sebesar 1.21709. Berdasarkan hasil proses penyaringan klip, 34 dari 100 klip komposisi model berhasil menyerupai komposisi manusia. Hasil tes &quot;identify the human&quot; menunjukkan bahwa 68% klip musik berhasil diidentifikasi oleh responden awam, dan 84% oleh responden ahli.
","Along with the development of technology, humans can now create various things with digital technology, one of which is entertainment in the form of music. Music can now be created using the machine learning process. One of the techniques commonly used is the neural network model with a recurrent neural network (RNN) cells and long short-term memory (LSTM) with various inputs. New research tries to represent input with a natural language processing (NLP) approach named notewise encoding. Research has also found that the RNN variant's performance called the gated recurrent unit (GRU), has a performance comparable to the commonly used LSTM but with a lower training time.
In this study, a model was created using an input representation notewise encoding to a neural network model using GRU cells. The models are trained using a dataset of MIDI files containing piano melodies created by classical composers. The trained model was compared between the GRU variants with the LSTM, and the best model-created music was tested on humans to determine how well the model was at creating music.
Based on the research conducted, it was found that the GRU model was a model with the best quantitative evaluation with a training time of 267.9972s / epoch and a loss of 1.22235 in the test data, compared to the LSTM model, which had a training time of 313.9038s / epoch and a loss of 1.21709 test data. Based on the clip screening process results, 34 of the 100 clips of the model composition managed to resemble human compositions. The &quot;identify the human&quot; test results showed that lay respondents identified 68% of music clips and 84% by expert respondents.
",,,146589,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177409,,IKHSAN BUDIYANTO,IDENTIFIKASI TWEET KEMACETAN LALU LINTAS MENGGUNAKAN ALGORITMA SUPPORT VECTOR MACHINE DAN K-NEAREST NEIGHBORS,,,"macet,identifikasi,support vector machine,k-nearest neighbors","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2019,1,"Salah satu permasalahan yang sering terjadi dan menjadi topik pembicaraan pengguna Twitter di Indonesia adalah kemacetan lalu lintas. Tak heran karena berdasarkan salah satu hasil survei Indonesia menempati urutan ke-11 sebagai negara dengan indeks kemacetan tertinggi di dunia. Dampak negatif yang ditimbulkan dari kemacetan lalu lintas ini dapat meliputi berbagai aspek, mulai dari kerugian waktu, ekonomi, bahkan hingga masalah psikologi. Oleh karena itu, perlu dikembangkan sebuah metode bagaimana mengidentifikasi tweet kemacetan lalu lintas yang diposting melalui Twitter sehingga mampu membedakan tweet tersebut menginformasikan kejadian kemacetan lalu lintas atau bukan.
Pada penelitian ini, proses identifikasi tweet kemacetan lalu lintas menggunakan algoritma klasifikasi Support Vector Machine dan K-Nearest Neighbors. Data yang digunakan merupakan tweet yang mengandung kata kunci macet dan dikategorikan kedalam 3 kelas yaitu macet lalu lintas sebagai kejadian, macet lalu lintas bukan sebagai kejadian dan macet bukan dalam konteks lalu lintas. Jumlah tweet yang digunakan sebagai data training sebanyak 1709 dan 419 sebagai data testing yang diambil pada 1 Juni - 31 Juli 2018.
Penelitian dilakukan dengan membandingkan algoritma Support Vector  Machine dan K-Nearest Neighbors dalam mengidentifikasi tweet kemacetan lalu lintas. Evaluasi model dilakukan dengan menggunakan metode 10-fold cross validation. Dari hasil pengujian didapatkan nilai akurasi 97.14% untuk algoritma Support Vector Machine dengan parameter  C = 1 dan 92.36% untuk algoritma K-Nearest Neighbors dengan parameter K = 50.","One of the most recurring and most discussed topic by Indonesian Twitter users is traffic jam. It is not surprising that Indonesia sits on the 11th place in countries with worst traffic congestion in the world, based on a survey. The negative impacts from traffic jam comprises certain aspects, from time and economical loss, even psychological issues. Therefore, a method to identify traffic jam tweets generated on Twitter is necessary to be developed, to distinguish between tweets that are informing traffic jam and tweets that are not.
In this research, Support Vector Machine and K-Nearest Neighbors classification algorithm are used to identify tweets corresponding to traffic jam. Data used are tweets that possess macet keyword, and categorized in 3 classes: traffic jam as an occurrence, traffic jam in any context except as an occurrence, and congestion that is not traffic-related. Tweets used as data training is 1709 tweets, and 419 tweets used as data testing, extracted between 1 June and 31 July 2018.
The research compares Support Vector Machine algorithm with K-Nearest Neighbors in identifying traffic jam tweets. Model evaluation is done with 10-fold cross validation method. The testing result comes with 97.14% accuracy for Support Vector Machine algorithm with C = 1 parameter, and 92.36% accuracy for K-Nearest Neighbors algorithm with K = 50 parameter.",,,128437,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
174338,,IRVAN NASHER ALIMI,Individual Personality Profiling Using Combination of Twitter Data and Psychological Test,,,"Twitter, Text Classification, Random Forest, Briggs-Myers Personality Test, Keirsey Temperament Sorter, WordCloud",Mardhani Riasetiawan ,2,3,0,2019,1,"In the fields of Psychology, the main concern is to study a person&acirc;€™s mind which relates to their personality and profile. A post on social media can either directly or indirectly reflect someone&acirc;€™s personality. To gather the data required for these profiling methods, the Information Technology can contribute. The research itself will be focused on data mining and data classification with the purpose of connecting both the psychology sector and the IT sector by profiling a group of individual with data mining. The test group will consist of 50% of the total subjects that are willing to give information on their Twitter. This research goal is to create a new data set for Psychologist to analyze. Relation between data or any psychological result will not be analyzed. This research utilizes Tweepy for Twitter data collection, Briggs-Myers Test for direct Psychological evaluation process, Keirsey Temperament Sorter for classification categories, several feature extraction models (Bag-of-Words, TF-IDF, Word2Vec and Doc2Vec), WordCloud for data representation and Random Forest classifier for machine learning process. For calculation process and data showcase, Microsoft Excel is used. The result of this research are a new data set consisting of subject&acirc;€™s Briggs-Myers Personality Type, Twitter Classification Accuracy, Tweet Percentage and WordCloud of Tweet according to Keirsey Temperament Sorter categories.","In the fields of Psychology, the main concern is to study a person&acirc;€™s mind which relates to their personality and profile. A post on social media can either directly or indirectly reflect someone&acirc;€™s personality. To gather the data required for these profiling methods, the Information Technology can contribute. The research itself will be focused on data mining and data classification with the purpose of connecting both the psychology sector and the IT sector by profiling a group of individual with data mining. The test group will consist of 50% of the total subjects that are willing to give information on their Twitter. This research goal is to create a new data set for Psychologist to analyze. Relation between data or any psychological result will not be analyzed. This research utilizes Tweepy for Twitter data collection, Briggs-Myers Test for direct Psychological evaluation process, Keirsey Temperament Sorter for classification categories, several feature extraction models (Bag-of-Words, TF-IDF, Word2Vec and Doc2Vec), WordCloud for data representation and Random Forest classifier for machine learning process. For calculation process and data showcase, Microsoft Excel is used. The result of this research are a new data set consisting of subject&acirc;€™s Briggs-Myers Personality Type, Twitter Classification Accuracy, Tweet Percentage and WordCloud of Tweet according to Keirsey Temperament Sorter categories.",,,125204,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175363,,RIZKI KURNIAWAN,ALGORITME DECISION TREE UNTUK PREDIKSI PENGEMBALIAN ATAU RETUR DALAM BELANJA ONLINE,,,"retur, toko online, decision tree ID 3, decision tree C4.5, decision tree CART","Aina Musdholifah S.Kom., M.Kom., Ph.D.",2,3,0,2019,1,"Belanja melalui toko online saat ini merupakan kegiatan belanja yang cukup populer. Kegiatan ini bisa dilakukan kapan saja dan dimana saja. Konsumen tidak memperlukan banyak waktu untuk berbelanja. Konsumen cukup mengunjungi website toko online dan membeli barang. Hal ini membuat belanja di toko online menjadi populer. Permasalahan utama pada belanja melalui toko online adalah retur. Retur pada belanja online tidak mudah. Hal ini membutuhkan waktu dan biaya yang mahal.
Algoritme decision tree dapat digunakan untuk memprediksi retur. Pada penelitian ini tiga jenis algoritme decision tree digunakan untuk memprediksi retur. Algoritme yang digunakan yaitu decision tree ID 3, decision tree C4.5, dan decision tree CART. Pada penelitian ini data yang digunakan untuk memprediksi merupakan data penjualan di tahun 2012 &acirc;€“ 2013. Ada dua data set yang digunakan. Data set pertama memiliki outlier dan missing value. Data set kedua tidak memiliki outlier dan missing value. Untuk data set pertama akurasi dari decision tree ID3 63,65%, C4.5 63,65%, dan CART  63,55%. Untuk data set kedua akurasi dari decision tree ID3 60,30%, C4.5 60,31%, dan CART  60,38%.","Shopping through an online shop are common these days. This activity can be done anytime and anywhere. Customer spend less time compared to conventional shopping. Customer visit online shop website and purchase the item. This convenience make online shopping popular. Main problem of online shopping is return goods. Return goods is not easy as conventional shopping.  This requires time and costly.
Decision tree algorithm can predict return goods. Three types of decision are used to predict return goods. The algorithm used in this research are decision tree ID3, decision tree C4.5, and decision tree CART. Data used in this research are sales from 2012 &acirc;€“ 2013. First data set has missing value and outlier. Second data set does not have missing value and outlier. For first data set accuracy decision tree ID 3 has 63,65%, C4.5 63,65%, and CART  63,55%. For second data set accuracy decision tree ID3 60,30%, C4.5 60,31%, and CART  60,38%.",,,126042,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175619,,NINDITO W BAGASKARA,Classifying Pro-vaccine and Anti-vaccine Posts in Social Media Using The TF-IDF Weighted Naive Bayes Classifier,,,"Naive Bayes, Term Frequency-Inverse Document Frequency, Classification","I Gede Mujiyatna S.Kom., M.Kom.",2,3,0,2019,1,"Sebuah tweet adalah sebuah posting yang dibuat oleh seorang user di media sosial bernama Twitter. Posting tersebut terdiri dari maksimum 280 karakter. Sebuah tweet dapat berisi segala sesuatu mulai dari informasi hingga opini. Beberapa peneliti telah mengevaluasi apakah tweet ini berisi pendapat yang dapat dianalisis dengan tujuan klasifikasi sentiment. Naive Bayes adalah sebuah algoritma yang sering digunakan untuk tujuan klasifikasi data Twitter. Sebuah Bayesian classifier memungkinkan pembobotan algoritma seperti TF-IDF untuk diimplementasikan di dalamnya untuk meningkatkan performa.
	Tujuan dari penelitian ini adalah untuk mengimplementasikan pembobotan TF-IDF pada suatu classifier Naive Bayes dengan tujuan meningkatkan nilai performa ketika digunakan untuk tujuan klasifikasi tweet pro-vaccine, anti-vaccine, dan netral. Hasil yang diperoleh dari penelitian ini menunjukkan kemampuan sistem mempunyai akurasi rata-rata 84% untuk membedakan antara tweet pro-vaccine, anti-vaccine, dan netral. Berdasarkan hasil penelitian, dapat disimpulkan bahwa meskipun  sebuah classifier Naive Bayes dengan pembobotan TF-IDF dapat mempunyai akurasi yang cukup baik, akan tetapi belum optimal ketika digunakan dalam suatu dataset yang berisi penuh dengan sarkasme dan ambigu.
","A tweet is a post made by a user on the social media called Twitter. It is a form of a message that is composed with a maximum of 280 characters. A tweet can contain anything from information to opinions. Several researches have been done to see whether these tweets containing opinions can be analyzed for the purpose of sentiment classification. Naive Bayes is an algorithm that is often used for the purpose of classification on twitter data. A Bayesian classifier allows a weighting algorithm such as TF-IDF to be implemented within it to improve performance.
	In this research, the goal is to implement TF-IDF weighting to a Naive Bayes classifier in order to produce a high performance score when used for the purpose of classifying pro-vaccine, anti-vaccine, and neutral tweets. The results obtained in this research gave the average accuracy of 84% for the system's ability of distinguishing between pro-vaccine, anti-vaccine, and neutral tweets. Based on the experiment results obtained, although the TF-IDF weighted Naive Bayes classifier can indeed be satisfactorily accurate, it is un-optimized to be used on a dataset that contains a large amount of sarcasm and ambiguity.",,,126363,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175629,,RIZA EGA SATYABUDHI,GraphQL Analysis and Implementation on Go-Life Microservices (Case Study: Go-Life),,,"GraphQL,REST,N+1 Problem,Response Time","Mardhani Riasetiawan, M.T., Dr ; Guntur Budi Herwanto, S.Kom., M.Cs",2,3,0,2019,1,"REST sering digunakan sebagai protokol komunikasi untuk arsitektur
berorientasi layanan seperti arsitektur microservice. Namun, REST memiliki dalam
keterbatasan fleksibilitas yang berujung pada terjadinya masalah N +1.
Dalam penelitian ini, sebuah studi kasus akan dilakukan pada microservices
Go-Life, sebuah perusahaan teknologi Indonesia yang memberikan layanan pijat
sesuai permintaan, layanan kebersihan, mekanik otomotif, dan layanan kecantikan,
dengan menerapkan GraphQL dalam arsitektur microservice Go-Life, kemudian
membandingkan dengan sistem yang ada yang hanya menggunakan REST.

Saat ini masalah N +1 terjadi dalam microservice Go-Life yang menggunakan
REST, di mana klien perlu melakukan beberapa permintaan jaringan HTTP ke
server untuk meminta data relasional antar microservice. Hal ini menghasilkan
waktu respons tinggi. Oleh karena itu implementasi GraphQL diusulkan untuk
memecahkan masalah N +1.

Perbandingan dalam penelitian ini didasarkan pada waktu respons yang
dibutuhkan oleh masing-masing sistem, yaitu sistem yang ada yang menggunakan
REST dan sistem yang diusulkan yang mengimplementasikan GraphQL, untuk
menangani permintaan data relasional dan non-relasional dari klien.

Dari penelitian ini, ditemukan bahwa pada arsitektur microservice Go-Life,
setelah mengimplementasikan GraphQL, waktu respons yang diperlukan untuk
menangani permintaan data relasional dan non-relasional dari klien dapat
diturunkan, karena masalah N+1 telah diselesaikan. Oleh karena itu disarankan
untuk mengimplementasikan GraphQL pada microservice Go-Life.","REST is often used as the communication protocol for service oriented
architectures such as the microservice architecture. However, REST imposes
restrictions in flexibility which leads to the occurrence of the N+1 problem.
In this research, a case study will be made on Go-Life's microservices, an
Indonesian tech company which delivers on-demand healthy massage, cleaning
services, automotive mechanic, and beauty services, by implementing GraphQL in
Go-Life's microservice architecture, then comparing it with their existing
system which only uses REST.

Currently the N+1 problem occurs within Go-Life's microservices which use
REST, where the client needs to do multiple HTTP network request round trips to
the server for requesting relational data between microservices. This leads to high
response time. Therefore the implementation of GraphQL is proposed to solve the
N+1 problem.

The comparison in this research is based on the response time needed by each
system, which are the existing system which uses REST and the proposed system
which implements GraphQL, to handle relational and non-relational data request
from the client.

From this research, it is found that on Go-Life's microservice architecture, after
implementing GraphQL the response time needed to handle relational and non-relational data request from the client are lowered, because the N+1 problem is
solved. Therefore implementing GraphQL on Go-Life's microservice is
recommended",,,126392,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182286,,FARAHSYIFA MUTIARA K,IMPROVING SUPERVISED LEARNING MODEL IN FRAUD DETECTION USING SMOTE,,,"Fraud Detection, Supervised Learning, SVM, Naive Bayes, Decision tree, Oversampling, SMOTE, Mobile money","Edi Winarko, Drs., M.Sc., Ph.D.",2,3,0,2019,1,"Alat transaksi keuangan saat ini telah banyak digunakan di seluruh dunia. Berbagai jenis layanan keuangan menawarkan solusi praktis untuk mengelola keuangan. Pertumbuhan teknologi memudahkan manusia untuk melakukan transaksi keuangan. Transaksi keuangan yang terjadi saat ini disajikan tidak hanya dalam bentuk kartu debit atau kredit tetapi juga dalam bentuk uang mobile. Dengan uang seluler, orang dapat dengan mudah melakukan transaksi seperti penarikan, debit, pembayaran, dan bahkan transfer di berbagai tempat hanya dengan menggunakan alat sederhana yang merupakan ponsel mereka. Namun, kemudahan yang ditawarkan oleh layanan seperti uang seluler juga membuka celah bagi mereka yang ingin melakukan kejahatan tertentu. Dengan menyadap sistem yang berisi informasi seseorang, informasi seseorang dapat dengan mudah disalahgunakan oleh penjahat untuk mengambil keuntungan yang tidak perlu.
Penelitian ini bertujuan untuk mengetahui metode terbaik untuk memecahkan masalah penipuan dalam sistem uang seluler sementara memecahkan masalah data yang tidak seimbang yang sering harus dihadapi dalam kumpulan data penipuan. Dalam penelitian ini, metode Oversampling menggunakan SMOTE Algorithm akan digunakan untuk menyelesaikan masalah ketidakseimbangan. Setelah itu, tiga metode klasifikasi, Support Vector Machine, Naive Bayes, dan Decision Tree akan digunakan untuk mengklasifikasikan dataset penipuan uang seluler. Kemudian kinerja klasifikasi akan dievaluasi dengan menghitung presisi, daya ingat, skor f1, akurasi, dan matriks kebingungan.
Hasil penelitian ini menunjukkan bahwa penggunaan model klasifikasi Decision Tree dengan SMOTE adalah yang terbaik dibandingkan dengan dua model klasifikasi lainnya. Dengan nilai rata-rata evaluasi adalah 0,99 untuk presisi, 0,99 untuk recall, 0,99 untuk skor f1, dan 0,99 untuk nilai akurasi.This research aims to find out the best method for solving fraud problems in the mobile money system while solving the imbalanced data problems that often have to face in the fraud datasets. In this research, the Oversampling method using SMOTE Algorithm will be used to solve the imbalanced problem. Afterward, three classification methods, Support Vector Machine, Naive Bayes, and Decision Tree will be used to classify the mobile money fraud dataset. Then the classification performance will be evaluated by calculating the precision, recall, f1-score, accuracy, and the confusion matrix.
The results of this study indicate that the use of the Decision Tree classification model with SMOTE is the best compared to the two other classification models. With an average value of evaluation are 0,99 for the precision, 0,99 for the recall, 0,99 for the f1-score, and 0,99 for the accuracy value.","In the present-time financial transaction appliance has been widely used around the world. Various types of financial services offer practical solutions for managing finances. The growth of technology makes it easier for the human to do a financial transaction. The financial transaction that happens nowadays is presented not only in the form of debit or credit card but also in the form of mobile money. With mobile money, people can easily do a transaction such as withdrawals, debit, payment, and even transfer in various places with just using a simple tool which is their mobile phone.  However, the easiness offered by services such as mobile money also opening a gap to those who want to commit certain crimes. By intercepting systems that contain someone's information, someone's information can be easily misused by the criminals to take an unnecessary advantage. 
This research aims to find out the best method for solving fraud problems in the mobile money system while solving the imbalanced data problems that often have to face in the fraud datasets. In this research, the Oversampling method using SMOTE Algorithm will be used to solve the imbalanced problem. Afterward, three classification methods, Support Vector Machine, Naive Bayes, and Decision Tree will be used to classify the mobile money fraud dataset. Then the classification performance will be evaluated by calculating the precision, recall, f1-score, accuracy, and the confusion matrix.
The results of this study indicate that the use of the Decision Tree classification model with SMOTE is the best compared to the two other classification models. With an average value of evaluation are 0,99 for the precision, 0,99 for the recall, 0,99 for the f1-score, and 0,99 for the accuracy value.
",,,133183,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
173585,,Muhammad Nirwandha D. P,PENENTUAN KEY PERSON DALAM PENYEBARAN INFORMASI FILM DI SOSIAL MEDIA DENGAN MENGGUNAKAN MODIFIKASI PAGERANK,,,"Social Network Analysis, PageRank, Centrality, Time Centrality, Key Person","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2019,1,"Algoritma PageRank merupakan algoritma yang diciptakan dan digunakan oleh mesin pencari Google untuk mengurutkan situs web mana yang lebih penting atau populer. Selain untuk menentukan kepopuleran serta seberapa penting situs web, algoritma PageRank juga dapat digunakan untuk melakukan Social Network Analysis. Social Network Analysis merupakan metode untuk menganalisa suatu jaringan sosial. Page Rank dapat diggunakan sebagai salah satu cara menganalisa untuk menentukan node mana yang lebih penting dalam suatu network pembicaraan di twitter, namun perhitungan PageRank yang hanya menilai sebuah node berdasarkan nilai node lain dianggap kurang komprehensif jika digunakan untuk menentukan node yang berpengaruh di twitter, dan adanya iterasi pada algoritma PageRank yang membuat nilai komputasinya cukup besar, serta tidak adanya unsur waktu dalam algoritma PageRank yang mengakibatkan metode tersebut kurang dinamis dengan perubahan struktur pada jaringan yang ada.
Pada penelitian ini akan dilakukan modifikasi terhadap algoritma PageRank untuk pemilihan key person pada jaringan sosial di twitter. Pemilihan key person dilakukan dengan menghitung nilai centrality dari tiap-tiap node yang berada pada suatu network serta menghitung kecepatan penyebaran informasi dari node. Pada penilitian ini juga akan dilakukan perbandingan algoritma PageRank, dengan PageRank yang telah dimodifikasi.
Penelitian ini menghasilkan metode penentuan key person dengan memodifikasi algoritma PageRank agar menjadi lebih komprehensif dengan kriteria di twitter dan efisien karena tidak menggunakan, serta lebih dinamis karena terdapat perhitungan unsur waktu.","PageRank Algorithm is an algorithm which is created and used by Google Search Engine to sort which website is more important or popular. Not only determine the popularity and the importance of a website, the PageRank Algorithm also can be used for Social Network Analysis. Social Network Analysis is a method to analyze the social network. PageRank is a algorithm that can be used to analyze and determine which node is more important in a Twitter network, but PageRank calculation which only rates a node based on other nodes is less relevant if it is used to determine the important nodes in Twitter. In addition, iteration in the PageRank Algorithm results in computation rate is big enough, and because there is no element of time in the PageRank Algorithm, the method is not dynamic enough with the changes of the structure in the established network.
In this research, it will do a modification to the PageRank Algorithm for the key person choice on Twitter. Choosing a key person will be carried out by counting the centrality rate from each node that exists in a network and also counting the booming speed of the node. Moreover, it will do the PageRank Algorithm comparison with the modified PageRank in the research.
The findings of the research results in a key person determination method by modifying the PageRank Algorithm method so that it will be more relevant and efficient with the criteria on Twitter because it does not use iteration and it is more dynamic because there is a calculation of time element.",,,124561,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182546,,GHIFFARI AGSARYA ARLIN,Sistem Pencari Berita Dengan Menggunakan Focused Web Crawler Dan Algoritma Breadth-First Search,,,"Web Crawler, Keyword Focused Web Crawler, Algoritma Breadth-First Search.","Dr. Andi Dharmawan, S.Si., M.Cs.",2,3,0,2019,1,"Data berita sangat dibutuhkan dalam beberapa penelitian sebagai objek penelitiannya. Data berita dapat diperoleh dengan mudah dari situs berita online dengan cara mengunjungi situs berita yang diinginkan dan mengambil berita satu per satu. Dikarenakan data berita yang dibutuhkan jumlahnya banyak, maka cara tersebut tidak efektif, sehingga dibutuhkan suatu program untuk memudahkan proses pencarian data berita.

Sistem pencari berita yang dibuat pada penelitian ini menggunakan metode keyword focused web crawler dan algoritme Breadth-First Search. Metode ini melakukan proses pencarian berita sesuai dengan masukan keyword dengan memanfaatkan fitur search bawaan dari situs berita online yang dapat ditemukan pada situs berita yang dijadikan sebagai seed Uniform Resource Locator (URL). Metode ini akan membantu crawler dalam pencarian informasi yang paling relevan berdasarkan kata kunci pada situs berita online tanpa benar-benar melakukan crawling terhadap banyak link yang tidak relevan. Relevansi yang didapatkan sangat tergantung pada fitur pencarian tiap situs berita, apakah mengandung keyword yang dimasukkan atau tidak.

Hasil dari penelitian ini adalah menyatakan bahwa performa dari keyword focused web crawler lebih baik dari web crawler biasa dalam segi waktu crawling dan hasil pencarian. Dengan menggunakan parameter yang sama yaitu 10 keyword, 50 parent, dan tingkat kedalaman 3, antaranews.com mencatatkan waktu tercepat yaitu 49 detik, sementara detik.com dengan waktu 1 menit 35 detik, dan tirto.id dengan waktu 12 menit 10 detik. Rata-rata banyaknya berita yang berhasil di-crawling&Atilde;‚&Acirc;&not; pada detik.com adalah 60 berita, antaranews.com dengan 72 berita, dan tirto.id dengan 171 berita. Rata-rata link yang diambil dari link ditawarkan pada antaranews.com adalah 14,43%, detik.com sebesar 14,43%, dan tirto.id sebesar 15,68%.
","News data is highly needed in several studies as the object of research. News data can be obtained easily from online news sites by visiting the desired news sites and retrieving news one by one. Due to a large amount of news data needed, this method is not practical; therefore, a program to facilitate the process of finding news data is needed.

The news search system in this study uses the keyword-focused web crawler method and the Breadth-First Search algorithm. This method performs the process of news searching according to keyword input by utilizing the default search feature of online news sites, which can be found on news sites that are made as seed Uniform Resource Locator (URL). This method will help crawlers in searching for the most relevant information based on keywords on an online news site without actually crawling many irrelevant links. The relevance obtained very much depends on the search feature of each news site, whether it contains keywords that are entered or not.

The result of this study is the performance of the keyword focused web crawler is better than web crawler in terms of crawling time and search results. By using the same parameters which are 10 keywords, 50 parents, and a depth level of 3, antaranews.com records the fastest time of 49 seconds, while detik.com has a time of 1 minute 35 seconds, and tirto.id with a time of 12 minutes 10 seconds. The average number of news that successfully crawled on detik.com is 60 news, antaranews.com with 72 news, and tirto.id with 171 news. The average link taken from the link offered at antaranews.com is 14.43%, detik.com is 14.43%, and tirto.id is 15.68%.
",,,133422,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183573,,MARISHA SALSABILA,Perbandingan Metode Prapemrosesan Citra Pada Convolutional Neural Network Untuk Pengenalan Wajah Berdasarkan Perbedaan Usia,,,"Age Invariant Face Recognition, Convolutional Neural Network, patch wajah","Moh Edi Wibowo, S.Kom, M.Kom, Ph.D.",2,3,0,2019,1,"Age Invariant Face Recognition (AIFR) menjadi tantangan dalam computer vision dalam melakukan pengenalan wajah. Perubahan usia memberikan perubahan yang signifikan terhadap visual wajah manusia. FG-Net merupakan dataset publik yang dapat digunakan untuk mengevaluasi model dalam melakukan pengenalan wajah berdasarkan perbedaan usia. Beberapa penelitian terdahulu telah menghasilkan model untuk mengevaluasi dataset FG-Net namun akurasi yang didapatkan masih kurang optimal. Pada penelitian ini dilakukan serangkaian eksperimen pada tahap prapemrosesan citra sebelum dilakukan ekstraksi fitur. Ekstrasi fitur pada penelitian ini menggunakan 7-layer Convolutional Neural Network dan klasifikasi dilakukan menggunakan Support Vector Machine. Dilakukan pula eksperimen penggunaan fitur patch wajah diantaranya set data seluruh wajah, area periocular serta area wajah bagian bawah untuk menjadi input model. Berdasarkan hasil percobaan, model evaluasi terbaik dengan dataset FG-Net dimiliki oleh model ukuran input 32x32 dengan prapemrosesan citra jenis RGB dengan enhancement sharpening menggunakan set data area periocular dengan parameter pada SVM dengan nilai C = 1 dan &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&sup3; = 0,01 menghasilkan akurasi sebesar 0,796.","Age Invariant Face Recognition (AIFR) is a challenge in computer vision in facial recognition. Changes in age provide significant changes to the visuals of the human face. FG-Net is a public dataset that can be used to evaluate models for facial recognition based on age differences. Several previous studies have produced a model to evaluate the FG-Net dataset, but the accuracy obtained is still not optimal. In this study a series of experiments were carried out at the image pre-processing stage before feature extraction was performed. Feature extraction in this study uses a 7-layer Convolutional Neural Network and classification is done using the Support Vector Machine. Experiments were also carried out using facial patch features including data sets throughout the face, periocular areas and lower face areas to be input to the model. Based on the experimental results, the best evaluation model with the FG-Net dataset is owned by a 32x32 input size model with pre-processing RGB image types with enhancement sharpening using periocular area data sets with parameters on SVM with values &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;C = 1 and &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&sup3; = 0.01 producing an accuracy of 0.796 .",,,134490,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171543,,ANDI ARISKA YOGATAMA,PERBANDINGAN KINERJA PENGGUNAAN STOPWORD REMOVAL DALAM KLASIFIKASI TEKS BERITA EKONOMI BERBAHASA INDONESIA MENGGUNAKAN METODE SUPPORT VECTOR MACHINE,,,"text mining, klasifikasi teks, Support Vector Machine, stopword removal","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2019,1,"Perkembangan teknologi yang semakin pesat membawa perubahan yang besar terhadap kehidupan masyarakat. Salah satunya adalah akses berita online yang semakin meningkat. Penelitian sebelumnya telah dilakukan mengenai klasifikasi teks berita ekonomi menggunakan metode Multinomial Naive Bayes. Hasil dari penelitian tersebut diperoleh rata-rata 74,77%. Dari hasil tersebut maka dilakukan penelitian mengenai klasifikasi teks berita ekonomi namun dengan metode yang berbeda.
Pada penelitian kali ini dilakukan klasifikasi teks berita ekonomi menggunakan metode Support Vector Machine. Dataset yang digunakan dalam penelitian ini adalah berita ekonomi online sejumlah 1233 berita yang terbagi dalam 9 kategori sektor ekonomi yang mengacu pada pendekatan produksi dalam menentukan angka Produk Domestik Bruto. Dalam penelitian kali ini juga dilakukan perbandingan performa antara penggunaan stopword removal dan tanpa stopword removal dalam preprocessing. Fitur yang digunakan adalah 1000 kata yang paling sering muncul dalam dataset dengan pembobotan tf-idf.
Dari fitur tersebut dilakukan klasifikasi dengan metode Support Vector Machine. Hasil dari pengujian didapatkan hasil rata-rata akurasi 75,27% untuk data tanpa stopword. Sedangkan untuk data dengan stopword didapatkan hasil rata-rata 75,43%.","The rapid development of technology has brought great changes to people's lives. One of them is increasing online news access. Previous research has been conducted on the classification of economic news texts using the Multinomial Naive Bayes method. The results of the study obtained an average of 74.77%. From these results, a study of the classification of economic news texts was conducted but with different methods.
In this research the economic news text classification was conducted using the Support Vector Machine method. The dataset used in this research is online economic news totaling 1233 news items which are divided into 9 categories of economic sectors that refer to the production approach in determining Gross Domestic Product figures. In this study also made a comparison between the performance of using stopword removal and without stopword removal. The feature that is used is 1000 words that appear most frequently in a dataset with weighted tf-idf.
From this feature classification is supported by the Support Vector Machine method. The results of these tests obtained an average accuracy of 75.27% for the dataset without stopword. While for testing dataset with stopword, the average yield is 75.43%.",,,122755,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177687,,ALFIAN TRYPUTRANTO ,PERBANDINGAN METODE EKSTRAKSI FITUR TEKSTUR  PADA SISTEM KLASIFIKASI CITRA BATIK ,,,"ekstraksi fitur tekstur, klasifikasi, batik, Gray Level Co-Occurrence Matrix, Haar Wavelet Transform, Local Binary Pattern","Moh Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2019,1,"Pada sistem klasifikasi citra, proses ekstraksi fitur memainkan peranan penting dalam menghasilkan tingkat keakuratan dan kecepatan pemrosesan klasifikasi yang baik. Fitur tekstur merupakan fitur penting dalam sebuah gambar yang merupakan informasi berupa susunan struktur permukaan suatu gambar. Beberapa penelitian telah dilakukan terhadap performa algoritma ekstraksi fitur tekstur citra Gray Level Co-Occurrence Matrix, Haar Wavelet Transform dan Local Binary Pattern. Akan tetapi belum ada penelitian yang melakukan perbandingan tingkat akurasi klasifikasi dan kecepatan pemrosesan ekstraksi dari ketiga algoritma tersebut. 
Dalam penelitian ini dilakukan perbandingan terhadap algoritma ekstraksi fitur tekstur Gray Level Co-Occurrence Matrix (GLCM), Haar Wavelet Transform (HWT) dan Local Binary Pattern (LBP) dalam membangun sistem klasifikasi citra batik Yogyakarta. Parameter yang dibandingkan adalah waktu eksekusi ekstraksi fitur tekstur dan akurasi klasifikasi. Fitur tekstur tiap citra kemudian diklasifikasi menggunakan algoritma Multi Layer Perceptron untuk menguji tingkat akurasi klasifikasi tiap algoritma ekstraksi fitur tekstur.
Digunakan 2120 citra batik yang terdiri dari 5 kelas motif yaitu kawung, nitik, truntum, parang dan tirta teja. Berdasarkan pengujian, algoritma GLCM menghasilkan akurasi klasifikasi paling baik yaitu 93.3%. Sedangkan algoritma HWT menjadi algoritma yang membutuhkan waktu eksekusi paling sedikit yaitu 30,55 detik. Ukuran citra yang paling baik untuk ekstraksi fitur tekstur citra batik dalam hal akurasi klasifikasi adalah 600x600 piksel.","In the image classification system, feature extraction processes are important role in producing a good level of accuracy and processing speed classification. Texture feature is an important feature in an image which is information in the form of a surface structure of an image. Several studies have been conducted on the performance of the texture feature extraction algorithm of Gray Level Co-Occurrence Matrix, Haar Wavelet Transform and Local Binary Pattern images. However, there are no research that compare the accuracy of classification and the processing time of extraction from the algorithms.
In this research a comparison of the Gray Level Co-Occurrence Matrix (GLCM) texture feature extraction algorithm, Haar Wavelet Transform (HWT) and Local Binary Pattern (LBP) was conducted in building a Yogyakarta batik image classification system. The parameters compared are the texture feature extraction execution time and classification accuracy. The texture features of each image are then classified using the Multi Layer Perceptron algorithm to test the classification accuracy of each texture feature extraction algorithm.
The data used were 2120 batik images consisting of 5 classes of motifs namely kawung, nitik, truntum, parang and tirta teja. Based on the experiments, the GLCM algorithm produces the best classification accuracy of 93.3%. Meanwhile the HWT algorithm is an algorithm that requires the least execution time of 30.55 seconds. The best image size for extraction of texture features of batik images in terms of classification accuracy is 600x600 pixels.",,,128463,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182041,,YUSUF SAFRIZAL,KOMPARASI PAGE OBJECT DESIGN PATTERN DAN SCREENPLAY DESIGN PATTERN DALAM AUTOMATED ACCEPTANCE TESTING (STUDI KASUS: E-LOK UGM),,,"utomated test, design pattern, regression test, eLok","Isna Alfi Bustoni, M.Eng.",2,3,0,2019,1,"Dalam sebuah siklus pengembangan software, perlu dilakukan sebuah
pengujian untuk memastikan apakah software yang dikembangkan telah memenuhi
spesifikasi yang diinginkan. Pengujian otomatis dapat mengurangi waktu untuk
melakukan regression test yang dikerjakan secara manual. Salah satu framework
pembuatan automated test adalah Serenity yang menggunakan prinsip Behavior
Driven Development (BDD). Salah satu design pattern pada framework ini adalah
Page Object yang merepresentasikan elemen-elemen web kedalam sebuah objek.
Kemudian dikembangkan design pattern Screenplay yang memperbaiki
kekurangan design pattern sebelumnya.
Pada penelitian ini, akan dilakukan pembuatan automated acceptance
testing pada website Elok UGM dengan menerapkan Page Object design pattern
dan Screenplay design pattern kemudian melakukan komparasi terhadap keduanya.
Jumlah test case yang digunakan sebanyak 26 skenario. Komparasi yang dilakukan
yaitu keseuaian hasil dengan manual testing, jumlah baris kode, dan performa
running time. Setelah hasil running time didapatkan, data diolah secara statistik agar
terlihat perbedaannya.
Hasil dari penelitian ini menunjukkan bahwa implementasi automated test
dapat menghasilkan hasil yang sama dengan manual testing. Kemudian jumlah
baris yang dibutuhkan untuk implementasi Screenplay design pattern lebih sedikit
daripada implementasi Page Object design pattern. Sedangkan berdasarkan hasil
uji statistik, hasil P-value mendapatkan 0,196 yang lebih besar dari pada nilai alpha
yang ditentukan 0,05 sehingga bisa diambil kesimpulan bahwa tidak terdapat
perbedaan running time antara Page Object design pattern dan Screenplay design
pattern.","In a software development cycle, a test needs to be done to ascertain
whether the software developed has met the desired specifications. Automatic
testing can reduce the time to do regression tests that are done manually. One of the
automated test manufacturing frameworks is Serenity which uses the principle of
Behavior Driven Development (BDD). One of the design patterns in this framework
is the Page Object which represents web elements into an object. Then the
Screenplay design pattern was developed which corrected the shortcomings of the
previous design pattern.
In this research, an automated acceptance testing will be made on the Elok
UGM website by applying Page Object design patterns and Screenplay design
patterns and then comparing them. The number of test cases used is 26 scenarios.
Comparisons made are the compatibility of the results with manual testing, the
number of lines of code, and running time performance. After the running time
results are obtained, the data is processed statistically to make the difference look.
The results of this research indicate that the implementation of automated
tests can produce the same results as manual testing. Then the number of lines
needed for the Screenplay design pattern implementation is less than the Page
Object design pattern implementation. While based on the results of statistical tests,
the P-value results get 0.196 which is greater than the alpha value determined 0.05
so that it can be concluded that there is no difference in running time between Page
Object design pattern and Screenplay design pattern.",,,132948,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175645,,Rokhana Diyah Rusdiati,PERINGKASAN TEKS OTOMATIS PADA ARTIKEL BERITA ONLINE BERBAHASA INDONESIA MENGGUNAKAN TEXTTEASER TERMODIFIKASI,,,"Peringkasan Teks Otomatis, TextTeaser, Stemming, RAKE, ROUGE","EDI WINARKO, Drs., M.Sc., Ph.D",2,3,0,2019,1,"Informasi yang tersedia di Internet semakin bertambah banyak setiap harinya, namun waktu yang dimiliki oleh manusia terbatas untuk bisa mengetahui semua informasi tersebut. Salah satu media untuk mendapatkan informasi adalah melalui berita yang dapat diakses dari situs penyedia berita online. Banyaknya informasi tersebut tidak lantas memudahkan manusia dalam mengetahui apa sedang terjadi baru-baru ini. Membuat ringkasan secara manual bukanlah suatu pekerjaan yang mudah. Oleh sebab itu diperlukan peringkas teks otomatis yang dapat memberikan ringkasan yang merepresentasikan keseluruhan isi berita yang ada.
Penelitian ini mengimplementasikan TextTeaser, yaitu peringkas teks otomatis yang dibuat oleh Balbin untuk meringkas artikel dalam bahasa Inggris. TextTeaser tidak menyertakan proses stemming dan proses ekstraksi kata kunci memberikan skor yang kecil. Penelitian ini melakukan modifikasi pada TextTeaser, yaitu dengan menambahkan proses stemming pada tahap prapemrosesan dan penggunaan algoritma Rapid Automatic Keyword Extraction (RAKE) pada tahap ekstraksi kata kunci. Peringkasan teks otomatis dalam penelitian ini terdiri dari beberapa proses, yaitu prapemrosesan, penghitungan skor kemiripan kalimat dengan judul berita, penghitungan skor panjang kalimat, penghitungan skor posisi kalimat, penghitungan skor kata kunci pada setiap kalimat, dan pemilihan kalimat sebagai hasil akhir ringkasan.
Pengujian dilakukan dengan menghitung nilai precision, recall, dan f-measure, dari ringkasan yang dihasilkan oleh sistem terhadap ringkasan ideal yang dihasilkan oleh manusia menggunakan metode ROUGE-1. Berdasarkan pengujian yang telah dilakukan, diketahui bahwa TextTeaser yang telah dimodifikasi (pada bagian prapemrosesan dengan menambahkan proses stemming dan penggunaan algoritma RAKE pada tahap ekstraksi kata kunci) memberikan hasil ringkasan yang lebih mirip dengan ringkasan ideal yang dihasilkan oleh manusia, yaitu dengan nilai f-measure sebesar 60,75%.","The information available on the Internet is increasing a lot every day, but human's time is limited to be able to know all the information. One of media that can be used to obtain information is through news that can be accessed from various online news provider sites. The amount of information does not necessarily make it easier for human to know what is happening recently. Therefore, we need automatic text summarizer which can provide summary that represents the entire content of the news.
This research implements TextTeaser, an automatic text summarizer made by Balbin to summarize articles in English. TextTeaser does not include stemming process and the keyword extraction process gives a small score. This research modified TextTeaser by adding stemming process at at preprocessing stage, and using Rapid Automatic Keyword Extraction (RAKE) algorithm at keyword extraction stage. The automatic text summarization in this research consists of several processes, namely preprocessing, calculating sentence similarity scores with news title, calculating sentence length scores, calculating sentence position scores, and selecting sentences as summary.
Testing is done by calculating the value of precision, recall, and f-measure, from the summary generated by the system to the ideal summary produced by humans usng ROUGE-1 method. Based on the test results, it is known that the modified TextTeaser (in preprocessing stage by adding stemming process and the use of RAKE algorithm in keyword extraction stage) provides summary result that is more similar to the ideal summary produced by humans, with the f-measure value of 60.75%.",,,126401,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182045,,RAKANANDITYA SAID,IMPLEMENTASI DAN ANALISIS MEDIA PENYIMPANAN TERDISTRIBUSI MENGGUNAKAN GLUSTERFS PADA DOCKER,,,"GlusterFS, Docker Container, Throughtput ","Dr.techn. Ahmad Ashari, M.Kom.",2,3,0,2019,1,"Media penyimpanan terdistribusi menawarkan penyimpanan yang pemarnen untuk saling berbagi berbagai file, dan membangun sebuah penyimpanan dengan menggabungkan sumber daya penyimpanan yang tersebar dalam jaringan. Dalam pembangunnya dibutuhkan resource yang cukup banyak. Disisi lain, teknologi container memberikan virtualisasi pada sistem. Teknologi container menyediakan fitur portabilitas bagi aplikasi  sehingga dapat berjalan pada sebagian besar platform.
	Pada penelitian ini dilakukan  pengujian  perbandingan performa nilai throughput pada salah satu penyedia media penyimpanan terdistribusi yaitu GlusterFS dengan menggunakan teknologi container, yang nantinya akan dibandingkan dengan GlusterFS yang dipasang secara native. Setelah nilai throughput didapat, data diolah secara statistik agar terlihat perbedaan performanya.
	Hasil dari penelitian ini menunjukkan bahwa GlusterFS yang dipasang menggunakan container lebih unggul dalam melakukan transfer pada single file transfer sedangkan pada native server lebih unggul pada multiple file transfer. Dalam beberapa kasus hasil pada container dengan native menerima hipotesa awal (H&Acirc;&not;&Acirc;&not;&Acirc;&not;0) sehingga nilai yang didapat tidak memiliki perbedaan yang signifikan.
","Distributed file system offers storage that is suitable for sharing various files, and builds storage by combining storage resources that are spread across the network. In the process needed quite a lot of resources. On the other hand, container technology provides virtualization on the system. Container technology provides portability features for applications so that they can run on most platforms.
	In this research, is done test of the performance of throughput values is performed on one of the distributed storage media providers, GlusterFS using container technology, which will be compared to GlusterFS installed native. After the throughput value is obtained, the data is processed statistically to make a difference in performance.
	The results of this study indicate that GlusterFS installed using containers is superior in transferring single file transfers while native servers are superior to multiple file transfers. In some cases the results in containers natively accept the initial hypothesis (H&Acirc;&not;&Acirc;&not;&Acirc;&not;0) so that the value obtained does not have a significant difference.
",,,132954,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177695,,ARDHINI MAHARANI,Sistem Informasi Geografis Berbasis Web Untuk Pemetaan Potensi Wisata Kabupaten Sleman,,,"Sistem Informasi Geografis, Potensi Wisata","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2019,1,"Setiap daerah perlu memiliki perencanaan matang terkait pengembangan potensi daerahnya, begitu juga dengan Kabupaten Sleman. Kurangnya fasilitas pemerintah yang mempromosikan potensi daerah seperti tempat wisata dan fasilitas umum yang tengah berkembang di wilayahnya membuat daerah tersebut sulit untuk berkembang. Apalagi ditambah dengan penduduk yang semakin banyak, pemerintah akan semakin sulit untuk memantau dan mengarahkan warganya untuk mengembangkan potensi daerahnya sekaligus meningkatkan kesejahteraannya.
Untuk menjawab permasalahan tersebut, dibuatlah suatu sistem informasi geografis berbasis web untuk memudahkan pemantauan dan promosi potensi wisata sehingga diharapkan potensi daerah di Kabupaten Sleman dapat semakin berkembang. Dalam pembuatan sistem informasi geografis berbasis web tersebut, penulis menggunakan bahasa pemrograman PHP dan leaflet.js dengan database MySQL. Tujuan akhir dari pembuatan sistem ini adalah agar potensi wisata daerah Kabupaten Sleman dapat lebih berkembang dan lebih banyak wisatawan yang berkunjung dari tahun ke tahunnya.
Penelitian ini menggunakan metode survei lapangan dan pembuatan peta menggunakan simbol titik, garis dan area. Survei lapangan dilakukan untuk mengetahui titik koordinat pasti dari objek wisata tersebut sekaligus pengambilan foto objek wisata. Untuk data dan deskripsi diambil dari Dinas Pariwisata Kabupaten Sleman. Hasil dari penelitian ini berupa suatu website yang berisi informasi terkait Pariwisata dilengkapi dengan peta yang mampu menunjukkan lokasi objek wisata yang berada di Kabupaten Sleman.","Each region needs to have careful planning related to the development of its regional potential, especially Sleman. Lack of government facilities that promote regional potential such as tourist attractions and public facilities that are developing in the region make it difficult for the area to develop. Moreover, coupled with an increasing population, the government will find it increasingly difficult to monitor and direct its citizens to develop their regional potential while increasing their welfare.
To answer these problems, a web-based geographic information sistem was created to facilitate monitoring and promotion of tourism potential so that the potential of the region in Sleman is expected to grow. In making the web-based geographic information sistem, the author uses the PHP programming language and leaflet.js with the MySQL database. The ultimate goal of making this sistem is so that the tourism potential of the Sleman Distric can be more developed and more tourists visit from year to year.
This study uses the field survei method and map making using symbols of points, lines and areas. Field surveis were conducted to determine the exact coordinates of the tourist attraction as well as taking photos of attractions. For data and descriptions taken from Dinas Pariwisata Kabupaten Sleman. The results of this study are in the form of a website that contains tourism information and maps that are able to show the location of attractions in Sleman Distric.",,,128445,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
168992,,FEMILIA HARDINA,Pengenalan Ekspresi Wajah Menggunakan Convolutional Neural Network,,,"Pengenalan Ekspresi Wajah, Jaringan Saraf Konvolusi, TensorFlow","Moh. Edi Wibowo, Ph.D.",2,3,0,2019,1,"Pengenalan ekspresi wajah merupakan salah satu topik yang sudah banyak diteliti sejak 10 tahun terakhir. Hal tersebut merupakan hal yang mudah dan sering dilakukan oleh manusia, namun belum cukup mudah dilakukan oleh komputer. Akurasi yang diperoleh pada metode yang digunakan pada penelitian-penelitian sebelumnya juga masih belum cukup tinggi, khususnya pada dataset wajah di
lingkungan yang tidak terkontrol. Oleh karena itu, diperlukan suatu algoritma yang bisa melakukan pengenalan ekspresi wajah dengan lebih baik pada dataset yang tidak terkontrol.
Pengenalan ekspresi wajah secara otomatis dapat dilakukan dengan mengklasifikasikan citra menjadi tujuh kelas ekspresi yaitu marah, jijik, takut, senang, sedih, terkejut, dan netral. Adapun penelitian ini akan berfokus pada penggunaan convolutional neural network untuk melakukan pengenalan ekspresi wajah dengan menggunakan TensorFlow.
Pengenalan ekspresi wajah secara otomatis pada dataset yang tidak terkontrol dengan menggunakan metode ini menghasilkan akurasi hingga 62,1%.","Facial expression recognition is one of the research topics that has been used since the last 10 years. It is such an easy and frequent thing to be done manually by human, but still found hard to be done automatically by computer. Beside that, the accuracy obtained by the previous researches are also still not very high, especially using uncontrolled face datasets. Therefore, an algorithm that can do facial expression recognition automatically with those uncontrolled datasets and obtain better accuracy is needed.
Automatic facial expression recognition can be done by classifying images into seven classes which are anger, disgust, fear, happiness, sadness, surprise, and neutral. As for this research will be focusing on the using of convolutional neural network in doing facial expression recognition using TensorFlow.
Automatic facial expression recognition with the uncontrolled dataset using this method can reach the accuracy up to 62,1%.",,,120156,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
172576,,MOHAMMAD NOFRIZAN,Algoritme Genetika Untuk Optimasi Permasalahan Transportasi,,,"masalah transportasi, optimasi, algoritme genetika","Drs. Janoe Hendarto, M.I.Kom",2,3,0,2019,1,"Masalah pendistribusian barang merupakan masalah yang cukup penting dalam kegiatan ekonomi. Namun, hambatan dalam proses pendistribusian barang adalah sulitnya menentukan alokasi distribusi barang agar segala penawaran (dari beberapa gudang) dapat memenuhi segala permintaan (dari beberapa toko). Untuk itu, diperlukan suatu kebijakan yang tepat pada proses distribusi. Salah satu metode yang dapat digunakan untuk menyelesaikan permasalahan pendistribusian barang ialah algoritme genetika.
Penelitian ini berisi penerapan algoritme genetika untuk optimasi permasalahan transportasi. Proses yang dilibatkan mulai dari pembangkitan populasi, seleksi, penyilangan, mutasi, hingga pada pemilihan individu terbaik sebagai solusi. Algoritme genetika diterapkan pada 2 buah kasus permasalahan transportasi (data penelitian Sari dkk. serta Xiangyu dkk.). Hasil yang diperoleh berupa biaya distribusi optimal dan alokasi distribusi barang. 
Pada penelitian ini juga dilakukan pembandingan antara hasil yang diperoleh melalui algoritme genetika dengan hasil yang diperoleh melalui metode optimasi deterministik (metode barat laut). Solusi yang diperoleh melalui algoritme genetika lebih optimal dibanding solusi yang didapat dari metode barat laut. Pada kasus A (data penelitian Sari dkk.), biaya transportasi lebih hemat sebanyak 19% dibanding metode barat laut, sedangkan pada kasus B (data penelitian Xiangyu dkk.), biaya transportasi lebih hemat sebanyak 23%. Namun, dari segi waktu komputasi, algoritme genetika memang membutuhkan waktu yang lebih lama, yakni 4,368387 detik berbanding 0,000911 detik pada kasus A dan 4,792465 detik berbanding 0,002401 detik pada kasus B. Pada bagian akhir juga dilakukan pengubahan parameter genetika untuk mengetahui pengaruhnya terhadap solusi. Hasilnya, parameter genetika ideal yang didapat yaitu probabilitas penyilangan 0,6, probabilitas mutasi 0,05, dan jumlah generasi pada kisaran 250 sampai 500.","The problem of distributing goods is an important problem in economic activities. However, the obstacles in the process of distributing goods are the difficulty of determining the allocation of goods distribution so that all offers (from several warehouses) can meet all requests (from several stores). For that, a good policy in the distribution process is needed. One method that can be used to solve item distribution problems is the genetic algorithm.
This study contains the application of genetic algorithm to optimize transportation problem. The process involved starts from population generation, selection, cross over, mutation, and the selection of the best individual as a solution. Genetic algorithm were applied to 2 cases of transportation problem (data from Sari et al. and Xiangyu et al.). The results obtained are in the form of optimal distribution cost and allocation of goods distribution.
In this study also made a comparison between the results obtained through genetic algorithm with the results obtained through deterministic optimization method (north-west corner method). The solution obtained through genetic algorithm is more optimal than the solution obtained from the north-west corner method. In case A (Sari et al. research data), transportation cost were more efficient at 19% compared to the north-west corner method, whereas in case B (Xiangyu et al. research data), transportation cost were more efficient by 23%. However, in terms of computational time, genetic algorithm requires a longer time, which is 4,368387 seconds compared to 0,000911 seconds in case A and 4,792465 seconds compared with 0,002401 seconds in case B. The final part also changes genetic parameters  to determine its effect on the solution. As a result, the ideal genetic parameters obtained are the probability of cross over  0.6, the probability of a mutation of 0.05, and the number of generations in the range 250 to 500.
",,,123777,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
181029,,KURNIAWAN EKA NUGRAHA,PENILAIAN JAWABAN ESAI OTOMATIS DENGAN ALGORITME FASTTEXT,,,"automated essay scoring, word embedding, fasttext, n-gram ","Dr. Mardhani Riasetiawan, S.E. Ak, M.T.;Guntur Budi Herwanto, S.Kom., MCs.",2,3,0,2019,1,"Penilaian esai memiliki hambatan untuk bisa diadopsi pada skala besar karena penilaian jawaban esai lebih kompleks dibanding pilihan ganda sehingga dikembangkan berbagai model Automated Essay Scoring (AES). Pada AES berbahasa Indonesia, metode bag-of-word dan tf-idf mampu mendapatkan hasil yang baik pada esai jawaban tertutup namun mengalami penurunan performa F1 signifikan pada esai jawaban terbuka. 
Dalam penelitian ini dilakukan penilaian esai otomatis pada tiga dataset Pusat Penilaian Pendidikan (PUSPENDIK) yaitu dataset Maccu Piccu yang merupakan esai jawaban tertutup, serta Jaket, dan Sepeda yang merupakan esai jawaban terbuka. Metode yang digunakan adalah Fasttext yang merupakan kombinasi word embedding dengan linear classifier. Grid Search Cross Validation dilakukan untuk mencari parameter model dengan F1 terbaik. 
Metode Fasttext mampu menambah performa pada AES Bahasa Indonesia pada jawaban terbuka di dataset Jaket mencapai 0.743 dan di dataset Sepeda mencapai 0.767. Metode Fasttext menghasilkan hasil lebih baik pada esai jawaban terbuka dibandingkan bag-of-word tf-idf.","Essay scoring has a drawbacks to be implemented in scale because essay scoring is more complex than multiple choice question. Therefore, Automated Essay Scoring (AES) has been researched. In AES Indonesian, bag-of-word dan tf-idf method could get a good result in closed-ended question but have a significant drip in F1 score in open-ended question.
This research done automated essay scoring in three dataset from Pusat Penilaian Pendidikan (PUSPENDIK) which is Maccu Piccu dataset as closed-ended question, and Jaket &amp; Sepeda as open-ended question. Fasstext method which combine word embedding and linear classifier is used. Grid search cross validation is done to search the model with the best F1.
Fasttext increases the performance of Indonesian AES in open ended question F1 0.743 in Jaket dataset and F1 0.767 in Sepeda dataset. Fasttext produces better result in open-ended question than its bag-of-word tf-idf counterpart.",,,131976,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
181542,,KARTIKA VIDYA RIAN PUTRI,SISTEM PAKAR BERBASIS WEB MENGGUNAKAN MODIFIKASI MODEL CERTAINTY FACTOR UNTUK DIAGNOSIS PENYAKIT JANTUNG KORONER,,,"sistem pakar, penyakit jantung koroner, certainty factor, fuzzy, forward chaining, expert system, coronary heart disease","Drs. Retantyo Wardoyo, M.Sc, Ph.D ",2,3,0,2019,1,"Penyakit jantung koroner telah menjadi penyebab kematian terbanyak di dunia. Penyakit ini disebabkan oleh adanya penumpukan plak pada arteri koroner yang menyebabkan aliran darah ke otot jantung menjadi terhambat. Apabila tidak segera ditangani dengan benar, plak ini dapat menyumbat pembuluh darah sehingga menyebabkan serangan jantung yang dapat berujung pada kematian. Oleh karena itu, pengenalan dini, pengobatan segera dan pencegahan faktor risiko sangat dibutuhkan untuk mengurangi morbiditas dan mortalitas penyakit jantung koroner.
Sistem pakar dapat digunakan untuk membantu proses diagnosis penyakit jantung koroner sejak dini karena dapat melakukan penalaran seperti seorang pakar (dokter spesialis jantung dan pembuluh darah). Untuk menangani ketidakpastian hasil diagnosis, dapat digunakan metode certainty factor. Namun metode ini sendiri masih memiliki kekurangan karena kurang memperhatikan peran dari masing-masing gejala yang ada.
Penelitian ini dibangun menggunakan metode certainty factor yang dimodifikasi. Modifikasi dilakukan dengan memberikan bobot untuk setiap gejala, yang menyatakan tingkat pengaruh masing-masing gejala dalam menentukan hasil diagnosis. Selain itu, modifikasi juga dilakukan pada nilai certainty factor masukan untuk gejala yang dapat diukur, seperti tingkat kolesterol, tekanan darah sistolik, gula darah, dll. Nilai certainty factor untuk gejala tersebut dihitung menggunakan fungsi keanggotaan fuzzy.
Pengujian sistem dilakukan menggunakan 25 data rekam medis pasien di Rumah Sakit Universitas Sebelas Maret Surakarta. Dari pengujian tersebut, didapatkan nilai akurasi hasil diagnosis sebesar 92%. Hal ini menunjukkan bahwa modifikasi certainty factor yang diterapkan dapat memberikan hasil yang baik.
","Coronary heart disease has been a number 1 leading cause of death worlwide. This disease is caused by plaque buildup in the coronary arteries which causes blood flow to the heart muscle to be blocked. If it doesn&acirc;€™t get treated immediately and correctly, the plaque can block the arteries and causing a heart attack that can lead to death. Therefore, early recognition, immediate treatment and prevention of risk factors are needed to reduce the morbidity and mortality of coronary heart disease.
An expert system can be a solution to help in diagnosing coronary heart disease due to its ability to do a reasoning like an expert (cardiologist). Certainty factor method can be used to handle the uncertainty of diagnosis results. However, the method itself has a drawback because it does not pay attention to the role of each symptoms that exist.
This research is built using a modified certainty factor method. Modifications are done by giving a score to each symptom, which states the influence of each symptom in determining the outcome of the diagnosis. In addition, modifications are also done to the value of the user&acirc;€™s certainty factors for measurable symptoms, such as cholesterol, systolic blood presure, blood sugar level, etc. The certainty factor value for these symptoms is calculated using fuzzy membership function. 
System testing was conducted using 25 medical records of patients at Sebelas Maret University Hospital, Surakarta. From these tests, the accuracy of diagnosis results obtained is 92%. It shows that the modified certainty factors applied has been able to give good results.
",,,132407,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175920,,LOVIANA IKA SARI,METODE UNTUK DETEKSI PENGHINAAN DALAM TWIT BERBAHASA INDONESIA,,,"text mining, klasifikasi, classification, tweet, twit, Twitter, penghinaan, insult, Multinomial Naive Bayes, Support Vector Machine","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2019,1,"Jumlah pengguna media sosial yang besar membuat kebebasan berbicara di media sosial menjadi tidak terkendali. Bahkan pada media sosial Twitter, seringkali terjadi perbedaan pendapat yang diungkapkan menggunakan bahasa kasar untuk menghina pengguna Twitter yang lain. Penyebab penghinaan yang tidak terkendali di media sosial khususnya Twitter adalah tidak adanya alat yang efektif untuk menyaring bahasa kasar yang menghina.

Oleh karena itu, pada penelitian ini akan dilakukan deteksi penghinaan dalam twit berbahasa Indonesia dengan studi kasus twit-twit dalam proses diskusi pada akun @tubirfess. Fitur kata pada twit-twit tersebut diambil untuk proses pembobotan kata menggunakan TF-IDF, kemudian hasil pembobotan kata dijadikan masukan pada proses klasifikasi. Metode klasifikasi yang digunakan adalah Multinomial Naive Bayes dan Support Vector Machine. Untuk pengujiannya menggunakan metode K-Fold Cross Validation dengan nilai k = 10.

Hasil pada penelitian ini adalah deteksi penghinaan twit pada fitur unigram menggunakan metode Multinomial Naive Bayes dengan parameter alpha = 1 memiliki akurasi rata-rata sebesar 83%, sedangkan metode Support Vector Machine dengan kernel linear memiliki akurasi terbaik, yaitu 90,07%.","The large number of social media users makes free speech on social media out of control. Even on Twitter, there are often differences of opinion that are expressed using abusive language that insults other Twitter users. The cause of this problem is that there is no effective tools to filter out abusive language.

Therefore, Indonesian insulting tweets on @tubirfess discussion board will be detected for this research. The word n-gram feature on the tweets will be collected and weighed using TF-IDF method then the weighed words will be collected as input into classification process. The classification method that were used in this research are Multinomial Naive Bayes and Support Vector Machine. For evaluation process will use K-Fold Cross Validation method with the value of k = 10.

The results of this research are insult detection on tweets on unigram feature using  Multinomial Naive Bayes method with parameters alpha = 1 having an average accuracy of 83%, while Support Vector Machine method with a linear kernel has best accuracy of 90.07%.",,,126694,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175154,,RAUUF DWI NUGROHO,PENGARUH IMAGE LINEARIZATION PADA NORMALIZED COMPRESSION DISTANCE UNTUK TEMU KEMBALI CITRA BERBASIS KONTEN,,,"NCD, Normalized Compression Distance, Image Linearization, CBIR, Image Retrieval","Wahyono, S.Kom., Ph.D.",2,3,0,2019,1,"Sekarang ini Normalized Compression Distance dapat digunakan untuk
mengukur jarak kemiripan antara 2 objek berdasarkan kompresi data. Pada data
citra, NCD dapat digunakan secara langsung dengan membaca format citra. Selain
itu, NCD juga dapat digunakan pada data citra dengan metode Image
Linearization. Metode tersebut bekerja dengan cara merubah data citra dari array
2 dimensi menjadi bentuk array 1 dimensi. Namun, perbandingan performa antara
penggunaan metode Image Linearization dan tanpa metode tersebut pada sebuah
sistem temu kembali citra berbasis konten belum diketahui.
Pada penelitian ini, peneliti melakukan perbandingan pengukuran jarak
kemiripan Normalized Compression Distance dengan metode Image Linearization
dan tanpa metode tersebut pada sistem Content-Based Image Retrieval (CBIR).
Metode Image Linearization yang digunakan adalah Row-Major. Kompresor yang
digunakan untuk kompresi citra pada NCD adalah DEFLATE. Dataset yang
digunakan dalam penelitian adalah GHIM-10k berformat PNG. Pengujian
dilakukan sebanyak 20 kali pada 20 citra dari 1000 citra dalam dataset, dan
dihitung tingkat akurasi sistem CBIR untuk 10 citra termirip setiap pengujian
dengan menggunakan Precision dan Recall serta dihitung juga waktu eksekusi
sistem.
Hasil dari penelitian menunjukan bahwa penggunaan metode Image
Linearization memiliki pengaruh terhadap akurasi sistem dengan nilai rata-rata
Precision dan Recall lebih tinggi masing-masing 79% dan 15.8% dibandingkan
dengan tanpa metode Image Linearization masing-masing 67.5% dan 13.5%.
Namun, penggunaan metode Image Linearization memiliki waktu eksekusi yang
lebih lama sebesar 1109 detik dibandingkan tanpa metode tersebut sebesar 189
detik.","The Normalized Compression Distance can be used to measure the
distance between two objects based on data compression. In image data, NCD can
be used directly by reading the image format. In addition, NCD can also be used
in image data using the Image Linearization method. The method works by
changing the image data from a 2-dimensional array into a 1-dimensional array.
However, the performance comparison between the use of the Image
Linearization method and without the method on a Content-Based Image
Retrieval system is unknown.
In this study, researcher conducted a comparison of the measurement of
the similarity of the Normalized Compression Distance with the Image
Linearization method and without this method on the Content-Based Image
Retrieval system. The method used for the Image Linearization is Row-Major.
The compressor used for image compression on NCD is DEFLATE. The dataset
used in the study is GHIM-10k in PNG format. Tests were carried out 20 times in
20 images from 1000 images in the dataset, and the accuracy of the CBIR system
was calculated for 10 similar images per test using Precision and Recall and also
the system execution time was calculated.
The results of the study showed that the use of the Image Linearization
method had an effect on the accuracy of the system with higher Precision and
Recall values of 79% and 15.8% respectively compared to without the Image
Linearization method of 67.5% and 13.5%, respectively. However, the use of the
Image Linearization method has a longer execution time of 1109 seconds than
without the method of 189 seconds.",,,125918,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
179766,,JOSUA ADITYA MUSTIKO,PARALELISASI PENGHITUNGAN LUAS SEGITIGA TERBESAR PADA POLIGON KONVEKS,,,"Geometri Komputasi, Komputasi Paralel, Luas Segitiga Terbesar, CUDA","Dr-Ing, MHD. Reza M.I. Pulungan, M. Sc.",2,3,0,2019,1,"Sebuah permasalahan klasik pada bidang geometri komputasional, pencarian luas segitiga terbesar pada poligon konveks, mempunyai sangat banyak metode penyelesaian, dengan kompleksitas yang beragam. Terdapat penelitian yang memecahkan permasalahan ini dengan kompleksitas yang baik, dengan mempertimbangkan setiap segitiga terbesar pada sebuah titik pada poligon, tetapi belum mengimplementasikan komputasi paralel yang dapat mempercepat waktu komputasi yang ada.

Pada penelitian ini, algoritma yang sudah ada sebelumnya diimplementasikan dan dibandingkan dengan algoritma yang diparalelkan. Paralelisasi yang dilakukan adalah dengan menggunakan CUDA, sehingga komputasi dapat dilakukan dengan bantuan GPU.

Setelah implementasi selesai, running-time setiap implementasi dibandingkan, dan dapat dilihat bahwa penggunaan GPU dalam proses komputasi ini dapat mempercepat waktu komputasi hingga hampir 50 kali lipat.","There is a classical problem in the study of computational geometry, to find the largest area of the triangle in convex polygon, that has many ways to solve it, with various complexity. There are research that solve the area with considering the largest area in a point, but have not implemented parallel computing that can make faster computing become possible.

In this research, the algorithm that has been invented before implemented and compared to the paralled algorithm. Parallelization will be done using CUDA, so that computing takes place in GPU.

After the implementation, running time is compared, and can be seen that GPU computing is much faster to compute and fasten the computing time to almost 50 times.",,,130517,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175415,,CANDRA DEWI JODISTIARA,Penjadwalan Mata Kuliah Menggunakan Metode Hybrid Algoritme Genetika dan Algoritme Cuckoo Search,,,"Penjadwalan, Mata Kuliah, Algoritme Genetika, Algoritme Cuckoo Search, hybrid, meta-heuristik","Dr. Drs. Suprapto, M.Kom",2,3,0,2019,1,"Algoritme genetika merupakan algoritme yang bekerja cukup baik dalam mengatasi permasalahan rumit, seperti penjadwalan mata kuliah yang tergolong sebagai permasalahan NP-Hard. Algoritme genetika dapat mengatasi masalah optimasi yang dengan ruang pencarian solusi yang sangat banyak. Namun, algoritme genetika dalam beberapa domain permasalahan, mudah terjebak pada titik optima lokal. Beberapa peneliti telah melakukan penelitian dengan menggunakan metode hybrid dari dua algoritme meta-heuristik untuk menyelesaikan permasalahan ini.
Pada penelitian ini, digunakan metode hybrid algoritme genetika dan algoritme cuckoo search dalam melakukan penjadwalan mata kuliah, dan dilakukan perbandingan dengan algoritme genetika. Penjadwalan mata kuliah dengan menggunakan metode hybrid algoritme genetika dan algoritme cuckoo search mampu memperoleh rata-rata nilai fitness yang lebih tinggi dibandingkan algoritme genetika saja, dan metode hybrid algoritme genetika dan algoritme cuckoo search mampu memperluas eksplorasi dalam ruang pencarian, sehingga memiliki peluang untuk menghasilkan solusi yang terhindar dari terjebak di optima lokal.
","Genetic algorithm performs well to solve complex problem, such as university course scheduling which is considered as NP-hard problem. Genetic algorithm excels in constrained optimization problems with large search space. But, the disadvantage of this algorithm, is that it can be easily trapped in the local optima. Researchers have done some research using hybrid method of two different meta-heuristic algorithms to avoid the local optima problem.
This research has been done to implement the hybrid method of genetic algorithm and cuckoo search algorithm to seek its performance in solving university course scheduling problem, and compares it with the performace of genetic algorithm. The hybrid method gives better average result than genetic algorithm does, and this method tends to do better in exploring the search space so that it has the possibility to prevent the solutions from being trapped in local optima.
",,,126656,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171066,,Vido Valianto,SISTEM PERHITUNGAN DAN KLASIFIKASI KENDARAAN BERMOTOR,,,"kalman filter, Convolutional Neural Network (CNN) ","Wahyono S. Kom, Ph. D.",2,3,0,2019,1,"Solusi  dari permasalahan lalu lintas yang terjadi Indonesia salah satunya adalah dengan mengatur jumlah kendaraan yang ada pada jalan. Kurangnya data dan masih manualnya sistem observasi yang dilakukan dengan kamera closed-circuit television (CCTV) membuat keputusan dan solusi terhadap permasalahan lalu lintas membutuhkan proses yang lama. Dengan berkembangnya pendekatan computer vision dan teknologi yang ada maka akan dibuat sebuah sistem yang mampu melakukan perhitungan kendaraan yang lewat pada jalanan.
Pada penelitian ini akan dibuat sebuah sistem yang menggunakan pendekatan computer vision, Kalman filter, dan Convolutional Neural Network (CNN) yang kemudian akan diujikan untuk menghasilkan perhitungan dan klasifikasi terhadap kendaraan yang lewat dengan akurat dan secara real-time.
","The solution to the traffic problems that occur in Indonesia is by regulating the number of vehicles on the road. The lack of data and the manual observation system carried out with closed-circuit television (CCTV) cameras make decisions and solutions to traffic problems requiring a long process. With the development of the computer vision approach and the existing technology, a system will be made that is capable of calculating vehicles passing on the road.
In this study, a system that uses a computer vision, Kalman filter, and Convolutional Neural Network (CNN) approach will be created which will then be tested to produce accurate and real-time calculations and classifications of passing vehicles.
",,,122531,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
180026,,ASHADI NUR HIDAYAT ,KLASIFIKASI BUAH-BUAHAN DARI CITRA BERDASARKAN  FITUR WARNA DAN BENTUK  MENGGUNAKAN NEAREST MEAN CLASSIFIER,,,"klasifikasi, buah, fitur, warna, bentuk, nearest mean classifier ","Moh. Edi Wibowo, S.Kom., M.Kom., Ph.D.",2,3,0,2019,1,"Prediksi kualitas buah membutuhkan klasifikasi buah berdasarkan jenisnya dengan akurasi yang optimal, sehingga dibutuhkan model klasifikasi yang mendukung hal tersebut. Sebagai langkah awal, maka dilakukan penelitian klasifikasi buah-buahan dari citra berdasarkan fitur warna dan bentuk. Untuk memperoleh keseluruhan fitur warna dan bentuk dari setiap buah, peneliti memotret citra dari berbagai sisi buah yang diwakili oleh 1 buah setiap jenisnya. 

Fitur warna yang digunakan adalah rerata dan deviasi standar warna sedangkan fitur bentuk yang digunakan adalah luas, perimeter, compactness dan momen invariant. Kemiripan diukur melalui pengukuran jarak vektor fitur. Pemilah yang digunakan adalah nearest mean classifier. Ada 8 kelas buah yang diperkenalkan pada klasifikasi buah-buahan. 

Dilakukan pengujian menggunakan 5-fold cross validation untuk melakukan klasifikasi buah-buahan yang telah diperkenalkan kategorinya. Dilakukan percobaan membandingkan performa klasifikasi berdasarkan fitur warna, fitur bentuk dan gabungan keduanya. Fitur warna memiliki akurasi lebih baik daripada fitur bentuk, yaitu sebesar 10,21%. Penggunaan fitur warna dan bentuk memiliki akurasi terbaik, yaitu mencapai 97,50%.
","Prediction on fruit quality requires classification of fruits with optimal accuracy. To support that,  classification modelling is important. As a first step, the fruits classification research from the images based on color and shape features is conducted. To obtain the color and shape features of each fruit, images is captured from various sides of the fruit. One class represented by one fruit.

The color features are the mean and standard deviation of the colors. The shape features are area, perimeter, compactness and invariant moment. The similarity is measured by feature vector distance. The proposed model uses nearest mean classifier. There are 8 fruit classes that have been recognized to the proposed model. 

The proposed model is tested by 5-fold cross validation to classify fruits that have been recognized their categories. An experiment was performed comparing the performance of classifications based on color features, shape features and a combination of both. The color feature has better accuracy than the shape feature, which is 10.21%. The use of color and shape features have the best accuracy with average accuracy reaches to 97.50%. 
",,,130776,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
179773,,Gama Candra Tri Kartika,"STUDI PERBANDINGAN METODE EKSTRAKSI FITUR, SELEKSI FITUR, DAN KLASIFIKASI PADA ANALISIS SENTIMEN SELAMA PEMILIHAN UMUM INDONESIA 2019",,,"analisis sentimen, seleksi fitur, ekstraksi fitur, klasifikasi teks / sentiment analysis, feature selection, feature extraction, text classification","Sri Mulyana, Drs., M Kom.",2,3,0,2019,1,"Data  dari  media  sosial  seperti  Twitter  dan  situs  portal  berita  seperti  Line-Today merupakan data yang objektif untuk diolah dikarenakan penggunanya yangsangat aktif dalam menyampaikan pendapatnya di kedua media sosial dan portal be-rita tersebut.  Namun perbandingan metode dalam analisis sentimen belum banyakditerapkan dalam domain Bahasa Indonesia dan dataset Twitter dan Line-Today.Tujuan dari penelitian ini adalah untuk mengetahui nilai terbaik dari metodeekstraksi fitur, seleksi fitur, dan metode pengklasifikasian serta pemilihan kombinasiyang terbaik dari proses seleksi fitur dan ekstraksi fitur, terhadap nilai analisis senti-men dengan berbagai metode pengklasifikasian.Hasil penelitian ini adalah pada dataset Line-Today,  pengklasifikasi terbaikpada masing-masing rata-rata semua nilai adalah Extra Tree (akurasi = 84.15 %), danExtra Tree Classifier (f1 score= 78.44 %). Kemudian pada dataset Twitter, pengkla-sifikasi terbaik pada masing-masing rata-rata semua nilai adalah Extra Tree (akurasi= 81.25 %) dan Extra Tree Classifier (f1 score= 78.45 %).","Data from social media such as Twitter and online news portal such as Line-Today are objective data to be processed because users are very active in expressingtheir opinions on both social media and online news portal.  But the comparison ofmethods  in  sentiment  analysis  has  not  been  widely  applied  in  the  Indonesian  lan-guage, Twitter and Line-Today dataset .The purpose of this study was to determine the best value of feature extractionmethods, feature selection methods, and classification methods and the selection ofthe best combination of feature selection and feature extraction processes, to the valueof sentiment analysis with various classification methods.The results of this study on the Line-Today dataset, the best classifiers for eachaverage of all values are Extra Trees (accuracy = 84.15 %), and Extra Tree Classifiers(f1 score = 78.44 %). Then on the Twitter dataset, the best classifiers in each averageof all values are Extra Trees (accuracy = 81.25 %), and Extra Tree Classifier ( f1 score= 78.45 %).",,,130519,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175172,,Kukuh Satrio Adi,Pembuatan Model Matchmaking pada Permainan Hearthstone Menggunakan Hybrid dan Reciprocal Recommender System,,,"Matchmaking, Recommender system, collaborative filtering, user profile","Khabib Mustofa, S.Si., M.Kom.,  Dr.Techn.; Isna Alfi Bustoni, M.Eng.",2,3,0,2019,1,"Matchmaking pada permainan multiplayer merupakan sistem yang membantu seorang pemain melawan atau bekerja sama dengan pemain lain yang sedang memainkan permainan yang sama pada waktu yang bersamaan, selain itu juga terdapat beberapa faktor yang mempengaruhi matchmaking. Pada permainan Hearthstone faktor yang mempengaruhi matchmaking adalah rating yang dibuat oleh permainan itu sendiri yang disebut MMR, namun itu tidak akan menjamin keseimbangan pada permainan. Recommender system adalah sistem yang merekomendasikan sesuatu atau seseorang ke seseorang berdasarkan apa yang disukai atau dibutuhkan. Dengan menggunakan recommender system untuk melakukan matchmaking akan mengatasi masalah seperti kekalahan atau kemenangan beruntun yang membuat pemain menjadi jenuh.  Pada Penelitian ini digunakan metode hybrid recommender system yang merupakan gabungan dari content-based dan user-based collaborative filtering pada data simulasi kumpulan permainan Hearthstone yang dilakukan oleh 9 orang dan berjumlah 204 match. Pemain yang direkomendasi merupakan pemain yang mempunyai profil pengguna. Lalu terdapat tahap Reciprocal recommender system yang merupakan rekomendasi orang-ke-orang, sehingga rekomendasi terjadi secara 2 arah. Pengujian performa akan dilakukan dengan recall, precision dan f-measure  Hasil dari penelitian ini merupakan rekomendasi matchmaking, yang selanjutnya diuji dengan simulasi permainan. Penelitian ini memiliki performa recall sebesar 77,7%, Precision sebesar 77,7% dan f-measure sebesar 77,7% dengan jumlah 20 rekomendasi matchmaking.","Matchmaking in multipayer games is a system that helps players meet their opponent or teammate in the same game at the same time. There are other factors that influence matchmaking. In Hearthstone, matchmaking is influnced by their own rating called MMR, but it does not guarantee the balance of the game. Recommender system is a system that recommend things or person to a person based on what they like or need. Using Recommender system for matchmaking will overcome problem such as, losing or winning streak that can make players discouraged. In this research, method that used is hybrid recommender system, which is a method that combines content-based and user-based collaborative filtering. Hybrid recommender system is going to be use on sets of games simulation data that were played by nine players and consisted of 204 matches. Only players who have user profile get recommendation. Furthermore, there is a phase called reciprocal recommender system, which is a people-to-people recommendation, hence making recommendation to be two way. Performance evaluation used in this research are recall, precision, and f-measure. Results on this research are matchmaking recommendation, which will be evaluate by simulating the games. This research has recall value of 77,7%, precision value of 77,7% and f-measure value of 77,7% with total of 20 matchmaking recommendation. ",,,126204,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167753,,FATAH NUR ALAM MAJID,Studi Komparasi Performa Load Balancer dengan Menggunakan Teknologi Docker Container,,,"Cloud, Load Balancer, Docker Container, Skalabilitas","Dr.techn. Ahmad Ashari, M.Kom.",2,3,0,2019,1,"Penerapan teknik load balancing yang bertujuan untuk melakukan distribusi beban kerja server memiliki peran yang sangat penting. Teknik load balancing merupakan solusi yang tepat dan efektif untuk menangani beban server yang sibuk dan dapat meningkatkan skalabilitas pada sistem terdistribusi. Namun untuk dapat memasang sistem load balancer diperlukan proses yang rumit dan menggunakan resource yang cukup banyak. Disisi lain, teknologi container menjadi sebuah topik hangat di dunia IT. Teknologi container sendiri merupakan salah satu teknik virtualisasi pada sistem, dimana pada teknik ini isolasi terjadi pada level sistem operasi dari mesin server yang dipakai. Teknologi container dianggap menarik karena dapat menyediakan fitur portabilitas bagi aplikasi perangkat lunak sehingga dapat berjalan pada sebagian besar platform.

Pada penelitian ini dilakukan pengujian perbandingan performa implementasi salah satu aplikasi load balancer dengan memanfaatkan teknologi container. Pengujian dilakukan pada sebuah environment secara bergantian sebanyak dua kali, yaitu load balancer dengan memanfaatkan teknologi container dan load balancer yang dipasang secara native. Terdapat dua skenario pengujian untuk mendapatkan data hasil uji, yaitu dengan membedakan jumlah request dari client terhadap server dengan selang waktu satu detik. Setelah data didapatkan selanjutnya dihitung rata-rata dan dilakukan beberapa pengujian terhadap data tersebut dalam beberapa parameter, yaitu response time, availability, dan distribusi request yang didapatkan oleh masing-masing server.

Hasil dari penelitian menunjukkan bahwa NGINX sebagai load balancer tidak memiliki perbedaan performa ketika dipasang secara native maupun dipasang dengan memanfaatkan container. Kedua load balancer yang diuji menunjukkan performa yang sama dengan tingkat keyakinan sebesar 99 persen dilihat dari aspek tingkat availability dan response time. Sehingga dapat direkomendasikan pemasangan load balancer dengan memanfaatkan container untuk mendapatkan fitur portabilitas.","Load balancing technique application used for distributing server load has very important role. Load balancing technique is becoming an effective solution for handling massively-busy server load and to increase scalability in distributed system. However, to implement those load balancer system, a complicated and resource-consuming processes is needed. At the other side, container technology is becoming trends in IT world. Container technology itself is a virtualization technique at system-level, where the isolation can be found in the OS-level on the server used. Container technology considered as the most interesting technology because it can provide portability for applications to run on most platforms.

In this research, is done comparison study of load balancer implementation using container technology. This research will be conducted in a single environment twice, that is load balancer using container technology and load balancer with native implementation. There will be two test scenarios to obtain statistics that differences in request count from client into server with interval of one second. After obtained, the statistics will be calculated to get the average and the data will be evaluated regarding some parameters, that is response time, availability, and request distribution for each server instance.

The result of this research shows that NGINX as load balancer doesn't have any significance differences in performance either when it is used natively or used by utilizing container technology. Both load balancer system evaluated shows equal performance with confidence interval (CI) of 99 percent based on availability and response time obtained. So it is recommended to deploy a load balancer using container based on equal performance given and portability provided by container.",,,118938,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175183,,Abdul Basith Basyiron,Pendeteksian Api Berbasis Pengolahan Citra Digital Menggunakan Fitur Warna dan Gerak,,,"deteksi api, model warna api, fitur gerak, support vector machine","Wahyono, S.Kom., Ph.D.",2,3,0,2019,1,"Sistem deteksi api pada umumnya menggunakan sensor asap dan suhu. Namun, sensor asap dan suhu tidak dapat menjangkau tempat yang luas dan terbuka sehingga penggunaan sensor asap dan suhu masih kurang efektif. Oleh karena itu, berkembangnya metode pengolahan citra digital dan didukung banyaknya kamera CCTV di berbagai tempat maka sistem deteksi api berbasis pengolahan citra digital dapat digunakan untuk menjangkau tempat yang luas dan terbuka. Beberapa penelitian telah melakukan deteksi api berbasis pengolahan citra digital namun hanya menggunakan fitur warna saja. Hal ini dapat menyebabkan terjadinya kesalahan deteksi jika terdapat objek bukan api yang memiliki warna seperti api.

Pada penelitian ini dilakukan deteksi objek api pada video menggunakan kombinasi segmentasi fitur warna dan gerak sehingga diharapkan dapat mengurangi kesalahan deteksi. Pada segmentasi berbasis fitur warna digunakan metode berbasis aturan dalam ruang warna RGB, HSI dan YCbCr. Kemudian, segmentasi berbasis fitur gerak menggunakan Mixture of Gaussian (MoG). Lalu, fitur luas, titik tengah, dan Hu Moments diekstraksi dari hasil kombinasi tersebut. Hasil dari proses ekstraksi digunakan untuk membuat model Support Vector Machines (SVM). 

Evaluasi pengujian dilakukan pada hasil segmentasi dan model SVM. Evaluasi segmentasi yang menggunakan kombinasi fitur warna dan gerak menghasilkan nilai rata-rata Intersection over Union (IoU) sebesar 0.5979, sedangkan yang hanya menggunakan fitur warna menghasilkan nilai rata-rata Intersection over Union (IoU) sebesar 0.5819. Evaluasi terhadap model SVM yang menggunakan kombinasi fitur warna dan gerak menghasilkan akurasi 98.4%, presisi 98.3%, recall 98.7%, dan f-measures 98.5%. Sementara itu, evaluasi terhadap model yang hanya menggunakan fitur warna meghasilkan akurasi 97.5%, presisi 96.6%, recall 98.6%, dan f-measures 97.6%. Hasil evaluasi segmentasi dan model SVM menunjukkan bahwa penggunaan kombinasi fitur warna dan gerak memberikan hasil yang lebih baik dibandingkan fitur warna saja.","Fire detection systems typically use smoke and temperature sensors. However, smoke and temperature sensors can not reach a wide and open places so that the use of smoke and temperature sensors are less effective. Therefore, the development of digital image processing methods and supported by the number of CCTV cameras in various places, the fire detection system based on digital image processing can be used to reach a wide and open places. Some studies have carried out fire detection based on digital image processing but only use color features. This can lead to detection errors if there is not a fire object that has a color like fire.

In this study, fire detection was performed on the video using a combination of color and motion features segmentation so that it was expected to reduce detection errors. In color feature based segmentation, rule-based methods are used in RGB, HSI and YCbCr color spaces. Then, the feature-based segmentation uses Mixture of Gaussian (MoG). Then, the area, centroid, and Hu Moments feature are extracted from the results of the combination. The results of the extraction process are used to create the Support Vector Machines (SVM) model.

Evaluation tests are carried out on the results of segmentation and SVM models. Evaluation segmentation using a combination of color and motion features produce an average value Intersection over Union (IOU) of 0.5979, while only using color features produce an average value Intersection over Union (IOU) of 0.5819. Evaluation of the SVM models that use a combination of color and motion features produce accuracy 98.4%, precision 98.3%, recall 98.7%, and f-measures 98.5%. Meanwhile, evaluation of models using only the color feature resulted in 97.5% accuracy, 96.6% precision, recall 98.6%, and f-measures 97.6%. The results of evaluation of segmentation and SVM models show that the use of a combination of color and motion features gives better results than just color features.",,,125937,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182352,,NADYA AVIRIANTA S,Perbandingan Metode Prapemrosesan Citra pada Convolutional Neural Network untuk Pengenalan Aksi Manusia,,,"Convolutional Neural Network, Prapemrosesan Citra, Pengenalan Aksi Manusia, Machine Vision","Drs. Janoe Hendarto, M.I.Kom; Wahyono, Ph.D",2,3,0,2019,1,"Jumlah penduduk dunia berusia 65 atau lebih (lansia) diproyeksikan akan meningkat menjadi hampir 1,5 miliar pada tahun 2050. Orang lanjut usia rentan terhadap berbagai risiko saat beraktivitas sehari-hari sehingga aktivitasnya perlu dipantau. Oleh karena itu, perlu pengenalan aksi secara otomatis dengan teknologi machine vision. Salah satu cara untuk melakukan pengenalan aksi adalah dengan Convolutional Neural Network (CNN). Akan tetapi, klasifikasi menggunaan CNN tanpa prapemrosesan akan menghasilkan akurasi yang buruk. Metode prapemrosesan yang digunakan mempengaruhi performa model yang dihasilkan. Oleh karena itu, perlu dilakukan penelitian mengenai berbagai metode prapemrosesan citra masukan pada CNN untuk mendapatkan model yang optimal.
Pada penelitian ini akan dilakukan pengujian parameter pada CNN. Kemudian dilakukan perbandingan metode prapemrosesan yaitu resizing, enhancement, pembuatan citra biner dan citra gradien, serta augmentasi data. Setelah itu dilakukan evaluasi dari model yang telah didapatkan. 
Dari hasil penelitian, didapatkan parameter terbaik model CNN untuk citra RGB yaitu ukuran input 64x64, learning rate 0,001, dropout rate 0,1, jumlah kernel pada lapisan konvolusi 64 dengan ukuran 3x3, dan jumlah neuron pada lapisan dense 128. Parameter terbaik model CNN untuk citra grayscale yaitu ukuran input 32x32, learning rate 0,002, dropout rate 0,2, jumlah kernel pada lapisan konvolusi 32 dengan ukuran 3x3, dan jumlah neuron pada lapisan dense 128. Dari hasil k-fold cross validation, metode prapemrosesan yang terbaik yaitu prapemrosesan citra grayscale berukuran 64x64 dengan sharpening dan augmentasi berupa horizontal flip dengan akurasi 0,852. Dari hasil pengujian, metode prapemrosesan yang menghasilkan akurasi terbaik yaitu prapemrosesan citra grayscale berukuran 64x64 dengan sharpening, dengan akurasi 0,660.
","The number of the world's population aged 65 or over (elderly) is projected to increase to almost 1.5 billion by 2050. Elderly is vulnerable to various risks on their daily activities so their activities need to be monitored. Therefore, it is necessary to automatically recognize actions with machine vision technology. One of the methods to do action recognition is with Convolutional Neural Network (CNN). However, using CNN without preprocessing will result in poor classification accuracy. The preprocessing method used affects the performance of the resulting model. Therefore, it is necessary to do research on various image preprocessing methods on CNN input to get the optimal model.
In this study, parameters are be tested on CNN. Then various preprocessing methods namely resizing, enhancement, creation of binary and gradient images, and data augmentation are compared. After that, evaluation of the model obtained are done. 
From the results of the study, the best parameters obtained for the CNN model for RGB images are input size 64x64, learning rate of 0.001, dropout rate 0.1, the number of kernels in the convolution layer 64 with size 3x3, and the number of neurons in the dense layer 128. The best parameters for the CNN model for grayscale images are input size 32x32, learning rate 0.002, dropout rate 0.2, the number of kernels in the convolution layer 32 with size 3x3, and the number of neurons in the dense layer 128. From the results of k-fold cross validation, the best preprocessing method is 64x64 grayscale image preprocessing with sharpening and augmentation in the form horizontal flip, with an accuracy of 0.852. From the test results, the preprocessing method that produces the best accuracy is the 64x64 grayscale image preprocessing with sharpening, with an accuracy of 0.660.
",,,133355,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
174420,,Lantang Satriatama,Komposisi Musik pada Protokol Musical Instrument Digital Interface Menggunakan Long Short-Term Memory Recurrent Neural Network,,,"MIDI, LSTM, Musik, Komposisi","Afiahayati, S.Kom., M.Kom., Ph.D.",2,3,0,2019,1,"Salah satu perkembangan teknologi pada musik adalah penggunaan protokol Musical Instrument Digital Interface (MIDI) sebagai protokol digital untuk penyimpanan notasi musik. Proses komposisi pada MIDI juga sudah berkembang, salah satunya adalah penggunaan proses pembelajaran mesin dengan model Long Short-Term Memory (LSTM) untuk komposisi musik. Namun sebagian besar peneliti menggunakan representasi per durasi not terkecil (frame) sebagai input model, sesuatu yang berbeda dengan standar notasi musik per not (notasi balok, ABC, dll). Terdapat penelitian yang sudah menggunakan representasi per not lagu dengan fitur delta, durasi, dan pitch, namun proses prediksinya masih dilakukan secara satu per satu untuk setiap fitur dalam sebuah not.

Pada penelitian ini, dibangun sebuah model LSTM yang mampu memprediksi semua fitur dalam sebuah not dalam satu langkah, dengan menggunakan batch normalization dan dropout sebagai metode normalisasi model. 

Setelah dilakukan proses latih, validasi, dan uji, model berhasil mendapatkan akurasi fitur (delta, durasi, dan pitch) sebesar 90,88%, 74,44%, dan 33,10% untuk dataset MIDI homogen (dengan struktur mirip antar lagu) dan sebesar 84,02%, 73,93%, dan 33,80% untuk dataset MIDI heterogen (dengan struktur berbeda-beda antar lagu).","One of technological advancement in music is the use of Musical Instrument Digital Interface (MIDI) as protocol to save musical notation. Music composition with MIDI is also developing, one of them is the usage of machine learning with Long Short-Term Memory (LSTM) model for music composition. But most researcher are using per frame representation for model inputs, something that is different from standard per note music notation (staff notation, ABC, etc). There are some research who already use per note representation with delta, duration, and pitch as note features, but the prediction of the features in a note is still predicted one by one. 

In this research, we created a LSTM model who can predict all features in a note at the same time (within a single timestep) with batch normalization and dropout for its normalization methods. 

After the model went through training, validation, and testing phase, the model successfully achieved features (delta, duration, and pitch) accuracy 90.88%, 74.44%, and 33.10% for homogen MIDI dataset (with similar structure between each songs) and 84.02%, 73.93%, and 33.80% for heterogen MIDI dataset (with different structure between each songs).
",,,125265,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
179285,,NANDINI WARA SASRI,ANALISIS SENTIMEN OPINI WARGANET TERHADAP INDIE GAME PADA TWITTER,,,"Analisis Sentimen, Twitter, Indie Games, Support Vector Machine, Multinomial Na&Atilde;&macr;ve Bayes","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2019,1,"Indie games semakin banyak bermunculan dalam industri games sehingga persaingan semakin ketat. Akan tetapi tidak banyak dari indie games yang sanggup mencapai peringkat dan kualitas yang tinggi, terutama dengan sedikitnya sumber daya yang dimiliki pengembang indie game dibandingkan dengan pengembang AAA. Maka dari itu diharapkan analisis sentimen sebagai kritik dan referensi bagi developer, dan menjadi acuan bagi pemain untuk memilih indie game.
Penelitian ini mencoba mengembangkan model analisis sentimen menggunakan data tweet dengan 3 judul indie game populer yang pernah mendapatkan penghargaan, yaitu Undertale, Night in the Woods, dan Minecraft. Model analisis dibangun dengan 2 metode, yaitu metode Support Vector Machine dan metode Multinomial Na&Atilde;&macr;ve Bayes. Penelitian ini juga akan membandingkan performa dari kedua metode.
Hasil penelitian menunjukkan nilai akurasi dari metode SVM lebih tinggi daripada metode NB, yaitu 70,68% untuk SVM dan untuk NB adalah 52,55%. Waktu proses dengan Support Vector Machine memiliki total lebih banyak dari Multinomial Na&Atilde;&macr;ve Bayes, yaitu 5,85 detik untuk SVM dan 0,02 detik untuk MNB.","Indie games has been emerging much in game industry so that the competition has become tough. However, not many from indie games which can reach high ranking and quality, especially since indie game developers only have so little
resources compared to AAA developers.
This research will develop sentiment analysis model using tweet data with 3 titles of popular indie game which has received award, which are Undertale, Night in the Woods, and Minecraft. Analysis Model built with 2 methods, Support Vector Machine and Multinomial Na&Atilde;&macr;ve Bayes. This research will also compare the performance of both methods.
The result shows that accuracy score for SVM is higher than MNB, with SVM being 70.68 % and 52.55% for MNB. Running time for SVM is higher than MNB, where SVM is 5.85 seconds and MNB is 0.02 seconds.",,,130048,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
170839,,RIGCHO AZHAR,Sistem Peramalan Konsentrasi Particulate Matter (PM 10) di Udara Menggunakan Algoritma Genetika,,,"prediksi, prediction, genetic algorithm, algoritma genetika, kualitas udara, air quality, PM10, Jakarta","Drs. Retantyo Wardoyo, M.Sc., Ph.D.",2,3,0,2019,1,"Pencemaran udara dapat menyebabkan berbagai masalah kesehatan. Salah satu jenis polutan yang dapat menyebabkan pencemaran udara adalah Particulate Matter 10 (PM10). Partikel yang berukuran &lt;10 &Acirc;&micro;m ini dapat terhirup ketika seseorang bernapas dan dapat menyebabkan meningkatnya resiko penyakit-penyakit kardiovaskuler, pernapasan, serta kanker paru-paru apabila seseorang terkena paparan terus-menerus.Menurut data BPS, Kota Jakarta merupakan salah satu kota di Indonesia yang memiliki kualitas udara kurang baik. Banyaknya kendaraan bermotor menjadi salah satu  faktor penyebabnya. Prediksi konsentrasi PM10 dapat menjadi salah satu upaya untuk mengantisipasi kurangnya kualitas udara.
Pada penelitian ini dibangun sebuah sistem untuk prediksi konsentrasi PM10 di udara Kota Jakarta menggunakan algoritma genetika. Algoritma ini terinspirasi dari proses evolusi untuk mencari solusi terbaik. Algoritma genetika digunakan untuk mengoptimasi koefisien dari fungsi objektif yang digunakan untuk menghitung prediksi konsentrasi PM10.
Penelitian dilakukan dengan menggunakan data konsentrasi PM10 pada tanggal 1-30 November 2016 yang bersumber dari website Laboratorium Lingkungan Hidup Daerah yang dikelola Dinas Lingkungan Hidup Provinsi DKI Jakarta. Sistem melakukan prediksi konsentrasi PM10 satu jam ke depan dengan menggunakan kromosom terbaik dari hasil pelatihan. Dari hasil pengujian, didapatkan rata-rata nilai fitness dari  10 kali percobaan adalah 0,01630, nilai MAD sebesar 5,54661, dan nilai MAPE sebesar 10,45010.
","Air pollution can cause many health problems. One type of pollutan that can cause air pollution is Particulate Matter 10 (PM10). These &lt;10 &Acirc;&micro;m sized particles can be inhaled when someone is breathing and can increase the risk of cardiovascular and respiratory disease. Someone can even get lung cancer due to continuous exposure of PM10. Acording to the data from BPS, Jakarta is one of the cities in Indonesia that have low air quality. The high number of vehicles is one of the contributing factor. Prediction of PM10 concentration can be one of the effort to anticipate the low air quality.
A system to predict the concentration of PM10 in the air of Jakarta using genetic algorithm will be built in this research. This algorithm is inspired by the evolution process to find the best solution. Genetic algorithm is used to optimize the coeficient of objective function which is used to count the prediction of PM10 concentration.
This research was conducted using data of PM10 concentration on 1-30 November 2016 from the website of Laboratorium Lingkungan Hidup Daerah which is organized by Dinas Lingkungan Hidup of DKI Jakarta Province. The system predicts the concentraion of PM10 one hour ahead using the best solution chromosome from the results of training process. From the testing result, the forecasting system gives the average of fitness score 0,01630, MAD score 5,54661, and MAPE value 10,45010.
",,,122183,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175192,,Andri Imanudin,PEMODELAN TOPIK ULASAN APLIKASI PADA GOOGLE PLAY MENGGUNAKAN BITERM TOPIC MODEL (BTM),,,"Kata Kunci: pemodelan topik, biterm topic model, topic coherence","Suprapto, Drs., M.I.Kom., Dr.",2,3,0,2019,1,"Google Play sebagai Mobile Application Market mengalami pertumbuhan yang pesat. Banyak developer telah mempublikasikan aplikasi mobile buatannya di Google Play. Para developer yang telah mempublikasikan aplikasi buatannya dapat memperoleh umpan balik dari para pengguna tentang aplikasi buatannya melalui fitur ulasan yang tersedia. Hanya saja jumlah ulasan tersebut terus bertambah dari waktu ke waktu. Untuk itu dibutuhkan pemodelan topik yang mampu menganalisis dan mengidentifikasi topik apa yang dibahas dalam ulasan aplikasi tersebut. Terlebih ulasan aplikasi dalam Google Play tergolong ulasan pendek dimana masalah data sparsity dapat terjadi, seperti kata pembentuk topik dalam teks pendek kebanyakan hanya muncul sekali dan hanya ada beberapa kata saja didalam teks pendek yang berhubungan dengan topik. 

Proses pemodelan topik dalam penelitian ini menggunakan Biterm Topic Model. Data yang digunakan adalah data ulasan aplikasi Instagram pada Google Play. Data diambil sebanyak 115.199 mulai dari tanggal 12 Februari 2019 hingga 8 April 2019. 

Pengujian pemodelan topik pada parameter passes atau iterasi dilakukan dengan melihat kestabilan coherence score yang dihasilkan, sementara pengujian pemodelan topik pada parameter jumlah topik dilakukan dengan melihat rata-rata coherence score tertinggi. Hasil pengujian didapatkan coherence score stabil pada passes atau iterasi ke-250 dan rata-rata topic coherence tertinggi pada saat banyaknya topik sama dengan 7 topik. Berdasarkan parameter tersebut dilakukan pemodelan topik dengan membandingan algoritma Biterm Topic Model dan Latent Dirichlet Allocation. Dari hasil pemodelan topik didapatkan rata-rata coherence score pada Biterm Topic Model lebih tinggi dari pada Latent Dirichlet Allocation dan pemodelan topik menggunakan Biterm Topic Model berhasil mengidentifikasi keluhan, permasalahan atau bug yang dirasakan oleh pengguna.","Google Play as a Mobile Application Market experienced rapid growth. Many developers have published mobile applications made on Google Play. Developers who have published their applications can get user feedback about their applications through the available review features. It's just that the number of reviews continues to grow over time. For this reason, topic modeling is needed that is able to analyze and identify what topics are discussed in the application review. Moreover, the app reviews in Google Play are classified as short reviews where data sparsity problems can occur, such as the words forming topics in short text mostly only appear once and there are only a few words in a short text related to the topic.

The topic modeling process in this study uses the Biterm Topic Model. The data used is the Instagram application review data on Google Play. Data taken as many as 115,199 starting from February 12, 2019 to April 8, 2019.

The topic modeling test on the passes or iterations parameters is done by looking at the stability of the coherence score produced, while the topic modeling testing on the parameter number of topics is done by looking at the highest average coherence score. The test results showed that the coherence score was stable in 250 passes and iterations and the highest topic coherence was the highest when the number of topics was 7 topics. Based on these parameters, topic modeling was conducted by comparing the Biterm Topic Model and Latent Dirichlet Allocation algorithms. From the topic modeling results, the average coherence score on the Biterm Topic Model is higher than the Latent Dirichlet Allocation and topic modeling using Biterm Topic Model successfully identifies complaints, problems or bugs perceived by the user.",,,125911,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177753,,SYAFINA NURUL AIDA,PENGARUH PENAMBAHAN FITUR TOPICS DALAM PERHITUNGAN NILAI KEMIRIPAN REPOSITORI PADA GITHUB,,,"Perhitungan Nilai Kemiripan, GitHub, Fitur Topik, TF-IDF, Fleiss Kappa, Skala Likert","Suprapto, Drs., M.Kom., Dr.",2,3,0,2019,1,"Pengembangan sistem deteksi kemiripan repositori GitHub dilakukan untuk mendeteksi repositori yang memiliki kemiripan fungsi satu sama lain dengan menghitung kemiripan fitur repositori. Dalam penelitian sebelumnya, fitur readme files, stargazers, dan waktu pemberian star digunakan untuk perhitungan kemiripan repositori.  Namun, diperlukan pengembangan lebih lanjut dalam proses perhitungan kemiripan repositori. Ditambahkan fitur topics untuk menghitung kemiripan repositori GitHub. Pada penelitian ini, dilakukan perhitungan besarnya pengaruh fitur topics sebagai tambahan dalam perhitungan kemiripan repositori.
Dilakukan perbandingan nilai success rate, confidence, dan presisi dari evaluasi output antara perhitungan kemiripan repositori dengan tiga fitur (readme files, stargazers, dan waktu pemberian star) dan dengan empat fitur (tiga fitur ditambah fitur topics) untuk mengetahui pengaruh penambahan fitur topics. Sebanyak 501 data repositori digunakan sebagai basis data dan sebanyak 20 repositori diantaranya digunakan sebagai kueri untuk menghasilkan output lima repositori teratas yang memiliki nilai kemirpan paling tinggi dibanding kueri. 
Pada proses evaluasi hasil pengujian, dilibatkan sebanyak empat orang penilai yang memberikan penilaian pada hasil output perhitungan kemiripan dengan nilai kesepakatan Kappa sebesar 0,47. Nilai evaluasi akhir dari semua penilai kemudian ditentukan menggunakan skala Likert. Perhitungan kemiripan repositori dengan penambahan fitur topics mendapatkan nilai success rate, confidence, dan presisi lebih tinggi jika dibandingkan dengan perhitungan kemiripan repositori tanpa fitur topics dengan nilai succes rate pada T=4 sebesar 55%, nilai succes rate pada T=5 sebesar 25%, mean confidence sebesar 2,35, median confidence sebesar 2,00, mean presisi sebesar 0,16, dan median presisi sebesar 0,20.
","The development of repository similarity detection system was conducted to detect similarity between repositories by calculating features similarity. In the research that has been done, readme files, stargazers, and timestamp of star addition can be used for repository similarity calculations. However, further development is needed in the process of repository similarity calculation. Topics feature was added to calculating the similarity of repositories. In this study, the influence of topics features to the repository similarity calculation was calculated. 
In this study, we conduct a comparison of success rate, confidence, and precision value of final evaluation score from the output between two systems, i.e. system with three features (readme files, stargazers, and timestamp) and one with four features (with topics feature) to determine the influence of topics feature. The number of repository data use as database was 501. From the database there were 20 repositories ID used as query and for each of the query in the similarity calculation there were five first highest value of similarity selected as an output.
The evaluation process involved four raters who gave an assessment of the output system with Kappa value was 0,47. The final evaluation score was calculated using Likert scale. System with topics feature addition obtained higher success rate, confidence, and precision values than system with only 3 features (without topics feature), with success rate obtained at T=4 was 55%, success rate at T=5 was 25%, mean confidence was 2,35, median confidence was 2,00, mean precision was 2,00, and median precision was 0,20. 
",,,128577,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175454,,WAVA CARISSA PUTRI,DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORK (DCGAN) MENGGUNAKAN TEKNIK TRANSFER LEARNING UNTUK PEMBUATAN GAMBAR,,,"Deep Learning, Deep Convolutional Generative Adversarial Network, Transfer Learning, Convolutional Network, Generative Model","Anny Kartika Sari, S.Si, M.Kom., Ph.D",2,3,0,2019,1,"Tingginya pemintaan terhadap gambar pada industri hiburan dan limitasi dalam mengakses dan mendapatkan data asli pada domain kecerdasan buatan, menyebabkan dibutuhkannya data gambar buatan untuk melengkapi kekurangan data asli. Model generatif adalah model yang dapat menghasilkan gambar data sintetis, dengan Deep Convolutional Generative Adversarial Networks (DCGAN) sebagai salah satu contohnya. Dibalik kemampuanyna dalam menghasilkan data baru, DCGAN membutuhkan durasi yang lama dan data dengan jumlah yang tinggi untuk tahap pelatihannya. Sebuah teknik pada pembelajaran mesin bernama transfer learning dapat membantu DCGAN agar dapat mempelajari data lebih cepat karena memberi pengetahuan tentang masalah yang sebelumnya telah dipelajari.

Pada penelitian ini dilakukan eksperimen terhadap  DCGAN  menggunakan menggunakan metode transfer learning untuk melihat hasil dan perfoma model terhadap dataset yang memiliki ukuran lebih rendah daripada ukuran dataset asli, dan dataset yang berbeda pada tahap pra-pelatihan dan pelatihan. Hasil penelitian menunjukkan bahwa eksperimen dengan dataset yang sama dan eksperimen dengan dataset berukuran kecil menghasilkan gambar yang lebih meyakinkan dibandingkan eksperimen dengan dataset yang berbeda.
","Due to the high demand of images for the entertainment industry and limitations in obtaining training data for the artificial intelligence industry, synthetic images are needed in order to cover the shortage of real images. Generative model is a model that could generate new synthetic images, with Deep Convolutional Generative Adversarial Networks (DCGAN) as one of the example. Despite of its ability on generating new data, DCGAN require a large amount of data and took a huge amount of time for its training process. A technique used in machine learning, transfer learning, can helped DCGAN to improve its ability to learn the data faster as it provides knowledge from its previous training process.

In this research, experiments are conducted using DCGAN with transfer learning to see the model&acirc;€™s performance and result on smaller-sized datasets and different combination of datasets on its pre-training and training process. This research shows that the experiment using the same dataset for the pre-training and training processes generates images with more recognizable features than the ones with different datasets. It also shows smaller amount of data resulted in more recognizable images.
",,,126253,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182632,,Klaudius Liasta,"ANALISIS PERBANDINGAN KINERJA ALGORITMA C4.5, NAIVE BAYES CLASSIFIER, DAN SUPPORT VECTOR MACHINE UNTUK SISTEM DETEKSI KEJAHATAN KARTU KREDIT",,,"Sistem Deteksi Kejahatan, Kartu Kredit, Support Vector Machine, Gaussian Naive Bayes, Decision Tree C4.5, Undersampling","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2019,1,"Kartu kredit merupakan salah satu pilihan pembayaran tanpa uang tunai yang paling banyak digunakan di seluruh dunia. Peningkatan pengguna kartu kredit sebagai alat pembayaran harus diimbangi dengan peningkatan dari sistem keamanannya. Permasalahan ini menjadi fokus utama bagi penyedia layanan kartu kredit agar pengguna tetap merasa aman dalam bertransaksi. Terdapat dua mekasnisme dalam menghindari kejahatan pada kartu kredit, sistem pencegahan kejatahatan dan sistem deteksi kejahatan.
Pada penelitian dilakukan analisa perbandingan menggunakan 3 algoritma klasifikasi, yaitu Decision Tree C4.5, Naive Bayes Classifier, dan Support Vector Machine. Kemudian dilakukan juga ujicoba undersampling untuk mengetahui dampak undersampling pada dataset dengan ketidakseimbangan data terhadap sistem deteksi kejahatan pada kartu kredit. Hasil klasifikasi yang akan digunakan sebagai pembanding berupa nilai accuracy, precision, recall dan f-1 score.
Hasil dari penelitian ini adalah pada data dengan kondisi tidak seimbang, SVM menjadi klasifikasi terbaik dengan nilai rata-rata recall sebesar 75,23%, f1-score sebesar 82,14%, precision sebesar 90,45%, accuracy sebesar 99,94%, dan dengan waktu yang dibutuhkan 54,86 detik. Sedangkan pada data dengan kondisi seimbang karena undersampling, GNB memjadi klasifikasi terbaik dengan nilai rata-rata recall sebesar 52,17%, f1-score sebesar 55,11%, precision sebesar 58,40%, accuracy sebesar 91,56%, dan dengan waktu yang dibutuhkan 0,01 detik.

","Credit cards are one of the most widely used cashless payment options worldwide. The increase in credit card users as a means of payment must be balanced with improvements from the security system. This problem is the main focus of credit card service providers so that users continue to feel safe in the transaction. There are two mechanisms in avoiding crime on credit cards, crime prevention systems, and crime detection systems.
The purpose of the research is analyzing the best classification for the fraud detection systems using 3 classification algorithms, such as Decision Tree C4.5, Naive Bayes Classifier, and Support Vector Machine. Then do comparation of the result on a dataset after undersampling. The classification results that will be used as a comparison consist of accuracy, precision, memory and f-1 scores.
The result of this study are SVM is a good choice to handle imbalance with average value of recall 75,23%, f1-score 82,14%, precision  90,45%, accuracy 99,94%, and consuming 54 seconds,. Otherwise, on dataset after undersampling, GNB be the best methode with average value of recall 52,17%, f1-score 55,11%, precision 58,40%, accuracy 91,56%, and consuming 0,01 seconds.
",,,133482,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175211,,Desthalia,Pemodelan Topik Kelompok Jawaban Kuesioner Pertanyaan Terbuka,,,"kuesioner, pertanyaan terbuka, coding, pemodelan topik, Latent Dirichlet Allocation, Guided Latent Dirichlet Allocation, questionnaire, open-ended, coding, topic modeling","Sri Mulyana, Drs., M.Kom.",2,3,0,2019,1,"Kuesioner, sebagai salah satu instrumen pengumpulan data, memiliki penggunaan yang sangat luas dalam berbagai bidang ilmu. Pada umumnya, kuesioner memiliki dua format pertanyaan, yaitu pertanyaan terbuka dan pertanyaan tertutup. Dalam menganalisis jawaban kuesioner, dilakukan sebuah proses yang disebut dengan coding. Pada praktiknya, penggunaan kuesioner pertanyaan terbuka terhitung jarang dibandingkan dengan kuesioner pertanyaan tertutup karena perlunya usaha coding yang ekstensif dan memakan waktu untuk menentukan kelompok jawaban, meski jawaban yang dihasilkan kuesioner pertanyaan terbuka dapat menginformasikan peneliti lebih dalam tentang motivasi seorang responden ketika memberikan jawaban tentang sebuah isu.
Pada penelitian ini dilakukan pemodelan topik terhadap jawaban dari kuesioner pertanyaan terbuka menggunakan Latent Dirichlet Allocation (LDA) dan Guided Latent Dirichlet Allocation (GuidedLDA), yang mana masing-masing merupakan pemodelan topik unsupervised dan semi supervised. Validasi model menggunakan metode uji reliabilitas Cohen&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s kappa. Data yang digunakan dalam penelitian ini adalah data Community Survey 2016 &amp; 2017 milik pemerintah kota Austin, Texas. 
Berdasarkan hasil penelitian, ditemukan alasan mahalnya proses coding diakibatkan oleh intepretasi manusia yang bermacam-macam tentang sebuah kelompok jawaban. Baik LDA dan GuidedLDA mampu mengidentifikasi kelompok jawaban yang terdapat dalam data dan masalah yang muncul dari proses identifikasi kelompok jawaban. Adapun GuidedLDA menghasilkan lebih banyak kelompok jawaban dibandingkan dengan LDA. Hasil topik GuidedLDA juga lebih baik dibandingkan dengan LDA karena nilai kappa dari uji reliabilitas milik GuidedLDA lebih tinggi.","Questionnaire is an instrument to collect data and used extensively in various fields. There are two well-known format of questionnaire, open-ended and close-ended. To categorize and analyze the responses given to a questionnaire, researchers usually use a procedure called coding. However, the use of open-ended questionnaire is comparatively rare compared to close-ended questionnaire because of the need for expensive and time-consuming coding efforts to determine groups of answers, although answers to open-ended questionnaires can provide researchers more nuance about the motivation of a respondent when giving answers to an issue.
In this research, the author applies topic modeling to answers of open question questionnaires using Latent Dirichlet Allocation (LDA) and Guided Latent Dirichlet Allocation (GuidedLDA), each is unsupervised and semi supervised topic modeling. The model validation uses the Cohen&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s kappa reliability test. The data used in this study are Community Survey 2016 &amp; 2017 data done by the city of Austin, Texas.
Based on the results of the study, the root cause of how expensive coding efforts for open-ended questionnaire is the varying degree of interpretation of a response category. Both LDA and GuidedLDA are found to be able to identify groups of answers contained in the data and address the challenges that come with identifying response categories. GuidedLDA can produce more response groups compared to LDA. The results of the GuidedLDA topic are also better because the Kappa value of the GuidedLDA is higher than those of LDA. ",,,126052,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175467,,Tio Rahaditya Luthfitama,PENGENALAN ENTITAS BERNAMA PADA TWEET TENTANG BENCANA MENGGUNAKAN CONDITIONAL RANDOM FIELD,,,"Pemrosesan Bahasa Alami (NLP), Pengenalan Entitas Bernama (NER), Conditional Random Field (CRF).","Drs. Edi Winarko, M.Sc., Ph.D",2,3,0,2019,1,"Sosial media merupakan salah satu media yang dapat digunakan untuk mencari informasi yang sedang terjadi di dunia. Salah satu jenis informasi yang dapat didapatkan adalah bencana yang sedang terjadi. Sayangnya, media sosial tidak memiliki struktur kalimat yang baku dalam penulisannya. Sehingga, akan dibutuhkan waktu yang cukup lama untuk mengenali entitas lokasi, bencana, dan tanggal terjadinya.
Dengan berkembangnya pendekatan Natural Language Processing (NLP) dan didukung teknologi yang ada, maka dibuat sebuah model yang mampu melakukan pengenalan entitas pada kumpulan tweet yang terindikasi sebagai tweet tentang bencana. Sedangkan tweet yang tidak termasuk dalam tweet tentang bencana, tidak digunakan dalam dataset.
Model ini menggunakan algoritma Conditional Random Field (CRF) dan dibandingkan dengan algoritma Multinomial Naive Bayes (MNB). Tiga entitas yang digunakan sebagai komponennya yaitu DIS (bencana apa yang sedang terjadi), LOC (dimana lokasi bencana itu terjadi), dan DATE (tanggal kejadian bencana tersebut). Pada penelitian ini, hasil dari evaluasi model CRF lebih baik dibandingkan dengan MNB. Nilai Evaluasi CRF yang dihasilkan adalah 0.93 untuk precision, 0.89 untuk recall dan 0.91 untuk f1-score.","Social media is one of the media that can be used to find
information that is happening in the world. One type of information that can be
obtained is a disaster that is happening. Unfortunately, social media does not
have a standard sentence structure in writing. So, it will take a long time to
recognize the location, disaster, and date of occurrence.
With the development of the Natural Language Processing
(NLP) and supported by existing technology, a model is able to
recognize entities to a tweets that indicated as tweets about disaster.
Tweets that not about disasters are not used in the dataset.
This model uses the Conditional Random Field (CRF)
algorithm and compared with the Multinomial Naive Bayes (MNB) algorithm. Three entities that are used as components are DIS (what disaster is happened), LOC (where the disaster happened), and DATE (the date of the disaster happened). In this study, the results of the evaluation of the CRF model were better than
MNB. The Evaluation Value of CRF produced is 0.93 for precision, 0.89 for
recall and 0.91 for f1-score.",,,126263,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171628,,Kadek Gemilang Santiyuda,PENYELESAIAN MULTIPLE OBJECTIVES MULTIPLE DEPOT OPEN VEHICLE ROUTING PROBLEM MENGGUNAKAN ALGORITME INTELLIGENT WATER DROPS TERMODIFIKASI (STUDI KASUS KULINA),,,"Transportasi, Multiple Objectives, MDOVRP, MO-IWD-SA, HMOEA, Pareto Optimal","Faizal Makhrus,S.Kom., M.Sc., Ph.D.; Suprapto, Drs., M.I.Kom. Dr.",2,3,0,2019,1,"Seiring berkembangnya kebutuhan masyarakat, tujuan lain (multiple objectives) mulai dipertimbangkan dalam pembuatan rute di bidang transportasi di samping optimisasi biaya pengiriman. Usaha perusahaan untuk mengoptimisasi biaya logistik dan transportasi juga berkembang. Salah satu usaha yang dilakukan adalah dengan menggunakan jasa transportasi tanpa memiliki armada dan kurir secara langsung (outsourcing) untuk menghemat biaya pengelolaan dan perawatan. Kegiatan oursourcing armada dan kurir dalam menyelesaikan permasalahan pembuatan rute dapat diformulasikan menjadi permasalahan multiple objectives multiple depot open vehicle routing problem (MDOVRP).

Modifikasi metode intelligent water drops (IWD) diusulkan pada penelitian ini untuk menemukan kumpulan solusi-solusi yang Pareto optimal (pareto front, PF) untuk permasalahan multiple objectives MDOVRP. Metode yang diusulkan mengaplikasikan probabilitas simulated annealling (SA) dan metode optimisasi PF strength pareto algorithm II (SPEAII) pada IWD (MO-IWD-SA). Metode yang diusulkan kemudian diuji dan dibandingkan dengan hibrida multiple objectives evolutionary algorithm (HMOEA) (Bi, Han dan Tang, 2017). Pengujian dilakukan dengan menggunakan kedua metode untuk menyelesaikan multiple objectives MDOVRP yang sedang dihadapi oleh perusahaan katering online di Jakarta yakni Kulina. Metode yang diusulkan terbukti dapat menghasilkan perbaikan terhadap kualitas PF yang dihasilkan
hingga 6.4% lebih baik daripada metode HMOEA.","Delivery cost was once the only concern in transportation, but as society needs grow multiple objectives are to be concerned in route planning. The effort of companies to optimize their transportation issues also grow. One of the effort is to utilize transportaion service without directly owning any vehicle and courier to cut management and mantainance cost. The outsourcing of vehicle and courier in solving multiple objectives route planning can be formulated into multiple objectives multiple depot vehicle routing problem (MDOVRP).

In this research, a modification of intelligent water drops (IWD) is proposed to find set of good solutions in the sense of Pareto optimality (pareto front, PF) for the multiple objectives MDOVRP. The proposed method applies simulated annealing (SA) probability and PF optimization method called SPEA2 into IWD (MO-IWD-SA). The proposed method is then tested with a modification of hybrid multiple objective evolutionary algorithm (HMOEA) (Bi, Han dan Tang, 2017). The two methods are tested to solve multiple objectives MDOVRP currently faced by Kulina, an online catering company in Jakarta. The proposed method proved to show improvement of quality of PF found up to 6.4%.",,,122860,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
180846,,RANDY KUSUMA PUTRA,PREDIKSI PERGERAKAN HARGA SAHAM MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK SATU DIMENSI,,,"Convolutional Neural Network, Prediksi Pergerakan Saham, Machine Learning, CNN 1 Dimensi, Prediksi Saham.","Afiahayati, S. Kom, M. Cs, Ph. D.",2,3,0,2019,1,"Pasar modal Indonesia telah menjadi perhatian banyak pihak belakangan ini. Namun pergerakan harga saham sangatlah cepat, sehingga pemegang saham harus terus memperhatikan harga dari saham yang dimilikinya agar tidak terjadi kerugian. Oleh karena itu dibutuhkan sebuah sistem yang dapat digunakan untuk memprediksi pergerakan harga saham. 
Pada penelitian ini akan membuat sebuah sistem untuk memprediksi pergerakan harga saham. Sistem dibuat dengan tujuan untuk dapat mempermudah memprediksi pergerakan harga saham. Sistem tersebut akan dibuat dengan menggunakan CNN 1 Dimensi dan hybrid antara CNN 1 Dimensi dengan SVM sebagai pengklasifikasinya. 
Hasil penelitian menunjukan bahwa prediksi pergerakan harga saham dengan menggunakan metode CNN 1 Dimensi menghasilkan akurasi sebesar 60.73 %. Sedangkan dengan menggunakan hybrid antara CNN 1 Dimensi dengan SVM menghasilkan akurasi sebesar 62.57%. Sehingga dengan menggunakan gabungan antara CNN 1 Dimensi dan SVM menghasilkan akurasi yang lebih tinggi.
","The Indonesian capital market is the concern of many parties. However, the share price must be recorded immediately. Therefore we need a system that can be used to predict stock price movements.
In this research, a system to predict stock price movements will be made. The system was created with the aim of being able to predict stock price movements. This system will be made using CNN 1 Dimension and a hybrid of CNN 1 Dimension with SVM as the classification.
The results showed that the stock price prediction using the 1 Dimension CNN method resulted in an accuracy of 60.73%. Whereas by using a hybrid between CNN 1 Dimension and SVM produces an accuracy of 62.57%. 1 Dimension CNN and SVM result in higher accuracy.
",,,131777,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182136,,GALIH PRADIPTO W,ESTIMASI KECEPATAN KENDARAAN BERBASIS PENGOLAHAN CITRA,,,"analisis geometri, pengolahan citra","Wahyono, Ph.D.",2,3,0,2019,1,"Salah satu hal penting dalam lalu lintas adalah kecepatan kendaraan. Kecepatan kendaraan menjadi hal yang sangat penting untuk mengetahui arus lalu lintas
dari suatu kota. Selain itu, kecepatan kendaraan juga menjadi hal yang sangat penting
karena berkaitan dengan keselamatan berkendara, dengan adanya aturan batas maksimal dan minimal kecepatan kendaraan yang harus dipatuhi. Maka dari itu, monitoring
kecepatan kendaraan merupakan hal yang sangat penting.
Pada penelitian kali ini dilakukan estimasi kecepatan kendaraan dengan menggunakan pengolahan citra. Proses yang dilakukan meliputi deteksi, tracking, dan
kalibrasi. Proses deteksi dilakukan dengan metode Mixture of Gaussian, proses tracking dilakukan dengan menggunakan moment kendaraan. Proses kalibrasi dilakukan
dengan transformasi proyektif. Untuk meningkatkan akurasi, dilakukan analisis geometri untuk mendapatkan titik acuan posisi kendaraan, sehingga didapatkan hasil
estimasi kecepatan yang lebih akurat
Hasil analisis geometri menunjukkan bahwa titik dengan ketepatan paling
tinggi adalah titik bagian depan bawah kendaraan. Hasil pengujian estimasi kecepatan pada 2 buah video uji, menunjukkan hasil ketepatan yang tinggi, yaitu 99,49%
untuk video pertama dan 99,39% untuk video kedua.","One of the important things in traffic analysis is vehicles speed. The speed
of the vehicle become very important to know the traffic of the city. It also very
important according to safety riding, related to maximum and minimum speed rule
which must be satisfied. Hence, vehicle speed monitoring is very important
In this research, vehicle speed estimation are conducted using image processing. The process consist of detection, tracking, and calibration. The detection process conducted using Mixture of Gaussian, the tracking process conducted using the
moment of the vehicle, and the calibration process are conducted using projective
transformation. The increase the accuracy, geometric analysis are conducted to find
the point that refer the position of vehicle, hence we could get the better accuracy
The result of the geometric analysis show that point with the highest accuracy
is the front bottom of the vehicle. The Result show that the accuracy of the speed
estimation from 2 video, reach 99,49% in first video, and 99,39% in the second
video",,,133416,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177021,,Julio Jeffer Maliangkay,PERANCANGAN ARSITEKTUR CNN SECARA OTOMATIS MENGGUNAKAN CARTESIAN GENETIC PROGRAMMING UNTUK PENGENALAN UCAPAN ORANG DYSARTHRIA,,,"speech recognition, convolutional neural network, dysarthria, cartesian genetic programming","Anny Kartika Sari, S.Si, M.Sc., Ph.D",2,3,0,2019,1,"CNN state-of-the-art biasanya dirancang oleh para ahli yang memiliki pengetahuan domain yang baik tentang data yang diselidiki maupun CNN. Karena kinerja CNN sangat bergantung pada data yang diteliti, trial-and-error atau pengetahuan ahli diperlukan dalam membangun arsitektur yang sesuai untuk dataset target. Untuk menangani ini, diperlukan algoritma yang memungkinkan peneliti untuk secara otomatis memperoleh CNN dengan kinerja terbaik untuk data yang diberikan. Penelitian ini bertujuan untuk membangun algoritma otomatis menggunakan Cartesian Genetic Programming untuk klasifikasi pengucapan digit terpisah bersifat speaker-dependent pada orang dengan dysarthria. Dysarthria sendiri merupakan gangguan berbicara motorik yang disebabkan oleh gangguan neurologis. 
CGP pada dasarnya adalah metode representasi genetik berbasis integer yang sangat sederhana dari suatu program dalam bentuk directed graph. Pada penelitian ini CGP digunakan untuk membangun CGP-CNN yaitu directed graph yang berisi lapisan CNN sebagai node yang saling berhubungan. Untuk mencari arsitektur yang optimal secara otomatis digunakan Evolutionary Strategy dengan akurasi klasifikasi sebagai fungsi fitness. 
Model dibangun dan dievaluasi menggunakan lima subjek dengan dysarthria dan dua subjek tanpa gangguan yang diperoleh dari UA Speech Database menghasilkan rata-rata akurasi pengucapan sebesar 94.9%. Untuk perbandingan, model CGP-CNN menghasilkan rata-rata akurasi model pada tiga subjek dysarthria mencapai 90.9%, yaitu lebih tinggi sebesar 2.33% dibandingkan dengan model CNN pada penelitian sebelumnya yang sebesar 88.57%. Berdasarkan hasil tersebut disimpulkan bahwa CGP-CNN berhasil membangun model yang lebih baik dibandingkan penelitian sebelumnya.","CNN state-of-the-art was specifically designed by experts who had good domain knowledge about CNN an the investigated data. Because CNN performance is very dependent on input data, trial-and-error or expert knowledge is required to build the architecture that is suitable for the target dataset. To allow this, an algorithm is needed for the researcher to obtain the best performance CNN for the data provided. This study was designed to build an automatic algorithm using Cartesian Genetic Programming for speaker-dependent digit pronunciation in people with dysarthria. Dysarthria itself is a motor speech disorder caused by a neurological disorder.
CGP is basically a very simple integer-based genetic representation method of a program in the form of directed graph. In this study CGP is used to build CGPCNN, namely a directed graph which contains components of the CNN layer as node that are interconnected to make the architecture. Evolution Strategy is used to find the optimal architecture with classification accuracy as its fitness function.
The model was built and evaluated using five subjects with dysarthria and two normal subjects obtained from UA Speech Database resulting in an average pronunciation accuracy of 94.9 %. For comparison, the CGP-CNN model produces an accurate average model on three subjects with dysarthria reaching 90.9%, higher by 2.33% compared to the CNN model in the previous study which was 88.57%. Based on these results, it was concluded that CGP-CNN managed to build a better model than previous research.",,,127761,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171650,,FILBERT UTOMO,Optimalisasi Permasalahan Alokasi Ruang Kerja Menggunakan Metode Algoritma Genetika,,,"Optimasi kombinatorial, metaheuristik, algoritma genetika, office space allocation","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2019,1,"Pengalokasian ruang kerja merupakan suatu penempatan sekumpulan entitas (manusia, alat, dan sebagainya) pada suatu ruang dengan tujuan untuk mengoptimalkan penggunaan ruang. Permasalahan alokasi ruang kerja merupakan permasalahan yang terjadi ketika perkembangan suatu organisasi atau institusi. Pengalokasian ruang kerja merupakan salah satu permasalahan optimasi kombinatorial, dimana pencarian solusi memiliki kompleksitas eksponensial sehingga membutuhkan waktu komputasi yang sangat lama. Salah satu pendekatan untuk menyelesaikan permasalahan tersebut yaitu dengan algoritma metaheuristik, salah satunya yaitu algoritma genetika. Algoritma genetika cocok digunakan untuk menyelesaikan permasalahan optimasi.
Pada penelitian ini, akan digunakan algoritma genetika untuk menyelesaikan permasalahan alokasi ruang kerja. Algoritma ini meliputi pembentukan populasi awal, perhitungan nilai kebugaran (fitness), seleksi orang tua, crossover antara kromosom orang tua, mutasi kromosom anak, dan seleksi survivor. Algoritma ini akan diujikan pada 7 dataset permasalahan alokasi ruang kerja. Hasil pengujian menunjukkan algoritma memberikan hasil yang belum lebih baik dibandingkan penelitian sebelumnya, namun dengan selisih yang tidak terlalu besar.
","Office space allocation is the task of allocating a set of entities (people, machine, and anyelse) into a space with the aim of optimizing the use of space. The problem of office space allocation is a problem that occurs when developing an organization or institution. Office space allocation is one of combinatorial optimization problems, where the search of solutions has exponential complexity that requires very long computing time. One approach to solving this problem is the metaheuristic algorithm, one of which is the genetic algorithm. Genetic algorithm is suitable to be used to solve optimization problem.
In this study, genetic algorithms will be used to solve the problem of office space allocation. This algorithm includes the formation of the initial population, calculation of fitness values (fitness), parent selection, crossover between parent chromosomes, child chromosome mutations, and survivor selection. This algorithm will be tested on 7 datasets for the allocation of work space. The test results show that the algorithm provides results that are not better than previous studies, but with a difference that is not too large.
",,,123023,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175492,,Rahmat Sholihin,SISTEM PAKAR UNTUK MEMBANTU DIAGNOSIS DEPRESI MENGGUNAKAN METODE DEMPSTER SHAFER,,,"Sistem pakar, forward chaining, dempster shafer, dan depresi.","Sri Mulyana, Drs., M.Kom.",2,3,0,2019,1,"Gangguan psikologis termasuk depresi menjadi kendala bagi seseorang dalam melakukan aktivitas. Pada kasus yang parah, depresi dapat mendorong penderitanya untuk bunuh diri. Diagnosis depresi hanya bisa dilakukan oleh psikolog/psikiater profesional, sedangkan jumlahnya di Indonesia terbatas. 

	Penelitian ini bertujuan untuk membangun sistem pakar yang akan digunakan untuk membantu psikolog atau psikiater dalam mendiagnosis depresi dengan menggunakan inferensi forward chaining dan metode dempster shafer. Metode dempster shafer merupakan salah satu metode dalam sistem pakar yang melibatkan perhitungan nilai belief atau keyakinan pakar pada suatu gejala. 

	Sistem yang telah dibangun di evaluasi dengan cara membandingkan keluaran hasil diagnosis oleh sistem dengan hasil diagnosis oleh pakar. Hasil evaluasi menunjukan bahwa sistem memiliki nilai akurasi sebesar 71,05 %. ","Psychological disorder, including depression makes it difficult for the someone to carry out his/her activities. In extreme cases, depression can push the sufferer into committing suicide. Diagnosis of depressive disorder can only be carried out by professional psychologists / psychiatrists. While the amount of experts in Indonesia is limited.

This research aims to build an expert system in aiding diagnosis of depressive disorder since the number of psychologists or psychiatrists in Indonesian are very limited. Forward chaining inferencing and dempster shafer method are being used. Dempster shafer method is involving calculation of belief value of a symptom.

System is evaluated by comparing its output and real experts diagnosis. Evaluation shows that the system has an accuration value of 71,05 %. ",,,126269,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177284,,SEPTIO FADHLUL ISMI,PENGEMBANGAN GAME MEMORI DENGAN METODE N-BACK TASK UNTUK PENYANDANG DISLEKSIA DI PANTI ASUHAN BINA SIWI BANTUL DAERAH ISTIMEWA YOGYAKARTA,,,"Dyslexia, N-Back Task, Software Development Method, Likert Scale","Isna Alfi Bustoni, M.Eng",2,3,0,2019,1,"Saat ini teknologi berkembang pesat dan memiliki dampak pada kehidupan
manusia. Namun, ditengah perkembangan teknologi yang pesat tersebut masih terdapat
sebagian masyarakat yang belum merasakannya. Berdasarkan survei oleh penulis,
ditemukan bahwa beberapa dari mereka mengalami kesulitan dalam membedakan dan
mengingat huruf (disleksia) pada Panti Asuhan Bina Siwi.
Penelitian ini menggunakan metode N-Back Task sebagai pedoman dalam
melakukan pengembangan perangkat lunak berupa game. Dalam proses
pengembangan game, metode pengembangan perangkat lunak digunakan dalam
tahapan-tahapan yang ada dalam metode N-Back Task. Skala likert digunakan untuk
menganalisa respon dari peserta terkait game yang telah dikembangkan.
Hasil penelitian ini menunjukkan bahwa persentase kelayakan perangkat lunak
adalah 60,7% dan berdasarkan Interpretasi Persentase Likert perangkat lunak
dikategorikan cukup layak.","Today technology is developing rapidly and has an impact on human life.
However, in the midst of rapid technological developments there are still some people
who have not felt the impact on their needs and constraints. Based on the survey by the
authors, it was found that some of them had difficulties in distinguishing and
remembering letters (dyslexia) at the Bina Siwi Institution.
This study uses the N-Back Task method as a guideline in developing software
in the form of games. In the game development process, software development methods
are used in the stages in the N-Back Task method. Likert scale is used to analyze
responses from participants regarding games that have been developed.
The results of this study indicate that the percentage of software feasibility is
60.7% and based on the Interpretation Percentage of software Likert is categorized as
quite feasible.",,,127951,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182148,,TOMMY WAHYU YUDIALIM,Sistem Pendukung Keputusan Penentuan Prioritas Bantuan Pada Balita Kurang Gizi Menggunakan Metode Promethee,,,"Malnutrition, stunting, decision support system, Promethee, toddler. Kurang gizi, stunting, balita, sistem pendukung keputusan, Android.","Drs. Retantyo Wardoyo, M.Sc., Ph.D.",2,3,0,2019,1,"Kurang gizi merupakan sebuah penyakit yang dapat menjadi berbahaya jika
tidak segera ditangani. Anak-anak khususnya balita sangat rentan terserang penyakit
kurang gizi yang ditandai dengan badan yang kurus serta sebagian besar diantaranya
memiliki tinggi badan dibawah rata-rata atau stunting. Oleh karena itu, diperlukan
upaya untuk dapat memberikan bantuan dengan cepat dan tepat kepada balita kurang
gizi.
Penelitian ini dilakukan dengan membangun Sistem Pendukung Keputusan
(SPK) untuk menentukan prioritas bantuan pada balita kurang gizi dengan menggunakan metode Promethee. Data yang digunakan merupakan data diri balita dari Puskesmas Kelarik yang berikutnya diolah dengan Indeks Antropometri sehingga menjadi kriteria Promethee. Sistem dibangun pada aplikasi Android dengan harapan dapat
memudahkan pengguna.
Keluaran dari sistem berupa rekomendasi balita untuk diberi bantuan beserta ranking dan skornya. Hasil pengujian didapatkan bahwa konfigurasi bobot yang
memprioritaskan tinggi badan berhasil mendapatkan akurasi 100%, sedangkan untuk
konfigurasi bobot yang bernilai sama mendapatkan akurasi tertinggi sebesar 60% dan
konfigurasi bobot yang memprioritaskan berat badan sebesar 40%. Berdasarkan hasil
pengujian dapat disimpulkan bahwa SPK dengan metode Promethee dapat digunakan
untuk menentukan prioritas bantuan pada balita kurang gizi.","Malnutrition is a disease that can be dangerous if not treated immediately.
Malnutrition attacks children especially toddlers which is indicated by having a thin
body and most of these children have below average height or also known as textit stunting. Therefore, efforts are needed to be able to provide quick and precise
assistance to malnourished children.
This research is conducted by building a Decision Support System (DSS) to
determine the priority of assistance in malnourished children using the Promethee
method. The data used are toddlers&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12; personal data obtained from Puskesmas Kelarik
which is then processed with the Anthropometry Index to become Promethee criteria.
The system is built on an Android application with the hope of making it easier for
users.
The output of the system is in the form of a toddler&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s recommendation to be
given assistance along with its ranking and score. The test results obtained that the
weight configuration that prioritizes height managed to get an accuracy of 100 %,
while for the equal weight configuration, gets the highest accuracy of 60 % and the
weight configuration that prioritizes weight gets 40 %. Based on the test results it can
be concluded that the DSS with the Promethee method can be used to determine the
priority of assistance in malnourished children.",,,133102,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167815,,ILHAM DWI TARA HIDAYAT,DETEKSI REVIEW DUPLIKAT PADA REVIEWS APLIKASI GOOGLE PLAY BERDASARKAN TINGKAT KEMIRIPAN MENGGUNAKAN ALGORITMA JARO WINKLER,,,"Kemiripan, Jaro Winkler, SPAM, HAM, SPAM Detection, Data Duplikat. ","Isna ALfi Bustoni, S. T., M. Eng.",2,3,0,2019,1,"Salah satu permasalahan dalam menguji performa aplikasi pada Google Play Store karena adanya review SPAM duplikat. Keberadaan review SPAM sendiri merugikan pengguna dan mengganggu para pengembang dalam melakukan evaluasi sebuah aplikasi. Hal ini dikarenakan review SPAM tidak memberikan informasi valid terkait fitur aplikasi yang diulas. Penelitian ini bertujuan membangun dan merancang sistem untuk mendeteksi SPAM duplikat pada ulasan Google Play dengan mengukur dan menganalisis tingkat kemiripan antar ulasan.

Dalam penelitian ini dilakukan penambahan sistem deteksi SPAM duplikat pada sistem klasifikasi yang menggunakan algoritma Support Vector Machine (SVM). Algoritma yang digunakan untuk mendeteksi SPAM duplikat adalah algoritma Jaro Winkler. Tingkat kemiripan antar reviews dapat dihitung menggunakan algoritma Jaro Winkler. Selanjutnya dipilih nilai treshold yang paling optimal untuk diterapkan pada sistem.

Hasil akhir dari penelitian ini menunjukkan terjadi peningkatan akurasi klasifikasi review sebesar 2,3 % setelah dilakukan penambahan sistem deteksi SPAM duplikat terhadap sistem klasifikasi SVM.

","One of the problems in testing application performance on the Google Play Store is because of a duplicate SPAM review. The existence of SPAM review harms users and interferes the developers in evaluating an application. This is because the SPAM review does not provide valid information regarding the application features reviewed. This study aims to build and design a system for detecting duplicate SPAM on Google Play reviews by measuring and analyzing the degree of similarity between reviews.

In this research, the addition of a duplicate SPAM detection system was added to the classification system which has been used the Support Vector Machine (SVM) algorithm. The algorithm used to detect duplicate SPAM is the Jaro Winkler algorithm. The level of similarity between reviews can be calculated using the Jaro Winkler algorithm. Then the most optimal threshold value is chosen to be applied to the system.

The final results of this study indicate an increase in the review classification accuracy of 2,3% after the addition of a duplicate SPAM detection system to the SVM classification system.

",,,119092,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
174731,,GUSTI RADITIA MADYA,Analysis and Implementation of Bin Packing Problem for Fleet Assignment Recommendation and Object Position in Fulfillment Service at PT. Global Digital Niaga (Blibli.com),,,"Fulfilment Service,Bin Packing Problem,Greedy Method,LAFF Algorithm","Retantyo Wardoyo, Drs., M.Sc.,Ph.D.;Faizal Makhrus, S.Si., M.Sc., Ph.D.",2,3,0,2019,1,"Layanan pemenuhan adalah layanan yang memungkinkan penjual untuk menyimpan produk mereka di gudang Blibli dan gudang akan menangani pengemasan dan pengiriman setiap kali ada pembelian produk. Aspek penting dalam proses inbound dari layanan pemenuhan adalah permintaan penjemputan, dimana logistik perlu untuk mengambil produk dari titik penjemputan penjual terlebih dahulu, sebelum diserahkan ke gudang. Namun, belum adanya sistem terintegrasi untuk menangani proses permintaan penjemputan, terutama dalam hal pemesanan jenis dan jumlah armada logistik yang diperlukan untuk mengangkut produk. Sistem untuk rekomendasi penugasan armada perlu dibuat untuk membantu dalam menentukan jenis dan jumlah armada yang dibutuhkan untuk menangani permintaan.

Dalam penelitian ini, dibuat suatu sistem yang mampu memberikan rekomendasi tentang jumlah dan jenis armada yang digunakan, serta posisi produk dalam armada menggunakan algoritma yang didasarkan pada greedy method dan algoritma LAFF. Greedy method digunakan untuk menetapkan produk ke armada mana dan algoritma LAFF digunakan untuk memposisikan produk di dalam armada. Data yang digunakan adalah data produk di Blibli.com dengan 10.000 varian produk. Pengujian dilakukan dengan melakukan tes pemrosesan data menggunakan 500, 1000, 1500 dan 2000 data dengan melihat persentase penggunaan ruang dalam armada dan running time.

Hasil penelitian ini menunjukkan bahwa sistem dapat memberikan rekomendasi kendaraan logistik dengan jumlah armada yang sedikit dan dapat menyediakan pemanfaatan ruang dengan tingkat pemanfaatan rata-rata pada 95%, dengan running time yang diperlukan untuk memproses data pada tingkat 1.2 ms/produk.","Fulfillment service is a service that allows merchants to store their products in Blibli`s warehouse and the warehouse will handle the packaging and shipping whenever there is a purchase of the products. One of the important aspect in inbound process of the fulfilment service is the pick up request---the logistics need to pick up the products from merchant`s pickup point first, before hand it over to the warehouse. However, there is no integrated system to handle the pickup request process, especially in terms of ordering the type and number of logistics fleets needed to transport the product. System for fleet assignment recommendation need to be made to help in deciding the type and the amount of fleet needed to handle the request. In the hope that it could give optimal result by providing few numbers of fleet yet maximizing the products inside each fleet.

In this study, a system is made that is able to provide recommendations on the number and type of fleet used, as well as the position of the product in the fleet using an algorithm based on the greedy algorithm and the LAFF algorithm. The greedy method is used to assign which product to which fleet and the LAFF algorithm is used to positioning the product inside the fleet. The data used is product data in Blibli.com with 10,000 product variants and 131,660 total products. The testing method is carried out by conducting data processing tests using 500, 1000, 1500 and 2000 data by looking at the percentage of space usage in the fleet and the running time.

The results of this study show that the system can provide logistics vehicle recommendations with few numbers of fleet and can provide space utilization with an average level of utilization at 95%, along with the time needed to process the data is at rate of 1.2 ms/product.",,,125529,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175500,,Harista Sriwahyuni,PENILAIAN SOAL ESAI SISWA SEKOLAH MENENGAH MENGGUNAKAN COSINE SIMILARITY DENGAN PREPROCESSING TERM FREQUENCY DAN N-GRAM,,,"Cosine Similarity, Term Frequency, N-gram, Penilaian Esai Otomatis",BAMBANG NURCAHYO PRASTOWO; GUNTUR BUDI HERWANTO,2,3,0,2019,1,"Ujian dengan sistem esai merupakan bentuk evaluasi di mana pilihan jawaban tidak disediakan, dan siswa harus menjawab dengan kalimat. Penilaian hasil ujian esai dapat bersifat subjektif dan memakan waktu yang lama jika dilakukan secara manual. Beberapa penelitian pengembangan sistem penilaian esai otomatis telah dikembangkan sejak tahun 1966. Salah satunya metode cosine similarity yang dikombinasikan dengan metode pembobotan TF-IDF pada jawaban esai berbahasa Inggris yang memiliki tingkat akurasi yang tinggi, sedangkan penerapan metode yang sama pada jawaban esai berbahasa Indonesia memiliki akurasi yang lebih rendah. Oleh karena itu, dibutuhkan pengembangan metode yang tepat untuk penilaian esai otomatis dalam Bahasa Indonesia.   Dalam penelitian ini akan dilakukan otomasi penilaian esai siswa sekolah menengah dengan text mining. Dataset yang digunakan adalah 6.777 jawaban esai siswa sekolah menengah yang didapatkan dari Pusat Penilaian Pendidikan (PUSPENDIK) atas persetujuan pihak terkait. Metode yang akan digunakan adalah metode Cosine Similarity yang dikombinasikan dengan metode pembobotan Term Frequence dan metode pembobotan n-gram. Nilai performa yang dicari adalah akurasi, presisi, recall, dan F1-Score.  Hasil dari penelitian ini adalah performa berupa akurasi sebesar 88,50%, presisi sebesar 93,22%, recall sebesar 87,30%, dan f1-score sebesar 90,16% untuk model term frequency, serta akurasi sebesar 86,36%, presisi sebesar 87,83%, recall sebesar 89,87%, dan f1-score sebesar 88,84% untuk model N-gram dengan n = 2. ","An essay examination is a form of evaluation where answer choices are not provided, and students must answer with sentences. Assessment of essay exam results can be subjective and take a long time if done manually. Several researches on the development of automated essay scoring systems have been developed since 1966. One of them is the cosine similarity method combined with the TF-IDF weighting method in English essay answers that have a high degree of accuracy, while the application of the same method to Indonesian essay answers has lower accuracy. Therefore, it is necessary to develop appropriate methods for automatic essay assessment in Indonesian. In this study, the assessment of high school student essays will be automated with text mining. The dataset used is 6,777 answers to high school student essays obtained from the Education Assessment Center (PUSPENDIK) with the consent of the relevant parties. The method that will be used is Cosine Similarity method which is combined with the Term Frequency weighting method and the n-gram weighting method. The performance sought is accuracy, precision, recall, and F1Score. The results of this study are performance in the form of accuracy of 88.50%, precision of 93.22%, recall of 87.30%, and f1-score of 90.16% for model term frequency, and accuracy of 86.36%, precision of 87.83%, recall of 89.87%, and f1score of 88.84% for N-gram models with n = 2. ",,,126347,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183694,,VIDISKIU FORTINO K,Mental Health Disorder Detection Through Facial Images Using Convolutional Neural Network,,,"Convolutional Neural Network, VGGFace, InceptionV3, Mental Health Disorder Detection, LIME Explainer","Edi Wibowo,S.Kom., M.Kom, Ph.D.",2,3,0,2019,1,"Gangguan kesehatan mental sering disembunyikan karena paradigma sosial di beberapa daerah karena dampak pada kesejahteraan dan citra seseorang. Oleh karena itu, banyak orang menyembunyikan gangguan mental mereka dan menyebabkan kondisi kesehatan mental mereka yang memburuk. (Venkataraman dan Paramswaran, 2018) Deteksi penyakit mental terbukti sulit ketika disembunyikan, tetapi penelitian oleh Ward dan Scott (2018) melakukan deteksi pada wajah netral. Penelitian ini bertujuan untuk membuat model CNN yang menerapkan metode transfer learning dengan model dasar VGG Face 2 dan InceptionV3 karena kelangkaan dataset. Implementasi data program augmentasi untuk membuat dataset dari 336 gambar dari mengubah ukuran, membalik, transformasi fotometrik, dan rata-rata wajah. Model pra-pelatihan dilatih lebih dari 20, 30, dan 40 epochs menggunakan pengoptimal Stochastic Gradient Descent dan Adam dengan skema validasi K-Fold. Model terbaik menghasilkan akurasi rata-rata 0,9762 dan 0,66 untuk VGG Face 2 dan Inception V3. Akhirnya, penelitian membuat keputusan model menggunakan pustaka LIME explainer. Model mengungkapkan daerah yang signifikan merupakan daerah mulut, mata dan hidung ketika memprediksi kecenderungan tinggi gangguan mental, sedangkan daerah ujung wajah dianggap mendukung prediksi kecenderungan rendah gangguan mental.","Mental health disorder is often concealed due to social paradigms in some regions due to the impact of on one's well being and image. Therefore, many individuals conceal their mental disorders and leads to the worsening condition of their mental health.(Venkataraman and Paramswaran, 2018) Detection of mental illness proves to be difficult when concealed, but research by Ward and Scott (2018) implicates the possibility of performing detection on neutral faces. This research aims to create CNN models that implement transfer learning method with base models VGGFace2 and InceptionV3 due to the scarcity of dataset. The implementation of the program augments the data to create a dataset of 336 images from resizing, flipping, photometric transformation, and face averaging. The pretrained models are trained over 20, 30, and 40 epochs using Adam and Stochastic Gradient Descent optimizers using K-Fold validation scheme. The best models produced 0.9762 and 0.66 average accuracy for VGGFace2 and InceptionV3 respectively. Finally, the research analyzes the model's decisions using LIME explainer library. The models expresses regions of interest on the area of the mouth, the eyes and and nose when predicting high tendency of mental disorder, while outer regions are considered to support prediction of low tendency of mental disorder.",,,134683,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175249,,ANISSA WULANDARI,PREDIKSI KESUKSESAN FILM  DENGAN SUPPORT VECTOR MACHINE,,,"Film, Support Vector Machine, Prediction of Film Success, Classification, Prediksi Kesuksesan Film","Sri Mulyana, Drs., M.Kom.",2,3,0,2019,1,"Kesuksesan sebuah film merupakan hal yag diharapkan bagi setiap pembuat filmnya. Namun faktor-faktor yang dapat memengaruhi kesuksesan film sangatlah banyak dan kompleks. Fitur-fitur tersebut dibagi menjadi fitur pre-released dan post&Acirc;&not;-released. Fitur pre-released adalah fitur yang tersedia sebelum film dirilis sedangkan fitur post-released merupakan fitur yang tersedia setelah film dirilis.
	Pada penelitian ini dilakukan prediksi kesuksesan film menjadi tiga kelas yaitu flop atau gagal, moderate, dan blockbuster atau sukses dengan menggunakan metode Support Vector Machine. Data yang digunakan sejumlah 666 data. Data tersebut merupakan data film yang dirilis di Amerika Serikat dalam kurun waktu 2006 hingga 2016. Klasifikasi kesuksesan film dilakukan dua kali yaitu klasifikasi dengan menggunakan sembilan fitur yang ada dan klasifikasi dengan menggunakan kombinasi tiga fitur pre-released. Hasil klasifikasi tersebut akan dievaluasi performanya dengan mencari nilai akurasi, presisi, recall, dan f1-score.
	Hasil penelitian ini adalah performa klasifikasi dengan seluruh fitur berupa akurasi sebesar 81,8%, presisi sebesar 78,9%, recall sebesar 74,3% dan f1-score sebesar 76%, serta akurasi sebesar 74,2%, presisi sebesar 52,4%, recall sebesar 85,6%, dan f1-score sebesar 80,8% untuk performa klasifikasi dengan kombinasi tiga fitur pre-released terbaik. Fitur-fitur pre-released yang krusial dalam memprediksi kesuksesan film adalah budget atau biaya produksi, sekuel dan Genre.
","Success of a film is something every filmmaker wants to achieve. But the factors that can effect film success are too many and too complex. There are two types of factors or features, pre-released and post-released. Pre-released features are features that existed before the movie was released and post-released features are features that only exist when the movie has been released.
	In this study, we predicted film success into three classes, they are flop, moderate, and blockbuster using the Support Vector Machine method. The data we used is consisted of 666 data. The data is released in the United States in the period of 2006 to 2016. The film classification is done twice, they are classification by using all nine existing features and classification using a combination of three best pre-released features. The results of the classification will be evaluated by looking for values of accuracy, precision, recall, and f1-score.
	The results of this study are classification performance with all features that has 81.8%of accuracy, 78.9% of precision, 74.3%  of recall and 76% of f1-score, and 74.2% of accuracy,  52, 4% of precision, 85.6% of recall and 80.8%  of f1-score for classification performance with the combination of the three best pre-released features. The crucial pre-released features in predicting film success are the budget, sequels and genre.
",,,126027,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
170133,,NELSON HALIM,Comparison between DBSCAN and Accelerated Hierarchical DBSCAN* for Text Clustering Unstructured Questionnaire Data,,,"Text clustering, DBSCAN, HDBSCAN*, GloVe","Mardhani Riasetiawan, M.T., Dr ",2,3,0,2019,1,"Questionnaires are often used for market research. Considering that the questionnaire data is in the form of text data and that large amounts of these data are produced, it becomes difficult to read the responses from questionnaires line per line. Text clustering can aid in getting an overview of the data. This research therefore aims to obtain a suitable clustering algorithm for text data. The text data has to be converted into a structured form using GloVe model, then the two clustering algorithms are implemented - DBSCAN and accelerated HDBSCAN*. Different sample sizes of the data are obtained from the same dataset to get sample sizes of 5,000, 6,000, 7,000, 8,000, 9,000 and 10,000 responses. These samples are clustered using the two different algorithms where evaluation of the clustering result is done by comparing the silhouette coefficient, Calinski-Harabaz index and run time for both the clustering algorithms to see which algorithm performs better. From this research, it is found that DBSCAN results in higher silhouette coefficient and higher Calinski-Harabaz index, showing that DBSCAN produces better clustering. The time taken to cluster for DBSCAN is longer than accelerated HDBSCAN*. DBSCAN is therefore a better clustering algorithm for text data compared to accelerated HDBSCAN*.","Questionnaires are often used for market research. Considering that the questionnaire data is in the form of text data and that large amounts of these data are produced, it becomes difficult to read the responses from questionnaires line per line. Text clustering can aid in getting an overview of the data. This research therefore aims to obtain a suitable clustering algorithm for text data. The text data has to be converted into a structured form using GloVe model, then the two clustering algorithms are implemented - DBSCAN and accelerated HDBSCAN*. Different sample sizes of the data are obtained from the same dataset to get sample sizes of 5,000, 6,000, 7,000, 8,000, 9,000 and 10,000 responses. These samples are clustered using the two different algorithms where evaluation of the clustering result is done by comparing the silhouette coefficient, Calinski-Harabaz index and run time for both the clustering algorithms to see which algorithm performs better. From this research, it is found that DBSCAN results in higher silhouette coefficient and higher Calinski-Harabaz index, showing that DBSCAN produces better clustering. The time taken to cluster for DBSCAN is longer than accelerated HDBSCAN*. DBSCAN is therefore a better clustering algorithm for text data compared to accelerated HDBSCAN*.",,,121395,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171926,,Sastra Anugrah Bimantara,KLASIFIKASI TWEET BERITA CLICKBAIT DAN NON-CLICKBAIT PADA TWITTER MENGGUNAKAN TEXT MINING,,,"Clickbait, Support Vector Machine, Multinomial Naive Bayes, TF-IDF, Text Document Classification","Dr. Andi Dharmawan, S.Si., M.Cs.",2,3,0,2019,1,"Clickbait adalah judul atau cuplikan berita yang kata-katanya menunjukkan fakta yang dilebih-lebihkan atau fakta yang tidak lengkap. Fenomena ini merugikan baik bagi pembaca maupun bagi penyedia berita lain yang masih kompeten. Salah satu penyebaran clickbait adalah melalui media sosial Twitter. Pendeteksian clickbait pada Twitter perlu dilakukan untuk meminimalisasi kerugian yang ditimbulkan oleh clickbait.
	Pada penelitian ini dilakukan klasifikasi tweet berita clickbait menggunakan data tweet dari beberapa akun Twitter portal berita menggunakan metode Support Vector Machine dan Multinomial Naive Bayes. Data yang digunakan adalah sejumlah 3.400 dengan komposisi yang seimbang. Pengambilan data dilakukan bersamaan untuk lima dari enam akun yaitu pada tanggal 29 Desember 2019, sementara akun sisanya diambil lebih akhir yaitu pada tanggal 22 Maret 2019. Metode ekstraksi fitur yang digunakan adalah metode TF-IDF dan nilai performa yang dicari adalah akurasi, presisi, recall, dan F1-Score.
	Hasil dari penelitian ini adalah performa berupa akurasi sebesar 85,6%, presisi sebesar 86%, recall sebesar 85,8% dan f1-score sebesar 85,6% untuk model Support Vector Machine, serta akurasi sebesar 76,4%, presisi sebesar 82,4%, recall sebesar 77,1% dan f1-score sebesar 75,6% untuk model Multinomial Naive Bayes.","Clickbait is a title or snippet of news which words indicate exaggerated facts or incomplete facts. This phenomenon is detrimental to both the reader and other competent news providers. One of the spreads of clickbait is through social media Twitter. Clickbait detection on Twitter needs to be done to minimize the loss caused by clickbait.
In this study the classification of clickbait news tweets is done using tweet data from several news portal Twitter accounts using the Support Vector Machine and Multinomial Naive Bayes method. The data used is a number of 3.400 with a balanced composition. Data retrieval is done simultaneously for five of the six accounts, which is on December 29, 2019, while the remaining account is taken later, which is on March 22, 2019. The feature extraction method used is the TF-IDF method and the performance values calculated are accuracy, precision, recall and f1-Score.
The results of this study are performances in the form of accuracy of 85.6%, precision of 86%, recall of 85.8% and f1-score of 85.6% for Support Vector Machine models, and accuracy of 76.4%, precision of 82.4%, recall of 77.1% and f1-score of 75.6% for the Multinomial Naive Bayes model.",,,123120,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
172958,,JEFFREY EVAN YULIANTO,PENGENALAN NOMINAL DAN KEASLIAN UANG KERTAS RUPIAH,,,"Convolutional Neural Network, Klasifikasi Nominal Uang Kertas Rupiah, Pengenalan Uang Asli, Pengolahan Citra Digital","Retantyo Wardoyo, Drs. M.Sc. Ph.D.",2,3,0,2019,1,"Peredaran uang palsu di Indonesia masih tinggi. Teknologi yang semakin canggih membuat pembuatan uang palsu semakin mudah. Oleh karena itu diperlukan sebuah sistem yang dapat mencegah peredaran uang palsu dengan menghentikan transaksi dengan uang palsu. Metode Convolutional Neural Network (CNN) yang sangat baik dalam mengklasifikasikan citra dapat digunakan untuk mengklasifikasikan citra uang asli dan palsu. Citra dari uang asli maupun palsu dapat diolah secara digital sehingga dapat diklasifikasikan dengan CNN. Citra uang asli memiliki ciri-ciri khusus yang dapat membedakannya dengan uang palsu. Salah satu ciri tersebut adalah tanda air. Tanda air merupakan gambar tersembunyi yang akan muncul pada uang rupiah asli bila diterawang. Hasil penelitian terbaik dengan parameter kernel konvolusi pertama 5x5 dan kernel konvolusi kedua 2x2, jumlah epoch 25 serta pembagian jumlah data training dan validasi 80% 20% dapat mengklasifikasikan 99.64% uang nominal rupiah baik asli maupun palsu.","Circulation of counterfeit money in Indonesia is still high. Increasingly sophisticated technology makes counterfeit money even easier. Therefore we need a system that can prevent the circulation of counterfeit money by stopping transaction with counterfeit money. The Convolutional Neural Network (CNN) method which is very good at classifying images can be used to classify genuine and counterfeit money. The image of either genuine or counterfeit money can be process digitaly so that it can be classified by CNN. The genuine money's image has special characteristics that can distinguish it from the counterfeit one. One of these characteristics is watermark. A watermark is a hidden image that will appear on the genuine money if it is exposed to light. The best results with parameter of first convolutional kernel 5x5 and second convolutional kernel 2x2, number of epoch 25, and division of training and validation data 80% 20% can classify 99.64% of nominal rupiah both the real and the counterfeit.",,,123966,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
173473,,MAHESSA BAGUS ARDIKHA,KOMPARASI USER EXPERIENCE PADA WEBSITE VOLUNTEERING DENGAN PENDEKATAN EVALUASI USABILITY DAN USER TESTING,,,"user interface, ,usability, user experience, UEQ, keystroke-level model","Andi Dharmawan,S.Si., M.Cs., Dr.",2,3,0,2019,1,"Dicetuskannya Sustainable Development Goals (SDG) 2030 oleh United Nations mendorong NGO untuk memiliki kemampuan menyebarkan isu dan informasi secara publik untuk dapat mengundang keikutsertaan masyarakat. Platform media penyebaran informasi sangat dibutuhkan untuk menunjang NGO dalam permasalahan ini, namun solusi yang kerap diimplementasikan yaitu penggunaan sosial media dirasa belum memenuhi kebutuhan pengguna. Untuk itu, penelitian ini mencoba memberikan alternatif platform media penyebaran informasi yang user-friendly dalam bentuk prototipe portal website.

Penelitian diawali dengan mendefinisikan kebutuhan pengguna melalui survei untuk membentuk persona atau user model. Persona tersebut digunakan sebagai acuan desain antarmuka yang juga menggunakan kaedah-kaedah user experience dalam menentukan pola desain yang dipilih. Pengujian prototipe antarmuka dilakukan menggunakan metode evaluasi UEQ dan GOMS-KLM.

Hasil pada tahap evaluasi menunjukkan antarmuka yang dirancang memiliki nilai guna yang baik berdasarkan metode evaluasi UEQ, dan memiliki keunggulan dibandingkan sistem kompetitor sejenis berdasarkan perhitungan operator interaksi KLM.","The estabalished Sustainable Development Goals (SDG) 2030 by United Nations pushes NGO to have an ability to convince people to be active in grassroot movement through story and awareness. A platform as a place to share informations is much needed and crucial for this issue, but the most implemented solution which is the usage of social media have yet to fulfil the user needs. For this reason, this study tries to provide an alternative user-friendly media platform in th form of a website portal prototype.

This research begins with defining user needs through survey to form a persona or user model. This persona then used as a reference when designing the proposed interface, which supported by user experience guideline to determine the chosen design pattern. The proposed prototype's evaluation utlitizes The UEQ and GOMS-KLM method.

Evaluations and tests at the end of the research showed that the proposed prototype have a good usability and user experience proven by user using UEQ, and showing an advantage in the KLM model compared to the estabilished and similar system.
",,,124385,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
182689,,M YUDHA GALANG F,METODE FUZZY TIME SERIES MARKOV CHAIN MODEL PADA PERAMALAN NILAI TUKAR MATA UANG RUPIAH,,,"peramalan, nilai tukar mata uang, fuzzy time series, Markov chain","Dr. Agus Sihabuddin, S.Si., M. Kom.",2,3,0,2019,1,"Peramalan adalah suatu kegiatan memprediksi nilai di masa yang akan datang menggunakan data-data nilai yang berkaitan di masa lalu. Terdapat banyak aplikasi dari peramalan, salah satunya adalah peramalan pada nilai tukar mata uang. Salah satu metode yang dapat digunakan untuk melakukan peramalan adalah metode fuzzy time series. Metode fuzzy time series versi awal memiliki beberapa kelemahan, diantaranya menggunakan operasi geometri yang boros, dan kurang dapat melakukan peramalan nilai tukar mata uang dengan baik.
Salah satu pengembangan metode fuzzy time series yang dapat menangani hal tersebut adalah metode fuzzy time series markov chain model. Pada penelitian ini, metode yang digunakan untuk melakukan peramalan adalah metode fuzzy time series markov chain model. Data yang akan digunakan adalah data nilai tukar mata uang Dolar Amerika dan Euro Eropa terhadap Rupiah Indonesia dalam periode 1 Januari 2019 sampai 30 September 2019. Untuk mengukur tingkat error dari model, digunakan metode mean absolute percentage error (MAPE), mean squared error (MSE)
dan directional statistics (Dstat).
Penelitian ini menghasilkan nilai MAPE sebesar 0,1516%, nilai MSE sebesar 741,77, nilai Dstat sebesar 86,15% dan berhasil melakukan peramalan dengan waktu 3,528 s untuk data uji nilai tukar mata uang USD-IDR, sedangkan untuk data uji nilai tukar mata uang EUR-IDR menghasilkan nilai MAPE sebesar 0,2312%, nilai MSE sebesar 2516,22, nilai Dstat sebesar 80,0% dan waktu peramalan selama 4,795 s.","Forecasting is an act to predict some values in the future using datas related to the values in the past. There are many applications of forecasting, one of them is foreign currency exchange forecasting. One of the methods to forecast is fuzzy time series. The original version of fuzzy time series has several weaknesses, two of the weaknesses are using geometric operations that wasteful and forecasting foreign exchange currency data poorly.
One of developed fuzzy time series methods which can handle those weaknesses is fuzzy time series markov chain model. In this study, fuzzy time series markov chain model is going to be used to forecast. Datas that are going to be used are exchange currency data between US Dollar and Euro to Indonesian Rupiah from 1st January 2019 until 30th September 2019. Three methods to measure the errors are mean absolute percentage error (MAPE), mean squared  error (MSE) and directional statistics (Dstat).
This study produces MAPE value of 0.1516%, MSE value of 741.77, Dstat value of 86.15% and forecasting time of 3.528 s for the test data of USD-IDR exchange currency. While for the test data of EUR-IDR exchange currency, it produces MAPE value of 0.2312%, MSE value of 2516.22, Dstat value of 80.0% and forecasting time of 4.795 s.",,,133578,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
169891,,Retno Palupi,KOMPARASI KINERJA VIRTUAL PRIVATE NETWORK (VPN) DAN NON VIRTUAL PRIVATE NETWORK (NON-VPN) MENGGUNAKAN PROTOKOL EOIP,,,"EoIP, Komparasi, VPN, QoS","Drs. Medi, M.Kom",2,3,0,2019,1,"Internet saat ini bukan lagi merupakan sesuatu yang awam dikalangan masyarakat, penggunaannya saat ini sudah meningkat dengan pesat seiring dengan pertumbuhan teknologi telekomunikasi. Disamping meningkatnya teknologi tersebut, kejahatan melalui internet yang erat kaitannya dengan jaringan komputer juga ikut meningkat, sehingga mengaplikasikan pengaman akses internet menjadi penting dilakukan agar akses internet dapat dilakukan sesuai dengan keinginan pengguna, yaitu memiliki kinerja yang handal dan juga aman.
Virtual Private Network (VPN) merupakan suatu pengaman jaringan yang memungkinkan pengguna melakukan akses ke jaringan public atau internet secara private yang berarti pengguna memblok jalur yang akan dipakai sendiri dalam melakukan akses ke jaringan. Metode yang digunakan dalam pembangunan jaringan VPN ini dilakukan dengan memanfaatkan Ethernet over Internet Protocol (EoIP) yang memiliki konsep kerja seperti membangun terowongan yang diumpamakan sebagai jaringan private yang aman. VPN dan EoIP digunakan untu merancang sebuah jaringan, dan sebuah jaringan lainnya akan dirancang tanpa pemanfaatan VPN dan EoIP, kemudian setiap rancangan yang dibangun akan menghasilkan data lalu lintas jaringan yang akan digunakan untuk perbandingan Quality of Service (QoS) yang akan memberikan gambaran kinerja paling baik dari kedua jaringan.
Hasil dari penelitian ini menunjukkan bahwa jaringan non Virtual Private Network memiliki kinerja yang lebih baik jika dibandingkan dengan jaringan Virtual Private Network. Nilai Throughput non-VPN lebih baik yaitu senilai 98% dibandingkan dengan Throughput jaringan VPN yaitu 95% dari jumlah 100 buah data yang dikirim, dan Packet Loss jaringan non-VPN lebih baik yaitu 2% dibandingkan dengan Packet Loss jaringan VPN yaitu senilai 4,25%. Dengan demikian terbukti bahwa jaringan dengan implementasi Virtual Private Network memiliki sisi keamanan yang lebih baik dari jaringan tanpa Virtual Private Network, namun dari sisi Quality of Service jaringan tanpa VPN memiliki kinerja yang lebih baik.","The internet is no longer a common thing among the people, its use today has increased rapidly along with the growth of telecommunications technology. In additional to the increase in technology, crime through the internet which is closely related to computer networks also increases, so applying internet access safeguards is important so that internet access can be carried out in accordanse with the wishes of users, namely having reliable and safe performance.
Vitual Private Network (VPN) in a network security that allows users to access public networks or the internet in private, which means users block paths that will be used alone in increassing the network. The method used in the construction on VPN networks is done by utilizing Ethernet over Internet Protocol (EoIP) which has a work concept such as building a tunnel that is linked to a secure private network. VPN and EoIP are use in design a network, and another network will be designed without the use of VPN and EoIP, then each design that is built will produce network traffic data that will be used for comparison of Quality of Service (QoS) which will give the best performance picture from both networks.
The result of this study indicate that non-Vitual  Private Network have better performance compared to with the implementation of a Virtual Private Network. The value of non-VPN throughput is better, which is worth 98% compared to VPN network throughput, which is 95% of the total 100 data sent, and the non-VPN network packet loss is better, whitch is 2% compared to VPN network packet worth 4,25%. thus it is proven that networks with the implementation os virtual private networks have better security side than networks without a virtual private network, but in terms of Quality os Service networks without VPNs have better performance.",,,121117,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
173476,,RAHMITA IFTAR RIZMA,DETEKSI KEASLIAN PENGGUNA MENGGUNAKAN TEKS DI TWITTER,,,"text mining, klasifikasi, author identification, Support Vector Machine, twit, Twitter","Edi Winarko, M.Sc., Ph.D.",2,3,0,2019,1,"Meningkatnya jumlah pengguna pada media sosial, menimbulkan beberapa masalah atau isu baru. Salah satunya berkaitan dengan maraknya akun-akun palsu. Orang-orang dari dunia media, hiburan, dan politik biasanya menjadi korban dari akun-akun palsu tersebut. 
Maka dalam penelitian kali ini akan dilakukan identifikasi terhadap teks dari media sosial berupa twit untuk menentukan keasliannya. Dari twit tersebut, kemudian akan diambil beberapa fitur seperti jumlah hashtag, jumlah mention, jumlah alamat web yang dicantumkan, jumlah masing-masing kelas kata, jumlah entitas bernama, jumlah abreviasi, dan jumlah akronim untuk selanjutnya diklasifikasi. Proses klasifikasi dilakukan dengan menggunakan metode Support Vector Machine (SVM) dengan kernel linear, RBF dan polinomial. Kemudian untuk pengujiannya digunakan metode K-Fold Cross Validation, dengan mengambil nilai k sebesar 10. Percobaan klasifikasi dilakukan pada dua jenis dataset, yaitu dataset 2 kelas dan dataset 12 kelas. 
Hasil dari penelitian ini adalah model klasifikasi dataset 2 kelas dapat menentukan yang mana akun tokoh publik asli dan akun yang hanya meniru akun asli berdasarkan twit yang diproses, dimana tingkat akurasi rata-rata tertinggi sebesar 86% dengan menggunakan kernel RBF. Sedangkan untuk hasil klasifikasi dataset 12 kelas, model klasifikasi dapat membedakan masing-masing akun tokoh publik dan bukan tokoh publik dengan tingkat akurasi tertinggi sebesar 46% menggunakan kernel RBF.
","The increase in the number of users on social media has raised several new problems or issues. One of them is related to the increase in the number of fake accounts. Celebrities in the  media, entertainment, and political domains are regular victims of these fake accounts. In this research, the identification of text from social media will be carried out in the form of tweets to determine its authenticity. 
Several features will be taken from each tweet, such as the number of hashtags, number of mentions, number of web addresses attached, number of each part-of-speech, number of named entities, number of abreviations, and number of acronyms. The classification process is carried out using the Support Vector Machine (SVM) method. For the kernel function, this research tried linear, RBF, and polynomial kernel. Then for the evaluation used the K-Fold Cross Validation method. The value of k is 10. This research did experiment using two types of datasets, which is 2-class dataset and 12-class dataset. 
As a result, the classification model of 2-class dataset can distinguish between real account and fake account of public figure from the tweet with the highest average of the accuracy is 86% using RBF kernel. And the 12-class dataset experiment can distinguish between user account from the tweet posted with highest accuracy of 46% using RBF kernel.
",,,124386,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
173479,,MUHAMMAD HUDA WIDODO,PEMODELAN RANTAI TOPIK BERITA BAHASA INDONESIA DENGAN MENGGUNAKAN LATENT DIRICHLET ALLOCATION,,,"Latent Dirichlet Allocation, pemodelan topik, topic chain, Jensen Shannon Divergence, Kullback Leibler divergence, sliding window.","Mardhani Riasetiawan, SE., Akt., MT., Dr.;Guntur Budi Herwanto, S. Kom., M. Cs.",2,3,0,2019,1,"Membaca berita adalah salah satu alternatif yang bisa dilakukan untuk mencari informasi di masa sekarang. Berita sekarang dapat diakses dengan mudah melalui internet. Diperlukan analisis dan identifikasi topik dari suatu berita untuk mengetahui topik apa yang ada dalam kumpulan berita. Pembuatan rantai topik dibutuhkan untuk mengidentifikasi dan menampilkan topik yang ada dalam periode waktu tertentu. Selain itu rantai topik juga diperlukan untuk mengidentifikasi masalah atau isu jangka pendek dan jangka panjang dari suatu data berita.
Dalam penelitian ini, penulis meneliti tentang pemodelan topik berita Indonesia. Studi kasus dari penelitian ini adalah portal berita Kompas.com. Analisis dan pemodelan topik dalam penelitian ini menggunakan metode Latent Dirichlet Allocation. Pengujian pemodelan topik menggunakan uji perplexity menghasilkan 10 passes sebagai jumlah passes terbaik. Selain itu pengujian parameter jumlah topik menggunakan pengujian log perplexity didapatkan kesimpulan bahwa 9 topik merupakan jumlah topik terbaik.
Hasil dari topik yang dihasilkan dalam pemodelan topik selanjutnya dibuat rantai topik dengan menggunakan Jensen-Shannon divergence dan Kullback Leibler divergence. Pembuatan rantai topik pada penelitian ini menggunakan parameter sliding window dengan nilai 2 dan 3. Selain itu parameter similarity cut yang digunakan dalam membuat rantai topik adalah 2.5 untuk Jensen Shannon Divergence serta 1.6 untuk Kullback Leibler Divergence dapat berhasil menemukan isu jangka panjang dan jangka pendek dari sekumpulan data berita.
","Reading news is one alternative that can be done to find information in the present. News can now be accessed easily via the internet. Analysis and identification of topics from a news are needed to find out what topics are in the news collection. The topic chain making is needed to identify and display topics that exist within a certain time period. In addition, the topic chain is also needed to identify short-term and long-term issues or issues from a news data.
In this study, the author research about topic modeling of Indonesian news. The case study of this study is the Kompas.com news portal. Analysis and topic modeling used in this study are the Latent Dirichlet Allocation method. The topic modeling test using the perplexity test produced 10 passes as the best number of passes. In addition, testing the number of topic parameters using the perplexity log test, concluded that 9 topics were the best number of topics.
The results of the topics produced in the topic modeling are made into topic chains using Jensen-Shannon divergence and Kullback Leibler divergence. The topic chain making in this study uses sliding window parameters with values of 2 and 3. The similarity cut parameters used in making the topic chain are 2.5 for Jensen Shannon Divergence and 1.6 for Kullback Leibler Divergence can successfully find long-term and short-term issues from a set of news data.
",,,124512,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
173736,,OGI YOGA ABI KUNCORO,DESAIN ANTARMUKA SITUS WEB E-COMMERCE TIPE BUSSINESS TO CONSUMER DENGAN PEDEKATAN USER CENTERED DESIGN PADA AGILE SOFTWARE DEVELOPMENT,,,"user interface, e-commerce, user centered design, agile software development, usability evaluation","Suprapto, Drs., M.I.Kom., Dr. s",2,3,0,2019,1,"E-commerce sangat berkembang di dunia, termasuk Indonesia. Tokopedia, Lazada, dan Bukalapak adalah contoh industri e-commerce tipe business to consumer (B2C) yang cukup besar di Indonesia. Penerimaan pengguna terhadap sistem e-commerce menjadi salah satu tolok ukur penting dalam bisnis digital.

Tolok ukur usability dapat digunakan untuk meningkatkan kualitas sistem informasi dalam hal kegunaan. Tingkat usability yang baik dapat dicapai dengan pengembangan desain antarmuka berbasis user centered design (UCD) pada agile software development (ASD). Pendekatan ini sangat populer dan saat ini diterapkan secara unik seperti LeanUX, Scrum, dan Design Sprint.

Pada penelitian ini dilakukan studi literatur terhadap metodologi berbasis UCD dan ASD. Diperoleh metodologi yang terdiri dari 3 proses dasar yaitu analisis, desain, dan evaluasi yang terdiri dari metode survei, task analysis, persona, product backlog, design guidelines, prototipe, user testing, dan heuristic evaluation. Metodologi tersebut kemudian diterapkan dalam pengembangan antarmuka situs web e-commerce tipe business to consumer. Ditemukan bahwa pendekatan terhadap UCD dan ASD yang digunakan dapat diterapkan untuk meningkatkan tingkat usability dan prototipe yang dihasilkan memiliki skor heuristic evaluation yang baik.","E-commerce is very popular activity in the world, including Indonesia. Tokopedia, Lazada, and Bukalapak is one of leading business to consumer (B2C) e-commerce in Indonesia. User acceptance to an e-commerce system become more and more important aspect in digital industry to be measured now days. 

Usability can be utilized to improve information system quality in the aspect of user use. Good usability can be achieved by using user centered design (UCD) and agile software development (ASD) in user interface development. This
approach is very popular in user experience research and applied in many forms such as LeanUX, Scrum, and Design Sprint. 

This research conduct a literature study to many UCD and ASD methodologies. The methodology found consists 3 main processes; analysis, design, and evaluation which consists of survei, task analysis, personas, product backlog, design guidelines, prototype, user testing, and heuristic evaluation. The methodology then applied in B2C e-commerce website development. This research found that our approach to UCD and ASD can be used to improve usability and prototype produced has good heuristic evaluation score.",,,124637,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
176811,,Rangga Sahadewa,Evaluasi Pengalaman Belajar Mahasiswa pada Gamifikasi E-Learning eLOK Menggunakan Logika Fuzzy,,,"gamifikasi, e-learning, logika fuzzy","Khabib Mustofa, S.Si., M.Kom., Dr. tech.; Isna Alfi Bustoni, M.Eng.",2,3,0,2019,1,"Gamifikasi merupakan penerapan dari sebuah atau sebagian dari desain game ke dalam permasalahan yang nyata.  Gamifikasi dapat diterapkan pada sistem e-learning.  Gamifikasi e-learning memiliki tujuan yaitu memberikan engagement atau ketertarikan pada mahasiswa agar mahasiswa terus melakukan proses pembelajaran pada e-learning.  Oleh karena itu, penentuan kondisi pada mahasiswa perlu dilakukan agar mahasiswa tidak merasa khawatir atau bosan saat melakukan proses pembelajaran pada e-learning.

Pada penelitian ini dilakukan evaluasi pengalaman belajar mahasiswa menggunakan eLOK sebagai situs e-learning, dan plugin LevelUp! Moodle sebagai plugin untuk gamifikasi e-learning.    Penelitian ini menggunakan logika fuzzy untuk menentukan kondisi pada tiap mahasiswa.  Data yang digunakan pada penelitian ini adalah data course log dan data course dedication.  Data course log merupakan data yang berisikan aktivitas mahasiswa saat melakukan proses pembelajaran di e-learning, sedangkan data course dedication merupakan data yang berisikan waktu yang dihabiskan oleh mahasiswa selama satu semester.  Penelitian ini menggunakan data course log sebesar 1392 data, dan data course dedication sebesar 34 data.  Metode fuzzy inference yang digunakan adalah Mamdani, serta nilai performa yang digunakan adalah nilai akurasi.

Hasil dari penelitian ini adalah pengujian berupa survei dibandingkan dengan sistem fuzzy yang dibuat.  Nilai performa yang diperoleh berupa akurasi sebesar 55.17% ","Gamification is the application of full or part of game design that implement it into real problems Gamification can be applied to e-learning systems. Gamification of e-learning has the purpose of giving engagement or interest to students in order to that students continue to carry out the learning process in e-learning. Therefore, the determination of the student condition on students needs to be done so that students do not feel bored or worried when doing the learning process in e-learning.

In this study, an evaluation of student learning experiences using eLOK as a e-learning website, and the LevelUp! Moodle plugin as e-learning gamification plugin. This study uses fuzzy logic to determine on each student. The data used in this study is the course log data and course dedication data. Course log data is data that contains student activities while doing the learning process in e-learning, while course dedication data is data that contains time spent by students for one semester. This study uses course log data of 1392 data, and course dedication data of 34 data. Fuzzy inference machines used is Mamdani, and the performance value used is the value of accuracy.

The results of this study are testing in the form of a survey compared to the fuzzy system created. The performance value obtained in the form of an accuracy of 55.17%.",,,128442,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
179123,,PUTRA PERDANA HARYANA,Pengembangan Framework Asesmen Dampak Perambatan Vulnerability pada Node Package Manager (npm),,,"npm, perambatan vulnerability, jaringan dependency","Sigit Priyanta, S.Si., M.Kom., Dr.",2,3,0,2019,1,"Node Package Manager (npm) adalah sebuah layanan package repository yang menopang keberlangsungan ekosistem Node.js secara khusus dan JavaScript secara umum. Sebagai repositori yang bersifat open source, npm memungkinkan siapapun untuk memublikasikan package karyanya agar dapat dimanfaatkan oleh orang lain tanpa ada proses screening. Hal tersebut, ditambah praktik code reuse (penggunaan bagian kode dari perangkat lunak lain) yang sangat umum di kalangan pengembang perangkat lunak, membuka jalan bagi vulnerability yang dimuat oleh suatu package untuk merambat ke package lain melalui jaringan dependency.

Pada penelitian ini akan dikembangkan framework identifikasi package pewaris vulnerability di jaringan dependency antar-package npm. Proses identifikasi mengadopsi pendekatan temporal agar alur evolusi package menjadi perhatian. Pendekatan komputasi big data diterapkan untuk mengakomodasi besarnya ukuran dataset jaringan npm.

Framework diaplikasikan untuk melakukan asesmen pada dataset snapshot npm pada tanggal 2 November 2017 dan laporan advisory snyk.io hingga tanggal 9 November 2017. Dari 267 package yang dilaporkan memuat vulnerability, framework berhasil mengidentifikasi sebanyak 187.694 package terdampak yang tersebar hingga pada 13 level kedalaman dependency. Hasil asesmen pada studi ini dapat dijadikan gambaran umum bagi pengembang perangkat lunak untuk mencegah risiko perambatan vulnerability.","Node Package Manager (npm) is a package repository service that supports the Node.js and JavaScript ecosystem. Being an open-source project, it allows everyone to publish their package to be used by anyone else without any screening procedure. With code reuse being a very common practice among software developers, it paves the path for vulnerabilities to propagate among packages in the dependency network.

This study aims to develop a framework to identify vulnerability propagation in transitively-dependent packages along the dependency network. Identification process adopts temporal approach to put more focus on package evolution. Big data computational approach is applied to accomodate the sheer size of whole npm ecosystem data.

The framework is then applied to assess the npm snapshot of November 2 2017 with snyk.io's advisory report until November 9 2017. From the initial 267 reportedly vulnerable packages, the framework identified 187.694 affected packages up to 13 dependency-level deep. This finding may give software developers a big picture on the security state of the npm ecosystem to anticipate vulnerability propagation risk.",,,129927,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171966,,MUHAMMAD REZA PAHLEVI,ANALISIS SENTIMEN PERTANDINGAN SEPAK BOLA PADA TWEET TWITTER MENGGUNAKAN TEXT MINING,,,"sepak bola, svm, mnb, rf, analisis sentimen","Andi Dharmawan, S.Si., M.Cs., Dr.",2,3,0,2019,1,"Sepak bola merupakan olahraga yang paling banyak digemari oleh penduduk di Seluruh Dunia. Permainan sepak bola tidak akan ramai jika tanpa sosok pemain ke-12 yaitu Pendukung. Pendukung bisa menaikan mental tim yang didukungnya. Jika tim yang didukung menang, maka akan tampak raut muka kebahagiaan, namun jika tim yang didukung kalah akan tampak kekecewaan. Euforia tersebut kemudian akan diluapkan melalui sosial media, sosial media yang sering dipakai adalah Twitter. Klasifikasi luapan euforia tersebut perlu dilakukan karena bisa menjadi acuan tim untuk berbenah.
Pada penelitian ini dilakukan klasifikasi sentimen pertandingan sepak bola menggunakan data tweet dari Twitter menggunakan metode Support Vector Machine (SVM), Multinomial Naive Bayes (MNB) dan Random Forest (RF) yang dikombinasikan dengan variasi parameter. Data yang digunakan adalah tweet pertandingan Tottenham Hotspur melawan Manchester United yang berjumlah 19.292 yang diambil secara waktu nyata dari Twitter. Pengambilan data dilakukan pada tanggal 13 Januari 2019 bersamaan dengan waktu pertandingan dilaksanakan. Metode ekstraksi fitur yang digunakan adalah metode TF atau TF-IDF dan nilai performa yang dihitung adalah accuracy, precision, recall, dan f1 score.
Hasil dari penelitian ini adalah performa accuracy sebesar 79,4%, precision sebesar 72,68%, recall sebesar 51,53% dan f1 score sebesar 60,3% untuk model SVM, MNB menghasilkan skor accuracy sebesar 75,48%, precision sebesar 71,11%, recall sebesar 67,14% dan f1 score sebesar 68,3%, sedangkan accuracy sebesar 78,7%, precision sebesar 77,2%, recall sebesar 70,02% dan f1-score sebesar 71,94% untuk model RF.
","Football is the most popular sport for people around the world. The game of football will not be interested without the figure of the 12th player, Supporters. Supporters can mentally raise the team they support. If the supported team wins, then there will be a look of happiness, but if the team supported by the defeat will appear disappointed. The euphoria will shared through social media, social media that is often used is Twitter. The euphoric overflow classification needs to be done because it can be a reference for the team to improve.
In this study football class sentiment classification was conducted using tweet data from Twitter using the Support Vector Machine (SVM), Multinomial Naive Bayes (MNB) and Random Forest (RF) combined with severals parameters. The data used is a tweet of Tottenham Hotspur's match against Manchester United which total 19,292 taken in real time from Twitter. Data collection is carried out on January 13, 2019 together with the time the match is held. The feature extraction method used is the TF or TF-IDF method and the calculated performance value is accuracy, precision, recall, and f1 score.
The results of this study are 79.4% performance accuracy, 72.68% precision, 51.53% recall and 60.3% f1 score for SVM models, MNB produces an accuracy score of 75.48%, precision equal to 71.11%, recall of 67.14% and f1 score of 68.3%, while accuracy is 78.7%, precision is 77.2%, recall is 70.02% and f1- score is 71.94% for RF models.",,,123123,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175551,,FENGKIE JUNIS,DIGITAL ART REGISTRY FOR OWNERSHIP AND SALEABILITY OF AR-BASED DIGITAL ART THROUGH BLOCKCHAIN-BASED SMART CONTRACT,,,"blockchain, ERC-1155, Ethereum, smart contract, augmented reality, digital art, digital scarcity","Suprapto, Drs., M.Kom., Dr.",2,3,0,2019,1,"Objek digital memiliki sifat yang berbeda dengan objek fisik Terutama, ia dapat dengan mudah disalin secara identik sebanyak mungkin. Dalam ranah karya seni digital, khususnya lukisan dan patung berbasis augmented reality, karena sifat digitalnya, untuk menentukan dan membuktikan kepemilikan karya seni digital bagi kolektor seni menjadi tantangan karena sifatnya yang tidak langka.
Bitcoin, uang digital peer-to-peer menangani masalah serupa dengan memanfaatkan teknologi blockchain. Perkembangan terkini dalam bidang ini yang salah satunya ialah smart contract berbasis blockchain telah memungkinkan jangkauan berbagai domain untuk mengimplementasikan solusi serupa dalam hal membuktikan kepemilikan dan kelangkaan informasi digital.
Penelitian ini mencoba untuk menerapkan teknologi ini dalam domain karya seni berbasis augmented reality dengan mengembangkan proof-of-concept sistem dalam platform Ethereum. Sistem ini memiliki 3 komponen utama yaitu: artwork registry, server, dan viewer. Sistem yang dikembangkan mampu mendaftarkan karya seni sebagai token unik dan membatasi jumlah kepemilikannya, membuatnya langka secara digital. Transfer kepemilikan untuk sebuah karya seni juga relatif mudah serta sistem ini juga kompatibel dengan standar ERC-1155 yang ada, sehingga siap untuk diperdagangkan di pasar digital manapun yang mendukung ERC-1155. Penggunaan sumber daya dari sistem tersebut (diukur dalam gas Ethereum) berada dalam kisaran 175.204 hingga 179.172 untuk pendaftaran karya seni dan dalam kisaran 25.384 hingga 59.352 untuk transfer kepemilikan.
","A digital object has different properties to a physical one. Most predominantly, it can easily be copied identically as much as possible. In the domain of digital artwork, particularly augmented reality based paintings and sculpture, due to its digital nature, to determine and prove its ownership for art collectors becomes a challenge because it is not scarce.
Bitcoin, a peer-to-peer digital money tackles the similar problem by utilizing the blockchain technology. Recent development in this field such as blockchain-based smart contract has enabled a wider range of domain to implement similar solution in regards of proving ownership and scarcity for digital information.
This research attempts to implement this technology in the domain of augmented reality based artworks by developing a proof-of-concept of such system on Ethereum platform. The system has 3 main components i.e.: artwork registry, server, and viewer. The developed system is able to register an artwork as a unique token and limit its number of ownership, making it digitally scarce. Ownership transfer for an artwork is also relatively easy and the system is also compatible to an existing standard ERC-1155, making it ready to be traded in any digital market that supports ERC-1155. The resource usage of such system (measured in Ethereum gas) is in range of 175,204 to 179,172 for artwork registration and in range of 25,384 to 59,352 for ownership transfer.",,,126410,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177601,,Muhammad Muhlas Abror," IMPLEMENTASI CONTAINER DAN KUBERNETES ORCHESTRATION DALAM PELUNCURAN, PENGAWASAN DAN PENGATURAN EKOSISTEM BIG DATA",,,"big data, container, orchestration, Kubernetes, cluster","Mardhani Riasetiawan, M.T., Dr.",2,3,0,2019,1,"Big data adalah kondisi dimana model penyimpanan basis data konvensional tidak dapat lagi menanggulangi data dengan jumlah yang besar. Big data memiliki karakteristik lima V, yaitu Volume, Velocity, Variety, Value, dan Veracity. Beberapa tahun terakhir ini, sudah terdapat banyak sekali framework - framework yang dapat digunakan untuk pemrosesan big data, bahkan pasar layanan big data diprediksi akan meluas pada tahun 2020. Framework big data dapat diinstal dalam sistem operasi yang telah tersedia saat ini atau dapat menggunakan Gamabox OS, tetapi sistem operasi memiliki kelemahan yaitu proses instalasi yang lama dan konfigurasi yang cukup rumit bagi orang awam.
Penelitian ini bertujuan untuk membuat suatu container yang berisi platform big data, serta sistem orkestrasi yang memiliki kemampuan untuk meluncurkan, mengatur, dan mengawasi platform big data tersebut. Pada penelitian ini, dilakukan pembuatan platform big data dalam container dan pembuatan paket terhadap platofrm tersebut agar dapat berjalan dalam lingkungan klaster. Metode yang digunakan untuk pembuatan paket ini adalah metode agile development dengan menggunakan Ubuntu 16.04 sebagai sistem operasi dasar, sedangkan sistem orkestrasi yang digunakan pada penelitian ini adalah Kubernetes. Container hasil pengembangan ini diberi nama Gamabox Container, sedangkan sistem orkestrasi hasil modifikasi ini diberi nama Gamacloud.
Berdasarkan hasil pengujian yang telah dilakukan, Gamacloud dapat meluncurkan hingga 6 buah layanan Gamabox Container. Hasil pengujian running time Gamabox container dan Gamabox OS menggunakan TestDFSIO dan Terasort memiliki perbedaan signifikan.","Big data is a condition where conventional database storage models can no longer cope with large amounts of data. Big data has five V characteristics, namely Volume, Velocity, Variety, Value, and Veracity. In recent years, there have been a lot of frameworks that can be used for big data processing, even the big data service market is predicted to expand in 2020. The big data framework can be installed in the operating system that is currently available or can use Gamabox OS, but the operating system has the disadvantage that the installation process is long and the configuration is quite complicated for ordinary people.
This study aims to create a container which contains a big data platform, as well as an orchestration system that has the ability to launch, organize, and supervise the big data platform. In this study, a big data platform was created in the container and the package was made on the platofrm to run in a cluster environment. The method used for making this package is the agile development method using Ubuntu 16.04 as the basic operating system, while the orchestration system used in this study is Kubernetes. The container from this development is named Gamabox Container, while the modified orchestration system is named Gamacloud.
Based on the results of tests that have been conducted, Gamacloud can launch up to 6 Gamabox Container services. The running time test result of Gamabox Container and Gamabox OS has a significance difference conducted with TestDFSIO and Terasort",,,128436,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
176323,,BINTANG ARIGO K U,OPINION MINING USING NAIVE BAYES ALGORITHM ON ONLINE ACCOMMODATION CUSTOMER REVIEWS,,,"Naive Bayes, Term Frequency-Inverse Document Frequency, TF-IDF, Opinion Mining, Online, Customer Reviews.","I Gede Mujiyatna S.Kom., M.Kom.",2,3,0,2019,1,"Ulasan pelanggan online berisi pendapat yang dibuat oleh pengguna yang diposting di situs web pemesanan akomodasi online. Dalam situs web pemesanan akomodasi seperti airbnb.com, terdapat informasi yang berlebihan dari jumlah ulasan dalam berbagai daftar akomodasi. Sistem penambangan opini dapat digunakan untuk menganalisis dan meringkas ulasan pelanggan online untuk memberikan pengguna klasifikasi klasifikasi sesuai dengan polaritas yang positif, negatif atau netral.
Dalam tujuan mengklasifikasikan teks, metode pembobotan istilah seperti TF-IDF dapat digunakan untuk membantu lebih lanjut dalam proses mengklasifikasikan teks. Dalam penelitian ini, tujuannya adalah untuk menerapkan sistem penambangan opini yang menggunakan pembobotan TF-IDF dan algoritma Naive Bayes untuk menghasilkan performa tinggi untuk setiap parameter evaluasi dengan tujuan mengklasifikasikan polaritas ulasan pelanggan online yang diperoleh dari airbnb.com. Dataset yang digunakan dalam penelitian ini terdiri dari 750 ulasan pelanggan online, yang terdiri dari 250 ulasan positif, 250 ulasan negatif dan 250 ulasan netral.
Hasil yang diperoleh dalam penelitian ini memberikan akurasi 69%, ketepatan 70%, recall 70% dan f-ukur 70%. Namun, setelah menerapkan pengurangan stop word dalam prosedur stop word removal, hasil yang diperoleh memberikan peningkatan keseluruhan dalam akurasi 72%, ketepatan 75%, penarikan kembali 72% dan pengukuran-f 73%.","An online customer review contains an opinion made by a user posted on an online accommodation booking website. Within an accommodation booking website such as airbnb.com, there is an overload of information from the amount of reviews within various accommodation listings. An opinion mining system can be used to analyze and summarize the online customer reviews in order to provide the users with classifications of the reviews accordingly the polarity being either positive, negative or neutral. 
Within the purpose of classifying text, a term weighting method such as TF-IDF can be utilized to further assist in the process of classifying the text. In this research, the goal is to implement an opinion mining system that utilizes TF-IDF weighting and Naive Bayes algorithm to produce a high performance for each evaluation parameter in the purpose of classifying the polarity of the online customer reviews acquired from airbnb.com. The dataset used in this research consists of 750 online customer reviews, in which consists of 250 positive reviews, 250 negative reviews and 250 neutral reviews. 
The results obtained in this research gave an accuracy of 69%, precision of 70%, recall of 70% and f-measure of 70%. However, upon implementing stop word reduction within the stop word removal procedure, the results obtained gave an overall improvement in accuracy of 72%, precision of 75%, recall of 72% and f-measure of 73%.",,,127059,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
178125,,BRILIANTO INDRAJAYA,Analisis Kepuasan Pelanggan Pemakai Apartemen Menggunakan Algoritma C4.5 dan Naive-Bayes,,,"algoritma C4.5,algoritma Na&Atilde;&macr;ve-Bayes,analisis kepuasan pelanggan","Anifuddin Aziz, S.Si., M.Kom",2,3,0,2019,1,"Ketersediaan lahan adalah suatu hal yang bersifat terbatas. Manusia membutuhkan lahan untuk dibangun tempat tinggal, sedangkan dengan bertambahnya jumlah populasi manusia jumlah lahan yang tersedia terus berkurang setiap tahunnya. Dengan semakin berkurangnya jumlah lahan tersedia terutama di tempat-tempat strategis banyak developer properti yang beralih dari pembangunan horisontal ke pembangunan vertikal. Salah satunya adalah untuk penghematan lahan agar lahan yang tersedia dapat ditempati oleh banyak keluarga dalam bentuk apartemen dibanding dalam bentuk perumahan horisontal. Berbagai developer melakukan pembangunan apartemen dengan standar kualitas yang berbeda-beda. Hal ini menjadi faktor adanya penelitian di bidang analisis kepuasan pelanggan pemakai apartemen. Berbagai kasus klasifikasi dan implementasi algoritma telah dilakukan, seperti algoritma C4.5 dan Na&Atilde;&macr;ve-Bayes. Di antara kedua algoritma tersebut belum ada informasi seccara pasti mana yang memiliki performa lebih baik dalam analisis kepuasan.

		 Penelitian ini mengimplementasikan algoritma C4.5 dan Na&Atilde;&macr;ve-Bayes dalam analisis kepuasan pelanggan pada suatu topik yang sudah ditentukan. Penelitian dilakukan dengan menyebar kuesioner kepada pemakai apartemen kota Semarang. Setelah itu data kuesioner diekstraksi dari data kontinyu menjadi data yang diskrit. Skenario pengujian yang dilakukan meliputi pengaruh peningkatan performa yang diukur dari segi akurasi,f-measure, dan waktu pengujian.

		Pada penelitian ini diperoleh hasil akurasi, f-measure, dan waktu pengujian seiring dengan bertambahnya jumlah data. Hasil evaluasi performa pengklasifikasian pada penelitian ini menunjukkan bahwa algoritma Na&Atilde;&macr;ve-Bayes memiliki performa pengklasifikasian lebih baik dibandingkan algoritma C4.5 pada analisis kepuasan pelanggan. Algoritma C4.5 menghasilkan tingkat akurasi 83.33% dan f-measure 88.80%. Sedangkan Algoritma Na&Atilde;&macr;ve-Bayes menghasilkan tingkat akurasi 92.67% dan f-measure  95.05%. Dalam segi waktu, algoritma C4.5 memilki waktu pengujian lebih lama dibandingkan dengan algoritma Na&Atilde;&macr;ve-Bayes seiring dengan bertambahnya jumlah dataset pengujian.
","Land is a thing that has limited amount. Humans need land to build a place to live, whereas with the increasing number of human populations the amount of available land continues to decrease each year. With the decreasing amount of available land especially in strategic places many property developers are shifting from horizontal development to vertical construction. One of them is to save land so that the available land can be occupied by many families in the form of apartments rather than in the form of horizontal housing. Various developers are building apartments with their different quality standards. This is a factor in the existence of research in the area of customer satisfaction analysis of apartment users. Various cases of classification and implementation of algorithms have been carried out, such as the C4.5 algorithm and Na&Atilde;&macr;ve-Bayes. Between the two algorithms, there is no certain information as to which one has better performance in the satisfaction analysis.

This research implements C4.5 and Na&Atilde;&macr;ve-Bayes algorithms in the analysis of customer satisfaction on a predetermined topic. The study was conducted by distributing questionnaires to Semarang city apartment users. Then the questionnaire data is extracted into discrete data that is not continuous. Test scenarios carried out include the effect of increasing performance that measured in terms of accuracy, f-measure, and test time.

In this study the results obtained are accuracy, f-measure, and testing time along with the increasing amount of data. The results of the classification performance evaluation in this study indicate that the Na&Atilde;&macr;ve-Bayes algorithm has a better classification performance than the C4.5 algorithm in the analysis of customer satisfaction. C4.5 algorithm produces 83.33% accuracy and 88.80% f-measure. Whereas the Na&Atilde;&macr;ve-Bayes algorithm produces an accuracy rate of 92.67% and an f-measure of 95.05%. C4.5 algorithm has a longer testing time compared to the Na&Atilde;&macr;ve-Bayes algorithm as the number of test datasets increases.
",,,132983,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171726,,Nadhifa Sofia,IMPLEMENTASI SISTEM WEB SEMANTIK QUESTION ANSWERING DALAM BIDANG RESTORAN DAN KULINER DENGAN METODE RULE-BASED (STUDI KASUS: DAERAH ISTIMEWA YOGYAKARTA),,,"Information Retrieval, Web Semantik, Sistem Pertanyaan Jawaban, Pengolahan Bahasa Alami, Sistem Berbasis Aturan","Azhari SN, Drs., MT., Dr",2,3,0,2019,1,"Sistem Question Answering (QA) telah dikembangkan dalam berbagai pendekatan, seperti menggunakan metode berbasis pengetahuan serta pembelajaran mesin. Namun, belum ada yang mengembangkan sistem QA berbasis pengetahuan dengan kombinasi analisis semantik seperti penggunaan metode rule-based yang yang diimplementasikan dengan pencarian makna pertanyaan untuk analisis semantiknya. Selain itu, penggunaan jenis pertanyaan juga bertujuan untuk mempermudah pencarian makna pertanyaan serta pencarian jawaban.
Penggunaan domain restoran dan kuliner ini ditujukan untuk bisa memberikan saran serta informasi mengenai restoran yang akan dikunjungi oleh masyarakat DI Yogyakarta maupun wisatawan yang akan berwisata kuliner karena restoran tidak hanya berfungsi sebagai tempat makan saja, namun juga untuk tempat berkumpul, mengerjakan tugas, maupun tempat wisata.
Sistem QA ini diimplementasikan dalam pendekatan rule-based serta analisis semantik dengan 309 pertanyaan dalam sembilan jenis pertanyaan. Untuk memproses kalimat pertanyaan tersebut, dibutuhkan metode dalam question analysis, information retrieval, serta answer extraction. Sistem ini memberikan hasil akurasi sebesar 96.76% terhadap pertanyaan dalam lingkup restoran dan kuliner. Jenis pertanyaan yang diujikan berupa lokasi, jam kerja, fasilitas, harga, menu, menu spesial, diskon, yang mengadakan diskon, serta konsep restoran.","Question Answering (QA) systems have been developed in various approaches, such as using knowledge-based methods and machine learning. However, no one has developed a knowledge-based QA system with a combination of semantic analysis such as the use of rule-based methods that are implemented by searching the meaning of questions for their semantic analysis. In addition, the use of this type of question also aims to facilitate the search for the meaning of questions and search for answers.
The use of restaurant and culinary domains is intended to be able to provide advice and information about restaurants that will be visited by DI Yogyakarta people as well as tourists who will go on culinary tours because the restaurant does not only function as a place to eat, but also for gathering places, assignments, or tourist attractions.
This QA system is implemented in a rule-based approach and semantic analysis with 309 questions in nine types of questions. To process the sentence of the question, a method is needed in question analysis, information retrieval, and answer extraction. This system provides results of accuracy of 96.76% in questions on the restaurant and culinary sphere. The types of questions tested were in the form of locations, working hours, facilities, prices, menus, special menus, discounts, discounts, and restaurant concepts.
",,,122967,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171215,,Inggar Riyandi Musyaffanto,Peringkas Teks Otomatis Ekstraktif pada Berita Berbahasa Indonesia Menggunakan Maximal Marginal Relevance dan Non-Negative Matrix Factorization,,,"peringkasan, ekstraktif, maximal marginal relevance, non-negative matrix factorization, berita","Guntur Budi Herwanto, S.Kom., M.Cs.",2,3,0,2019,1,"Pesatnya perkembangan internet memunculkan banyak situs berita online dan tidak jarang beritanya memiliki judul yang tidak sesuai dengan isi berita, sehingga pembaca harus membaca keseluruhan teks untuk memahami teksnya. Masalah tersebut bisa diatasi secara otomatis dengan automatic text summarization. 
Pada penelitian ini teks berita akan dilakukan pra-pemrosesan data berupa sentence segmentation, case folding, stopword removal, dan stemming. Setelah itu digunakan Maximal Marginal Relevance (MMR) untuk merangkum teks secara otomatis. Pada pembentukannya terdapat dua macam query yaitu yang berasal dari judul dan yang berasal dari hasil Non-Negative Matrix Factorization (NMF) untuk mengatasi judul yang tidak sesuai dengan isi berita. 
Berdasarkan hasil evaluasi dengan ROUGE-1 pada rangkuman sistem yang hanya menggunakan MMR didapatkan nilai recall sebesar 62.7%, precision sebesar 69.59%, dan f-measure sebesar 63.58% serta untuk ROUGE-2 didapatkan nilai recall sebesar 53.11%, precision sebesar 58.61%, dan f-measure sebesar 55.21%. Sedangkan untuk rangkuman sistem yang menggabungkan MMR dan NMF pada pengujian ROUGE-1 didapatkan nilai recall sebesar 67.15%, precision sebesar 69.39%, dan f-measure sebesar 67.73% serta untuk ROUGE-2 didapatkan nilai recall sebesar 58.50%, precision sebesar 58.40%, dan f-measure sebesar 58.05%.
","The rapid development of internet has led to many online news sites and the title of the news is frequently misleading, so readers have to read all the text to understand the meaning of the text. This problem can be overcome automatically with automatic text summarization. 
This research did data preprocessing for the news text such as sentence segmentation, case folding, stopword removal, and stemming. After that, Maximal Marginal Relevance (MMR) is used to summarize the text automatically. To form the summaries, there are two types of query. The first one is from the title of the news and the second one is from the Non-Negative Matrix Factorization (NMF)'s result to overcome the misleading title.
Based on evaluation using ROUGE-1, system summary that only use MMR got 62.70% for recall, 69.59% for precision, and 63.58% for f-measure also based on evaluation using ROUGE-2, it got 53.11% for recall, 58.61% for precision, and 55.21% for f-measure. While in system summary that combine MMR and NMF, based on evaluation using ROUGE-1 got 67.15% for recall, 69.39% for precision, and 67.73% for f-measure also based on evaluation using ROUGE-2, it got 58.50% for recall, 58.40% for precision, and 58.05% for f-measure.
",,,122508,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
176593,,FAISAL MALIK WIDYA P,Binary Classification on Encrypted Image Data using Paillier's Homomorphic Encryption and Convolutional Neural Network,,,"Machine Learning, Cloud System, Homomorphic Encryption, Paillier's Homomorphic Encryption, Convolutional Neural Network","Anny Kartika Sari, S.Si., M.Sc., Ph.D.",2,3,0,2019,1,"Algoritma pembelajaran mesin telah dikembangkan dan digunakan untuk memecahkan banyak masalah dunia nyata. Penerapan pembelajaran mesin di platform cloud computing juga meningkatkan kekuatan pembelajaran mesin untuk memecahkan masalah tersebut secara daring. Sayangnya, penyebaran pembelajaran mesin untuk data pribadi dalam platform cloud computing terbatas karena masalah privasi dan kerahasiaan pada sistem cloud. Praktik umum pendekatan sistem cloud yang rahasia dan privat menggunakan banyak langkah untuk memodifikasi data dan tidak dapat mengambil keuntungan dari cloud computing karena komputasi dilakukan di komputer lokal.
Homomorphic encryption dapat digunakan untuk menyelesaikan masalah ini. Homomorphic encryption memungkinkan komputasi pada ciphertext. Ini memungkinkan perhitungan yang aman dilakukan di server cloud. Sayangnya, karena batas operasi yang dapat dilakukan dalam data terenkripsi homomorfik, pendekatan saat ini dari algoritma pembelajaran mesin dalam data terenkripsi homomorfik masih memiliki masalah. Karena mereka menggunakan fully homomorphic encryption yang hanya dapat melakukan operasi biner antara ciphertext, model hanya dapat menganalisis data yang dienkripsi menggunakan public key yang sama dengan data training.
Penelitian ini mengajukan solusi untuk mengimplementasikan algoritma pembelajaran mesin, khususnya convolutional neural network untuk mendiagnosis pneumonia pada gambar CXR terenkripsi menggunakan cryptosystem Paillier. Cryptosystem Paillier memiliki dua sifat homomorfik utama yang memungkinkan CXR terenkripsi untuk dioperasikan pada model CNN tanpa mengenkripsi model dan melakukan training ulang pada setiap public key yang berbeda. Ada beberapa modifikasi yang diperlukan pada model CNN untuk memungkinkan operasi pada CXR terenkripsi menjadi mungkin. Evaluasi akan dilakukan untuk kinerja keseluruhan dari model yang diajukan dengan membandingkannya dengan model yang menganalisis CXR tidak terenkripsi, yaitu CNN asli.","Machine learning algorithm has been developed and used for solving many real-world problems. Deployment of machine learning in cloud computing platform also enhance the power of machine learning to solve those problems online. Unfortunately, deployment of machine learning for private data in cloud computing platform is limited due to cloud system privacy and confidentiality issue. Common practice of confidential and private cloud system approach takes a lot of steps to modify data and unable to take an advantage of cloud computing since the computation is done in local computer and in this case.
Homomorphic encryption can be used to solve this problem. Homomorphic encryption allows computation on ciphertext. This allows secure computation to be performed in cloud server. Unfortunately, due to the limit of operations that can be performed in homomorphic encrypted data, current approach of machine learning algorithm in homomorphic encrypted data still has a problem. Since they are using fully homomorphic encryption which can only perform binary operation between ciphertext, the model can only analyze data that is encrypted using the same public key as the training data.
This research proposes a solution to implement machine learning algorithm, especially convolutional neural network for diagnosing pneumonia on encrypted CXR image using Paillier's cryptosystem. Paillier's cryptosystem has two main homomorphic properties which allow the encrypted CXR to be operated on CNN model without encrypting the model and perform retraining on each different public key. There are several modifications that are needed on CNN model in order to make the operation on encrypted CXR possible. Evaluation will be conducted for the overall performance of the proposed model by comparing it with model that analyzes unencrypted CXR, i.e. the original CNN.",,,127545,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
183763,,RIMBA ERLANGGA,Pengaruh Phase-Encoding Bijektif pada Jaringan Syaraf Bernilai Kompleks,,,"bijektif, jaringan syaraf bernilai kompleks, klasifikasi citra, phase-encoding","Anifuddin Azis, S.Si., M.Kom.",2,3,0,2019,1,"Jaringan Syaraf Bernilai Kompleks (JSBK) adalah satu variasi jaringan syaraf tiruan yang menggunakan bilangan kompleks sebagai representasi parameternya, alih-alih menggunakan bilangan real seperti pada umumnya. Representasi input berupa bilangan real dapat diubah terlebih dahulu ke bilangan kompleks sebelum dilakukan proses pelatihan pada JSBK. Salah satu cara pengubahan ini adalah dengan menggunakan phase-encoding, yang mengubah informasi pada input yang berupa bilangan real menjadi informasi berupa fase bilangan kompleks hasil pengubahannya. Sejumlah penelitian menggunakan phase-encoding yang bersifat non-bijektif, dimana terdapat kemungkinan bahwa dua atau lebih bilangan real berbeda pada input yang diperlakukan sebagai sebuah bilangan kompleks yang sama. Padahal, dua atau lebih input bilangan real tersebut merupakan informasi yang saling berbeda.

Penelitian ini mengusulkan sebuah phase-encoding yang bersifat bijektif, dimana satu nilai input bilangan real dipetakan ke tepat satu bilangan kompleks. Hasil dari JSBK yang menggunakan phase-encoding ini dibandingkan dengan yang menggunakan phase-encoding yang bersifat non-bijektif. Lebih jauh, penelitian ini juga membandingkan pengaruh penggunaan phase-encoding dengan tidak melakukan pengubahan representasi input ke bilangan kompleks sama sekali. Percobaan diterapkan pada permasalahan klasifikasi citra dengan menggunakan dataset MNIST handwritten digits, CIFAR-10, dan dataset tambahan yang diperoleh dari CCTV Dishub Kabupaten Sukoharjo. Dari hasil penelitian yang diperoleh, walaupun phase-encoding yang diusulkan pada penelitian ini memberikan hasil yang tidak lebih baik dibandingkan phase-encoding yang bersifat non-bijektif, secara umum penggunaan phase-encoding mampu mempercepat konvergensi dan meningkatkan akurasi model, dibandingkan dengan tidak mengubah representasi input ke bilangan kompleks sama sekali.","Complex-valued Neural Network (CVNN) is a variant of neural networks that use complex numbers as the representation of its parameter, instead of the real number that widely used. The representation of the real-valued input could be encoded into a complex number before doing the training phase. One of the encodings available is phase-encoding, where the information contained in real-valued inputs are encoded into information in the form of the phase of the resulting complex number. A number of research papers used phase-encoding with non-bijective property, that two or more distinct values of real number in inputs are treated as the same complex number. Yet, those values of real numbers actually are different information.

This work proposed a new phase-encoding with bijectivity property, which maps a real-valued input to exactly one complex number. The result of the CVNN using this proposed phase-encoding then compared with another CVNN with non-bijective phase-encoding. Furthermore, this work also studied the effect of the usage of phase-encoding, compared with no change in real-valued inputs at all. Experiments have been applied to image classification problems using MNIST handwritten digits and CIFAR-10 datasets, and an additional dataset retrieved from CCTV Dishub Kabupaten Sukoharjo. Although the proposed phase-encoding did not give better results than the non-bijective phase-encoding, it showed that generally, the usage of phase-encoding led to faster convergence and higher accuracy of the model, compared with no change in real-valued inputs at all.",,,134663,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
181975,,GHINA INDAH FITHRIYAH,Perbandingan Sentiment Analysis pada Akun Instagram menggunakan Pembobotan Term Frequency Inverse Document Frequency (TF-IDF) Dan Naive Bayes Classifier (NBC),,,"sentiment analysis, naive bayes, multinomial naive bayes, tf-idf, naive bayes classifier","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2019,1,"Dalam era ini internet menyebabkan peningkatan jumlah pengguna jejaring sosial. Instagram salah satu platform yang populer dalam masa kini. Dalam perkembangannya, Instagram kini memiliki banyak komunitas, diantaranya komunitas beauty, travel, automotive, fashion hingga food. Komunitas yang paling pesat perkembangannya di Indonesia adalah komunitas pencinta makanan (food). Mereka berkumpul dan menghadirkan para influencer yang sering kita sebut sebagai instafood, dan mereka akan membagikan informasi mengenai menu rekomendasi dari food stand sampai restoran sehingga dapat mempengaruhi bisnis makanan itu sendiri.
Pada penelitian ini dilakukan proses sentiment analysis menggunakan Multinomial Naive Bayes Classifier dan Naive Bayes Classifier pada data komentar dari dua akun instafood yaitu kulineryogya dan jogjaculinary. Data ini menggunakan sampel komentar post dari bulan Januari 2019 hingga Maret 2019 dan diambil sebanyak 5807 komentar. Setelah dilakukan text processing didapatkan jumlah vector dari fitur kedua akun tersebut sebanyak 3097 kata. 
Pengujian dalam dua akun tersebut menggunakan Multinomial Naive Bayes dan Naive Bayes Classifier sebagai classifier dan metode validasi Cross Validation. Dari hasil pengujian ini didapatkan akurasi sebesar 0,9006 pada akun jogjaculinary dan 0,9424 pada akun kulineryogya. Hasil klasifikasi dengan menggunakan Naive Bayes Classifier lebih tinggi dibandingkan dengan menggunakan Multinomial Naive Bayes.  Pada Multinomial Naive Bayes bahwa nilai sentimen positif untuk akun jogjaculinary sebesar 97,23%, lebih tinggi jika dibandingkan dengan akun kulineryogya yang hanya sebesar 91,14%. Pada Naive Bayes Classifier bahwa nilai sentimen positif untuk akun kulineryogya sebesar 99.08%, lebih tinggi jika dibandingkan dengan akun jogjaculinary yang hanya sebesar 97,71%. Perbedaan ini tidak signifikan karena kedua akun tersebut mengekspose niche yang sama di kota yang sama.","Era of the Internet caused an increase of social media users. Instagram is one of the most popular platforms of social media.Nowdays, Instagram now has many community, including beauty, travel, automotive, fashion, and food. Food community is the fastest developing community in Indonesia. They are gathering and inviting the influencers that we often refer as an instafood. They will share about reccomended food from foods stand to restaurant that improve the food business itself.
This research studying about sentiment analysis process was carried out using the Multinomial Naive Bayes classifier dan Naive Bayes Classifier on comments data from two instafood account named kulineryogya and jogjaculinary. The data uses the comments from the post on January 2019 to March 2019 as sample and is taken as many as 5807 comments. The result of calculating vector features from the two accounts were 3097 words after going through the text processing.
Multinomial Naive Bayes and Naive Bayes Classifier as classifier and Cross validation method was used to tested in these two accounts uses. From the results of this test we kows that accuracy 0.9006 on the jogjaculinary account and 0.9424 on the kulineryogya account. The classification results using Naive Bayes Classifier was higher than the Multinomial Naive Bayes. In Multinomial Naive Bayes, positive sentiment value for the jogjaculinary account was 91.23%, it was higher than the kulineryogya account which was only 91.14%. In Naive Bayes Classifier, positive sentiment value for the kulineryogya account was 99.08%, it was higher than the jogjaculinary account which was only 97.71%. The difference is not significant because both accounts are on the same niche in the same city.",,,132893,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171739,,ABDUL HASAN MUBAROK,ALGORITMA AKS TERIMPROVISASI PADA PENGUJIAN BILANGAN PRIMA,,,"AKS, probabilistik, pengujian primalitas","Dr-Ing, Reza Pulungan, S.Si, M. Sc.",2,3,0,2019,1,"Salah satu penemuan yang bermanfaat dalam teori bilangan adalah algoritma RSA. Algoritma ini membantu menjaga informasi agar aman. Dalam algoritma ini diperlukan sebuah bilangan prima bernilai besar untuk membangkitkan kunci yang akan digunakan untuk melakukan enkripsi dan dekripsi. Untuk mendapatkan bilangan prima tersebut, pendekatan yang digunakan adalah dengan membangkitkan bilangan bulat secara acak kemudian dilakukan pengujian bilangan prima. Pengujian bilangan prima adalah algoritma untuk menentukan apakah suatu bilangan merupakan bilangan prima atau bukan.
Algoritma yang digunakan adalah yang diusulkan oleh Han Wei Wu, dkk. Algoritma pengujian bilangan prima dibagi menjadi dua macam, yaitu probabilistik dan deterministik. Algoritma probabilistik dapat melakukan pengujian dengan cepat namun sangat rentan terjadi kesalahan. Sementara algoritma deterministik dapat melakukan pengujian secara tepat namun kecepatan pengujiannya sangat rendah. AKS termasuk algoritma pengujian bilangan prima deterministik yang sangat lambat sehingga tidak dapat digunakan dalam praktik. Dalam penelitian ini akan dilakukan improvisasi terhadap AKS sehingga dapat digunakan dalam praktik meskipun menjadi bersifat probabilistik.","One of useful finding in number theory is RSA algorithm. This algorithm is needed to keep information secure. This algorithm requires a big prime to generate a key that will be used to encrypt and decrypt. The approach used to get the prime number is to randomly generate an integer and then perform the primality test. Primality test is an algorithm to determine whether a number is a prime or not.
The algorithm used is proposed by Han Wei Wu, et al. The primality testing algorithm is divided into two types, probabilistic and deterministic. The probabilistic algorithm is fast but its misjudgment probability is high. While deterministic algorithm can perform the test precisely but its test speed is so low. AKS is a deterministic primality test algorithm that is so slow that it is not useful in practice. This research will improvise AKS so that it can be used in practice although probability-based.",,,123012,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175837,,DEFFI ALFIANI M,PREDIKSI STRUKTUR SEKUNDER PROTEIN MENGGUNAKAN SUPPORT VECTOR MACHINES (SVM),,,"SVM, classification, protein secondary structure prediction.","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2019,1,"Protein merupakan elemen yang sangat penting di dalam tubuh makhluk hidup. Fungsi protein dapat terlihat apabila telah terjadi pelipatan sekuens protein menjadi struktur tiga dimensi. Sehingga, dengan memahami bagaimana pelipatan tersebut terjadi, akan sangat membantu untuk mengetahui fungsi protein, yang selanjutnya dapat dimanfaatkan lebih lanjut, seperti dalam pembuatan obat untuk penyakit tertentu. Pada penelitian ini dilakukan prediksi struktur sekunder protein menggunakan SVM. 
Preprocessing yang dilakukan menggunakan metode sliding window dan dua metode pengkodean, yaitu orthogonal input profile dan BLOSUM62 matrix input profile. Fungsi kernel digunakan dalam penelitian ini untuk memetakan fitur ke ruang dimensi tinggi. Terdapat tiga fungsi yang digunakan, yaitu linear kernel, polynomial kernel, dan RBF kernel. Adapun untuk menghitung akurasinya, digunakan Q3 yang dapat menghitung akurasi sistem prediksi struktur sekunder protein untuk klasifikasi dengan tiga kelas.
Hasil akurasi Q3 terbaik adalah sebesar 63,6638% yang diperoleh dengan preprocessing menggunakan sliding window berukuran tiga belas asam amino dan pengkodean BLOSUM62 matrix input profile. Fungsi kernel yang digunakan yaitu RBF kernel dengan parameter c=32 dan gamma=0,0078125.
","Protein is a very important element in the body of living things. The function of protein can be seen if there has been a folding of the protein sequence into a three-dimensional structure. So, by understanding how folding occurs, it is help us to know the function of proteins, which can be used, such as in making drugs for certain diseases. In this study, the protein secondary structure prediction using SVM method.
Preprocessing is done using the sliding window method and two encoding methods, orthogonal input profile and BLOSUM62 matrix input profile. The kernel function is used in this study to map features to high-dimensional space. There are three functions that are used, linear kernel, polynomial kernel, and RBF kernel. As for calculating its accuracy, Q3 is used which can calculate the accuracy of protein secondary structure prediction for classification with three classes.
The best Q3 accuracy results were 63.6638% obtained by preprocessing using thirteen amino acid sliding windows and BLOSUM62 matrix input profile encoding. The kernel function used is the RBF kernel with parameters c = 32 and gamma = 0.0078125.
",,,126600,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
176098,,CERLANG GEMINTANG,AUTOMATIC CREDIT CARD FRAUD DETECTION SYSTEM USING DBSCAN OUTLIER DETECTION,,,"DBSCAN, K-Means, Clustering, Outlier Detection, Fraud Detection",Mardhani Riasetiawan,2,3,0,2019,1,"Penipuan kartu kredit dapat didefinisikan sebagai penggunaan akun kartu kredit orang lain yang menipu atau tidak sah dalam upaya mencuri uang, barang, atau layanan. Penipuan kartu kredit adalah masalah utama, terutama di negara-negara berkembang dan menyebabkan kerugian jutaan dolar setiap tahun. Sistem deteksi penipuan kartu kredit akan sangat membantu dalam memerangi ancaman ini dan mengurangi kerusakan yang ditimbulkannya.
Penelitian ini berfokus pada penggunaan DBSCAN untuk melakukan deteksi outlier pada dataset untuk tujuan mendeteksi penipuan. Sistem ini diujicobakan dengan berbagai parameter dan transformasi data, dan kemudian dibandingkan dengan algoritma pengelompokan terkenal lainnya, K-means untuk menentukan efektivitas kinerjanya.
Hasil penelitian menunjukkan bahwa DBSCAN mampu lebih akurat menentukan transaksi penipuan daripada K-means. Selain itu, penelitian menunjukkan bahwa memanfaatkan transformasi PCA pada dataset terlebih dahulu meningkatkan akurasi clustering dan deteksi outlier.","Credit card fraud can be defined as deceptive or unauthorized use of another person's credit card account in an attempt to steal money, goods, or services. Credit card fraud is a major problem, especially in developing countries and causes millions of dollars in losses annually. A credit card fraud detection system would help greatly in combatting this threat and reducing the damage it causes.
This research focuses on the use of DBSCAN is order to perform outlier detection on a dataset for the purposes of detecting fraud. The system was experimented on with various parameters in addition with data transformations, and then compared with another well-known clustering algorithm, K-means in order to determine the effectiveness of its performance.
The results of the research showed that DBSCAN was able to more accurately determine fraudulent transactions than K-means. In addition, the research showed that utilizing PCA transformation on the dataset first improves accuracy of the clustering and thus of outlier detection.",,,126935,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
174827,,Rizky Agung Dwi Putranto,PENINGKATAN AKURASI SIAMESE NETWORK UNTUK VERIFIKASI WAJAH DENGAN FACE ALIGNMENT,,,"Verifikasi wajah, convolutional neural network, siamese network, variasi pose, face alignment","Wahyono, S.Kom, Ph.D.",2,3,0,2019,1,"Verifikasi wajah merupakan topik yang populer pada bidang computer vision dan aktif diteliti saat ini. Verifikasi wajah adalah proses untuk menentukan apakah kedua citra wajah berasal dari orang yang sama atau tidak. Berbagai penelitian dilakukan untuk membangun sistem verifikasi wajah dengan akurasi yang tinggi sehingga reliable untuk diterapkan pada sistem keamanan dan autentifikasi secara otomatis. Terdapat berbagai kondisi pada citra yang dapat mempengaruhi tingkat akurasi dari verifikasi wajah secara signifikan. Salah satu kondisi tersebut adalah variasi pose. Variasi pose dapat menurunkan akurasi karena kondisi dan posisi wajah yang berbeda-beda pada masing-masing citra.

Convolutional Neural Network (CNN) memiliki keunggulan dibandingkan metode konvensional lainnya seperti SVM karena dapat melakukan ekstraksi fitur secara otomatis. Siamese network yang ditujukan untuk menentukan kesamaan kedua input sesuai dengan proses verifikasi wajah itu sendiri. Sehingga siamese network yang dibangun menggunakan CNN dapat digunakan untuk proses verifkasi wajah.

Dalam penelitian ini dilakukan proses face alignment untuk mengatasi variasi pose pada citra. Implementasi face alignment dilakukan menggunakan hard frontalization sedangkan implementasi verifikasi wajah dilakukan menggunakan CNN dengan arsitektur siamese. CNN digunakan untuk melakukan proses ekstraksi fitur citra wajah dan arsitektur siamese digunakan pada proses verifikasi wajah untuk menghitung jarak antara kedua fitur. Hasil dari penelitian ini diperoleh akurasi sebesar 50% ketika dilakukan tanpa proses alignment dan 68,9% ketika menggunakan alignment.","Face verification is one of the popular topic in computer vision. Many researches has done to build a verification system with good accuracy and thus can be implemented for autonomous security and authentification system. However, many number of variations in image can be hardly decreasing the accuracy of the system. One of such variation is pose variation. Pose variation affects the accuracy due to different angle dan position of the face in each images.

Convolutional Neural Network (CNN) has some advantages than other conventional methods such as SVM that CNN can be trained to do feature extraction task automatically. On the other hand, the siamese network which aims to differentiate two or more input image is suitable for verification task. So siamese network built from CNN's can be trained on face verification task.

In this research, face alignment is done to resolve pose variation problem. Face alingment is implemented using \textit{hard frontalization} while the face verification system is implemented using CNN with siamese architecture. CNN is used for feature extraction of the face image and siamese is used for measure the distance of both extracted features. The results of this research is 50.0% accuracy with no alignment and 68.9%  with alignment. ",,,125626,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
181739,,ANANDRATHA HARMEDAZIWA,ANALISIS SENTIMEN ENTITAS BERNAMA PADA KALIMAT BERITA ONLINE BERBAHASA INDONESIA MENGGUNAKAN METODE SUPPORT VECTOR MACHINE,,,"pengenalan entitas, analisis sentimen, entity recognition, sentiment analysis, SVM, ClassifierBasedTagger","Suprapto, Drs., M.Kom., Dr.",2,3,0,2019,1,"Artikel berita online sudah menjadi pilihan masyarakat dalam membaca berita dikarenakan kemudahan akses yang ditawarkan. Hal ini berdampak pada jumlah artikel berita yang tersedia di internet menjadi semakin banyak. Di dalam sebuah artikel berita biasanya terdapat entitas bernama yang menjadi subjek berita, seperti nama orang, lokasi dan organisasi. Selain itu, di dalam artikel berita juga mengandung sentimen tertentu, diantaranya positif, netral dan negatif. Pengenalan entitas bernama dan analisis sentimen terhadap entitas bernama pada artikel berita online dapat menjadi acuan oleh masyarakat dalam memberikan penilaian terhadap seseorang, lokasi dan suatu organisasi.
Pada penelitian ini metode ClassifierBasedTagger diimplementasikan untuk melakukan pengenalan entitas bernama dan metode Support Vector Machine (SVM) diimplementasikan untuk melakukan analisis sentimen dari hasil pengenalan entitas bernama. Pengujian terhadap model klasifikasi menggunakan Confusion Matrix pada pengenalan entitas bernama menghasilkan nilai akurasi 88,4%. Pada analisis sentimen, pengujian model klasifikasi dengan pembobotan TF-IDF dengan parameter Unigram dan Bigram masing-masing menghasilkan nilai akurasi sebesar 76,4 % dan 79,2%.
","Online news articles have become the people's choice in reading news because of the ease of access. This has an impact on the number of news articles available on the internet. In a news article there is a named entity that is become the subject of the news, such as the name of the person, location and organization. In addition, news articles also contain certain sentiments, including positive, neutral and negative. Named entities recognition and sentiment analysis of named entities in online news articles can be a reference by the community in providing an assessment of a person, location and an organization.
In this study the ClassifierBasedTagger method is implemented to perform the named entity recognition and the Support Vector Machine (SVM) method is implemented to perform sentiment analysis of the results of the named entity recognition. Testing the classification model using the Confusion Matrix on the named entity recognition produces an accuracy value of 88.4%. In sentiment analysis, the classification model testing by TF-IDF weighting with the parameters of Unigram and Bigram each resulted in an accuracy value of 76.4% and 79.2%.
",,,132605,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
172781,,Wibisana Wiratama,Sistem Pendukung Keputusan Kesesuaian Varietas Padi Terhadap Lahan menggunakan Metode Analytical Hierarchy Process dan Profile Matching,,,"Analytical hierarchy process, Kesesuaian lahan, Profile matching, SPK, Varietas padi","Sigit Priyanta, S.Si., M.Kom., Dr,",2,3,0,2019,1,"Setiap tanaman mempunyai syarat tumbuh berbeda untuk dapat tumbuh dengan baik. Pada tanaman padi, masing-masing varietas memiliki syarat tumbuh yang berbeda-beda. Karakteristik lahan diperlukan sesuai dengan kebutuhan varietas padi agar dapat tumbuh dengan baik. Lima kriteria digunakan sebagai evaluasi kesesuaian yang terbagi ke dalam three kelompok utama, yaitu unsur lahan, iklim di sekitar lahan, dan fisik dari lahan.
Salah satu metode yang dapat digunakan sebagai penyelesaian masalah dengan banyak kriteria atau biasa disebut Multi-criteria Decision Making (MCDM) adalah gabungan metode Analytical Hierarchy Process (AHP) dengan Profile Matching. Metode AHP digunakan untuk membangun matriks perbandingan berpasangan. Matriks ini berisi hasil penghitungan bobot relatif antara kriteria satu dengan yang lainnya. Metode Profile Matching digunakan untuk mencari seberapa besar tingkat kecocokan lahan dengan kebutuhan tanaman. Matriks hasil dari AHP kemudian diolah untuk didapatkan bobot relatif yang akan digunakan di dalam proses Profile Matching. Penggabungan metode AHP dan Profile Matching bertujuan untuk meningkatkan akurasi dari analisis kesesuaian lahan. 
Dalam penelitian ini, telah dibuat sistem pendukung keputusan yang mampu menghasilkan informasi sesuai atau tidak sesuainya varietas padi terhadap lahan dalam bentuk tabel peringkat yang mudah dibaca. Metode gabungan antara metode AHP dengan Profile Matching telah diimplementasikan dengan baik ke dalam sistem. Dari 6 lahan yang diteliti, tidak didapatkan varietas padi yang dominan. Pengujian hasil sistem dilakukan dengan membandingkan sistem SAW-Profile Matching, dengan hasil akurasi 83%.
","Every plant has different requirements to grow well. For rice plants, each variety has different requirements. The characteristics of land are needed in accordance with the needs of plant in order to grow well. Five criteria were used as suitability evaluation which were divided into three main groups, soil, climate around the soil, and physical form of the soil. These criteria will be compared between the needs of varieties with the characteristics of land.
One of many method that can be used to solve problems with multiple criteria or commonly called Multi-criteria Decision Making (MCDM) is a combination of Analytical Hierarchy Process (AHP) and Profile Matching. AHP is used to build pairwise comparison matrix. This matrix contains the comparison results between one criteria to others, which are called relative weights. Profile Matching is used to calculate how much the suitability of the land is compared to the needs of the plant. Matrix, which are the result of AHP, then processed to find the relative weights that will be used by Profile Matching. Combination of AHP and Profile Matching aims to improve the accuracy of land suitability analysis.
In this research, a decision support system has been created that is able to produce information on the suitability of rice varieties to land in form of ranking table that easy to read. Combination of AHP and Profile Matching has been well implemented into the system. No dominant varieties were found in the 6 fields studied. Testing have been done by comparing the system to SAW-Profile Matching&acirc;€™s system, resulting in accuracy of 83%. 
",,,123847,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
171758,,KRISNA ATTAYENDRA,Komparasi RESTful API dan GraphQL Dalam Arsitektur Database (Studi Kasus: Mamikos),,,"RESTful API, GraphQL, perbandingan performa","Dr.Techn. Khabib Mustofa, S.Si., M.Kom.",2,3,0,2019,1,"Pemilihan model arsitektur dan sistem yang digunakan merupakan komponen dalam membangun sebuah web service. Terlebih pada perusahaan berjenis startup yang memiliki mobilitas tinggi dalam pengembangan produk dan fiturnya. Salah satu sistem yang umum digunakan dalam membangun sebuah web service adalah RESTful API, yang juga diterapkan oleh salah satu perusahaan startup bernama Mamikos. Mamikos adalah sebuah platform pemesanan kost yang memiliki lebih dari dua juta pengguna aktif dan tujuh puluh ribu properti didalam servisnya.
Dalam membangun sebuah web service, selain RESTful API terdapat GraphQL, query language yang dipublikasikan oleh Facebook. GraphQL dianggap mampu melakukan handling request yang menyesuaikan kebutuhan klien (Lee Byron, 2015). Dengan handling request yang baik, maka ada kemungkinan untuk dilakukan optimasi response time dan package size yang dihasilkan.
Oleh karena optimasi response time dan package size yang memungkinkan untuk dilakukan, Mamikos ingin melihat apakah GraphQL bisa digunakan untuk menduplikasi struktur data dari arsitektur database mereka yang menggunakan sistem RESTful API dan seberapa signifikan optimasi yang didapatkan. 
Metode yang digunakan adalah komparasi performa response time dan package size. Setelah melakukan implementasi staging untuk menduplikasi sistem yang sudah berjalan, hipotesis diambil berdasarkan uji statistik two-tailed t-Test dan melihat P-value yang dihasilkan. 
Berdasarkan hasil uji statistik, diketahui bahwa P-value dari data pengujian response time dan package size lebih rendah dari nilai alpha sama dengan 0.05 yang telah ditentukan. Kemudian bisa diambil kesimpulan bahwa hipotesis-0 bisa ditolak dan terdapat perbedaan performa response time dan package size yang signifikan antara RESTful API dan GraphQL.","The selection of architectural models and systems used are components in building web services. Especially for startups that have high mobility in developing products and features. One system that is commonly used in building web services is RESTful API, which was also implemented by one of the startup companies called Mamikos. Mamikos is a boarding booking platform that has more than two million active users and seventy thousand properties in its service.
In building a web service, in addition to the RESTful API there is GraphQL, a query language published by Facebook. GraphQL can be responsible for handling requests that match client needs (Lee Byron, 2015). With good handling requests, it is possible to optimize response time and the resulting package size.
Because of the possible optimization of response time and package size, Mamikos wants to see whether GraphQL can be used to duplicate data structures from their database architectures that use RESTful API systems and how significant the optimization is.
The method used is the comparison of the response time performance and package size. After implementing the staging implementation to duplicate the system that is already running, the hypothesis is taken based on the two-tailed t-Test statistical test and sees the P-value produced.
Based on the results of the statistical test, it is known that the P-value of the response time test data and the package size lower than the alpha value equals 0.05. Then it can be concluded that hypothesis-0 can be rejected and there is a significant difference in the performance of response time and package size between RESTful API and GraphQL.
",,,122960,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
177646,,KANIA KHAIRUNNISA,"IMPLEMENTASI ALGORITME AES, RSA, DAN SHA1 PADA STUDI KASUS PENGIRIMAN DOKUMEN PENAWARAN PADA SISTEM PENGADAAN SECARA ELEKTRONIK (E-PROCUREMENT)",,,"Sistem pengadaan, E-Procurement, Penandaan Digital","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2019,1,"Beberapa tahun yang lalu, pelaksanaan pengadaan barang dan jasa masih diselenggarakan secara manual. Pengadaan barang dan jasa secara manual memiliki banyak kekurangan, yaitu proses administrasi dan penyerahan dokumen penawaran melalui tatap muka, kerahasiaan peserta lelang kurang terjamin, tranparansi rendah, serta proses monitoring yang sulit. Untuk itu dibutuhkan suatu sistem pengadaan barang dan jasa (e-procurement) yang dapat diakses secara elektronik (berbasis internet) agar kerahasiaan peserta dapat terjaga.

Dalam penelitian ini akan diimplementasikan konsep-konsep kriptografi algoritme AES, RSA, dan SHA1 pada proses pengiriman dokumen penawaran dalam sistem e-procurement. Sistem ini memiliki tiga tahap proses, yaitu pembuatan kunci pengguna menggunakan algoritme RSA yang dilakukan di website e-procurement, pembangkitan digital signature yang dilakukan di aplikasi pengaman dokumen berbasis desktop, dan verifikasi digital signature dilakukan di website e-procurement. Data yang digunakan pada penelitian ini berupa dokumen-dokumen digital yang berasal dari microsoft word, microsoft excel, dan pdf yang dikompresi oleh sistem. 

Hasil dari penelitian ini menunjukkan bahwa sistem yang dibuat sudah mampu membangkitkan kunci pengguna, membangkitkan dan melakukan verifikasi digital signature. Layanan keamanan yang dimiliki oleh sistem ini ialah kerahasiaan, otentikasi, integritas, dan anti penyangkalan. Pengujian yang dilakukan terhadap layanan-layanan keamanan tersebut meliputi uji otentikasi, uji kerahasiaan, uji integritas serta pengujian terhadap anti penyangkalan. 
","Several years ago, the procurement of stuff and services was manually. Manually procuring stuff and services has many disadvantages, administration process and submission of bidding documents face to face, the confidentiality participants is less guaranteed, low transparency, and difficult monitoring process. For this reason, a system of procurement of stuff and services is needed (e-procurement) that can be accessed electronically (internet based) so the confidentiality participants can be guaranteed.
In this research, AES, RSA and SHA1 cryptographic algorithm concepts will be implemented in the process of sending bidding documents in the e-procurement system.This system has three process, making user keys using RSA algorithm in the e-procurement website, generating digital signature in desktop based document security application, and verifying digital signature in the e-procurement website. The data used in this research are digital documents from word, excel, and pdf compressed by system. 
The results of this research indicate that the system created capable to generate user keys, generate and verify digital signatures. The security services owned by this system are confidentiality, authentication, integrity, and anti-denial. Tests conducted on security services include authentication tests, confidentiality tests, integrity tests and non repudiation test.
",,,128462,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
172274,,DAMAR ADI PRABOWO,Deteksi Kesamaan Pertanyaan pada Forum Online menggunakan Convolutional Neural Network,,," Convolutional Neural Network, Siamese Neural Network, Quora online forum, Word Embeddings, GloVe","Guntur Budi Herwanto, S.Kom., M.Cs.",2,3,0,2019,1,"Forum online adalah platform untuk berkumpul, berbagi informasi dan berdiskusi antar pengguna mengenai suatu topik tertentu. Pengguna pada forum online dapat bertanya mengenai suatu topik, kemudian pengguna lain yang ahli mengenai topik yang ditanyakan menjawab pertanyaan tersebut. Akan tetapi, karena pengguna dapat menanyakan pertanyaan dengan cara yang berbeda - beda, pengguna terkadang menanyakan pertanyaan yang sebelumnya sudah pernah ditanyakan oleh pengguna lain. Oleh karena itu, diperlukan model untuk mendeteksi kesamaan pertanyaan pada forum online. 
Pada penelitian ini metode yang digunakan untuk mendeteksi kesamaan pertanyaan yaitu Convolutional Neural Network (CNN). Kalimat pertanyaan dijadikan vektor dengan menggunakan word embedding. Pretrained word embedding yang digunakan yaitu GloVe word vectors. Pasangan pertanyaan yang sudah menjadi word embeddings digunakan sebagai masukan CNN yang kemudian dibandingkan kesamaannya dengan Siamese Neural Networks. Optimasi model dilakukan dengan menggunakan Stochastic Gradient Descent. 
Model deteksi kesamaan pertanyaan yang dibuat menghasilkan akurasi sebesar 79%. Akurasi model CNN pada dataset yang digunakan terbukti lebih tinggi dari model yang dibuat dengan algoritma Jaccard Similarity dan Multilayer Perceptron. Penggunaan dimensi word embedding yang tinggi membutuhkan jumlah epoch yang lebih tinggi pada proses pelatihan.","Online forums are platforms for gathering, sharing information, and discussing between users on a particular topic. Users in online forums can ask questions about a topic, then other users who are experts on the topic of that question would answer the question. However, because users can ask questions in different ways, users sometimes ask questions that other users have previously asked. Therefore, a model is needed to detect the semantic similarity of questions in online forums. 
In this study, the method used to detect the semantic similarity of questions is Convolutional Neural Networks (CNN). The question sentence transformed into a vector using word embeddings. Pretrained word embeddings used is GloVe word vectors. The pair of questions that have transformed into word embeddings are used as input for the CNN then the similarities of the questions pair compared with Siamese Neural Networks. Model Optimization is done using Stochastic Gradient Descent. 
The semantic similarity detection model of questions resulted in an accuracy of 79%. The accuracy of the CNN model on the dataset proved to be higher than models made with the Jaccard Similarity and Multilayer Perceptron algorithms. The use of a higher word embedding dimensions requires a higher number of epochs in the training process.",,,123404,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
175351,,ARIE VARIAN AKBARI,IMPLEMENTASI SISTEM REKOMENDASI MENGGUNAKAN INDIVIDUAL BASED REGULARIZATION (STUDI KASUS : YELP DATASET),,,"Sistem Rekomendasi, Hubungan Sosial, Yelp Dataset","Guntur Budi Herwanto, S.Kom., M.Cs",2,3,0,2019,1,"Banyaknya data mengakibatkan user mengalami kesulitan dalam menentukan pilihannya. Untuk mengatasi masalah tersebut, maka diperlukan sistem rekomendasi yang dapat membantu user dalam menentukan pilihannya. Pada kehidupan sehari-hari kita sering bertanya kepada teman untuk menentukan pilihan, hal ini adalah sistem rekomendasi dalam kehidupan sehari hari. Penelitian ini mencoba untuk mengimplementasikan  sistem rekomendasi individual based regularization dengan menggunakan hubungan sosial sebagai regularization pada Yelp Dataset.
Penelitian dilakukan dengan menggunakan metode individual based regularization dari (Ma, dkk., 2011) dengan membandingkannya dengan metode low-matrix factorization dari (Koren, dkk., 2009) untuk mengetahui performanya. Hasil dari penelitian ini adalah besar parameter regularization pada individual based regularization paling optimal adalah &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&sup2; = 0,1. Hasil terbaik diperoleh pada learning rate sebesar &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&plusmn; = 0,005 pada low-rank matrix factorization dengan hasil RMSE rata-rata sebesar = 1,010565 dan learning rate sebesar &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&plusmn; = 0,001 pada Individual based regularization dengan hasil RMSE rata-rata sebesar = 1,001444.
","Lot of data causes users to experience difficulties in making their choices. To overcome this problem, a recommendation system is needed that can help users determine their choices. In daily life we often ask friends to make choices, this is a recommendation system in daily life. This research attempted to implement an individual based regularization recommendation system by using social relations as a regularization on Yelp Dataset.
The research was conducted using the individual based regularization method of (Ma, dkk., 2011) by comparing it with the low-matrix factorization method of (Koren, dkk., 2009) in order to find out the performance of the system recommendations. The results of this research are that the most optimal regularization parameters for individual based regularization &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&sup2; = 0.1. The best results were obtained at the learning rate &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&plusmn; = 0.005 on low-rank matrix factorization with the results of the average RMSE = 1.010565 and learning rate &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&plusmn; = 0.001 on the Individual based regulation with the result of average RMSE = 1.001444.
",,,126126,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158466,,RAMA JAKARIA,PENGGUNAAN ALGORITMA-ALGORITMA MACHINE LEARNING UNTUK KLASIFIKASI EKPRESI WAJAH,,,"Facial Expression Recognition, Preprocessing, SVM, Naive Bayes, KNN.","Dr. Drs. Suprapto, M.I.Kom.",2,3,0,2018,1,"Tidak seperti metode komunikasi verbal, ekspresi wajah didapatkan dari sifat alami manusia sejak lahir. Hal ini dibuktikan dengan kesamaan raut wajah bayi saat menggunakan ekspresi wajah yang sama walaupun lahir dengan ras dan budaya yang berbeda. Dengan algoritma machine learning, ekspresi manusia kemudian dapat diklasifikasikan dengan memodelkan wajah manusia menggunakan posisi dan bentuk fitur wajah. Berbagai penelitian tentang klasifikasi ekspresi wajah manusia telah dilakukan sebelumnya menggunakan dataset yang berbeda. Namun kebanyakan hanya yang menggunakan dataset yang tidak beragam. Sehingga model yang dibuat menjadi lebih bias kearah suatu ras ataupun budaya.
Penelitian ini mencoba mengimplementasikan algoritma-algoritma machine learning dengan dataset yang beragam untuk pengklasifikasian ekspresi wajah. Selain itu, berbagai preprocessing juga ditambahkan dengan keragaman bentuk wajah berbagai ras sebagai acuan. Pengklasifikasi yang digunakan untuk perbandingan adalah SVM, Naive Bayes dan KNN.
Pada penelitian ini, diperoleh hasil dengan SVM lebih unggul dari Naive Bayes dan KNN pada metric performa seperti akurasi, recall, presisi, dan f1. Jarak akurasi dan f1 antara 3 algoritma tersebut sebesar 5% dan 7%. Namun diantara 3 algoritma tersebut, Naive Bayes unggul pada waktu fitting dan scoring dengan perbedaan mencapai 95%. Selain itu, hasil preprosessing pada dataset berefek positif pada model di hampir semua metrik performa. Akurasi sebelum dan sesudah dilakukan preprocessing naik sebesar 2-75%. Sedangkan presisi dan f1 pada Naive Bayes naik hingga 445%.","Unlike verbal communication method, face expression acquired from the nature of human since he was born. It is can be proven by the shape similarity of the face between different babies while using the same expression even though the babies were born with different race and culture. By using machine learning algorithm, human expression can be classified by modelling human expression using location and shape of face feature. Many kinds of previous researches have been done with many different datasets. But, the most of the researches has not been done it yet using diverse dataset. As the result, the model that has been created has more bias to a certain race or culture.
This research attempt to implementing machine learning algorithms with diverse dataset to classify face expression. Furthermore, the preprocessing with the diversity of facial shape as the reference is also added to. The classifiers that are used for the comparison will be SVM, Naive Bayes, and KNN.
In this research, the result was obtained with SVM is better than Naive Bayes and KNN on many performance metrics like accuracy, recall, precision, and f1. The accuracy gap between those 3 algorithms is about 5-7%. But among those 3 algorithms, Naive Bayes is better on fitting and scoring time with the difference to 95%. Moreover, the preprocessing for the dataset affect the model positively almost on every performance metric. The accuracy of the model after the preprocessing raise about 2-75%. While precision and f1 of Naive Bayes raise up to 445%.",,,109903,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
161794,,AWANG BRILIAN BRANTAS,PENGARUH ALGORITME RANDOM NUMBER GENERATOR TERHADAP AKURASI SIMULASI,,,"PRNG, simulasi, simulation, RANDU, MRG32k5a, Mersenne Twister, teori antrean, queueing theory","Dr Suprapto, M.I.Kom",2,3,0,2018,1,"Dalam sebuah zaman yang masyarakatnya mengolah semakin banyak sumber daya, baik itu manusia, bahan baku, maupun informasi, rekayasa sistem amat diperlukan untuk menghadapi tantangan yang semakin rumit. Simulasi merupakan salah satu metode terbaik yang dapat dimanfaatkan oleh pengambil keputusan yang bertanggung jawab atas rekayasa dan pengoperasian sistem yang rumit. Model simulasi perlu, sejauh yang dimungkinkan secara praktis, bersesuaian dengan apa yang disimulasikan, Akan tetapi, hal itu tidak berarti setelah model tersebut dibangun peneliti kemudian bisa mengabaikan detail teknis, misalnya penggunaan algoritme pseudo-random number generator yang baik.
Tugas akhir ini akan membahas perbandingan akurasi tiga PRNG (RANDU, MRG32k5a, dan Mersenne Twister) dalam menjalankan sebuah model antrean. Parameter uji yang digunakan adalah kesesuaian rata-rata data cacah observasi, rata-rata waiting time, simpangan baku waiting time, serta rata-rata panjang antrean dengan nilai closed form teoretis keempat nilai kinerja tersebut untuk arrival rate, service rate, dan rentang waktu yang telah ditentukan. 
Dari semua parameter uji yang digunakan, didapati bahwa meskipun ketiga PRNG tersebut tidak menghasilkan nilai rata-rata yang secara signifikan menyimpang dari nilai teoretis, RANDU menghasilkan variansi yang secara mencolok berbeda dari kedua PRNG yang lain pada semua aspek kecuali cacah observasi.","In an era which society processes ever more resources, be it human, material, or information, system engineering is highly required to face ever more complicated challenges. Simulation is one of the best methods that decision makers respnsible for engineering and operating complicated systems can use. A simulation model needs, as far as practically possible, to correspond to what is being simulated. However, this does not mean that after the model is constructed the researcher then can ignore technical details, for example the usage of a good pseudo-random number generator algorithm.
This final project will discuss the comparison of the accuracy of three PRNGs (RANDU, MRG32k5a, and Mersenne Twister) in running a queueing model. The test parameters used are whether the averages of observation count, waiting time average, waiting time standard deviation, and queue length conforms to theoretical closed form values of the four performance measures for a given arrival rate, service rate, and time span.
From all test parameters used, it was found that although all three PRNGs does not produce values that significantly differs from theoretical values, RANDU produces variances that noticeably differs from the other two PRNGs in all aspects but observation count.",,,112754,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
164612,,MUAMAR HANAFI,Sistem Presensi Mahasiswa Berbasis Location Based Service Menggunakan Perangkat Android,,,"Android, Location Based Service, Global Positioning System, Geofence, Sistem Presensi","Medi, Drs., M.Kom.",2,3,0,2018,1,"Saat ini terdapat banyak metode yang digunakan dalam sebuah sistem presensi, namun untuk sistem presensi mahasiswa pada khususnya masih banyak yang menggunakan metode konvensional. Hal ini dirasa kurang efektif jika dilihat dari perkembangan teknologi pada masa ini, sehingga diperlukan sebuah metode alternatif untuk menggantikan metode tersebut. Sebuah metode yang menjadi sebuah pemikiran adalah dengan menggunakan perangkat android, dimana android pada saat ini menjadi sistem operasi yang paling populer dan mempunyai banyak pengguna.

Pada penelitian ini dilakukan pengembangan sebuah sistem presensi mahasiswa berbasis Location Based Service yang memanfaatkan teknologi Global Positioning System, sebuah fitur yang terdapat pada perangkat android. Implementasi proses presensi dilakukan dengan membuat perimeter virtual menggunakan metode geofencing yang dapat mendeteksi saat perangkat android mahasiswa memasuki area perkuliahan. Sistem ini memiliki dua antarmuka, yaitu aplikasi android yang digunakan oleh mahasiswa dan aplikasi web dimana admin dapat melakukan pengolahan data pada server.

Sistem ini diuji pada beberapa ruangan kuliah yang terdapat di Fakultas MIPA UGM. Pengujian dilakukan dengan melakukan tes akurasi pendeteksian perangkat saat pengguna berada di dalam maupun di luar ruangan, dengan menggunakan berbagai nilai radius pada perimeter virtual yang telah dibuat. Dari hasil pengujian yang telah dilakukan dapat dikatakan bahwa sistem ini dapat bekerja dengan cukup baik, meskipun masih dapat dikembangkan lebih jauh lagi.","Currently there are many methods used in a attendance system, but for student attendance system in particular there are still many who use conventional methods. This is considered less effective when viewed from the perspective of technological developments at this time, so it needs an alternative method to replace that method. A method that becomes a thought is to use android devices, android at this time is the most popular operating system and has many users.

In this research was developed a student attendance system based on Location Based Service that utilizes Global Positioning System technology, a feature in android device. Implementation of the attendance process is done by making a virtual perimeter using geofencing method that can detect when the student's android device enters the lecture area. This system has two interfaces, android applications used by students and web applications where the admin can perform data processing on the server.

This system has been tested in several lecture rooms located at the Faculty of Mathematics and Natural Sciences of UGM. Testing is done by performing device detection accuracy tests when the user is inside or outdoors, using various radius values on the virtual perimeter that have been created. From the results of tests that have been done can be said that this system can work pretty well, although it can still be developed further.",,,115714,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
164358,,REIFITA AYU PRASETIO,ANALYSIS OF NETWORK BANDWIDTH MANAGEMENT BASED ON HIERARCHICAL TOKEN BUCKET ALGORITHM IN SOFTWARE-DEFINED NETWORK,,,"Network bandwidth management, hierarchical token bucket, SDN, and QoS","Medi, Drs., M.Kom.",2,3,0,2018,1,"Pengalokasi bandwidth dalam memperoleh jaringan yang efisien dan berkinerja sangat baik adalah hal yang wajar dicari dari segala pihak. Kondisi yang mungkin di mana setiap pengguna mengalami kegagalan transmisi data, kehilangan paket, atau dikenal sebagai kemacetan jaringan, penerapan manajemen bandwidth jaringan berfungsi sebagai salah satu dari banyak pendekatan yang ada untuk menyelesaikan masalah tersebut. Meskipun dalam kenyataannya, keterbatasan terhadap keselarasan komponen perangkat keras dan perangkat lunak membawa beberapa masalah bagi berbagai pihak, dengan demikian, Software-Defined Networking (SDN) berfungsi sebagai salah satu solusi untuk mensimulasikan sejumlah skenario kehidupan nyata, sebelum diimplementasikan ke jaringan yang ada. Simulasi dilakukan sesuai dengan parameter yang menentukan Quality of Service (QoS) jaringan yang dikeluarkan oleh Telecommunication and Internet Protocol Harmonization Over Network (TIPHON).
Dalam penelitian ini, penulis mencoba untuk mengimplementasikan salah satu algoritma pembatas bandwidth yang ada dikenal sebagai algoritma Hierarchical Token Bucket (HTB). Proses implementasi dilakukan di lingkungan SDN, di mana fokusnya adalah pada kinerja aktivitas jaringan dengan implementasi algoritma yang akan menyampaikan hasil yang sesuai dengan standar QoS yang ditentukan. Penelitian ini menyimpulkan bahwa di bawah bandwidth yang telah ditentukan, 100Mbit sebagai rate dan 80 Mbit sebagai ceil dalam hierarki parent, 25 Mbit sebagai rate dan 20 Mbit sebagai ceil dalam hirarki leaf, kinerja ini menghasilkan 0% paket loss (sangat baik), &lt;0,15 ms dalam kinerja delay (sangat baik) dengan rata-rata 0,0506 ms di bawah traffic TCP dan 0,02236 ms di bawah traffic UDP, 0 - 0,075 s (bagus) dalam kinerja jitter dengan rata-rata 0,00003856 s di bawah traffic TCP dan 0,0000142 s, dan 100 % throughput dengan rata-rata 9062.4277289 Kbit / s di bawah traffic TCP dan 8969.6035262 Kbit / s di bawah traffic UDP (sangat baik).","The allocation of bandwidth capacity in regard to acquire and efficient and excellent-performing network is a constant demand throughout the years. The likely condition in which any users experience failure of data transmission, packet losses, or otherwise known as network congestion, the implementation of network bandwidth management serves as one of the many existing approaches to resolve such issues. Although in reality, limitations towards the harmonization of both hardware and software components convey numerous drawbacks for several parties, thus, Software-Defined Networking (SDN) serves as a solution for simulating a number of replicative real-life scenarios rather than implementing to an existing network. The simulation is done in accordance to the parameters that define a network's Quality of Service (QoS) issued by the Telecommunication and Internet Protocol Harmonization Over Network (TIPHON). 
In this research, the author attempts to implement one of the existing bandwidth limiting algorithm widely known as the Hierarchical Token Bucket (HTB) algorithm. The process of implementation is conducted in a SDN environment, where the focus is on whether or not performance of the network activity with the implementation of algorithm would suffice a satisfactory result based on the standard applied. The research concludes that under predetermined bandwidth of 100Mbit as rate and 80 Mbit as ceil in the parent hierarchy, 25 Mbit as the rate and 20 Mbit as the ceil in the leaf hierarchy, the performance is resulted in 0% packet loss (excellent), &lt; 0.15 ms in delay performance (excellent) with an average of 0.0506 ms under TCP traffic and 0.02236 ms under UDP traffic, 0.075 s (good) in jitter performance  with an average of 0.00003856 s under TCP traffic and 0.0000142 s, and 100% throughput with an average of 9062.4277289 Kbit/s under TCP traffic and 8969.6035262 Kbit/s (excellent).",,,115467,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132111,,FAJAR ULIN NUHA,IMAGE GENERATION USING DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORK,,,"generative model, adversarial training, deep learning, convolutional nets, dataset size","Afiahayati, S.Kom., M.Cs., Ph.D.",2,3,0,2018,1,"Dalam beberapa tahun terakhir, generative model yang menggunakan pendekatan jaringan syaraf tiruan telah menjadi bidang yang menarik dalam pembelajaran mesin, selain kegunaan dalam pembuatan konten di industry permainan, ia juga memiliki banyak aplikasi potensial seperti resolusi gambar super dan image to image translation. Namun, model generatif telah dikenal tidak stabil dan sangat sulit untuk dilatih, kemudian penelitian yang meneliti pengaruh pengurangan dataset untuk pelatihan tersebut juga belum pernah dilakukan sebelumnya.
Dalam penelitian ini, penulis mencoba merancang suatu metode yang menghasilkan arsitektur jaringan syaraf tiruan yang stabil untuk menghasilkan gambar wajah dan jembatan dengan menggunakan kombinasi CNN dan GAN, selain itu penulis juga mencoba bereksperimen dengan berbagai jumlah dataset untuk mendapatkan gambaran tentang bagaimana kecil jumlah dataset yang dibutuhkan agar GAN dapat bekerja.
Telah ditunjukkan bahwa model dari kedua dataset tersebut dapat menghasilkan gambar realistis yang layak dan pengurangan dataset pelatihan akan mempengaruhi pelatihan jika pengurangannya terlalu tinggi, namun ditemukan bahwa pengurangan sekitar lima puluh ribu dataset telah menunjukkan hasil yang lebih baik dari pelatihan menggunakan dataset utuh.","In recent years, generative model using neural network has become an interesting field in machine learning, besides of its potential benefit in creating content for game industry, it has also many potential applications like super- image resolutions and image to image translation. However, generative model has been known as very hard to train, secondly there have not been known study that investigate the effect of reducing dataset for such training.
In this research, the writer tries to design a method that produces a neural network architecture for generating images of face and bridge using combination of CNN and GAN, additionally the writer also tries to experiment with various amount of dataset to get the idea of how small is the amount of dataset required for a GAN to work.
It has been shown that the model can produce a decent realistic images from both dataset and the reduction of training dataset would affect the training if the reduction was too high, but it is found that the reduction to around fifty thousand dataset has shown a better result than a full amount dataset.",,,83113,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
160015,,MARIA GABRIEL OKTA YULIANA,DETEKSI EMOSI PADA TWEET UNTUK IDENTIFIKASI AWAL BUNUH DIRI DENGAN SUPPORT VECTOR MACHINE DAN MULTINOMIAL NAIVE BAYES,,,"klasifikasi emosi, klasifikasi bunuh diri, support vector machine, naive bayes","Faizah, S.Kom., M.Kom",2,3,0,2018,1,"Salah satu bentuk dari berkomunikasi adalah mengungkapkan perasaan atau emosi kepada orang lain. Banyak media yang membantu kita dalam berkomunikasi, salah satunya media sosial Twitter. Tweet sebagai representasi pemikiran pengguna dapat dimanfaatkan untuk menganalisis kondisi emosi serta tendensi pengguna terhadap perilaku bunuh diri.
Pada penelitian ini, telah dikumpulkan 7500 tweet dari pengguna yang sudah meninggal dengan penyebab bunuh diri dan kecelakaan. Proses klasifikasi emosi dan bunuh diri dilakukan menggunakan algoritma Support Vector Machine (SVM) dan Multinomial Naive Bayes (MNB). Term Frequency-Inverted Document Frequency (TF-IDF) dan Total Emotions Intensity (TEI) menjadi fitur pada klasifikasi emosi, sedangkan output dari klasifikasi emosi dan bigram menjadi fitur pada klasifikasi bunuh diri. 
Penambahan fitur TEI pada klasifikasi emosi dapat meningkatkan akurasi pada klasifikasi emosi dengan menggunakan algoritma SVM metode One-vs-One, dengan hasil 92,32%. Hal ini dikarenakan skor TEI mengindikasikan emosi apa yang paling representatif pada dokumen. Pada klasifikasi bunuh diri dihasilkan akurasi terbaik sebesar 55,05% dengan menggunakan algoritma SVM. Pelaku bunuh diri tidak selalu menunjukkan kecenderungan terhadap emosi tertentu, namun cenderung kondisi emosi yang sering berubah.","Communication is a human need. One of the ways is expressing emotion towards others. There are many media that helps us to communicate and twitter is on of them. A tweet is representing user's idea, emotion or opinion. These tweets can be used to analyze something about the user.
Support Vector Machine (SVM) and Multinomial Naive Bayes (MNB) algorithm will be used in this classification. The data obtained from dead user, caused by suicide and accident. There are 7500 tweets in total. Term frequency-Inverted Document Frequency (TF-IDF) and Total Emotion Intensity (TEI) will be the features for emotion classification, meanwhile bigram wil be the feature for suicide classification. 
TEI improve the result of emotion classification from 77,86% (Chirawichitchai, 2014) to 92,32% by using SVM One-vs-One method. TEI calculate intensity for each emotion, hence the highest score will indicate which emotion represent the tweet. The best result for suicide classification is 55,05% by using SVM algorithm. The data shows that  the suicidal user does not always show a tendency towards a particular emotion, but tends to a changing emotional state quickly.
",,,110972,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
156432,,FAHREZA LERIAN,USB FLASH DRIVE DATA ACQUISITION FOR COLLECTING EVIDENCE BY RECOVERING DELETED DATA FROM UNALLOCATED SPACE ACCORDING TO DIGITAL FORENSIC PROCEDURES,,,"Digital forensic, USB Flash Drive, Deleted Files, The Sleuth Kit.","Dr.techn. Ahmad Ashari, M.Kom",2,3,0,2018,1,"Penggunaan flash disk sebagai media transfer data sangat mendunia, namun penggunaan flash disk dapat disalahgunakan untuk kejahatan, seperti transaksi informasi pribadi yang sangat rahasia, transaksi pornografi dan lainnya. File dapat dihapus di dalam flash drive atau memformat perangkat dapat dilakukan untuk membuat penjahat tidak dapat dilacak, di mana file dapat berisi informasi penting mengenai penjahat. Menghapus file di dalam flash drive dan mengosongkan tempat sampah membuat file tidak dapat diakses untuk pengguna, pemulihan dilakukan untuk mengumpulkan data dari penyimpanan. Pengajuan untuk bukti elektronik diterima untuk memenjarakan para penjahat.
Penelitian ini mencakup beberapa proses penyelidikan: Akuisisi dengan membuat bit-by-bit disk image menggunakan perintah dd, verifikasi keaslian menggunakan algoritma MD5 dan menganalisis gambar dan pemulihan file menggunakan The Sleuth Kit. The Sleuth Kit (TSK) adalah alat sumber terbuka untuk menyelidiki gambar, TSK dapat digunakan untuk memulihkan file yang dihapus dan melakukan fungsi lain.
Hasil penelitian ini menginformasikan perolehan dan pemulihan file bukti dapat dilakukan dari gambarnya, selama file tidak ditimpa dan telah melalui proses format yang baik (zeroing perangkat). Penelitian ini juga menginformasikan hasil akuisisi dan pemulihan dalam beberapa keadaan dan berbagai jenis dan ukuran bukti. Dengan menggunakan implementasi penelitian ini, prosesnya dapat dilakukan dengan alat sederhana dan biaya lebih murah.","The use of flash disk as a data transfer media is very global, yet the use of flash disk can be misused for crimes, such as transactions of a highly confidential personal information, transactions of pornography and others. Files can be erased inside a flash drive or formatting a device could be performed in order to make the criminals untraceable, where the files may contain important information regarding the criminal. Deleting files inside flash drive and emptying the trash bin make files inaccessible for user, recovery is performed in order to gather data from storage. The submission for electronic evidence is accepted to imprison the criminals.
This research includes some processes of investigation: Acquisition by creating bit-by-bit disk image using dd command, authenticity verification using MD5 algorithm and analyzing image and files recovery using The Sleuth Kit. The Sleuth Kit (TSK) is an open source tools to investigate the image, the tools can be used to recover deleted file and perform other tasks.
The result of this research informs the acquisition and file recovery of evidence can be done from its image, as long as the files is not overwritten and been through a good formatting process (zeroing the device). The research also informs the result of acquisition and recovery under few circumstances and various type and size of evidence. By using this research implementation, the process can be done with a simple tools and less cost.
",,,107502,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
162064,,GHANI FARADHA,PERFORMANCE ANALYSIS BETWEEN HIVE AND CASSANDRA FROM TWITTER DATA WAREHOUSE,,,"Data Warehouse, Twitter, Apache Hive, Real-time","Suprapto, Drs., M.I.Kom., Dr.",2,3,0,2018,1,"Pertumbuhan pesat aktivitas jejaring sosial telah menyebabkan beberapa perusahaan mulai menggunakannya untuk mendukung pengambilan keputusan mereka secara real-time. Twitter adalah salah satu media sosial paling populer. Namun, sebagian besar alat yang dikembangkan dari penelitian berbasis Twitter masih spesifik dalam beberapa tugas saja. Database tradisional belum menyediakan fungsionalitas untuk melakukan analisis data ukuran besar secara real time. Oleh karena itu, kami memerlukan data warehouse yang mendukung penyimpanan, pemrosesan, dan konversi data secara real time untuk data besar.
Penelitian ini menggunakan Apache Hive, salah satu teknologi data warehouse untuk penyimpanan dan pengolahan data besar secara real time. Data yang akan digunakan adalah data Twitter. Penelitian ini kemudian membandingkannya dengan Cassandra.
Studi ini mengeksplorasi kemampuan untuk menyimpan dan query data yang terkandung dalam Apache Hive dan Cassandra. Hasil penelitian ini menunjukkan bahwa Cassandra memiliki kinerja query terbaik, dengan rata-rata 29.68653325 detik untuk Cassandra dan 3890.7075 detik untuk Hive. Dalam pengambilan data, Cassandra lebih unggul dari Hive secara keseluruhan dan hanya mengarah ke data kecil jika dibandingkan dengan Hive. Saat mengumpulkan data streaming, Hive mendapat total data 40946, sementara Cassandra berjumlah total 67365 data.","The rapid growth of social networking activity has led some companies to start using it to support their decision-making in real-time. Twitter is one of the most popular social media. However, most of the tools developed from Twitter-based research are still specific in some tasks only. The traditional database has not provided the functionality to perform large-size data analysis in real time. Therefore, we need a data warehouse that supports storage, processing, and data conversion in real time for large data.
This study uses Apache Hive, one of the data warehouse technology for storage and processing of large data in real time. The data to be used is Twitter data. This study then compares it to Cassandra.
This study explores the ability to store and query data contained in Apache Hive and Cassandra. The results of this study show that Cassandra has the best query performance in Hive, with mean 29.68653325 seconds for Cassandra and 3890.7075 seconds for Hive. In data retrieval, Cassandra is superior to Hive as a whole and only leads to small data when compared to Hive. When collecting streaming data, Hive got a total data of 40946, while Cassandra totaled 67365 data.",,,113077,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
162839,,NUKE KUS YUANITA,PERBANDINGAN ALGORITMA FP-GROWTH DAN APRIORI UNTUK MENENTUKAN ASSOCIATION RULES PADA DATA PENJUALAN  (STUDI KASUS : GRIYA BATIK LUAR BIASA SOLO),,,"association rule, Apriori, FP-Growth","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2018,1,"Kebutuhan masyarakat akan informasi dan pengolahan data yang akurat dan efisien semakin meningkat. Data transaksi yang biasanya hanya digunakan untuk melihat laporan penjualan masih menyimpan informasi lain yang berguna, salah satunya adalah pola belanja konsumen. Data mining hadir membantu dalam pencarian pola yang diinginkan dalam basis data. Association rule miningadalah teknik data mining yang digunakan untuk menemukan polakombinasi dari data transaksi dengan menggunakan algoritma yang ada seperti Apriori dan FP-Growth.
Pada penelitian ini dilakukan perbandingan kinerja algoritma Apriori dan FP-Growth untuk pencarian association rule. Perbandingan dilihat dari segi jumlah ruleyang dihasilkan dan lama waktu proses. Data yang digunakan untuk penelitian ini adalah data penjualan di Griya Batik Luar Biasa Solo.Penelitian dilakukan dengan melakukan sepuluh kali percobaan menggunakan nilai minimum support dan minimum confidence berbeda. Hasil percobaan dengan algoritma Apriori dan FP-Growth kemudian dibandingkan.
Dari hasil pengujian yang telah dilakukan, jumlah association rules yang dihasilkan dari dua algoritma berbeda. Jumlah association rules yang dihasilkanalgoritma FP-Growth enam kali lebih banyak dibandingkan hasil dari algoritma Apriori. Lama waktu proses algoritma FP-Growth lebih singkat yaitu hanya seperempat kali dari waktu algoritma Apriori, sehingga algoritma FP-Growth dinilai lebih efisien.
","The community needs for accurate and efficient data processing are increasing. Transaction data which is usually only used to view sales reports still holds other useful information, one of them is consumer spending patterns. Data mining is present to help in finding the pattern in the database. Association mining rules are data mining techniques that are used to find a combination pattern of transaction data using existing algorithms such as Apriori and FP-Growth.
This study was done comparison between Apriori algorithm and FP-Growth algorithm in finding association rules.Comparison is seen in terms of the number of rules generated and the length of process time. The data used for this research is sales data at Griya Batik Luar Biasa Solo.The study was done by doingten trials using different minimum support value and minimum confidence.The experimental results with Apriori and FP-Growth algorithms were then compared.
From the test results that have been done, the number of association rules from two algorithms are different. The number of association rules produced by FP-Growth algorithm is six times more than Apriori's.The duration of FP-Growth algorithm process is shorter, only a quarter time of Apriori's.Therefore,FP-Growth algorithm is more efficient than Apriori algorithm.
",,,113874,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159768,,MOHAMMAD IRFAN RADEN,METODE IMPROVED HOLT WINTERS PADA PRAKIRAAN KEJADIAN DEMAM BERDARAH DI KABUPATEN SLEMAN,,,"prakiraan, twitter, improved Holt Winters, demam berdarah, Holt Winters.","Agus Sihabuddin, S.Si., M.Kom., Dr",2,3,0,2018,1,"Penyakit berbahaya demam berdarah terjadi setiap tahun di wilayah Kabupaten Sleman. Untuk membantu kesiapan masyarakat dalam menghadapi jumlah penderita demam berdarah dibutuhkan suatu prakiraan. Penelitian ini bertujuan untuk membuat prakiraan jumlah kejadian demam berdarah di Kabupaten Sleman dengan menggunakan data jumlah kejadian demam berdarah dan jumlah tweet mengenai demam berdarah. Proses prakiraan dengan menggunakan metode Improve Holt Winters yang merupakan pengembangan dari metode Additive Holt Winters. Hasilnya, data jumlah kejadian demam berdarah memiliki korelasi dengan jumlah tweet mengenai demam berdarah dengan nilai korelasi 0,335 lebih besar dari nilai kritis 0,284 dengan tingkat signifikansi 95%. Prakiraan dengan menggunakan metode Improve Holt Winters pada data jumlah kejadian demam berdarah menghasilkan nilai MAPE sebesar 5,94%, MSE sebesar 715,63 dan RMSE sebesar 26,75 dengan nilai awal &Icirc;&plusmn;, &Icirc;&sup2;, &Icirc;&sup3; terbaik (0,95; 0,05; 0,01). Dan pada data jumlah tweet menghasilkan nilai MAPE sebesar 4,01%, MSE 5,28 dan RMSE sebesar 2,3 dengan nilai awal &Icirc;&plusmn;, &Icirc;&sup2;, &Icirc;&sup3; terbaik (0,9; 0,02; 0,01).","Danger of dengue fever occurs every year in the region of Sleman Regency. To assist readiness community in handling the number of DHF patients requires forecasts. This research aims to predict the number of dengue incidence in Sleman District by using data on the number of dengue fever and number of tweets about dengue fever. Forecasting process using Improve Holt Winters method which is the development of Additive Holt Winters method. As a result, data on the number of dengue fever cases has a correlation with the number of tweets about dengue fever with a correlation value of 0.335 greater than the critical value of 0.284 with a 95% significance level. Forecast by using Improve Holt Winters method on data of dengue fever number resulted MAPE value of 5.94%, MSE of 715.63 and RMSE of 26.75 with best initial value of &Icirc;&plusmn;, &Icirc;&sup2;, &Icirc;&sup3; (0.95, 0.05 ; 0.01). And on tweet data yields MAPE value of 4.01%, MSE 5,28 and RMSE 2,3 with best initial value &Icirc;&plusmn;, &Icirc;&sup2;, &Icirc;&sup3; (0,9; 0,02; 0,01)",,,110706,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159771,,GUNAWAN PROBO S,IMPLEMENTASI MARKDOWN FILE SEBAGAI PENGGANTI DATABASE PADA SISTEM BLOGGING,,,"blog, php, markdown file, alokasi sumber daya","Moch Edi Wibowo S.Kom., M.Kom., Ph.D.",2,3,0,2018,1,"Beberapa tahun terakhir, popularitas blog semakin tinggi. Dengan menggunakan blog, pengguna dapat mengekspresikan pemikiran mereka ke dalam tulisan, serta dapat bertukar pendapat dengan orang lain. Salah satu keunggulan blog ialah anonimitas, sehingga pengguna tidak akan takut diketahui identitasnya.
Faktor penunjang blog ialah internet, di mana internet mampu menjadi perantara penulis dan pembaca. Sehingga komunikasi dan pertukaran informasi dapat dilakukan dengan mudah. Di Indonesia internet sudah menjadi  hal yang penting bagi setiap individu, namun standar kecepatan internetnya masih tergolong lambat. Kecepatan internet Indonesia menduduki peringkat 77 di dunia.
Penelitian ini, bertujuan untuk membuat aplikasi blog sederhana berbasis php, yang dapat memenuhi kebutuhan pengguna seperti blog pada umumnya. Aplikasi ini dapat dijalankan dengan kebutuhan sumber daya dan bandwidth data yang relatif kecil. Perbedaan aplikasi ini dengan blog lain, ialah dengan implementasi markdown file sebagai pengganti database pada sistem blog.
Hasil dari penelitian ini adalah sebuah aplikasi blog yang ringan, sederhana, dan memiliki performa yang baik. Aplikasi tersebut memililki kebutuhan sumber daya yang relatif kecil dan mampu mengakomodir beban pengguna yang tinggi. ","In recent years, the popularity of blogs is getting higher. By using blogs, users can express their thoughts into writing, and can exchange opinions with others. One of the benefits of blogs is anonymity, so users will not be afraid of identity.
Factors that supporting the blog is the internet, where the internet is able to become the intermediary writer and reader. So communication and information exchange can be done easily. In Indonesia the internet has become an important thing for every individual, but the internet speed standard is still relatively slow. The internet speed in Indonesia ranked 77 in the world.
This research, aims to create a simple blog application based on php, which can meet the needs of user like blog in general. This application can be run with relatively small resources and data bandwidth requirements. The difference of this application with other blog is with the implementation of file markdown as a database replacement on the blog system.
The result of this research is a blog application that is light, simple, and has good performance. The application has relatively small resource requirements and able to accommodate a high user load.",,,110705,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157982,,MOHAMAD RIZKY IRFIANTO,PREDIKSI NILAI TUKAR MATA UANG USD/IDR MENGGUNAKAN TEKNIK PENGELOMPOKAN DAN FEEDFORWARD NEURAL NETWORK DENGAN OPTIMASI FLOWER POLLINATION ALGORITHM,,,"prediksi, nilai tukar USD/IDR, pengelompokan, regresi linear, feedforward neural network (FNN), flower pollination algorithm (FPA)","Faizal Makhrus, S.Kom., M.Sc., Ph.D",2,3,0,2018,1,"Pertukaran mata uang asing terjadi di seluruh dunia selama 24 jam setiap hari dengan jumlah transaksi yang besar. Pada April 2016 jumlah pertukaran mata uang mencapai 5,1 triliun dolar Amerika. Keuntungan yang bisa diperoleh dengan pengambilan keputusan yang tepat dapat didukung oleh prediksi yang tepat. Penelitian ini berfokus pada prediksi nilai tukar mata uang dengan bantuan pengelompokan untuk memperbaiki hasil prediksi. Prediksi dilakukan pada data nilai tukar mata uang USD/IDR pada tahun 2016 dengan data latih dari Januari 2001 sampai Desember 2015. Proses pengelompokan diawali dengan pembagian data menggunakan sliding window lalu dikelompokan berdasarkan range koefisien gradien dan trend harga. Model prediksi yang digunakan adalah Feedforward Neural Network (FNN) dengan satu lapisan tersembunyi. Flower Pollination Algorithm (FPA) akan digunakan sebagai algoritma optimasi bobot. Hasil k-fold cross validation terbaik diperoleh pengelompokan range koefisien gradien dan trend harga (gabungan) dengan optimasi FPA. Pada prediksi tahun 2016, metode pengelompokan trend dengan optimasi backpropagation mendapatkan RMSE paling rendah yaitu 51,526686 dengan Dstat paling tinggi sebesar 0,58.","Currency exchanges take place around the world 24 hours a day for massive transaction amount. In April 2016 the amount of currency exchange reached 5.1 trillion US Dollars. Advantages to be gained by proper decision making can be supported by precise prediction.
This research focuses on exchange rate prediction with the aid of clustering to improve prediction results. Prediction is performed on 2016 USD/IDR exchange rate data with training data from January 2001 to December 2015. Classification process begins with exchange rate data partitioning using sliding window, then the data will be clustered based on gradient coefficient and price trend. Prediction model that will be used is Feedforward Neural Network with one hidden layer. Flower Pollination Algorithm (FPA) will be used as weights optimization algorithm. K-fold cross validation result shows that clustering based on gradient coefficient and price change (combined) with FPA outperforms other methods. In the 2016 prediction, the trend-based clustering method with backpropagation optimization earns the lowest RMSE of 51.526686 with Dstat as high as 0.58.",,,108928,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157983,,GUNAWAN SETIADI,PERBANDINGAN METODE EVALUASI OTOMATIS AKURASI MESIN PENERJEMAH,,,"Bahasa, Bahasa Indonesia, Bahasa Inggris, Mesin Penerjemah, Evaluasi Otomatis, Language, Indonesian, English, Machine Translation, Automatic Evaluation","Sri Mulyana, M.Kom",2,3,0,2018,1,"Bahasa Inggris yang digunakan masyarakat secara global saat ini merupakan lingua franca, yaitu bahasa penyambung bagi dua pihak yang memiliki bahasa ibu berbeda. Sebagai bahasa ibu bagi lebih dari 360 juta orang dan bahasa asing bagi lebih dari 600 juta orang, Bahasa Inggris membutuhkan Mesin Penerjemah (MP) dengan kualitas tinggi dalam penerjemahan publikasi dari bahasa lain, termasuk di antaranya dari Bahasa Indonesia. Terdapat beberapa MP Bahasa Indonesia &acirc;€” Bahasa Inggris yang telah dikembangkan dan dapat diakses secara daring, beberapa di antaranya adalah Google Translate dan Tradukka. 
Dalam memastikan MP tersebut memiliki kualitas terjemahan yang tinggi, digunakan beberapa metode evaluasi secara otomatis oleh komputer. Tiga buah metode evaluasi otomatis yang digunakan pada penelitian ini adalah Match Accuracy (MAcc), Bilingual Evaluation Understudy (BLEU), dan Metric for Evaluation of Translation with Explicit Ordering (METEOR). Dari antara ketiga metode tersebut, dilakukan perbandingan untuk menentukan metode evaluasi otomatis terbaik dalam mengevaluasi MP. Perbandingan dilakukan menggunakan nilai Korelasi dan nilai Mean Absolut Persentase Error (MAPE) dengan didukung oleh data dari ahli bahasa.
Penelitian ini mengevaluasi hasil terjemahan kedua MP terhadap teks Referensi yang telah ditentukan sebelumnya. Lebih lanjut, nilai evaluasi yang didapatkan dari metode BLEU, MAcc, dan METEOR terhadap 150 kalimat dibandingkan menggunakan perhitungan Korelasi dan MAPE. Hasil yang didapatkan menunjukkan metode BLEU pada lingkup unigram dengan nilai mean MAPE 0.220563866 merupakan metode evaluasi otomatis terbaik dibandingkan dengan metode evaluasi otomatis MAcc dan METEOR. Hasil selanjutnya yang didapatkan menunjukkan MP Google dengan nilai MAPE lebih rendah lebih baik dibandingkan dengan MP Tradukka pada nilai mean BLEU yang sama.
","English used by the global community nowadays is a lingua franca, a connecting language for two parties with different mother tongues. As the mother tongue for more than 360 million people and foreign languages for over 600 million people, English requires a high quality Machine Translation (MT) in translating publications from other languages, including from Indonesian. There are several Indonesian &acirc;€” English MT that has been developed and could be accessed online, some of which are Google Translate and Tradukka.
In ensuring said MT has a high-quality translations, several methods of automatic evaluation are used by a computer. Three of the automatic evaluation methods used in this research are Match Accuracy (MAcc), Bilingual Evaluation Understudy (BLEU), and Metric for Evaluation of Translation with Explicit Ordering (METEOR). Among those three methods, a comparison is performed to determine the best automatic evaluation method in evaluating MT. Comparisons are performed using Correlation value and Mean Absolute Percentage Error (MAPE) value supported with data from the linguists.
This research evaluates the results of both MT&acirc;€™s translation against a predetermined reference text. Furthermore, the evaluation values obtained from BLEU, MAcc, and METEOR methods towards 150 sentences are compared using Correlation and MAPE calculations. The results gained indicated that BLEU method on unigram scope with MAPE Mean value of 0.220563866 is the best automatic evaluation method compared to the MAcc and METEOR automatic evaluation method. Next results obtained shows that Google MT with a lower MAPE value are better than Tradukka MT with the same value of BLEU Mean.
",,,108923,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
164899,,ABU BAKAR,Komparasi Quality of Service dari Jaringan Hotspot VPN yang Diimplementasikan pada Singleboard Raspberry-Pi dengan Single-Configuration VPN,,,"Raspberry-Pi, Hotspot, VPN, Wireshark, Wi-Fi, OpenVPN, QoS","Guntur Budi Herwanto, S.Kom., M.Cs",2,3,0,2018,1,"Virtual Private Network (VPN) adalah suatu koneksi antara satu jaringan dengan jaringan lainnya secara private melalui jaringan public (internet). Disebut private network karena jaringannya bersifat pribadi dan hanya pihak tertentu saja yang dapat mengaksesnya. Banyak perusahaan rela membayar pihak ketiga untuk menyewa VPN agar data yang mereka kirimkan aman sampai tujuan dan menjamin QoS (Quality of Service) agar paket yang dikirimkan diterima dengan baik. 
Pada tugas akhir ini, dilakukan analisa QoS pada jaringan Hotspot VPN yang diimpementasikan pada singeboard Raspberry-Pi, sebagai data pembanding yaitu penggunaan VPN pada komputer biasa (single-configuration VPN) yang keduanya berbasis OpenVPN. Sebelum melakukan pengujian, yang pertama dilakukan adalah mengkonfigurasi client VPN dengan Hotspot VPN Raspberry-Pi dan single-configuration VPN pada komputer yang dilakukan secara bergantian. Yang kedua adalah menjalankan video-streaming pada private network melalui jaringan Internet dengan menggunakan VPN. Yang ketiga adalah menganalisi hasil dari software monitoring (Wireshark) yang dipasang pada PC Server dan PC Client untuk menghitung delay, throughput, dan packet loss. Hasil analisis pada Hotspot VPN Raspberry-Pi dan single-configuration VPN dengan ISP Tekom Indihome yaitu tidak ada perbedaan delay, throughput, dan packet loss yang terlalu besar, ini dikarenakan pengiriman protokol Hotspot VPN Raspberry-Pi dan single-configuration VPN sama-sama menggunakan enkripsi OpenVPN dan melewati jaringan cloud internet, namun terdapat perbedaan dari segi fungisonalitas, Hotspot VPN Raspberry-Pi mampu menyediakan layanan VPN dengan konfigurasi yang lebih simple hanya menyambungkan beberapa perangkat dengan Hotspot VPN Raspberry-Pi maka perangkat masuk dalam VPN dibandingkan dengan single-configuration VPN, yang mengharuskan per-perangkat melakukan single-configuration VPN terlebih dahulu untuk masuk dalam VPN.","Virtual Private Network (VPN) is a connection between one network and another network privately through a public network (internet). Called a private network because the network is private and only certain parties can access it. Many companies are willing to pay third parties to rent a VPN so that the data they send is safe to the destination and guarantees QoS (Quality of Service) so that the package sent is received properly.
In this final project, QoS analysis is performed on a Hotspot VPN network implemented on a Raspberry-Pi singeboard, as a comparative data that is the use of VPN on ordinary computers (single-configuration VPN), both of which are based on OpenVPN. Before testing, the first thing to do is to configure the VPN client with Raspberry-Pi VPN Hotspot and single-configuration VPN on a computer that is done alternately. The second is running video-streaming on a private network through an Internet network using VPN. The third is analyzing the results of monitoring software (Wireshark) that is installed on the PC Server and PC Client to calculate delay, throughput, and packet loss. The results of the analysis on Raspberry-Pi VPN Hotspot and single-configuration VPN with ISP Tekom Indihome are that there is no difference in delay, throughput, and packet loss that is too large, this is due to the delivery of the Raspberry-Pi VPN Hotspot protocol and single-configuration VPN both using OpenVPN encryption and over the internet cloud network, but there are differences in terms of functionality, Raspberry-Pi VPN Hotspot is able to provide VPN services with a simpler configuration that only connects multiple devices with Raspberry-Pi VPN Hotspot then devices enter in VPN compared to single-configuration VPN , which requires the device to do a single-configuration VPN first to enter the VPN.",,,116052,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159528,,JESY .S. AMELIA,KLASIFIKASI JENIS KELAMIN MANUSIA PADA CITRA WAJAH UNTUK MENDUKUNG MAJALAH DINDING DIGITAL,,,"Support Vector Machine, Local Binary Pattern, Preprocessing, Pengenalan wajah secara online","Bambang Nurcahyo Prastowo, Drs., M.Sc.",2,3,0,2018,1,"Digital signage merupakan aplikasi konten digital yang digunakan untuk
menampilkan informasi atau pesan dalam bentuk tontonan visual yang menarik dan
atraktif. Pada era ini, digital signage dimanfaatkan sebagai media informasi
khususnya pusat perbelanjaan dalam bentuk periklanan. Periklanan yang dibuat
harus bersifat interaktif dan adaptif secara real time. Salah satu caranya dengan
mendeteksi jenis kelamin dari pengamat. Hasil pendeteksian ini digunakan untuk
menentukan konten yang akan ditampilkan.
Untuk membedakan jenis kelamin pengamat diperlukan pengolahan citra
wajah. Tahap awal yaitu preprocessing citra wajah laki-laki dan perempuan.
Preprocessing terdiri dari dua tahapan yaitu mengubah ukuran citra menjadi
grayscale dan ukuran seragam. Setelah itu, pendeteksian wajah menggunakan Haar
Cascade Classifier. Area wajah didapatkan, dilakukan diekstraksi menggunakan
Local Binary Pattern (LBP) dan didapatkan fitur unik dari citra. Kemudian citra
direduksi menggunakan PCA dan dinormalisasi. Hasil dari proses ini digunakan
untuk proses klasifikasi menggunakan Support Vector Machine (SVM)
dengankernel linear.
Tingkat akurasi tertinggi diperoleh ketika ekstraksi LBP radius 3 dengan
ukuran citra 100x100 piksel adalah 87.20% menggunakan 1000 citra wajah hasil
pengunduhan. Kemudian dilakukan pengujian pada dataset wajah lokal (hasil
pengenalan wajah secara online ) dan dataset diperoleh melalui pengunduhan
dengan masing-masing terdiri dari 360 citra (terdiri dari 180 citra perempuan dan
laki-laki). Perbandingan dilakukan pada kedua dataset tersebut. Tingkat akurasi
tertinggi diperoleh pada citra berukuran 100x100, radius 3 sebesar 87.5% pada
dataset wajah lokal. Sementara untuk dataset hasil pengunduhan menghasilkan
tingkat akurasi sebesar 85.0%.","Digital signage is a digital content application used to display information
or messages in the form of an attractive and interesting visual spectacle. In this era,
digital signage used as a medium of information, especially shopping centers in the
form of advertising. Created advertising must be interactive and adaptive in real
time. One way to detect the sex of the observer. The results of this detection used
to determine the content to be displayed.
To distinguish the sex of observer required processing of face image. The
initial stage is preprocessing the male and female facial image. Preprocessing
consists of two stages of resizing the image to grayscale and uniform size. After
that, face detection using Haar Cascade Classifier. The face area was obtained,
extracted using Local Binary Pattern (LBP) and obtained unique features of the
image. Then the image reduced using PCA and normalized. The result of this
process is used for classification process using Support Vector Machine (SVM)
with linear kernel.
The highest level of accuracy is obtained when the extraction of LBP radius
3 with the image size of 100x100 pixels is 87.20% using 1000 face image of the
download. Then tested the local facial dataset (face recognition results online) and
the dataset was obtained by download with each of 360 images (consisting of 180
images of women and men). Comparison is done on both datasets. The highest
accuracy level is obtained in 100x100 image, 3rd radius of 87.5% in the local face
dataset. As for the dataset results downloads to produce an accuracy of 85.0%.",,,110492,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158250,,DINA HANIYAH,Sistem Pendukung Keputusan menggunakan Metode AHP dan TOPSIS dalam Penentuan Saham Prioritas,,,"Sistem Pendukung Keputusan, AHP, TOPSIS, Saham, Decision Support System, Stock","Anifuddin Azis, S.Si., M.Kom.",2,3,0,2018,1,"Saham merupakan salah satu investasi yang populer baik bagi individu maupun industri. Analisis pemilihan terhadap saham perusahaan diperlukan sebelum melakukan investasi sehingga saham yang dipilih mampu menghasilkan keuntungan yang lebih optimal. Analisis fundamental menjadi salah satu jenis analisis yang digunakan dalam pemilihan saham yang berdasarkan pada faktor-faktor fundamental perusahaan. Salah satu aspek dalam analisis fundamental adalah analisis rasio keuangan yang menggambarkan mengenai keadaan keuangan dan kemampuan suatu perusahaan. Dibutuhkan waktu yang cukup banyak dan tingkat ketelitian yang cukup tinggi untuk dapat mengetahui saham perusahaan terbaik diantara perusahaan lain yang dibandingkan dengan menggunakan rasio keuangan.
Pengambilan keputusan yang cepat dan tepat dibutuhkan dalam menyelesaikan permasalahan ini, sehingga pada penelitian ini dikembangkan sistem pendukung keputusan dengan menggunakan metode AHP (Analytical Hierarchy Process) dan TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) dalam menentukan saham prioritas. Metode AHP digunakan dalam menentukan bobot prioritas terhadap kriteria dan subkriteria, sedangkan metode TOPSIS digunakan untuk menentukan urutan alternatif saham prioritas.
Hasil dari penelitian ini menunjukkan bahwa sistem dapat memberikan urutan alternatif saham prioritas yang dapat digunakan sebagai salah satu acuan bagi pengambil keputusan dalam memilih saham prioritas dan sistem dapat menyesuaikan perubahan terhadap subkriteria dan alternatif saham yang digunakan oleh pengambil keputusan dalam proses perhitungan.","Stocks are one of the most popular investments for both individuals and industries. An analysis of the selection of a company's stock is required before investing, so the selected stock can yield a more optimal profit. Fundamental analysis is one of the types of analysis used in stock selection based on the company&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s fundamental factors. One aspect of the fundamental analysis is the analysis of financial ratios that describe the financial condition and ability of a company. It takes a lot of time and the level of accuracy is high enough to be able to know the best company stock among other companies compared to using financial ratios. Quick and appropriate decision-making is needed to solve this problem. In this research a decision support system developed by using AHP (Analytical Hierarchy Process) and TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) methodology to determine priority stock. The AHP method is used in determining the priority weighting against the criteria and subcriteria, whereas the TOPSIS method is used to determine the order of priority stock alternatives. The results of this study indicate that the system can provide an alternative sequence of priority stocks that can be used as a reference for decision-makers in choosing priority stock and the system can adjust changes to subcriteria and alternative stocks used by decision-makers in the calculation process",,,109501,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159018,,HANGGA BUWANA B W,APLIKASI ANDROID JADWAL KAJIAN AGAMA ISLAM BERBASIS LOKASI DI YOGYAKARTA,,,"Jadwal kajian Agama Islam, Agama Islam, Jadwal Kajian, Android, Web, LBS / Schedule Islamic studies, Islamic Studies, Study Schedule, Android, Web, LBS","Dr. Sigit Priyanta, S.Si., M.Kom.",2,3,0,2018,1,"Agama Islam merupakan agama yang berlandaskan dalil dari Al Qur'an dan hadits. Setiap individu muslim memiliki kewajiban untuk menuntut ilmu agama. Ada banyak penghalang bagi seorang muslim untuk menuntut ilmu agama, salah satunya malas dan tidak ada inisiatif untuk mencari jadwal kajian ilmu agama. Padahal lumayan banyak jadwal kajian agama yang sesuai dengan Al Qur'an dan hadist. Di samping dengan adanya teknologi LBS, maka penyampaian informasi kajian harusnya menjadi lebih mudah.

Tujuan dari skripsi ini berusaha membuat aplikasi yang mempermudah pengecekan jadwal kajian melalui Device Android. Sistem ini dibuat menggunakan pemograman web dan Android. Pada sisi web dipergunakan untuk mengelola data kajian dan pada sisi Android dipergunakan oleh pengguna untuk melihat jadwal kajian serta melihat lokasi kajian menggunakan fitur LBS. 

Sesuai hasil implementasi, dapat disimpulkan bahwa aplikasi telah mampu menampilkan lokasi kajian terdekat sesuai lokasi pengguna. Selain itu daftar kajian juga telah disaring sesuai hari dan tanggal saat itu. Lokasi kajian dapat ditampilkan pada peta Google Maps. Akan tetapi masih ada ruang untuk pengembangan aplikasi ini.
","Islam is a religion that is based on the Al Qur'an and Hadith. Each and every Muslim has a duty to study about Islam. There are some barriers for a Muslim to study about islam, one of that is lazy and no initiative to find a schedule in order to study their religion. Yet quite a lot of Islamic studies schedule in accordance with the Al Qur'an and hadith. In addition to the existence of LBS technology, the submission of assessment information should be easier.

The purpose of this minithesis seeks to make an application that makes it easy to check the schedule for the study through the Android Device. The system is built using web programming and java Android. On the web side is used to manage data assessment and on the Android used by users to view study schedule and the location using LBS features.

Based on result of implementation, can be concluded that the application has been able to display the location of the closest study according to user location. In addition the list of reviews has also been filtered according to the day and date at that time. The location of the review can be displayed on the Google Maps.
",,,109982,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
165931,,MUHAMMAD ASADUDDIN H,RECURRENT NEURAL NETWORK DAN EXTENDED KALMAN FILTER UNTUK PERAMALAN NILAI TUKAR MATA UANG,,,"Peramalan, Nilai Tukar Mata Uang, Recurrent Neural Network, Extended Kalman Filter, Sliding Window","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2018,1,"Jaringan saraf tiruan (JST) digunakan untuk memprediksi nilai tukar mata uang. Jenis JST yang digunakan pada penelitian ini adalah Recurrent Neural Network (RNN) dimana RNN dinilai baik digunakan untuk data time series. Ini disebabkan karena RNN menggunakan luaran dari waktu sebelumnya untuk dijadikan masukan pada waktu saat ini. algoritme pembelajaran yang umum digunakan pada JST adalah Stochastic Gradient Descent (SGD). Salah satu kelebihan dari SGD adalah waktu komputasional yang dibutuhkan relatif singkat. Namun SGD juga memliki kelemahan, antara lain SGD membutuhkan beberapa hyperparameter seperti parameter regularisasi. Selain itu SGD relatif membutuhkan epoch yang banyak untuk mencapai konvergen.
Pada penelitian ini diusulkan penggunaan Extended Kalman Filter (EKF) sebagai algoritme pembelajaran pada RNN menggantikan SGD. Beberapa penelitian mengungkapkan bahwa penggunaan JST dengan algoritme pembelajaran EKF memberikan tingkat akurasi dan laju konvergensi yang lebih baik dibandingkan JST dengan algoritme pembelajaran SGD.
Penelitian ini menggunakan data nilai tukar IDR/USD dari 31 Agustus 2015 hingga 29 Agustus 2018 dengan 70% data sebagai data latih dan 30% data sebagai data uji. Untuk mengetahui performa dari model yang dibangun, akan digunakan MSE, RMSE, MAE, dan Dstat sebagai metrik evaluasi. Dari hasil penelitian, diperoleh arsitektur untuk peramalan nilai tukar mata uang adalah arsitektur 3-6-1 dengan memperoleh nilai rata - rata Dstat sebesar 65,42% dan nilai Dstat terbaik sebesar 73,06%, MSE 2425,64, RMSE 49,25, dan MAE 35,05. Penelitian juga menunjukkan RNN-EKF menghasilkan akurasi yang lebih baik dibandingkan dengan RNN-SGD.","Artificial Neural Network (ANN) is used to predict currency exchange rate. The type of ANN that is used in this research is Recurrent Neural Network (RNN) where RNN gives good result when used with time series data. This is because RNN uses the output from the previous step as the input for the current step. Commonly used learning algorithm for ANN is Stochastic Gradient Descent (SGD). The advantage of using SGD is it have relatively small computation time. Nevertheless, SGD also have some disadvantage such as the need of hyper parameter. SGD also need relatively many epoch to converge.
In this research, RNN will be implemented along with Extended Kalman Filter (EKF) as learning algorithm in place of SGD. Some result prove that use of EKF as learning algorithm gives better accuracy and convergence rate when compared to SGD.
This research IDR/USD daily exchange rate data from August 31st 2015 until August 29th 2018 with 70% data used as training data and 30% used as testing data. To know the performance from the model, MSE, RMSE, MAE, and Dstat are used as evaluation metrics. This research found that 3-6-1 architecture produces the best result for currency exchange rates forecasting which have average Dstat value equals to 65,42% with the best Dstat  73,06%, 2425,64 in MSE, 49,25 in RMSE, and 35,05 in MAE. This research also shows that RNN-EKF gives better accuracy compared to RNN-SGD.",,,117098,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
164910,,MUHAMMAD Z P W,Qur'an Link : Ekstraksi dan Visualisasi Sitasi Al-Qur'an pada Artikel Islam di Web,,,"information extraction, big data visualization, regular expression, named entity recognition, search engine, Al-Qur'an, ekstraksi informasi, ekstraksi sitasi, visualisasi big data, ekspresi reguler, mesin pencari","Edi Winarko, M.Sc., Ph.D",2,3,0,2018,1,"	Internet telah dijadikan media dakwah untuk pencarian informasi tentang Islam oleh ummat muslim. Mesin pencari yang ada saat ini belum mengakomodasi sumber ayat Al-Qur'an pada perangkingan pencarian. Ekstraksi sitasi Al-Qur'an pada artikel di web mungkin dapat mengakomodasi sumber Al-Qur'an pada sistem pemeringkatan mesin pencari. Penelitian ini mengusulkan sebuah kerangka kerja untuk ekstraksi sekaligus visualisasi sitasi Al-Qur'an dari artikel yang dipublikasikan di web. 

	Sistem ekstraksi yang diusulkan pada penelitian ini menggunakan aturan ekspresi reguler yang telah ditentukan untuk mengekstraksi sitasi. Selanjutnya algoritme koreksi ejaan diterapkan untuk pengenalan entitas nama surat. Pada penelitian ini juga dirancang mesin pencari untuk  membuat artikel islam dapat dicari. Penelitian ini menyediakan metode untuk mereduksi kompleksitas visualisasi data besar hasil ekstraksi dengan memvisualisasikan setiap artikel dari hasil pencarian bersama dengan sitasi Al-Qur'an hasil dari ekstraksi setiap dokumen.

	Hasil ekstraksi sitasi berupa graf dapat digunakan untuk mempengaruhi nilai relevansi dalam proses pemeringkatan hasil pencarian. Sistem mampu mengekstraksi 31.062 sitasi dari 9.022 artikel yang dikumpulkan dari empat website islam yang berbeda. Sistem ekstraksi menunjukkan hasil kinerja dengan Presisi 0,996, Recall 0,991, F-measure 0,993, dan Fallout 0,075. Visualisasi interaktif pada penelitian ini juga dapat digunakan untuk analisis dan eksplorasi pengetahuan Al-Qur'an pada penelitian selanjutnya.","	Internet has been used for searching of Islamic knowledge by moslem community. Currently, most search engine has not accommodated source of Al-Qur'an for scoring in their ranking system yet. Extraction of Qur'an citations from articles on the web may be able to acccommodate source of Al-Qur'an to relevance score in search engine's ranking system. This research proposes a framework to extract and visualize the citation of Al-Qur'an from articles published on the web.

	The extraction system proposed in this research uses the given rules of regular expression to extract the citation. After extraction, we use spelling correction algorithm in order to recognize named entity of surat's name of Al-Qur'an. A search engine has been designed in order to make the article searchable. We provide a method to reduce the complexity of big data visualization in extraction's results by visualizing each article from search engine's results together with citations of Al-Qur'an which extracted from individual document.

	Result of extraction represented as graph of article citing verses from Al-Qur'an can be used for formulating the relevance score in ranking process of search engine too. System is able to extract 31,062 citations from 9,022 articles collected using our crawler from four different islamic website. Performance of extraction in this research shows Precission 0.996, Recall 0.991, F-measure 0.993, and Fallout 0.075. Interactive visualization proposed in this research can also be used for analysis and exploration of Quranic knowledge in future study.",,,116007,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
162865,,OKTAVIAN GALANG PRAS,PERANCANGAN PURWARUPA SISTEM NOTIFIKASI CLAN WAR PADA GAME WORLD OF TANKS BERBASIS SMS GATEWAY,,,"Notifikasi, API, SMS Gateway, Permainan","Triyogatama Wahyu Widodo, S.Kom., M.Kom.",2,3,0,2018,1,"World of Tanks merupakan salah satu permainan massively multiplayer online buatan Wargaming dan cukup populer di Indonesia. Salah satu mode permainan yang paling digemari para pemain adalah clan war dimana pemain harus online sesuai jadwal yang ditentukan oleh server untuk dapat mengikuti mode permainan ini. Agar para pemain dapat mengetahui jadwal clan war, dibutuhkan suatu notifikasi kepada pemain. Untuk dapat mengakses data jadwal clan war langsung dari server, Wargaming menyediakan API yang dapat digunakan oleh para pengembang pihak ketiga.
Pada penelitian ini, dibangun sebuah purwarupa sistem notifikasi clan war dengan memanfaatkan SMS sebagai media notifikasinya. Sistem ini memiliki fitur registrasi service, aktivasi service, deaktivasi service, cek status service, serta fitur utama yaitu notifikasi clan war. .SMS dipilih sebagai media alternatif dari sistem notifikasi yang sudah disediakan oleh World of Tanks dengan alasan daya jangkaunya yang lebih luas. 
Dari hasil pengujiannya, sistem ini telah berjalan sesuai dengan rancangan. Semua kebutuhan fungsional dari sistem ini telah terpenuhi. Dari segi performa, sistem ini masih membutuhkan peningkatan seiring bertambahnya jumlah data yang harus diproses oleh sistem.
","World of Tanks is one of the most popular massively multiplayer online games in Indonesia, developed by Wargaming. One of the most favorite game mode is clan war, where players should be online according to the schedule to be able to participate in this game mode. To let players know when the clan war is, a notification is required. To be able to access clan war data directly from the server, Wargaming provides an API that can be used by third party developers.
In this research, a notification system prototype is created by using SMS as its media. This system has several services including registration service, activation service, deactivation service, status check service and the main service itself is the notification service. SMS is selected as an alternative media of the notification system already provided by the World of Tanks with its broader terms of power.
Based on the test result, this notification system has been implemented in accordance with the design itself. All of the functional requirements of the system have been met. In terms of performance, this system still needs some improvements along with the increasing amounth of data that must be processed by the system.
",,,113898,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167217,,SUCI NUR AINI,Analisis Sentimen Commit Logs di Github Menggunakan Metode Support Vector Machine dan Gaussian Naive Bayes,,,"Analisis Sentimen, GitHub, Support Vector Machine, Gaussian Naive Bayes","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2018,1,"GitHub merupakan salah satu layanan web untuk repositori pengembangan perangkat lunak. Pengembang dapat berbagi source code, mengetahui perubahan source code dan memberi komentar pada source code yang diubah. Pada GitHub terdapat beberapa perintah yang dapat dilakukan oleh pengembang, salah satunya adalah commit. Perintah commit digunakan untuk mencatat perubahan apa saja yang terjadi pada repositori lokal. Terkadang, pengembang mengekspresikan emosi mereka mengenai kesulitan dalam menangani suatu proyek maupun kepuasan suatu proyek pada message tersebut. Message tersebut dinamakan commit logs. Namun, commit logs tidak dapat memberikan kesimpulan sentimen mengenai emosi pengembang secara otomatis, maka diperlukan analisis sentimen. Proses analisis sentimen memerlukan algoritma klasifikasi yang memiliki performa tinggi untuk menghasilkan hasil klasifikasi yang akurat.
Pada penelitian ini metode Support Vector Machine dan Gaussian Naive Bayes diimplementasikan untuk membandingkan metode mana yang lebih baik dari kedua metode klasifikasi tersebut untuk kasus analisis sentimen commit logs di GitHub. Penelitian ini menggunakan data commit logs di GitHub dalam rentang tahun 2007-2013 dengan 70% data sebagai data latih dan 30% data sebagai data uji. Penerapan metode Support Vector Machine memiliki performa analisis sentimen commit logs di GitHub yang lebih baik dengan akurasi, presisi dan recall masing-masing sebesar 0,97 dibandingkan dengan metode Gaussian Naive Bayes dengan akurasi 0,93, recall 0,93 dan presisi 0,94. Waktu rata-rata yang dibutuhkan pada proses pelatihan menggunakan metode SVM sebesar 30,872 detik. Sedangkan, pada metode Gaussian Naive Bayes hanya dibutuhkan waktu rata-rata sebesar 0,045 detik.","GitHub is one of the web services for software development repositories. Developers can share the source code, know the changes in source code and make comments on the modified source code. On GitHub, there are several commands that can be done by the developer, one of which is commit. The commit command is used to record any changes that occur in the local repository. Sometimes, developers express their emotions about the difficulties in handling a project and the satisfaction of a project on the message. The message is called commit logs. However, commit logs cannot conclude developer sentiments automatically, so sentiment analysis is needed. The sentiment analysis process requires a classification algorithm that has high performance to produce accurate classification results. However, commit logs cannot provide conclusions about sentiments towards developers' emotions automatically, so sentiment analysis is needed. The sentiment analysis process requires a classification algorithm that has high performance to produce accurate classification results.
In this study, the Support Vector Machine and Gaussian Naive Bayes method are implemented to compare which methods are better between the two classification methods for the case sentiment analysis of logs on GitHub. This research use GitHub's commit logs data from 2007 to 2013 where 70% data as train data and 30% data as test data. SVM method has better performance for GitHub's commit logs sentiment analysis with accuracy, precision, and recall each get 0,97 points whereas Gaussian Naive Bayes method get 0,93 for accuracy, 0,93 for recall, and 0,94 for precision. The average time needed in the training process using SVM method is 30,872 seconds while Gaussian Naive bayes method need 0,045 seconds.",,,118410,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
154676,,FARIS LUTHFI DEWRAS,SENTIMENT ANALYSIS FROM TWITTER.COM USING MULTINOMIAL NAÃƒï¿½VE BAYES ALGORITHM FOR CRYPTOCURRENCY BITCOIN NEWS,,,"sentiment analysis, cryptocurrency, machine learning, multinomial na&Atilde;&macr;ve bayes","Agus Sihabuddin, M.Kom., Dr",2,3,0,2018,1,"Pada awal 2017, Bitcoin memasuki masa keemasannya sebagai salah satu instrumen investasi populer yang banyak dibahas kalangan investor dan pedagang. Saat ini adalah kesempatan bagi pemain dalam dunia kriptocurrency untuk mendapatkan keuntungan paling banyak. Dinamika harga bitcoin yang fluktuatif
membuat investor dan trader membutuhkan strategi khusus dalam proses penentuan transaksi pasar. Salah satu strategi yang bisa diimplementasikan adalah melalui
analisis berita terbaru dari media sosial twitter.com tentang bitcoin untuk kemudian bisa memprediksi probabilitas berita berdampak pada harga bitcoin. Strategi ini dapat dilakukan dengan menggunakan model analisis sentimen.

Dalam penelitian ini, penulis mencoba untuk menentukan sentimen berita menggunakan algoritma Machine Learning Multinomial Na&Atilde;&macr;ve Bayes. Penelitian ini menggunakan data berita dari twitter.com per tanggal 21 Agustus 2017 sampai 5 Oktober 2017 dengan 30% data pelatihan dan 70% sebagai data uji.

Total data yang didapat adalah 772 tweet, yang membuat kumpulan data pelatihan terdiri dari 233 data dan data uji terdiri dari 542 data. Karena penelitian ini menggunakan supervised machine learning, dataset pelatihan diberi label secara manual.

Sebagai hasil dari penelitian ini, model tersebut mendapatkan 346 sentimen positif tweets, 72 tweet sentimen netral, dan yang terakhir namun tidak kalah
pentingnya adalah 124 sentimen negatif. Sebanyak 542 tweet.","In early 2017, Bitcoin entered its golden age as one of the popular investment instruments that many discussed among investors and traders. This moment is an opportunity for players in the world of cryptocurrency to gain the most profit. The dynamics of bitcoin prices that are fluctuated make investors and traders need a special strategy in the process of determining market transactions. One of the strategies that can be implemented is through the analysis of the latest news from social media twitter.com about bitcoin to then be able to predict the probability of news impact on bitcoin price. This strategy can be done by using sentiment analysis.

In this research, the author tries to determine sentiment news using machine learning algorithm Multinomial Na&Atilde;&macr;ve Bayes. This study uses news data from twitter.com per date 21th of August 2017 until 5th of October 2017 with 30% of training data and 70% as test data.

The total of data obtained is 772 tweets, which make the training dataset consist of 230 data and test dataset of 542 data. Because this research using supervised machine learning, the training dataset are labelled manually.

As a result of this study, the model gain 346 positive sentiment tweets, 72 neutral sentiment tweets, and last but not least 124 negative sentiment tweets. Which the total of 542 tweets.",2018-02-15 00:00:00,53,105739,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158007,,DHAYU ANGGI SETYOKO,ANALISIS PERFORMANSI JARINGAN SENSOR NIRKABEL YANG HETEROGEN BERBASIS WIFI,,,"jaringan sensor nirkabel, perangkat heterogen, performa jaringan","Drs. Bambang Nurcahyo P, M.Sc.",2,3,0,2018,1,"Konsep Internet of Things sangat berkembang dan dianggap sebagai revolusi baru dunia komputer. Seiring dengan perkembangan IoT tersebut, konsep jaringan sensor nirkabel juga ikut berkembang pesat sebagai salah satu bagian dari penerapan IoT. Ini memunculkan persaingan diantara pengembang perangkat sensor sehingga makin banyak bermunculan produk perangkat sensor baru dari perusahaan-perusahaan yang mulai melirik sektor ini. Keanekaragaman perangkat sensor ini melahirkan sebuah pengembangan konsep pada jaringan sensor nikabel, yakni konsep jaringan sensor nirkabel yang heterogen atau penggunaan perangkat berbeda pada suatu rangkaian jaringan sensor. Para pengembang jaringan sensor mulai melakukan penelitian mengenai perbedaan hasil yang didapat dengan konsep tersebut selain karena penghematan biaya, salah satunya ialah performa jaringan dari jaringan sensor nirkabel yang heterogen itu sendiri. 

Untuk menunjang keberagaman perangkat tersebut dibutuhkan sistem komunikasi yang fleksibel. Pemilihan sistem komunikasi WiFi dianggap cukup tepat dikarenakan perangkat-perangkat yang digunakan atau perangkat masa kini biasanya sudah dilengkapi dengan modul WiFi. Pada penelitian ini digunakan tiga buah perangkat yang berbeda, yakni Raspberry Pi 3, NodeMCU, dan WiDo. Raspberry Pi dengan sistem operasi Raspbian dilengkapi dengan modul wireless LAN IEEE 802.11n 2.4GHz yang tertanam pada chip Broadcom BCM43438. NodeMCU dengan sistem operasi XTOS berbahasa Lua dilengkapi dengan modul WiFi ESP8266. Dan WiDo dengan mikrokontroller ATMEL ATmega32U4 berbahasa Arduino dilengkapi dengan modul wireless LAN IEEE 802.11b/g 2.4GHz yang tertanam pada chip WG1300. Rangkaian yang dibangun pada penelitian ini ialah sink &acirc;€“ source, dengan WLAN disediakan oleh sink node. 

Performa jaringan yang diukur pada penelitian ini ialah waktu pengiriman atau round trip times (RTT), availability, dan throughput, yang diukur pada sebuah ruangan lab dan diukur selama rentang waktu 30 menit atau minimum pengiriman paket sebanyak 1000 buah data paket hasil penginderaan sensor. Pada penelitian ini, didapatkan nilai round trip times rata-rata 10ms dan maksimum throughput sebesar 19.300bps. Serta ketersediaan jaringan didapatkan hingga maksimum traffic terdapat pada interval minimum 8ms dengan besar paket hasil sensor suhu LM35.","The concept of the Internet of Things is highly developed and considered as a new revolution in the information technology. Along with the development of the IoT, the concept of wireless sensor networks is also growing rapidly as one part of the application of IoT. This led a competition among developers of sensor devices so that more and more emerging new sensor device products from companies that began to glance at this sector. The diversity of these sensor devices bring to a conceptual development on a wireless sensor network, namely as heterogeneous wireless sensor networks or the use of different devices on a series of sensor networks. The developers of the sensor network began research on the difference in results obtained with the concept in addition to cost savings, one of which is the network performance of the heterogeneous wireless sensor network itself.

To support the diversity of these devices required a flexible communication system. The choice of WiFi communication system is considered as a quite appropriate because the devices used or the new devices are usually equipped with WiFi module. In this study used three different devices, there are Raspberry Pi 3, NodeMCU, and WiDo. Raspberry Pi with Raspbian operating system is equipped with 2.4GHz IEEE 802.11n wireless module embedded on Broadcom BCM43438 chip. NodeMCU with LUA's XTOS operating system comes with WiFi module ESP8266. And WiDo with ATMEL ATmega32U4 microcontroller equipped with wireless LAN IEEE 802.11b/g 2.4GHz module embedded on WG1300 chip. The set built on this research is sink - source, with WLAN provided by sink node.

Network performance measured in this study was delivery time or round trip times (RTT), availability, and throughput in a laboratory and measured over a time span of 30 minutes or minimum packet delivery of 1000 pieces of sensor data. In this study, the average round trip rate is 10ms and maximum throughput of 19.300bps. And the availability of the network is obtained up to the maximum traffic at a minimum interval of 8ms with a large package of LM35 temperature sensor results.",,,108931,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
161847,,DINA YUNIDHA ISMALIA,"DETEKSI OUTLIER DENGAN METODE MEDIAN RULE UNTUK PENEMPATAN PEGAWAI YANG MENGEFISIENKAN JARAK TEMPUH (Studi Kasus Penempatan Guru SMA, SMK dan MA di Kota Yogyakarta)",,,"data mining, outlier, deteksi outlier, reposisi guru, jarak,  Median Rule, Greedy, outlier detection, teacher replacement, distance","Nur Rokhman, S.Si., M.Kom., Dr.",2,3,0,2018,1,"Jarak rumah guru ke sekolah merupakan hal yang perlu diperhatikan dalam penentuan penempatan guru. Jarak rumah yang terlalu jauh akan berdampak pada kesehatan, konsentrasi kerja, kedisiplinan waktu, aktivitas untuk berinteraksi sosial dan tingkat stres pegawai. Pertukaran penempatan kerja antara guru satu dengan yang lainnya masih sangat memungkinkan jika memang itu lebih memberikan dampak positif. Pada beberapa kasus yang ada pada penerimaan calon guru dan penempatannya dapat menimbulkan ketidakmerataan persebaran guru sehingga terjadi kelebihan jumlah guru di sekolah tertentu dan kekurangan guru di sekolah yang lainnya.
Outlier merupakan observasi yang sangat berbeda dengan observasi yang lainnya sehingga menimbulkan kecurigaan bahwa observasi dihasilkan oleh mekanisme yang berbeda. Penelitian ini menguji deteksi outlier dengan menerapkan  metode Median Rule pada studi kasus penempatan guru SMA, SMK dan MA di Kota Yogyakarta. Data yang termasuk ke dalam outlier penelitian adalah data guru dengan jarak rumah jauh dari sekolah jika dibandingkan dengan data lainnya. Kemudian memanfaatkan algoritma Greedy untuk pertukaran pasangan guru untuk mendapatkan solusi jarak optimum.
Hasil dari penelitian ini adalah metode Median Rule berhasil melakukan deteksi outlier pada penempatan guru SMA, SMK dan MA di Kota Yogyakarta dan didapatkan 81 data outlier. Hasil tersebut dapat digunakan untuk menentukan guru mana saja yang disarankan untuk direposisi. Hasil algoritma pertukaran guru untuk skenario pertama adalah saran untuk reposisi guru berdasarkan peringkat kombinasi penjumlahan jarak guru ke sekolah dari pasangan pertukaran guru dan didapatkan jarak yang lebih efisien dengan penurunan jarak tempuh sebesar 4,16%.","Teacher's home distance to school is a matter of concern in determining teacher placement. Teacher's home distance that is too far from the school will have an impact on health, work concentration, time discipline, activities for social interaction and stress levels. The exchange of work placements between teacher is still very possible if it gives more positive impact. In some cases, there is an inequality in teacher distribution and placement resulting in an excess of teachers in a particular school and a lack of teachers in other schools.
Outliers are observations that significantly differed from the rest of observations, thus raising the suspicion that observations are produced by different mechanisms. This research test outlier detection by applying Median Rule method in case study of teacher placement of SMA, SMK and MA in Yogyakarta City. Data included in the research outliers is teacher data with home distance away from school when compared to other data. Greedy's algorithm was subsequently applied for teacher pair exchanges to get the optimum distance solution.
The result of this research is Median Rule method succeeded in detecting outlier at SMA, SMK and MA teacher placement in Yogyakarta City and obtained 81 data outliers. These results can be used to determine which teachers are suggested for repositioning. The results of the teacher exchange algorithm are suggestions for teacher repositioning based on teacher exchange partner ratings to streamline the home mileage to school and obtained more efficient distance with decreased mileage amounting to 4.16%.",,,112785,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158008,,DSK FEBY INKA PUTRI,Klasifikasi Pasangan Pertanyaan Menggunakan Long Short-Term Memory Recurrent Neural Network,,,"sentence classification, LSTM, RNN","Drs. Sri Mulyana, M.Kom",2,3,0,2018,1,"Dalam sebuah situs tanya jawab, permasalahan yang umumnya muncul adalah ketika salah satu atau beberapa pengguna lainnya mengajukan pertanyaan yang sama dalam diskusi yang berbeda. Hal ini akan menjadi sebuah permasalahan dalam efisiensi sebuah situs tanya jawab serta penggunaan ruang penyimpanan yang besar untuk informasi yang sama. Pertanyaan-pertanyaan dengan maksud yang sama akan tersebar di berbagai forum diskusi, padahal jika digabungkan akan lebih baik dan memaksimalkan jawaban oleh pengguna lain. 
Solusi awal dari permasalahan tersebut adalah dengan membangun sebuah sistem yang melakukan deteksi otomatis terhadap pasangan pertanyaan yang duplikat. Dimana pada penelitian ini, dilakukan pemodelan pada bahasa alami untuk menemukan kalimat yang duplikat satu sama lainnya dengan menggunakan Long Short-Term Memory Recurrent Neural Network. Data yang digunakan dalam penelitian ini adalah dataset yang dirilis oleh situs tanya jawab Quora, dimana data terdiri dari pertanyaan-pertanyaan yang telah dipasangkan satu sama lain untuk mengetahui apakah kedua pertanyaan tersebut duplikat atau tidak.
Dari hasil penelitian diperoleh arsitektur terbaik dalam pembelajaran pasangan pertanyaan Quora, dengan nilai akurasi yang dicapai sebesar 76,01%.","In a question and answer site, a common problem arises when one or more other users ask the same question in different discussions.. This will be an issue in the efficiency of a question and answer site and the use of large storage space for the same information. Questions with the same intentions will spread across different discussion forums, which would be better if they are combined in a one canonical page and it will maximize answers by other users.
The initial solution to the problem is to build a system that performs automatic detection of duplicate pair of questions. Where in this study, modeling on natural language is done to find sentences that duplicate one another by using Long Short-Term Memory Recurrent Neural Network. The data used in this study is the dataset released by the Quora question and answer site, where the data consists of questions that have been paired with each other to see whether the two questions are duplicate or not.
From the research results obtained the best architecture in learning Quora questions pair, with the value of accuracy achieved by 76,01%.",,,108920,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
164408,,ANANG PRASETYO,Klasifikasi EEG Epilepsi Menggunakan LSTM Fully Convolutional Network,,,"Epilepsi, EEG, Klasifikasi Runtun Waktu, Fully Convolutional Network, LSTM","Edi Winarko, M.Sc., Ph.D",2,3,0,2018,1,"	Epilepsi adalah salah satu jenis gangguan syaraf yang menyerang otak manusia dan menyebabkan beberapa reaksi pada tubuh pengidapnya. Berbagai deteksi otomatis dapat digunakan untuk mengenali indikasi penyakit epilepsi salah satunya dengan sinyal Electroencephalogram (EEG). EEG adalah sinyal yang merekam aktivitas otak manusia dari waktu ke waktu dan tergolong sebagai suatu data runtun waktu.
Dalam deteksi otomatis penyakit epilepsi terdapat dua tahap utama yakni ekstraksi fitur dan klasifikasi. Dewasa kini penelitian mengenai klasifikasi data  khususnya dengan objek data runtun waktu telah banyak dilakukan dan mempunyai berbagai metode. Pada penelitian ini digunakan suatu arsitektur pengembangan dari jaringan saraf tiruan yaitu berupa jaringan LSTM-Fully Convolutional Network untuk melakukan klasifikasi data EEG epilepsi tersebut.
Performa arsitektur yang dihasilkan tersebut diukur seberapa baiknya dengan menggunakan ukuran akurasi dan fungsi biaya berupa loss. Lebih lanjut untuk mengetahui kemampuannya secara umum, performa yang dihasilkan oleh arsitektur LSTM-FCN dibandingkan dengan dua arsitektur penyusunnya sendiri, yakni LSTM dan FCN apabila bekerja secara terpisah sehingga terdapat 3 arsitektur yang dibandingkan.
Setelah dilakukan berbagai skenario pelatihan dan pengujian menggunakan skema 5-fold-cross-validation, hasil terbaik diperoleh dari arsitektur LSTM-FCN yang disusun dari 6 encoder FCN dengan kernel berukuran 3 dan 8 unit LSTM dengan regularisasi dropout 0,8. Arsitektur tersebut memiliki tingkat akurasi 0,89 dengan waktu tiap epoch pelatihannya selama 2,009 detik. Disisi lain setelah dilakukan pelatihan pada dua arsitektur LSTM dan FCN secara terpisah diperoleh kesimpulan bahwa arsitektur utama LSTM-FCN tersebut mengungguli dua arsitektur pembandingnya tersebut.","Epilepsy is one of neurological disorder that attacks the human brain and causes some reactions in the human body. Various ways to detect interference by using the Electroencephalogram signal (EEG). An EEG is a signal that detects human brain activity over time and is incorporated into time series data.
There are two main functions of feature extraction and classification in the automatic detection of epilepsy. Nowadays research on data classification especially timeseries has been done and held various methods. In this study, we proposed a method from development in artificial neural networks named LSTM-Fully Convolutional Network for classify the class of the EEG epilepsy data.
Performance of the resulted architectural are measured by using accuracy and a cost function called loss. Furthermore, in order to know its general capability, the performance of the generated by resulted LSTM-FCN architecture compared with its own LSTM and FCN architecture but work separately. So, there are 3 architectures to compare.
After a several of training and testing scenarios using the 5-fold-cross-validation scheme, the best results were obtained from the LSTM-FCN architecture composed of 6 FCN encoders with 3 kernels and 8 LSTM units with a dropout of 0.8. The architecture has an accuracy of 0,89 with time of each epoch learning for 2,009 seconds. On the other hand after training on two LSTM and FCN architectures separately it was concluded that the LSTM-FCN main architecture outperformed the two comparative architectures.",,,115538,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
163642,,MOHAMAD ANDIKA B,PERFORMANCE ANALYSIS OF LATENT SEMANTIC ANALYSIS ALGORITHM AND K-MEANS CLUSTERING ALGORITHM TO CATEGORIZE DATA RESULT OF IT AUDIT EVIDENCE,,,"Text Clustering, K-means Algorithm, Singular Value Decomposition, Matrix Decomposition.","Mardhani Riasetiawan, M.T., Dr ",2,3,0,2018,1,"Dalam suatu organisasi, proses evaluasi menjadi komponen penting yang mendukung jalannya operasi perusahaan. Evaluasi untuk sistem informasi ini dapat dilakukan dengan Audit TI. Pengawasan dalam sistem informasi perusahaan berguna untuk memantau penggunaan sistem informasi, menjaga integritas perusahaan, hingga keamanan data perusahaan. Namun, bukti dokumen yang dihasilkan oleh audit TI dapat menyebabkan manipulasi yang beragam. Dokumen yang tidak diatur dengan semestinya dapat mengganggu Auditor dan dapat menyebabkan Auditor bekerja tidak efisien. Untuk dapat mengatasi masalah ini diperlukan suatu metode untuk dapat mengelompokkan dokumen.
Terdapat 2 kumpulan data yang digunakan dalam penelitian ini yaitu 36 dokumen dalam bentuk .docx dan 106 dokumen dalam bentuk .docx dan .pdf. Dokumen tersebut akan melaui proses penghilangan kata umum, pemotongan kata dan ekstraksi fitur dari dokumen tersebut. Penelitian ini difokuskan untuk membandingkan hasil Algoritma K-Means Clustering untuk metode pengelompokan data menggunakan beberapa input matriks tereduksi dari dekomposisi matriks yang dihasilkan oleh Singular Value Decomposition. 
Dari hasil evaluasi, menunjukkan bahwa hasil Sum of Squared Error dan Silhouette tidak jauh berbeda dalam menentukan jumlah cluster terbaik. Jumlah kluster terbaik untuk 36 dokumen adalah 4 dan jumlah kluster terbaik untuk 106 dokumen adalah 3.
","In an organization, the evaluation process becomes an important component supporting the running of the company's operations. Evaluation for this information system can be done with IT Audit. Supervision in the information system of a company is useful for monitoring the use of information systems, maintaining corporate integrity, up to the security of corporate data. However, the documents produced by IT audit for evidence can cause diverse manipulation. Documents that are not arranged accordingly may interrupt the Auditor and can cause Auditor to work inefficiently. To be able to overcome these problems a method is needed to be able to group documents.
There are 2 data sets used in this research, which are 36 documents in the form of .docx and 106 documents in the form of .docx and .pdf. These documents are going through a process of common words remover, reducing inflected words and feature extraction of documents. This research focused on comparing the results of the K-Means Clustering Algorithm for data categorization method using several reduced matrix inputs from matrix decomposition generated by the Singular Value Decomposition.
From the evaluation results, shows that the results of the Sum of Squared Error and Silhouette are not much different in determining the best number of clusters. The best number of clusters for 36 documents is 4 and the best number of cluster for 106 documents is 3.
",,,114664,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157756,,A MUHAMMAD NURFAJRIN,KOMPARASI KINERJA JARINGAN LTE DI FAKULTAS MATEMATIKA DAN ILMU PENGETAHUAN ALAM UNIVERSITAS GADJAH MADA,,,"4G, LTE, performance comparation, throughput, latency, jitter, delay, RSRP,RSRQ","Dr.techn. Ahmad Ashari, M.Kom",2,3,0,2018,1,"Tingkat kepuasan seorang konsumen terhadap sebuah layanan telekomunikasi seperti teknologi LTE ini dipengaruhi oleh kecepatan koneksi internetnya. Kecepatan koneksi internet ini biasanya dipengaruhi oleh beberapa hal seperti latency. Walaupun maksimal data bandwidth sudah tetap, tetapi dalam kenyataanya data yang mengalir di dalam jaringan biasanya berbeda-beda setiap waktunya disebut throughput. Dengan terjadinya ini, maka pengalaman mengggunakan teknologi LTE dalam hal terhubung dengan internet akan terganggu dan terasa kurang maksimal.
Pada penelitian ini, dilakukan pengujian dengan cara pengambilan data dan analisis terhadap parameter kinerja jaringan LTE seperti latency, delay, jitter, throughput, serta kekuatan sinyal. Dari penelitian yang dilakukan di kampus FMIPA UGM ini, akan diperoleh pula perbandingan kinerja dari tiga operator seluler penyedia layanan LTE yakni Telkomsel, Tri, dan XL Axiata. 
Di mana hasil dari penelitian ini, dari dua kali pengambilan data, Telkomsel, Tri maupun XL Axiata secara bergantian menjadi operator dengan kinerja terbaik untuk masing-masing kinerja pada lokasi-lokasi pengambilan data.
","The level of satisfaction of a consumer to a telecommunication service such as LTE technology is influenced by the speed of internet connection. The speed of this internet connection is usually influenced by some things like latency. Although the maximum data bandwidth is fixed, the data flowing in the network usually varies each time (called throughput). So the experience of using LTE technology in terms of connecting with the internet will be disrupted and felt less than the maximum. 
In this research, tested by way of data retrieval and analysis of LTE network performance parameters such as latency, delay, jitter, throughput, and signal strength. From the research conducted at FMIPA UGM campus, there will be a comparison of performance from three LTE service providers namely Telkomsel, Tri, and XL Axiata. 
	The results of this study, from twice data retrievals, Telkomsel, Tri and XL Axiata alternately become the best performing operators for each of performance criteria.
",,,108726,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
155965,,NOURMA JENNIE AISYIYAH,PENGGUNAAN METODE K-MEANS DENGAN PROBABILITY SAMPLING DAN PARAMETER DENSITAS UNTUK CLUSTERING JAWABAN URAIAN,,,"Clustering Teks, Algoritme K-means, Clustering Jawaban Uraian","Mardhani Riasetiawan, M.T., Dr.",2,3,0,2018,1,"Evaluasi merupakan salah komponen penting yang digunakan untuk mengukur keberhasilan suatu proses pembelajaran. Salah satu cara yang dapat dilakukan dalam proses evaluasi adalah ujian tertulis menggunakan soal uraian. Soal uraian dianggap sebagai cara yang efektif karena melibatkan kemampuan siswa dalam pemecahan masalah dan penalaran. Akan tetapi, dengan banyaknya jawaban dari soal uraian, sulit bagi guru untuk mengetahui kemampuan dan dan variasi jawaban siswa secara keseluruhan. Clustering merupakan salah satu cara yang dapat digunakan untuk mengetahui variasi jawaban siswa secara keseluruhan. Dengan mengetahui variasi jawaban siswa, guru dapat mengetahui kemampuan siswa secara keseluruhan dan mengetahui kesalahan konsep yang mungkin terjadi sehingga dapat dilakukan evaluasi pada proses pembelajaran. 
Pada penelitian ini digunakan algoritme K-means dengan metode probability sampling dang perhitungan parameter densitas pada proses inisialisasi pusat klaster. Dari hasil pengujian dengan menggunakan pengukuran nilai SSE, metode inisialisasi dengan parameter densitas menghasilkan klaster yang lebih baik dibandingan dengan metode probability sampling. Namun dengan menggunakan pengukuran nilai silhouette, metode probability sampling menghasilkan klaster yang lebih baik dibandingkan metode parameter densitas. Dengan karakteristik dataset yaitu adanya kemiripan antar jawaban, metode parameter densitas lebih tepat digunakan karena mengutamakan kedekatan data dengan data-data lain dalam membentuk klaster.
","Evaluation is an important things for education quality measurement. One of the evaluation method is using descriptive or essay question. It is well known that there is greater for educational evaluation from essay question because from this question, teachers could find out the ability of students to solve a problem and to give a reason of their answer. But it&acirc;€™s hard for teachers to find out the insight from hundreds or thousands answers. One approach to addressing this is to automatically clustering the answers based on answer&acirc;€™s similiarity. From the cluster result, teachers could find out the insights of all answers. Also, teachers can find modes of understanding or misunderstanding concept so it can be used for teaching evaluation. 
This research focus on comparing probability sampling and density paramater for centroid initialization of K-means algorithm. Using SSE value measurement, clustering result of density parameter method is better than probability sampling method. But using silhouette value measurement, probability sampling method is better than density parameter method. Based on answer similarity in dataset, density parameter is an appropriate method because it consider closeness of data to others when choosing initial centroid.
",,,107079,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158018,,RYAN SURYOTOMO,ANALISIS SENTIMEN UNTUK MENGETAHUI ELEKTABILITAS TOKOH POLITIK MENGGUNAKAN METODE MULTINOMIAL NAIVE BAYES,,,"sentiment analysis, electability, multinomial naive bayes, chi square, TF-IDF","Faizah, S.Kom., M.Kom",2,3,0,2018,1,"Salah satu tolak ukur kandidat tokoh politik yang akan mengikuti pilgub, pilkada atau pemilu adalah elektabilitas. Sekarang ini, metode untuk mengukur elektabilitas tokoh politik masih dilakukan secara konvensional dan tidak objektif sehingga hasilnya kurang merepresentasikan tokoh politik tersebut.Sementara metode yang lebih modern dan objektif seperti analisis sentimen menggunakan data Twitter dan berita untuk mengukur elektabilitas masih sedikit dilakukan. Data Twitter dan berita dipilih karena dapat mempengaruhi opini publik tokoh politik. Pada penelitian ini dilakukan analisis sentimen menggunakan data tweet dan berita dari masing-masing tokoh politik untuk mengetahui elektabilitasnya menggunakan metode Multinomial Naive Bayes. Tokoh politik yang digunakan dalam penelitian adalah 10 tokoh politik yang dianggap populer di Indonesia.
Dataset yang digunakan berjumlah 16.523 data training dan 6.550 data testing. Data tweet didapatkan menggunakan tool tweetcatcher dan berita didapatkan dari 3 situs berita di Indonesia yaitu tribunnews.com, tempo.co, dan viva.co.id menggunakan tools scrapper dalam kurun waktu 17 November 2016 sampai 1 November 2017. Setelah data terkumpul, dilakukan tahap preprocessing dan filtering. Lalu
dilakukan seleksi top-n kata fitur menggunakan metode chi square dan TF-IDF. Selanjutnya adalah pembentukan model klasifikasi dan proses testing dengan membandingkan hasil elektabilitas tiap tokoh politik tanpa seleksi fitur dan dengan
seleksi fitur chi square dan TF-IDF. Hasil penelitian ini menunjukan bahwa nilai performa model menggunakan metode seleksi fitur chi square lebih tinggi dengan rata-rata nilai akurasi 85,24% , presisi 88,84% , recall 91,65% dan f-measure 90,17% dibandingkan dengan menggunakan metode seleksi fitur TF-IDF dengan rata-rata nilai akurasi 78,11% , presisi 87,41%, recall 87,79% dan f-measure 87,54% serta jika dibandingkan tanpa seleksi fitur dengan nilai rata-rata akurasi 74,69% , presisi 87,40%, recall 84,88% dan f-measure 84,72%.","One of politician candidates benchmark to join in election is electability. Recently, the method to measure politicians electability was done conventionally and not objectively, so the result were less representative to the politicians figure. Meanwhile, method that was more modern or objective like sentiment analysis to measure the electability was less used. Twitter and news data were chosen because it's could influence politicians public opinions. Sentiment analysis was performed in this research with tweet and news data for every politicans to measure the electability by using Multinomial Naive Bayes algorithm. The number of politicians used in this research were 10 politicians that
considered as popular politicians in Indonesia. The data set consists of 16.523 training data and 6.550 testing data. The tweet data were collected by using tweetcatcher tool and the news data were collected from 3 news site : tribunnews.com, tempo.co, and viva.co.id by suing scrapper tool in the period of 17th November 2016 until 1st November 2017. Once collected, processing and filtering phase were performed. Then, top-n word features were performed by using chi square and TF-IDF algorithm. The next phase was forming classification models and testing process that compared electabilities result of each politicians with chi square and TF-IDF feature selection or without feature selection. The result of this research showed that average performance of chi square features selection model was higher with 85,24% accuracy, 88,84% precision, 91,65% recall and 90,17% f-measure compared to TF-IDF features selection which had average value of 78,11% accuracy , 87,41% precision , 87,79% recall and 87,54
f-measure and without features selection model which had average value of 74,69% accuracy , 87,40% precision , 84,88% recall and 84,72% f-measure",,,108972,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
162374,,ARIF DINULSYAH PUTRA,Sentiment Analysis Using Ensemble Learning to Classify Airport Service Experience From an Air Traveler Perspective,,,"sentiment analysis, ensemble learning, airport services, random forests, extremely randomized tree","Afiahayati S.Kom, M.Cs., Ph.D",2,3,0,2018,1,"Saat ini bandara tidak hanya menjadi tempat bagi pesawat terbang untuk lepas landas dan mendarat namun fasilitas publik yang diharapkan untuk melakukan pengoperasian layanan yang memadai dan berkualitas tinggi bagi penumpang bandara. Sehingga diperlukan pengukuran persepsi penumpang terhadap layanan bandara untuk memberikan masukan yang berharga bagi bandara tersebut. 
Pada penelitian ini, penulis mencoba menerapkan pembelajaran ensemble menggunakan Random Forests and Extremely Randomized Tree untuk mengklasifikasikan pengalaman terhadap pelayanan bandara. Kemudian pada eksperimen diterapkan perbandingan hasil performa dari algoritma ensemble dan menganalisa evaluasi metrik yang dihasilkan dengan kombinasi dari beberapa parameter pada praproses, ekstraksi fitur, jumlah pohon dan algoritma pembelajaran.
Telah diperlihatkan bahwa Exteremely Randomized Tree menghasilkan model dengan performa yang sedikit lebih baik dibandingkan algoritma Random Forests. Lalu hasil evaluasi dari seluruh  eksperimen memperlihatkan bahwa dengan menggunakan kombinasi dari parameter seperti nostopword, Tf-idf , n_tree dengan jumlah 250 dan Exteremely Randomized Tree telah menghasilkan hasil terbaik dibandingkan kombinasi parameter yang lain dan hasil evaluasi pada precision, recall dan accuracy berturut-turut adalah 91%, 88%, 89%.
","Nowadays airports are not only a place for airlines to take off and landing, whereas public facilities that are expected to operate a sufficient service organization providing efficient and high-quality services for airport passengers. So that measuring the passenger perception of airport services provides a valuable feedback to an airport.  
In this research, the author tries to apply ensemble learning model using Random Forests and Extremely Randomized Tree to classify airport services experience based on sentiment. Additionaly the experiment tries to compare performance results of these ensemble algorithm and  observing the evaluation metrics produced in combination of parameters using distinct preprocessing, feature extraction, number of trees and algorithm. 
It has been shown that Extremely Randomized Tree can produce a slightly better performance model compared to Random Forests algorithm. Afterward The evaluation results from all of  the experiment showed that using the combination of parameters like nostopword, Tf-idf , n_tree equals to 250 and Extremely randomized Tree algorithm have produced the best result among other combinations and the result in order of precision, recall, and accuracy respectively 91%, 88%, 89%.",,,113377,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132680,,PUTRI MAYANG SARI,Prediksi Fluktuasi Nilai Tukar Rupiah terhadap US Dollar Berbasis Analisis Sentimen Berita Finansial,,,"klasifikasi, analisis sentimen, prediksi, fluktuasi, SVM, MNB, KNN / classification, sentiment analysis, prediction, fluctuation, SVM, MNB, KNN","Edi Winarko, Ph.D ",2,3,0,2018,1,"Berita merupakan salah satu kebutuhan wajib manusia yang selalu membutuhkan informasi. Dalam setiap berita terdapat ekspresi subyektif yang menggambarkan sentimen penulis terhadap isi berita, yang mana bisa jadi sentimen tersebut turut mempengaruhi pembaca secara kolektif. Analisis sentimen sangat berguna untuk mengetahui respon yang akan diberikan oleh pembaca terhadap suatu entitas, kejadian, atau sifat. Salah satunya dalam bidang finansial, antara lain untuk melakukan pemantauan terhadap dinamika bidang finansial pihak-pihak yang langsung berhubungan dengan fluktuasi nilai tukar Rupiah terhadap US Dollar. 

Pada penelitian ini dilakukan percobaan untuk melakukan prediksi fluktuasi nilai tukar mata uang Rupiah terhadap US Dollar setiap harinya dengan menganalisa sentimen berita finansial yang terbit setiap hari dari 1 Agustus hingga 31 Desember 2015. Algoritma yang digunakan untuk membangun model klasifikasi dan prediksi adalah algoritma Support Vector Machine (SVM), Multinomial Na&Atilde;&macr;ve Bayes (MNB), dan K-Nearest Neighbors (KNN). Untuk analisis sentimen digunakan fitur unigram dengan pembobotan TF-IDF. 

Dari pengujian prediksi fluktuasi nilai tukar Rupiah terhadap US Dollar berbasis analisis sentimen berita finansial didapatkan hasil dengan SVM memberikan akurasi 54.8%, dengan MNB 54.3%, dan KNN 54.6%. Akurasi yang kecil untuk prediksi fluktuasi nilai tukar Rupiah terhadap US Dollar ini dapat berarti bahwa sentimen berita finansial bisa jadi tidak terlalu mempengaruhi fluktuasi nilai tukar Rupiah terhadap US Dollar.

Dalam penelitian ini didapatkan kesimpulan bahwa untuk melakukan prediksi fluktuasi nilai tukar Rupiah terhadap US Dollar dari klasifikasi sentimen berita finansial, algoritma Support Vector Machine lebih unggul daripada K-Nearest Neighbors dan Multinomial Na&Atilde;&macr;ve Bayes.
","News is one of the most important part of human being whom always in need of information flow. In every single news, there is the subjective expression which is draw the sentiment of the writer towards the news itself, which could affect the reader collectively. Sentiment analysis is useful to take  out the responses given by the readers over an entity, story, or other thing. One of the many, occur in financial world, such as to discover the relation of the exchange rate fluctuation between Indonesian Rupiah to US Dollar.

In this research, system is tested to predict the fluctuation between Indonesian Rupiah and US Dollar in daily basis using sentiment analysis of financial news issued from August 1st until December 31st 2015. The method being used is Support Vector Machine (SVM), Multinomial Na&Atilde;&macr;ve Bayes (MNB), and K-Nearest Neighbors (KNN). To analyze the sentiment in news, the system uses unigram feature with TF-IDF weighting.

In testing step of exchange rate fluctuation prediction using sentiment of the news it is discovered that  SVM algorithm resulting in accuration of 54.8%, MNB 54.3%, and KNN 54.6%. Thus, the poor accuration is most likely telling that news sentiment is not always correlated in exchange rate fluctuation because news is not a prime parameter in financial world.

In this research, it can be summarize that prediction of  Rupiah and US Dollar exchange rate fluctuation using sentiment analysis, Support Vector Machine gives better accuration rather than K-Nearest Neighbors and Multiomial Na&Atilde;&macr;ve Bayes algorithm.
",,,104685,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158280,,ROZAN ROIF,MULTILEVEL EXPERT SYSTEM USING CERTAINTY FACTOR AND FORWARD BACKWARD CHAINING FOR QUALITY CONTROL OF PLANTATION EQUIPMENT,,,"Expert system, Certainty factor, Multi-level architecture, Plantation machinery","Aina Musdhalifah, M.Kom., Ph.D",2,3,0,2018,1,"Sistem Cerdas telah digunakan secara umum untuk menyelesaikan berbagai macam masalah termasuk diantaranya diagnosa penyakit, yang juga berarti dapet digunakan untuk keperluan industri, dengancara memperlakukan mesin sepagai pasien dan permasalahannya sebagai penyakit. Penelitian ini mendiskusikan perkembangan sistem ceday untuk meintenance peralatan berat, yaitu quality control untuk sebuah alat pabrik. Sistem ini akan dibagi menjadi beberapa level, dimana setiap level mempunyai data set yang berbeda untuk setiap knowledge base. Dan juga setiap level akan mempunya inference method yang berbeda untuk inference engine-nya, yaitu forward chaining dan backward chaining.Selanjutnya, metode managemen ketidakpastian yaitu certainty factor akan digunakan oleh sistem untuk memberikan keputusan akan kondisi dari sebuah unit mesin, karena banyak faktor tidak terduga di dalam mendiagnosa kondisi sebuah unit
        Penelitian ini juga mendiskusikan aplikasi dari multi-level architechture dan pengelompokan data. Multi-level architecture untuk sebuah sistem cerdas belum terlalu diketahui atau diapakai. Hanya beberapa peneliti yang menyentuh subjek ini, dan tidak banyak dari mereka telah mengaplikasikan konsep ini dalam pembuatan sistem cerdas. Multi level architecture juga memberikan kemampuan untuk memakai lebih dari satu inference method dalam satu sistem. Dengan memberikan kemampuan untuk mepunyai inference method yang berbeda, sistem tersebut akan diharapkan untuk meningkatkan efisiensi dalam data processing, maka memberikan hasil yang lebih reliable
        Sistem cerdas ini di coba dengan menggunakan data lapangan dari sebuah pabrik tertentu, laporan tersebut adalah kompilasi dari laporan selama 9 tahun. hasil dari experimen tersebut menunjukkan bahwa production rule based inference menghasilkan hasil yang mirip dengan naratif dari laporan asli, dan keputusan yang diambil cukup akurat. Tetapi, hasul dari keputusan berbasis dari CF masih terlalu samar-samar untuk dianggap menjadi hasil untuk perusahan pabrik untuk mengambil sebuah keputusan","An expert system has been widely used for solving many problems including disease diagnosis, which could also means it could be used for machinery or industries, with treating each machine as patients and their problems as diseases. This research discusses the development of expert system for maintenance of a heavy machinery, namely the quality control of a plantation equipment. The system is divided into several level, which each level having different data sets for the knowledge base, and also has a different inference method for the inference engine, namely the forward chaining and backward chaining. In addition, Uncertainty management method namely certainty factor is used by the system for giving out final verdict on the final condition of a machine unit, because the many uncertain factor in diagnosing the condition of unit
	This research also discusses the application of multi-level architecture and data clustering. Multi-level architecture for an expert system has not been widely known nor used. Only select researcher has been touching on this subject, and not many of them has applied this concept in creation of an expert system. The multi-level architecture also allows it to use more than one inference method in one system. By allowing the inference method to be different, it was hoped to increase the efficiency in data processing, thus produce a more reliable result. 
	The expert system is tested based on the compilation of field report on a specific plantation, the report is compiled from a time period of 9 years. The experiment result shows that the production rule based inference generate a similar result to the narrative form of the original report, and the decisions are accurate enough. However, the result of the verdict based on CF is still too vague to be deemed negligible to be a base for the plantation company to make any decisions.
",,,109179,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167240,,MUHAMMAD HAFIZ A A,PERBANDINGAN METODE NAIVE BAYES DENGAN SUPPORT VECTOR MACHINE DALAM KATEGORI SENTIMEN TERHADAP TOKOH POLITIK PADA TWITTER,,,"Klasifikasi kategori, Naive Bayes , Support Vector Machine, Tokoh politik","Medi, Drs., M.Kom.",2,3,0,2018,1,"Keberadaan Twitter telah digunakan secara luas oleh berbagai lapisan masyarakat dalam beberapa tahun terakhir. Kebiasaan masyarakat mem-posting tweet untuk menilai tokoh politik adalah salah satu media yang merepresentasikan tanggapan masyarakat terhadap tokoh politik. Menjelang pemilihan umum, biasanya ada pihak-pihak tertentu yang ingin mengetahui sentimen dan tanggapan terhadap tokoh politik. Tokoh politik yang dinilai adalah tokoh yang dianggap layak dan memiliki kemampuan untuk dipilih menjadi pemimpin. Oleh karena itu, penelitian ini mencoba menganalisis tweet berbahasa Indonesia yang membicarakan tentang tokoh politik. Analisis dilakukan dengan melakukan klasifikasi tweet yang berisi sentimen masyarakat tentang tokoh tertentu. 
Metode klasifikasi yang digunakan dalam penelitian ini adalah Naive Bayes Classifier. Naive Bayes Classifier dikombinasikan dengan fitur untuk dapat mendeteksi negasi dan pembobotan menggunakan term frequency serta TF-IDF. Klasifikasi tweet pada penelitian ini diperoleh berdasarkan kombinasi antara kelas sentiment.
Klasifikasi sentimen terdiri dari positif dan negatif sedangkan klasifikasi kategori terdiri dari kapabilitas, integritas, dan akseptabilitas. Hasil pengujian pada aplikasi yang dibangun dan pada tools Rapidminer memperlihatkan bahwa akurasi dengan metode term frequency dan TF-IDF terhadap metode Support Vector Machine menghasilkan akurasi performansi yang lebih baik daripada metode Naive Bayes baik dalam klasifikasi sentimen maupun dalam klasifikasi kategori. Namun demikian, secara keseluruhan penggunaan metode Support Vector Machine dan Naive Bayes sama-sama memiliki performansi yang cukup baik untuk melakukan klasifikasi tweet. 
","Twitter has been worldwide by many different types of people in recent years. The habit of people that posting for judge political figure is one way to representative community response to political figure. Toward general election, there are usually some party that want to know sentiment and response toward political figures. Political figures that being judge are the figure that considered worthy and have the ability to be chosen. This research tries to compare performance ability between two methods in category classification that contained tweets in Indonesian Language that talking about political figures. The comparation being done by classifying the tweet that contain people sentiments toward political figure.
There are two method that being used in this research, Naive Bayes and Support Vector Machine that combine with some features using term frequency and TF-IDF. Tweets Classification in this research obtained based on combination of sentiment classes
Sentiment Classification consists of positive and negative and category classification consists of capability, integrity, and acceptability. Test results on Rapidminer aplication showing that Support Vector Machine better than Naive Bayes. However, overall Naive Bayes and Support Vector Machine methods are having good performance in accord to tweet classification.
",,,118862,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158025,,RAHMAT ALBARIQI,Prediksi Perubahan Harga Bitcoin Menggunakan Jaringan Syaraf Tiruan,,,"cryptocurrency, jaringan syaraf tiruan, multilayer perceptron, recurrent neural networks","Edi Winarko, M.Sc., Ph.D",2,3,0,2018,1,"Dalam beberapa tahun terakhir, Bitcoin terus meningkat dan menjadi investasi populer bagi para pedagang finansial (trader). Tidak seperti saham atau valuta asing, Bitcoin sangat fluktuatif, terutama karena waktu perdagangan 24 jam sehari tanpa waktu tutup. Untuk meminimalisir risiko dan memaksimalkan keuntungan, trader dan investor membutuhkan cara untuk memprediksi tren harga Bitcoin secara akurat.

Penelitian ini mencoba menghasilkan model jaringan syaraf tiruan untuk memprediksi perubahan harga Bitcoin, apakah harga mendatang akan naik atau turun. Arsitektur yang digunakan adalah Multilayer Perceptron dan Recurrent Neural Networks. Data yang digunakan adalah data blockchain Bitcoin berjumlah 1300 data, yaitu dalam time-series dari Agustus 2010 hingga Oktober 2017 dengan periode 2 hari. Selain itu, model yang dihasilkan juga memprediksi untuk perubahan harga jangka pendek dan jangka panjang, dari 2 hari hingga 60 hari.

Hasilnya menunjukkan bahwa prediksi jangka panjang memiliki hasil yang lebih baik daripada prediksi jangka pendek, dengan akurasi terbaik untuk Multilayer Perceptron ketika memprediksi perubahan harga 60 hari berikutnya dan Recurrent Neural Network ketika memprediksi perubahan harga 56 hari berikutnya. Multilayer Perceptron mengungguli Recurrent Neural Networks dengan accuracy 81,3%, precision 81% dan recall 94,7%.","In recent years, Bitcoin is rising and become popular investment for traders. Unlike stocks or foreign exchange, Bitcoin is fluctuated, mainly because of its 24-hours a day trading time without close time. In order to minimize the risk involved and maximize capital gain, traders and investors need a way to predict the Bitcoin price trend accurately.

This research tries to generate neural networks model to predict the Bitcoin price change, whether the future price increase or decrease. Neural networks used are Multilayer Perceptron and Recurrent Neural Networks. Data used are Bitcoin&acirc;€™s blockchain from August 2010 until October 2017 with 2-days period and total amount of 1300 data. Additionally, the models generated are predicting both for short-term and long-term price change, from 2-days until 60-days. 

The result shows that long-term prediction have better result than short-term prediction, with the best accuracy in Multilayer Perceptron when predicting the next 60-days price change and Recurrent Neural Networks when predicting the next 56-days price change. Multilayer Perceptron outperform Recurrent Neural Networks with accuracy of 81.3%, precision 81% and recall 94.7%.",,,108956,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159305,,SINTA FITRIANINGRUM,ANALISIS SENTIMEN BERDASARKAN TOPIK MENGGUNAKAN LATENT DIRICHLET ALLOCATION (LDA) DAN NAIVE BAYES CLASSIFIER (NBC),,,"analisis sentimen, naive bayes, pemodelan topik, laten dirichlet allocation","Guntur Budi Herwanto, S.Kom, M.Cs",2,3,0,2018,1,"Perkembangan internet semakin pesat menyebabkan semakin meningkatnya pengguna media sosial. Salah satu media sosial yang populer adalah Twitter.  Sebanyak 40% pengguna media sosial Twitter menggunakan media sosial mereka untuk mengakses berita. Tidak hanya membaca berita namun mereka juga bisa menanggapi berita tersebut sesuai opini mereka masing-masing. Dari opini-opini tersebut dapat dicari topik utama atau tren yang sedang dibicarakan serta bagaimana pandangan masyarakat terhadap tren tersebut. Oleh karena itu, dibutuhkan analisis sentimen yang dapat menilai sentimen dari opini yang disampaikan masyarakat. Dibutuhkan juga pemodelan topik untuk melihat tren topik yang sedang dibicarakan.
Proses analisis sentimen menggunakan algoritam Naive Bayes, dan proses pemodelan topik akan maenggunakan Latent Dirichlet Allocation. Data yang digunakan adalah data tweet yang di-post dan me-mention akun berita @bbcbrk, @cnnbreaking, dan @nytimes. Data diambil sebanyak 5505 dan diambil mulai 2 April 2018 Hingga 8 April 2018.
Pengujian klasifikasi sentimen dilakukan dengan membandingka Multinomial Naive Bayes dan Bernouli Naive Bayes. Pengujian klasifikasi sentimen menggunakan 10-Fold Cross Validation dengan iterasi sebanyak 10 kali. Dari hasil penjuian klasifikasi sentimen didapatkan akurasi terbesar dihasilkan oleh algoritma Bernoulli Naive Bayes dengan rata-rata akurasi sebesar 85,29%. Pengujian pemodelan topik pada parameter passes dilakukan dengan menlihat tren kestabilan preplexity. Pengujian pemodelan topik pada parameter jumlah topik dilakukan dengan menlihat rata-rata preplexity terendah. Hasil pengujian didapatkan tren perplexity mulai stabil pada passes ke 10 dan rata-rata perplexity  terendah pada saat jumlah topik sama dengan 35.","As the development of the Internet increasingly rapidly led to the increasing social media users. One of the popular social media is Twitter.  40% of Twitter users use their social media to access news. Not only read the news but they can also respond to the news according to their respective opinions. From these opinions can be searched the main topics or trends being discussed and how people view the trend. Therefore, it takes a sentiment analysis that can assess the sentiments of the opinions conveyed by the community. It also takes topic modeling to see the trending topics being discussed.
The process of sentiment analysis uses the Naive Bayes algorithm, and the topic modeling will process using Latent Dirichlet Allocation. The data used are tweet data which is post and mention to news account @ bbcbrk, @cnnbreaking, and @nytimes. Data taken as many as 5505 and taken from 2 April 2018 Until 8 April 2018.
Sentimental classification testing is done by multiplying the Multinomial Naive Bayes and Bernouli Naive Bayes. Testing sentiment classification using 10-Fold Cross Validation with iteration 10 times. From the result of classification of sentiments obtained the greatest accuracy produced by Bernoulli Naive Bayes algorithm with average accuracy of 85.29%. Testing of topic modeling on parameter passes is done by looking at stability trends of preplexity. The topic modeling test on the topic number parameter is performed by looking at the lowest preplexity average. The test results showed a trend of perplexity starting to stabilize at the 10th passes and the lowest perplexity average when the number of topics equals 35. ",,,110287,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
161097,,EDGAR ANAROSSI,Klasifikasi Kerusakan DNA pada Comet Assay Menggunakan Convolutional Neural Network,,,"Deep learning, Convolutional Neural Network, klasifikasi, comet assay, transfer learning, support vector machine ","Afiahayati, S.Kom., M.Cs., Ph.D",2,3,0,2018,1,"	Pengembangan software dalam bidang medis telah banyak dilakukan guna mempermudah praktisi biologi untuk mempelajari informasi yang terdapat pada citra medis. Salah satu hasil citra medis yang diteliti adalah comet assay, yaitu hasil elektroforesis cairan tubuh yang didapat dari tubuh manusia. Telah terdapat beberapa tool yang dikembangkan untuk mengklasifikasi tingkat kerusakan DNA pada citra comet assay namun akurasi yang didapat pada tool tersebut masih kurang akurat untuk diguanakan.
	Deep learning adalah sebuah model jaringan syaraf tiruan yang sekarang mulai banyak dikembangkan karena kemajuan kecepatan komputasi. Salah satu varian deep learning, yaitu Convolutional Neural Network telah menunjukkan hasil yang baik dalam menyelesaikan permasalahan yang berkaitan dengan data citra.
	Pada penelitian ini, diimplementasikan beberapa model klasifikasi yaitu Convolutional Neural Network (CNN), SVM, dan model berbasis Transfer Learning untuk mengklasifikasi tingkat kerusakan DNA pada citra comet assay. Nilai akurasi yang didapat dari beberapa model tersebut kemudian dibandingkan dengan nilai akurasi yang didapat pada tool open source OpenComet untuk mencari model terbaik saat dilatih pada dataset dengan jumlah yang sangat kecil yaitu berjumlah 73 data. Nilai akurasi yang didapat pada penelitian ini adalah 63.5% pada model CNN, 62.3% pada model SVM, dan 70.5% pada model Transfer Learning.
","	Development of software for medical field has been done to ease biology practitioner at studying informations contained inside a medical image. One kind of medical image that has been studied is comet assay, the result of using electrophoresis at bodily fluids. There have been several tools developed to classify comet assay's DNA damage, but the results are still too inaccurate to be used.
	Deep learning is an artificial neural network model that is now starting to be developed following the improvement of computation speed. Convolutional Neural Network is a variant of deep learning that has shown good results at solving problems related to image.
	In this research, several classification models have been implemented including Convolutional Neural Network (CNN), Support Vector Machine, and Transfer Learning based model to classify comet assay's DNA damage. Accuracy produced by the machine learning based models trained at a small quantity dataset (73 data) will then be compared to the accuracy produced by the open source tool OpenComet. From the results of the conducted experiments, CNN produces an accuracy of 63.5%, SVM produces an accuracy of 62.3%, and Transfer Learning based model produces an accuracy of 70.5%.
",,,112051,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
164430,,RAFIF REYHANDHIA Y,Pengelompokkan Musik Berdasarkan Genre dan Mood Menggunakan Fuzzy C-Means,,,"Musik,Clustering,Inisialisasi,FCM,MPC,Purity","Guntur Budi Herwanto, S.Kom., M.Cs",2,3,0,2018,1,"Seiring dengan berkembangnya teknologi, industri musik digital juga ikut berkembang pesat. Dalam mendengarkan musik, pengguna cenderung mendengarkan musik berdasarkan suatu kemiripan misalnya genre atau mood. Berbagai pendekatan pun dilakukan untuk mengelompokkan musik yang memiliki kemiripan tersebut, contohnya clustering.
Pada penelitian ini, pengelompokkan musik dilakukan dengan metode clustering. Metode Fuzzy C-Means (FCM) dipilih untuk melakukan pengelompokkan fitur fitur musik yang didapatkan dari proses ekstraksi fitur. Untuk meningkatkan performa FCM, dilakukan modifikasi dalam proses inisialisasinya. Pengujian dilakukan dengan menghitung nilai Modified Partition Coefficient (MPC) dan Purity nya.
Hasil dari penelitian ini menunjukkan bahwa clustering menggunakan FCM dan fitur yang ditentukan menunjukkan hasil yang cukup baik untuk cluster mood yaitu nilai purity 0,8625 sedangkan hasil dari cluster genre cukup buruk dengan nilai purity hanya 0,2. Selain itu metode inisialisasi juga terbukti meningkatkan performa FCM dilihat dari segi waktu dan banyak iterasinya.","With the growing of technology, digital music is also going through a rapid growth. When listening to musics, user tends to listen to similar kind of music, for example music with similar genre or mood. Many approach have been done to group these musics with similiarities, one of them is clustering.
In this research, music is clustered so that groups of musics with high similiarities among its members while having low similiarities with the oher groups is obtained. Fuzzy C-Means Algorithm is chosen as the method to cluster the features that are extracted through feature extraction process. To improve the performance of the FCM algorithm, a modification was done on the initialization process. The Modified Partition Coefficient and Purity is calculated to evaluate the cluster result.
The result of this research shows that FCM with certain features can produce a good cluster based on mood with purity 0,8625, while the purity from genre based cluster shows poor result with only 0,2 purity. Also, in this research it is also proven that initialization can improve FCM performance looking from time and iteration constraint
",,,115541,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
168270,,MUHAMMAD ARIF RAHMAN,THE EFFECTS OF IMAGE PRE-PROCESSING ON GALAXY MORPHOLOGICAL CLASSIFICATION,,,"image processing, convolutional neural network, elliptical galaxy, spiral galaxy, irregular galaxy","Azhari, Drs., MT., Dr., Wahyono, S.Kom., Ph.D.",2,3,0,2018,1,"Astronom menggunakan istilah 'morfologi' untuk merujuk pada sifat struktural galaksi. Ini adalah alat fundamental dalam astronomi yang dapat digunakan untuk membantu memahami lebih jauh tentang sejarah alam semesta. Pembentukan galaksi melibatkan kombinasi efek yang kompleks, yaitu, pendinginan radioaktif, pembentukan bintang, penggabungan benda-benda langit asing, dan lain-lain. Dengan mengklasifikasikan galaksi ke dalam berbagai kategori, para ilmuwan dapat membangun pemahaman yang lebih dalam tentang mereka terbentuk dan berevolusi dan bahkan membuat perkiraan jumlah waktu yang telah berlalu sejak 'Big Bang' hingga saat ini.

Bentuk paling umum dari klasifikasi morfologi galaksi ditemukan oleh Edwin Hubble pada tahun 1926, yang disebut urutan Hubble. Skema rangkaian Hubble membagi galaksi menjadi tiga kelas luas berdasarkan tampilan visualnya. Ketiga kelas luas ini dapat diperluas ke dalam cabang-cabang dari berbagai jenis galaksi. Penelitian ini bertujuan untuk mengklasifikasikan gambar dari berbagai jenis galaksi menjadi tiga kategori umum: Elliptical, Spiral dan Irregular.

Jumlah total gambar yang digunakan dalam penelitian ini adalah 206 gambar galaksi elips, 320 gambar galaksi spiral dan 184 gambar galaksi tidak beraturan. Gambar-gambar ini diunduh dari pencarian Google menggunakan ekstensi browser yang mempercepat proses pengunduhan.
Sebelum gambar diklasifikasikan, mereka harus melalui tiga tahapan pemrosesan gambar yang berbeda untuk membuat gambar invarian sendiri dan untuk melihat apakah pemrosesan gambar akan membantu meningkatkan akurasi evaluasi dari penelitian ini.

Hasil pengujian menunjukkan bahwa gambar yang melalui pemrosesan gambar menunjukkan akurasi pengujian yang agak buruk dibandingkan dengan tidak menggunakan bentuk pemrosesan gambar. Akurasi pengujian rata rata yang diperoleh penelitian ini adalah 59%.
","Astronomers use the term 'morphology' to refer to the structural properties of galaxies. It is a fundamental tool in astronomy that can be used to help further understand the history of the universe. The formation of galaxy involves a complex combination of effects, namely, radioactive cooling, star formation, merging of foreign celestial bodies, and etc. By classifying galaxies into different categories, scientists can build a deeper understanding of how they form and evolve and even make an estimation of the amount of time that had passed since the 'Big Bang' until the present. The most commonly used form of galaxy morphology classification was invented by Edwin Hubble in 1926, called the Hubble sequence. Hubble's sequence scheme divides galaxies into three broad classes based on their visual appearance. These three broad classes can be further extended into branches of different types of galaxies. This research aims to classify images of different types of galaxies into three more general categories: Elliptical, Spiral and Irregular. 
	The total number of images that were used in this research were 206 images of elliptical galaxies, 320 images of spiral galaxies and 184 images of irregular galaxies. These images were downloaded from Google search using a browser extension that sped up the process. Before the images are classified, they had to go through three different image processing stages to create invariant images of themselves and to see if image processing would help improve the evaluation accuracy of this research. The test result showed that images that went through image processing showed a rather poor testing accuracy compared to not using any form of image processing. The overall testing accuracy that this research obtained was 59%.
",,,119511,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
161106,,ILHAMZI,GATED RECURRENT UNIT-RECURRENT NEURAL NETWORK UNTUK PERAMALAN DATA BANDWIDTH,,,"time series forecasting, gated recurrent unit, recurrent neural network","Isna Alfi Bustoni, ST., M.Eng.",2,3,0,2018,1,"Lalu lintas internet merupakan hal penting dalam sektor jaringan komputer,
dengan melakukan peramalan pada lalu lintas internet terutama pada data
bandwidth kita dapat mengetahui seberapa besar alokasi bandwidth yang
dibutuhkan sesuai dengan pemakaian. Maka dari itu dibutuhkan model peramalan
yang dapat memprediksi nilai bandwidth mendatang. Teknik peramalan dengan
menggunakan data masa lalu untuk memprediksi nilai yang akan datang. Jaringan
Syaraf tiruan (JST) adalah salah satu model yang dapat digunakan untuk melakukan
permalan. Gated Recurrent Unit merupakan perkembangan dari JST yang dapat
mengatasi masalah vanishing gradient yang terkadang muncul saat pelatihan.
Pada penelitian ini digunakan model Gated Recurrent Unit (GRU-RNN)
untuk meramalkan lalu lintas internet MIPA UGM. Model dilatih dan dievaluasi
dengan menggunakan data bandwidth dari 18 Mei 2016 hingga 2 Oktober 2016
dengan 70% sebagai data latih dan 30% sebagai data uji.
Dari hasil penelitian, diperoleh arsitektur GRU-RNN yang optimal untuk
peramalan data bandwidth MIPA UGM dengan jumlah masukan sliding window 7
dan 1 hidden layer dengan jumlah hidden unit sebanyak 9. Hasil peramalan
menghasilkan nilai unjuk kerja MAPE sebesar 36.74 %.","Internet traffic is important in the computer network sector, by doing
internet traffic forecasting, especially on data bandwidth we can determine the
amount of bandwidth required in accordance with usage. Therefore it takes a
forecasting model that can predict future bandwidth value. Forecasting techniques
using past data to predict future values. Artificial neural network (ANN) is one of
the models that can be used to do the work. Gated Recurrent Unit is the development
of ANN which can overcome the problem of gradient disappearance that arises
during training.
This research used Gated Recurrent Unit (GRU-RNN) model to predict
internet traffic at MIPA UGM. Model was trained and evaluated with bandwidth
data from May 18, 2016 to October 2, 2016 with 70% as training data and 30% as
test data.
The result of this research shows that GRU-RNN architecture with the
number of sliding window input 7 and 1 hidden layer with number of hidden units
9 gives the best result fr forecasting. The result of MAPE performance value is
36.74%.",,,112055,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159574,,RISNA DWI HAPSARI S,Comparative Analysis of Kernel Functions on Support Vector Machine for Opinion Mining on YouTube Video Comments,,,"kernel function, support vector machine, opinion mining, YouTube","Aina Musdholifah,S.Kom.,M.Kom.,Ph.D",2,3,0,2018,1,"Komponen terpenting dalam SVM adalah ketepatan dalam pemilihan fungsi kernel dan parameternya. Pemilihan fungsi kernel mempengaruhi akurasi yang dihasilkan. Penelitian ini membandingkan beberapa fungsi kernel SVM untuk mengetahui kernel apa yang terbaik untuk kasus opinion mining pada komentar YouTube. 
Fungsi kernel yang dibandingkan adalah kernel Linear, Polynomial, RBF, Sigmoid, Rational Quadratic, dan Inverse Multiquadratic. Parameter C yang digunakan adalah 1, 5, 10, 25, 50, dan 100. Berdasarkan percobaan dalam penelitian ini, kernel Linear menghasilkan akurasi yang paling tinggi dibandingkan 5 fungsi kernel lainnya dengan akurasi mencapai 81.90% untuk kategori SENTIMENT, 69.80% untuk kategori TYPE, dan 69.80% untuk kategori ALL.","The most critical component of SVM is the choice of an appropriate kernel and its optimal parameters. The selection of the kernel affects the resulting accuracy. This research is comparing several kernel functions of Support Vector Machine (SVM) to find which kernel function gives the best performance for opinion mining. 
The kernel functions that were compared are Linear kernel, Polynomial kernel, RBF kernel, Sigmoid kernel, Rational Quadratic kernel, and Inverse Multiquadratic kernel. The parameter C that were used in this research are 1, 5, 10, 25, 50, and 100. Based on the experiment, linear kernel works the best for opinion mining compared to RBF kernel, sigmoid kernel, inverse multiquadratic kernel, and rational quadratic kernel, with accuracy of 81.90% for SENTIMENT category, 69.80% for TYPE category, and 62.75% for ALL category",,,110588,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
154712,,NUR MUHAMMAD AL AKBARI,ANALISIS METODE NORMALIZED COMPRESSION DISTANCE (NCD) DALAM DETEKSI PERNYATAAN FAKTA,,,"Normalized Compression Distance, Hoax Detection, Berita Kesehatan","Azhari SN, Drs., M.T., Dr",2,3,0,2018,1,"Banyakya berita palsu yang beredar akan memberikan dampak negatif dalam masyarakat. Hal ini terlihat dari kecenderungan masyarakat yang sangat suka mendapatkan informasi khususnya secara online. Seiring dengan hal tersebut berkembang berbagai macam situs berita online yang sangat membantu dalam mendapatkan informasi secara aktual. Informasi yang ada pun tidak hanya berasal dari satu aspek saja, melainkan dari banyak aspek seperti politik, ekonomi, budaya, kesehatan dan masih banyak lagi lainnya. Semua aspek tersebut ada aspek yang jika telaah lebih lanjut, merupakan aspek penting dalam kehidupan, yaitu aspek kesehatan. Tentunya informasi dalam aspek kesehatan ini tidak boleh bersifat palsu atau tidak benar, melainkan harus bersifat fakta agar tidak berdampak buruk pada kehidupan sehari-hari.
Pada penelitian ini, akan diterapkan identifikasi dan pengklasifikasian berita kesehatan tersebut sehingga dapat diketahui mana yang bersifat fakta dan mana yang bersifat hoax. Metode yang dipakai adalah metode Normalized Compression Distance (NCD). Metode tersebut akan memproses file berita yang dijadikan sumber dan file berita input untuk dicari nilai similaritas nya. Proses kompresi kedua file tadi nantinya akan menghasilkan sebuah nilai antara 0-1 yang kemudian diolah selanjutnya agar dapat menggolongkan berita tersebut ke dalam berita fakta maupun hoax. Hasil dari data yang sudah diproses tadi akan dilakukan pengujian dengan perhitungan precision, recall dan accuracy untuk melihat apakah metode efektif jika diterapkan pada kasus ini.
Hasil penelitian ini menunjukan bahwa metode Normalized Compression Distance (NCD) kurang tepat jika digunakan untuk pendeteksian hoax dalam sebuah berita karena tidak memenuhi beberapa syarat uji secara sistematis dan logika. Hal ini dikarenakan metode Normalized Compression Distance (NCD) hanya mencari nilai similaritas dalam struktur file yang di proses dan tidak relevan jika digunakan untuk membandingkan file input yang masuk dan file sumber guna penentuan berita fakta ataupun hoax.","A lot of false news spreading in the community can cause negative effects. It is related to the community's nowadays trend of getting news from the internet. That is why there are a lot of news situs in Indonesia. This news situs covers wide varieties of news such as politics, economics, cultural, and medical-related news. Medical-related news can be crucial to one's life since it is referred a lot. That is why this kind of news can't happen to be false so there won't be any damage done to the readers.
 The Identification and classification of false news from medical-related field will be done in this research. Normalized Compression Distance (NCD) will be used for those purpose. This method will proceed the news input and look for the similarity that will be measured from 0-1 scale then it can be determined that the news whether or not is a false news. From the processed data, the precision, recall dan accuracy also being determined to know whether or not this method is suitable to do such analysis.
The result showed that the Normalized Compression Distance is likely to be not accurate to do an analysis for false news since it doesn't fulfill the sistematic and logical criteria. This method can only determine the similarities of the news input that can't be used for the determination of whether or  not the news is likely to be false.",2018-02-15 00:00:00,53,105759,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
155992,,GABRIEL GREGORIUS,FORECASTING KEBUTUHAN STOK RAW MATERIAL CAFE DENGAN METODE HOLT-WINTER'S TRIPLE EXPONENTIAL SMOOTHING STUDI KASUS COKLAT CAFE,,,"peramalan, persediaan, bahan baku, cafe, metode holt-winter's triple exponential smoothing","Drs. Sri Mulyana, M.Kom",2,3,0,2018,1,"Perkembangan usaha bisnis cafe di Yogyakarta saat ini semakin pesat. Semakin banyak persaingan bisnis cafe di bidang yang berkaitan dengan kualitas suatu produk, kemampuan untuk memenuhi permintaan konsumen dan kemampuan persaingan harga produk antar cafe. Peramalan stok barang yang digunakan untuk memenuhi permintaan konsumen merupakan salah satu bagian penting dalam persaingan bisnis cafe saat ini. Hal ini sangatlah penting sehingga barang tidak menumpuk di gudang ataupun terjadi kekurangan barang yang menyebabkan permintaan konsumen tidak terpenuhi dan membuat konsumen tidak kembali lagi ke cafe tersebut. Penelitian ini menggunakan Coklat Cafe sebagai objek penelitian. Perusahaan selama ini masih mengandalkan pengalaman atau kemampuan manajer dalam melakukan peramalan kebutuhan stok barang.
	Data yang digunakan dalam penelitian ini adalah data pemakaian barang raw material harian yang terdiri dari 132 barang. Berdasarkan karakteristik data historis, metode peramalan yang cocok untuk digunakan pada penelitian ini adalah metode kuantitatif runtun waktu (time series), yaitu Holt-Winter's Triple Exponential Smoothing. Sementara itu, untuk menghitung nilai error digunakan 5 metode evaluasi, yaitu MAE (Mean Absolute Error), MSE (Mean Squared Error), MAPE (Mean Absolute Percentage Error), MFE (Mean Forecast Error) dan TS (Tracking Signal). 
	Hasil penelitian ini menunjukkan bahwa metode Holt-Winter's Triple Exponential Smoothing adalah metode yang tepat untuk meramalkan 132 barang yang ada pada Coklat Cafe dari tanggal 1 Januari 2016 sampai 30 September 2017, dengan menghasilkan nilai rata-rata metode evaluasi untuk semua barang masing-masing adalah MAPE=12,0284%, MAE=12,74892, MSE=752,0799, dan MFE=38,00837. Penelitian ini juga menunjukkan bahwa metode peramalan tersebut memiliki keakuratan yang lebih baik daripada metode peramalan Holt's Double Exponential Smoothing.
","Development of cafe business in Yogyakarta is now growing rapidly. A lot of competition in cafe business that related with quality of products, capability to fulfill consumer demand and capability to compete the price of the products with other cafe. A stock needs forecast to fulfill consumer demands is an important part in cafe business competition. Forecasting is very important so the item in inventory not pilling up in inventory or even a shortage item that cause consumer demands not fulfilled and made consumer dissatisfied. This research used a Coklat Cafe as an object. This company still rely on experience or manager capability to make a forecast requirement of raw material stock
	Datas that used in this research are a data of actual used raw material item in a day that consists of 132 item. Based on characteristic of historical data, a suitable forecasting method for this research is a quantitive method of time series, Holt-Winter's Triple Exponential Smoothing. Meanwhile, to measure the error this research used 5 evaluation method that are MAE (Mean Absolute Error), MSE (Mean Squared Error), MAPE (Mean Absolute Percentage Error), MFE (Mean Forecast Error) and TS (Tracking Signal).
	The result of this research indicate that the suitable forecast method for forecasting 132 raw material item in Coklat Cafe is Holt-Winter's Triple Exponential Smoothing method with an average score each evaluation methods for all item from 1st January 2016 until 30th September 2017 is MAPE=12,0284%, MAE=12,74892, MSE=752, 0799, dan MFE=38,00837. This research also showed that this method have a better accuracy then Holt's Double Exponential Smoothing.
",,,107092,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
154714,,ARIF ARDIASMONO,ANALISIS SELF ORGANIZING MAP (SOM) UNTUK IMPLEMENTASI DATA ANALYSIS PADA BASIS DATA TERDISTRIBUSI DI SISTEM BIG DATA,,,"self organizing map, map reduce, hbase, hadoop, distributed database","Mardhani Riasetiawan, M.T., Dr",2,3,0,2018,1,"Banyak perusahaan beralih dari penggunaan basis data relasional menuju basis data terdistribusi karena memuat lebih banyak data serta dapat tersebar di berbagai lokasi. Cepatnya pertumbuhan data menuntut adanya peningkatan waktu pemrosesannya. Analisis perlu dilakukan untuk mengetahui adanya pola atau kemiripan antardata multi-dimensi. Beberapa penelitian menunjukkan Self Organizing Map (SOM) dapat digunakan untuk mengklaster data berdimensi tinggi. SOM melatih data secara unsupervised sehingga tidak diperlukan variabel yang ditargetkan. Dengan metode ini, sekumpulan data multi-dimensi dapat dikelompokkan berdasarkan kemiripannya dan tersebar merata dalam klaster.
Pada penelitian ini diambil dataset multi-dimensi seperti abstrak penelitian yang diimpor dalam basis data terdistribusi HBase secara map-reduce. Ekstraksi fitur kemudian dilakukan agar dataset dapat dilatih dengan metode SOM. Selama pelatihan dilakukan pencatatan waktu untuk mengetahui kecepatan kinerjanya. Setelah pelatihan, record dalam basis data diuji dan dicatat jumlah data di tiap klasternya untuk mengetahui pesebaran hasil pengelompokkan. Terakhir kedua parameter uji tersebut dihitung korelasinya untuk mengetahui tingkat keterkaitannya.
Hasil analisis didapatkan waktu kompleksitas SOM sebesar T(k,r ) = O(k^2r) + O(k^2) + O(r) dan dari hasil pengujian didapatkan bahwa algoritma SOM dapat diimplementasikan pada basis data terdistribusi untuk dataset internet dan ETD dengan perbandingan ukuran 1,85 : 180,77 dan jumlah record 1014 : 84354 dihasilkan perbandingan waktu komputasi dengan rata-rata 71,98 : 5474,72 dan simpangan baku sebaran jumlah data sebesar 73,12 : 8556,51. Pengujian korelasi menghasilkan adanya keterhubungan antara kedua parameter yang dinalisis, yaitu sebesar -0,52 untuk dataset internet dan -0,63 untuk dataset ETD.
","Many companies shift from using relational databases to distributed databases because they contain more data and can be scattered in multiple locations. Rapid data growth requires an increase in processing time. Analysis needs to be done to determine the pattern or similarity of multi-dimensional data. Several studies show Self Organizing Map (SOM) can be used to cluster high-dimensional data. SOM trains data unsupervisely so that no targeted variables are required. With this method, a set of multi-dimensional data can be reduced and grouped by their similarity and spread evenly in clusters.
In this study, HBase distributed databases import high-dimensional dataset of  research abstract using map-reduce. Feature extraction then performed on the dataset in order to be trained with the SOM method. Recording time during the training is done to determine the speed of its performance. After the training, records in the database are tested and recorded the amount of data in each cluster to know the spread of grouping results. Finally, the computation time parameter and standard deviation between clusters correlation coefficient are calculated to know the level of connectedness.
The result shows that the SOM complexity is T(k,r ) = O(k^2r) + O(k^2) + O(r) and from the test result shows that SOM algorithm can be implemented in distributed database for internet dataset and ETD with the ratio size of 1.85 : 180.77 and the number of records 1014 : 84354 computational time is generated with an average of 71.98 : 5474,72 and standard deviation of the distribution of data amounted to 73.12 : 8556.51. The correlation test resulted in a correlation between the two parameters analyzed, ie -0.52 for the internet dataset and -0.63 for the ETD dataset.
",,,105755,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159325,,ANANTA TYASTAMA,Analisis Modifikasi Stepsize Pada Flower Pollination Algorithm Dengan Menggunakan Chaos Map dan Metode Newton Termodifikasi,,,"Flower Pollination Algorithm, Metode Newton, Chaos Map, optimisasi.","Faizal Makhrus, S.Kom, M.Sc, Ph.D.",2,3,0,2018,1,"Algoritma optimisasi memiliki permasalahan terjebak di dalam local optima, akibatnya algoritma akan mengalami konvergensi dini dan nilai optimal yang didapat bersifat lokal. Salah satu algoritma yang mampu meminimalisir terjebak di local optima adalah Flower Pollination Algorithm (FPA), namun algoritma ini memiliki batasan berupa penentuan stepsize yang terbatas pada distribusi Levy. Di sisi lain penerapan Chaos Map dan Metode Newton Termodifikasi berpotensi membantu FPA untuk dapat lebih baik lagi dalam meminimalisir terjebak di local optima.

Pada penelitian ini diusulkan 4 buah metode baru untuk menentukan stepsize dari FPA, yaitu CMFPA1, CMFPA2, CMFPA3 dan MNFPA. FPA termodifikasi ini kemudian diujikan untuk menyelesaikan permasalahan klasifikasi data menggunakan jaringan saraf tiruan. Hasilnya didapat bahwa metode terbaik untuk meningkatkan akurasi adalah CMFPA3 menggunakan Singer Map, metode terbaik untuk meminimalkan iterasi adalah MNFPA dan metode terbaik untuk mempercepat waktu konvergensi adalah CMPFA1 menggunakan Circle Map.","The optimization algorithm is that it is has a chance to be trapped in local minima. As a result, the algorithm will suffer early convergence and the optimal value obtained is not global. One of the algorithms that is able to minimize local optima problem is Flower Pollination Algorithm (FPA). However, this algorithm has limitation due to its stepsize that is limited to Levy distribution. Some other method that potentially help minimize the chance of getting trapped in local minima in FPA are Chaos Map and Modified Newton Method.

This research proposed 4 new methods on determining FPA stepsize, namely CMFPA1, CMFPA2, CMFPA3 and MNFPA. These modified FPAs were tested to solve classification problem using artificial neural network. The result is the best method to improve accuracy is CMFPA using Singer Map, the best method to minimize iteration is MNFPA and the best method to speed up the convergence is CMFPA1 using Circle Map.",,,110275,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157791,,IGNASIUS HARVEY PRATAMA GUNAWIJAYA,Pengembangan Metode Visualisasi Adaptif Dari Hasil Pengolahan Data dengan MapReduce,,,"Big Data, Visualisasi Adaptif, MapReduce","Mardhani M. Riasetiawan, M.T., Dr",2,3,0,2018,1,"Pada lingkungan Big Data, perlu untuk menggali wawasan yang tersembunyi untuk membantu manusia dalam proses pengambilan keputusan. Namun, data terstruktur maupun tidak terstruktur terus bertumbuh sehingga muncul permasalahan bagaimana mengolahnya dalam waktu yang terukur dengan tetap mempertahankan nilai yang berarti di dalamnya. Diperlukan sebuah metode pemrosesan data dan visualisasi untuk mempermudah dalam pencarian wawasan. Visualisasi yang diperlukan bukan hanya memiliki tampilan yang simpel dan mudah dimengerti, namun juga harus merepresentasikan data secara menyeluruh dan interaktif.  
Pada penelitian ini diusulkan sebuah metode visualisasi adaptif dari hasil pengolahan data dengan MapReduce. Pemrosesan data dengan MapReduce digunakan untuk meningkatkan performa proses visualisasi agar dapat ditampilkan dalam waktu yang terukur. Dalam proses pembuatan visualisasi adaptif, dilakukan pendeklarasian komponen visualisasi beserta implementasi fitur adaptif yang diinginkan pengguna. Semua komponen visualisasi akan beradaptasi sesuai dengan perubahan nilai dalam visualisasi.  
Hasil pengujian menunjukkan bahwa MapReduce mampu melakukan agregasi data dengan tetap mempertahankan nilai-nilainya secara garis besar untuk menghasilkan visualiasi adaptif.  Dalam implementasinya, visualisasi adaptif yang diujikan pada Plotly Dash lebih cepat 271.34% dibandingkan dengan Tableau Desktop. Visualisasi adaptif yang dihasilkan mampu beradaptasi untuk menampilkan informasi yang diinginkan sesuai dengan berbagai masukan dari pengguna.","In the Big Data environment, it is necessary to extract hidden insights to assist people in decision-making process. However, structured and unstructured data continue to grow so there is a problem from how people can cultivate it in measurable time and constantly maintain meaningful value in it. We need a method of data processing and visualization to facilitate in search of insight with ease. Visualization required not only has a simple and easy to understand display, but also can represent the data thoroughly and interactively. 
In this research, is proposed an adaptive visualization method from data processing result using MapReduce. Data processing with MapReduce is used to improve the visualization performance to work in measurable time. In the process of making adaptive visualization, visualization components are declared with adaptive implementation features that the user wants. All visualization components will be presented according to the values in the visualization itself. 
Test results show that MapReduce is able to aggregate data while maintaining its values generally to produce adaptive visualization. In its implementation, adaptive visualization tested on Plotly Dash is 271.34% faster than on Tableau Desktop. Adaptive visualization that is produced, is able to adapt to display information according to various inputs from the user.",,,108743,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159583,,FARHAN RAMADHANI,PERBANDINGAN KINERJA ALGORITMA K-NEAREST NEIGHBORS DECISION RULES DAN MULTINOMIAL NAIVE BAYES CLASSIFIER UNTUK KLASIFIKASI BERITA ONLINE BERDASARKAN CABANG OLAHRAGA,,,"klasifikasi, text mining , chisquare, TF-IDF , K-Nearest Neighbors Decision Rules","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2018,1,"Peningkatan jumlah artikel berita yang ada di internet memberikan permasalahan baru terhadap tugas seorang editor dalam mengkategorikan berita. Metode text mining mampu melakukan proses pengkategorian suatu data teks secara otomatis menggunakan proses klasifikasi. Akan tetapi, permasalahan yang timbul dari proses klasifikasi teks  adalah ukuran dimensi dari data yang bisa mempengaruhi performa dari proses klasifikasi itu sendiri. Sehingga ukuran dimensi dari data harus dikurangi dengan melakukan seleksi terhadap atribut-atribut yang ada. Metode ini disebut sebagai seleksi fitur.
Seleksi fitur akan mengurangi dimensionalitas dari suatu data dengan mengabaikan fitur-fitur yang dianggap tidak representatif terhadap label manapun dari suatu data. Beberapa studi menunjukkan proses seleksi fitur bisa meningkatkan efektivitas dan akurasi. Pada penelitian ini digunakan metode seleksi fitur TF-IDF dan chi square. Hasil fitur yang telah diseleksi dari kedua metode, selanjutnya akan digunakan untuk klasifikasi oleh algoritma K-Nearest Neighbors Decision Rules. Hasil evaluasi performa dari algoritma klasifikasi K-Nearest Neighbors Decision Rules akan dibandingkan dengan Multinomial Naive Bayes dengan mengamati parameter evaluasi akurasi, presisi, recall dan f-measure.
Hasil penelitian ini menunjukkan metode seleksi fitur chisquare &amp; TF-IDF mampu meningkatkan akurasi, presisi, recall dan f-measure dari algoritma K-Nearest Neighbors Decision Rules. Akurasi dari klasifikasi menggunakan chisquare meningkat 10.68% sedangkan menggunakan TF-IDF meningkat 11.32%. Parameter presisi meningkat 5.8% dan 5.6% menggunakan TF-IDF &amp; chisquare secara berututan. Recall meningkat 10.50% menggunakan TF-IDF dan meningkat 10.7% menggunakan chisquare. Parameter f-measure meningkat 9.9% menggunakan metode seleksi fitur chisquare,  TF-IDF meningkat kan f-measure hingga 8.7%. Kedua metode seleksi fitur juga meningkatkan performa running time dari algoritma K-Nearest Neighbors Decision Rules. Namun peningkatan evaluasi parameter dari K-Nearest Neighbors Decision Rules belum mengungguli dari performa klasifikasi Multinomial Naive Bayes dalam mengklasifikasikan berita olahraga berbahasa Indonesia.
","The increasing number of news articles that exist on the internet gives new challenges to the editor when categorizing the news. Text mining method is able to categorize a text data automatically using the classification process. However, the problem arising from the text classification process is the size of the dimension of the data that may affect the performance of the classification process itself. So the dimension of the data should be reduced by selecting some attributes that are considered representative. This method is called feature selection.
Feature selection will reduce the dimensionality of the data by ignoring features that are deemed unrepresentative to any label of a data. Several studies shows that feature selection method can improve effectiveness and performance. This research used TF-IDF feature selection method and chisquare. Both feature selection method is continued by the K-Nearest Neighbors Decision Rules algorithm to perform classification. The performance evaluation results of K-Nearest Neighbors Decision Rules algorithm will also be compared by Multinomial Naive Bayes Classifier by observing each of its accuracy, precision, recall and f-measure.
The results show that the chisquare &amp; TF-IDF feature selection method improves the accuracy, precision, recall &amp; f-measure of K-Nearest Neighbors Decision Rules algorithm. The accuracy of the classification using chisquare increased by 10.68% while using TF-IDF increased 11.32%. Precision improves up to 5.8% and 5.6% using TF-IDF &amp; chisquare respectively. Recall increased 10.50% using TF-IDF and increased 10.7% using chisquare. F-measure parameter increased by 9.9% using chisquare feature selection method, TF-IDF increased f-measure up to 8.7%. Both feature selection methods also improve the running time performance of the K-Nearest Neighbors Decision Rules algorithm. However, the increased evaluation of the parameters of K-Nearest Neighbors Decision Rules has not outperformed the classification performance of Multinomial Naive Bayes in classifying Indonesian sports news.
",,,110556,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158048,,MUHAMMAD REIHAN F,MENS FASHION BRAND RECOMMENDATION USING SENTIMENT ANALYSIS,,,"Keywords: information retrieval, sentiment analysis, social media, fashion, fashion week, twitter, VADER sentiment, tweets","Edi Winarko M.Sc., Ph. D",2,3,0,2018,1,"Dalam industri Fashion, untuk memilih produk yang dijual, toko, menugaskan buyer untuk melihat produk dalam acara yang disebut Fashion Week di 4 kota mode besar Milan, New York, Paris, dan London. Tetapi, untuk membuat keputusan semacam itu buyer hanya menggunakan preferensi sendiri. Di era saat ini, dengan jumlah informasi yang diposting terkait dengan mode di media sosial, hal ini memengaruhi pelanggan dalam memilih produk. Oleh karena itu, ini akan menjadi proses pengambilan keputusan yang lebih baik jika pembeli dapat menggunakan semua informasi itu sebelum memilih produk. Salah satu media sosial terbesar saat ini adalah Twitter, karena banyak variasi pos, banyak pengguna yang mengirim gaya, perasaan mereka tentang produk, atau apa yang ingin mereka beli.
Dalam penelitian ini, sistem mengumpulkan tweet dari Twitter dengan mencari dengan setiap merek untuk setiap kota Fashion Week yang telah diambil dari setiap situs web Fashion Week. Kemudian, mencocokkan tweet berbahasa Inggris dengan nama merek. Lalu, penilaian dengan menggunakan analisis sentimen dengan algoritma Valence Aware Dictionary dan Sentiment Reasoner (VADER). Dan akhirnya, ditentukan 10 merek dengan nilai tertinggi untuk setiap Fashion Week sebagai hasil rekomendasi.
Berdasarkan sistem yang telah dilakukan, tweet yang berhubungan dengan fesyen yang diproses hingga sampai hasil memiliki presentase rata-rata 82.8% tweet yang berhubungan dengan fesyen yang sudah dicocokkan dengan merek. Sedangkan pada analisis sentimen hasil rata-rata adalah: precision 95.5%, recall 93.3%, f-measure 94.3%, and accuracy 91.3%.
","In the fashion industry, in order to pick products to sell a store assign a buyer to see the products a season before it sales in an event called Fashion Week in 4 major fashion cities Milan, New York, Paris, and London. But there is a problem, for making that kind of decision the buyer only uses his/her own preferences. In today&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s era, with the amount of information related to fashion posted in social media, it starts to influence customers on choosing the product to buy. Therefore, it will be a better decision-making process if the buyer can use all that information before selecting the products. One of the biggest social media today is Twitter, because of its variation of post, many users posted their styles, their feeling about a product, or what they wanted to buy.
In this research, the system collects tweets from Twitter by searching every brand for each city Fashion Week which already scraped from each Fashion Week websites. Then, match the tweets that are English with brands name. Scoring by using sentiment analysis with Valence Aware Dictionary and Sentiment Reasoner (VADER) algorithm. And finally, determine top 10 brands with the highest score for each city Fashion Week as the recommendation.
After being matched with brands, the tweets that are processed have an average of 82.8% tweets related to fashion from all tweets. While on sentiment analysis the average results are: precision 95.5%, recall 93.3%, f-measure 94.3%, and accuracy 91.3%.
",,,108998,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159584,,RONI FERNANDO,ENKRIPSI VIDEO MENGGUNAKAN VEA (VIDEO ENCRYPTION ALGORITHM) YANG DIMODIFIKASI DENGAN ALGORITME AES (ADVANCED ENCRYPTION STANDARD) DAN ALGORITME RC4 (RIVEST CIPHER 4),,,"Video, Frame, Kriptografi, VEA, AES, RC4, PBKDF2","Janoe Hendarto, Drs., M.Kom.",2,3,0,2018,1,"Kini perkembangan teknologi internet semakin pesat, ini ditunjukkan oleh meningkatnya pengguna internet sebesar 10% tiap tahunnya. Namun setiap informasi yang berada di internet perlu mendapatkan pengamanan, agar tidak disalahgunakan oleh pihak yang tidak berwenang. Terdapat berbagai macam untuk menjaga kerahasian data informasi. Salah satu cara menjaga informasi tersebut dapat menggunakan enkripsi. Enkripsi dapat dibagi menjadi dua jenis berdasarkan pembagian kuncinya, diantaranya adalah simetris dan asimetris. Pemilihan algoritme enkripsi tidaklah mudah. Setiap enkripsi memiliki kelebihan dan kekurangan masing-masing.
	Video Encryption Algorithm (VEA) merupakan algoritme enkripsi yang diimplementasikan pada citra bergerak atau video. Pada penelitian ini bertujuan untuk menganalisis perbandingan modifikasi VEA dengan algoritme enkripsi lainnya, yakni Advanced Encryption Standard (AES) dan Riverst Cipher 4 (RC4). Analisis yang dilakukan adalah analisis histogram, koefisien korelasi, kualitas enkripsi, waktu komputasi dan frame rate limit.
	Berdasarkan hasil penelitian didapatkan hasil pada algoritme modifikasi AES frame rate yang baik didapat saat untuk resolusi besar sedangkan pada resolusi kecil hasil yang didapatkan sangat tidak rendah. Dari tahapan analisis yang dilakukan algoritme VEA yang telah dimodifikasi dengan RC4 berada pada peringkat pertama dalam keseluruhan pengujian, akan tetapi proses menggunakan modifikasi RC4 tidak cocok untuk resolusi besar karna memiliki nilai frame rate yang rendah. 

","Now the development of Internet technology is increasing rapidly, this is indicated by the increase of internet users by 10% each year. But any information that is on the internet need to get security, so as not to be abused by unauthorized parties. There is a wide variety to keep information confidential. One way to preserve the information can be to use encryption. Encryption can be divided into two types based on key divisions, such as symmetric and asymmetric. Selection of encryption algorithms is not easy. Each encryption has its own advantages and disadvantages.
Video Encryption Algorithm (VEA) is an encryption algorithm that is implemented on a moving image or video. In this study aims to analyze the comparison of VEA modification with other encryption algorithms, namely Advanced Encryption Standard (AES) and Riverst Cipher 4 (RC4). The analysis is histogram analysis, correlation coefficient, encryption quality, computation time and frame rate limit.
Based on the results of the research obtained results on algoritme modification AES frame rate is good gained moment for large resolution whereas at small resolution result obtained is not very low. From the analysis stage, the modified VEA algorithm with RC4 is ranked first in the whole test, but the process of using RC4 modification is not suitable for large resolution because it has a low frame rate.",,,110535,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159840,,REZA A M,"PERBANDINGAN AKURASI NAIVE BAYES, DECISION TREE DAN RANDOM FOREST CLASSIFIER DALAM MEMPREDIKSI HASIL PERTANDINGAN LEAGUE OF LEGENDS BERBASIS SPESIFIKASI KARAKTER",,,"classification, prediction, MOBA, LOL, BNB, DT, RFC, AdaBoost","Edi Winarko, M. Sc., Ph. D.",2,3,0,2018,1,"League of Legends (LOL) sebagai permainan MOBA paling populer yang dibuat dan dikembangkan oleh RIOT Games dan mengalami pembaharuan setiap jangka waktu tertentu, menjadikan game ini memiliki daya tarik tinggi bagi pemain, penggemar dan peneliti. Selain itu salah satu pembaharuan terbesar yang terjadi adalah perubahan pada spesifikasi karakter dalam permainan. Hal ini tentunya menjadi topik diskusi yang hebat berupa kekecewaan maupun antusiasme, seperti para pemain yang mencoba memainkan karakter favorit mereka diluar dari spesifikasi yang ditentukan.
Pada penelitian ini dilakukan percobaan untuk melakukan prediksi pertandingan LOL berbasis spesisifikasi karakter yang telah mengalami pembaharuan dengan menggunakan dataset yang diunduh dari laman Kaggle. Dataset yang dijadikan objek penelitian adalah data permainan patch 7.17 dari region EUW atau Eropa Barat. Algoritma yang digunakan untuk membangun model klasifikasi adalah algoritma Bernoulli Naive Bayes (BNB), Decision Tree (DT), Random Forest(RF) dan AdaBoost yang digunakan untuk boosting BNB dan DT sebagai estimator dasar yang dianggap weak learner.
Dari pengujian prediksi pertandingan LOL didapatkan hasil dari BNB setelah boosting mendapatkan rata-rata akurasi 50,63%, DT setelah boosting dengan 49,84% dan RFC dengan 50,48%. Akurasi yang kecil untuk prediksi pertandingan LOL ini dapat diartikan bahwa spesifikasi karakter yang mengalami pembaharuan tidak terlalu mempengaruhi peluang kemenangan.
Dalam penelitian ini dapat disimpulkan bahwa untuk melakukan prediksi pertandingan LOL berdasarkan spesifikasi karakter, Bernoulli Naive Bayes dengan boosting lebih unggul daripada Decision Tree dengan boosting dan Random Forest Classifier.
","League of Legends as one of the MOBA games in the world which created and developed by RIOT Games and updated frequently, have made this game has so much charm for player, fans and researcher. Moreover, one of the biggest update ever happened is a change to in-game character specification. This make for a hot topic in form of dissapointment and enthusiasm, like how players are trying to play with their favorite character outside of role and specification given.
In this research, an attempt to predict LOL match based on updated character specification by using dataset received from Kaggle. Dataset used for research object are from 7.17 patch in Western Europe or EUW region. Method being used are Bernoulli Naive Bayes (BNB), Decision Tree (DT), Random Forest (RF) and AdaBoost which used for boosting BNB and DT as base estimator which considered weak learner or classifier.
In testing step of predicting LOL match resulted in Boosted BNB with highest average result of 50,63%, Boosted DT with 49,84% and RFC with 50,48%. On the other hand, poor accuration result can be interpreted that updated character specification doesn&acirc;€™t have significant impact for the result of LOL match.
This research also conclude that for predicting LOL match result based on character specification, Boosted BNB gives better accuration than Boosted DT and RFC.
",,,110996,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
165728,,DAVID JOURDAN MANALU,IMPLEMENTASI METODE SPLINE BIKUBIK DALAM PENGOLAHAN DATA RAPAT ARUS EKUIVALEN VERY LOW FREQUENCY ELECTROMAGNETIC,,,"Interpolasi, Spline Bikubik, Very Low Frequency, Elektromagnetik Geofisika","Drs. Janoe Hendarto, M.Kom",2,3,0,2018,1,"Very Low Frequency Electromagnetic (VLF-EM) merupakan suatu metode geofisika yang digunakan untuk mengetahui keadaan dibawah permukaan bumi dengan menghitung nilai rapat arus ekuivalen. Metode interpolasi dibutuhkan untuk menghitung nilai rapat arus ekuivalen pada titik-titik di area pengukuran VLF-EM. Setiap metode interpolasi menghasilkan grafik permukaan yang berbeda-beda dan menghasilkan akurasi yang berbeda-beda. 
Penelitian ini menggunakan data rapat arus ekuivalen VLF-EM yang disurvei pada 5 - 12 September 2015 dan 12 - 18 Agustus 2017 di Desa Kasihan, Tegalombo, Pacitan, Jawa Timur. Untuk mengetahui akurasinya, nilai RMSE dan MAE dari metode interpolasi spline bikubik dibandingkan dengan nilai RMSE dan MAE dari metode Krigging dan metode Inverse Distance Weighted (IDW).
Metode interpolasi spline bikubik menghasilkan nilai RMSE dan MAE yang lebih kecil dibandingkan dengan RMSE dan MAE dari metode Krigging dan metode IDW.
","Very Low Frequency Electromagnetic (VLF-EM) is a geophysical method used to determine the condition beneath the earth's surface by calculating the equivalent current density value. The interpolation method is needed to calculate the equivalent current density value at points in the VLF-EM measurement area. Each interpolation method produces different surface graphics and produces different accuracy.
This study uses VLF-EM equivalent current density data surveyed on 5-12 September 2015 and 12-18 Agust 2017 in Kasihan Village, Tegalombo, Pacitan, East Java. To find out the accuracy, the RMSE and MAE values of the bicubic spline interpolation method are compared with the RMSE and MAE values of the Krigging interpolation method and Inverse Distance Weighted method.
The spline bipubik interpolation method produces smaller RMSE and MAE values compared to the RMSE and MAE of the Krigging method and IDW method.
",,,116865,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
156001,,FAJAR YUDHISTIRA P,PERANCANGAN HIGH FIDELITY PROTOTYPE PLN MOBILE DENGAN PENDEKATAN USER-CENTERED DESIGN MENGGUNAKAN METODE AHP DAN TOPSIS,,,"PLN Mobile, user-centered design, usability testing, AHP, prototyping","Guntur Budi Herwanto S,Kom., M.Cs",2,3,0,2018,1,"Prototype adalah model dari produk yang digunakan untuk memberikan konsep atau gambaran dari ide yang akan dibuat. Proses perancangan prototype perlu mempertimbangkan kebutuhan dari calon pengguna. Aplikasi PLN Mobile merupakan salah satu aplikasi yang para penggunanya memiliki beberapa masalah saat menggunakannya. Untuk memecahkan masalah yang ada, penelitian ini menggunakan pendekatan user centered design, metode Analytic Hierarchy Process (AHP), dan metode Technique for Order Performance of Similarity to Ideal Solution (TOPSIS)  . 
Pada penelitian ini menggunakan metode google design sprint sebagai langkah langkah yang digunakan dalam merancang prototype PLN Mobile. Hasil analisis terhadap aplikasi PLN Mobile versi lama didapatkan dari pelaksanaan usability testing. Hasil usability testing menjadi acuan dalam membuat tiga rancangan prototype PLN Mobile versi baru. Pemilihan rancangan terbaik dibantu oleh sistem pendukung keputusan yakni menggunakan Analytic Hierarchy Process (AHP) dan Technique for Order Performance of Similarity to Ideal Solution (TOPSIS). Proses usability testing melibatkan lima partisipan. Hasil usability testing antara PLN Mobile versi baru dan PLN Mobile versi lama akan dibandingkan.
Hasil perhitungan AHP dan TOPSIS menunjukkan bahwa rancangan terbaik memperoleh rata rata usability score sebesar 7.933 dengan nilai prioritas sebesar 0.93636. Jumlah tap minimum pada PLN Mobile versi baru lebih sedikit 19.23% dari PLN Mobile versi lama. Usability score PLN Mobile  versi baru mengalami peningkatan sebesar 53,43% dari PLN Mobile versi lama. Berdasarkan data yang diperoleh dari perbandingan usability testing dan penilaian user melalui usability score, dapat diambil kesimpulan bahawa perancangan dan pemilihan prototype dengan pendekatan terhadap pengguna dapat menghasilkan rancangan yang lebih baik.
","Prototype is a model of the product used to provide the concept or description of the design to be made. Prototype design process needs to consider the user needs. PLN Mobile one of application that its users have some problem when using it. To solve the existing problem, this research uses user-centered design approach, Analytic Hierarchy Process (AHP) method, and Technique for Order Performance of Similarity to Ideal Solution (TOPSIS) method.
In this study using google design sprint method as a step that is used in designing prototype PLN Mobile. The results of the analysis of the old version of PLN Mobile application obtained from the implementation of usability testing. The results of usability testing become the reference in making three new PLN Mobile prototype designs. The best design selection is assisted by a decision support system that uses Analytic Hierarchy Process (AHP) and Technique for Order Performance of Similarity to Ideal Solution (TOPSIS). The usability testing process involved five participants. The result of usability testing between the old and PLN Mobile versions of the new version will be compared. 
The results of AHP and TOPSIS calculation shows that the best design obtained an average usability score of 7.933 with a priority value of 0.93636. The number of minimum taps on PLN Mobile of the new version is 19.23% less than the old version of the PLN Mobile. the usability score of the new version of PLN Mobile has increased by 53.43% from the old version of the Technique for Order Performance of Similarity to Ideal Solution (TOPSIS)PLN Mobile. Based on the data obtained from the comparison of usability testing and user assessment through usability score, it can be concluded that the design and  selection of prototype with approach to the user can produce a better design.
",,,107094,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167527,,ADITYA M N,ALGORITMA A* PADA APLIKASI SAMBUNG TITIK BERBASIS ANDROID,,,"Android, A*(A Star), Connect The Dots","Agus Sihabuddin, S.Si., M.Kom., Dr.",2,3,0,2018,1,"Pembelajaran untuk anak usia dini bukan berarti anak harus disekolahkan pada
umur yang belum seharusnya, dipaksa untuk mengikuti pelajaran yang akhirnya justru
membuat anak menjadi terbebani dalam mencapai tugas. Pembelajaran untuk anak usia
dini pada dasarnya adalah pembelajaran yang diberikan pada anak agar anak dapat
berkembang secara wajar (Rahmadonna,Sisca.2008). Dewasa ini para pendidik sudah
mengembangkan metode belajar bagi anak usia dini. Metode belajar sambil bermain
diharapkan anak tidak akan merasa bahwa dirinya sedang belajar, hingga membuat
kegiatan belajar mengajar lebih luwes dan tidak kaku. Lingkungan belajar dibuat
bersahabat untuk anak, sehingga mereka merasa tidak asing (Widyastuti,Susan.2010).
Perkembangan pada industri game semakin luas dengan semakin tingginya
kebutuhan akan game yang lebih kompleks dan menarik. Salah satu faktor
pendukungnya ialah kecerdasan buatan (AI). Pada kasus ini ialah sebuah Pathfinding,
pathfinding ialah sebuah istilah untuk pencarian jalan dengan solusi terbaik. Pencarian
jalan pada game komputer ataupun game mobile telah diteliti dan digunakan bertahuntahun. Telah banyak algoritma pada bidang pencarian jalan, contohnya ialah algoritma
Greedy Search, Depth First Search, Breadth First Search. Algoritma A* ialah salah
satu yang paling terkenal di algoritma pencarian jalan pada kecerdasan buatan.
Sambung titik merupakan sebuah permainan yang menggunakan teknik pencarian
(searching) yaitu algoritma A*. Tujuan dari penelitian ini adalah penulis dapat
menerapkan algoritma A* pada game sambung titik, dan juga membangun game
sambung titik yang berbasis android. Lalu pada tahap akhir pengembangan dilakukan
evaluasi terhadap proses dan hasilnya pada bagian akhir laporan ini.","Learning for early childhood does not mean that children should be sent to school
at an age that should not be, forced to follow the lesson that ultimately makes children
become burdened in achieving the tasks. Early childhood learning is basically the
learning given to children so that children can develop naturally (Rahmadonna,
Sisca.2008). Today educators have developed learning methods for early childhood.
Learning while playing methods is expected so that the child will not feel that he is
learning, to make teaching and learning activities more flexible and not rigid. Learning
environment is made friendly to children, so they feel familiar (Widyastuti,
Susan.2010).
The developement of the game industry became vast with the demand for more
complex and interesting game. One of the supporting factors is Artificial Intelligence
(AI). In this case is a pathfinding, pathfinding is a term for finding a path with the best
solution. Pathfinding in computer games or mobile games has been investigated and
used for many years. There are various algorithm in the pathfinding field, for example
greedy search algorithm, depth first search, breadth first search. A* algorithm is one of
the most popular pathfinding algorithm in artificial intelligence.
Connect the dots is a game that uses searching techniques (searching) the A*
algorithm. The purpose of this research is the authors can apply the A * algorithm on
the connect the dots game, and also build connect the dots game based on android.
Then, in the final stages of development, an evaluation of the process and the results
will be done at the end of this report.",,,118740,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
156265,,FACHRAN DHANI ACHMAD,THE INFLUENCE OF DIFFERENT PREPROCESSING TECHNIQUES FOR CLASSIFYING BATIK USING CONVOLUTIONAL NEURAL NETWORKS,,,"batik, CNN, deep learning, image processing, M.Sc.","Drs. Bambang Nurcahyo Prastowo, M.Sc.",2,3,0,2018,1,"Batik merupakan salah satu warisan terpenting di Indonesia. Dikenakan oleh tua dan muda itu digunakan untuk banyak kesempatan. Desainnya yang unik telah membuatnya populer di seluruh dunia, bahkan dipakainya oleh selebriti diluar Indonesia. Tetapi, tak banyak orang mengerti dan membedakan berbagai jenis motif dan corak.
Dalam penelitian ini, penulis mencoba menggunakan Convolutional Neural Convolutional (CNN) untuk mengklasifikasikan gambar ceplok, kawung dan parang dan menganalisa jika melakukan pra-pengolahan citra tersebut dengan menggunakan grayscale conversion, canny edge-detection dan histogram equalization untuk meningkatkan fitur dan karakteristik dari gambar akan berpengaruh pada kinerja klasifikasi CNN.
Telah ditunjukan bahwa mengolah data citra terlebih dahulu menghasilkan akurasi yang lebih rendah dibandingkan dengan menggunakan dataset citra mentah, terutama dengan menggunakan canny edge detection. Tetapi, saat menggunakan dataset pengujian keakuratan grayscale conversion, histogram equalization dan dataset mentah sangat mirip. Meningkatnya epoch memiliki pengaruh yang besar terhadap keakuratan, sedangkan peningkatan dataset mempengaruhi kinerja dengan jumlah kecil. 
","Batik is one of Indonesia's most important heritage. Worn by old and young it used for many occasions. Its unique design has made it popular globally, even being worn by non-Indonesian celebrities. Although, not many people understand and differentiate the various types of motifs and patterns.
In this research, the writer attempts to use Convolutional Neural Network (CNN) to classify images of ceplok, kawung and parang and analysing if pre-processing those images using grayscale conversion, canny edge detection and histogram equalization to enhance the features and characteristics of the image would have an effect on the classification performance of the CNN.
It was shown that pre-processing the image dataset resulted in a lower accuracy compared to using the raw image dataset, especially using canny edge detection. Although, when using the testing dataset the accuracy of the grayscale conversion, histogram equalization and the raw dataset were very similar. Increasing the epoch had a great effect on the accuracy, whereas increasing the dataset impacted the performance by a small amount.
",,,107334,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159337,,HERDIAN DEWANGGA S,Batik Pattern Classification Using Gray Level Co-occurrence Matrix and Modified K-Nearest Neighbor,,,"modified k-nearest neighbor, image classification, machine learning, gray level co-occurrence matrix, k-nearest neighbor","Bambang Nurcahyo Prastowo., M.Sc., Drs",2,3,0,2018,1,"Batik adalah salah satu kebudayaan asli Indonesia yang sangat populer baik di kalangan lokal maupun internasional. Hal itu terbukti dengan diakuinya batik sebagai warisan budaya internasional oleh UNESCO pada 2 Oktober 2009. Batik di Indonesia sendiri memiliki banyak jenis, contohnya kawung, parang, truntum, dan lain-lain. Maka dari itu, batik sudah seharusnya dilestarikan salah satunya dengan pengenalan pola/jenis batik di Indonesia. Pengenalan pola dapat dilakukan dengan media komputer/mesin. 
	Gray Level Co-occurrence Matrix adalah salah satu metode dalam ekstraksi fitur. Gray Level Co-occurrence Matrix atau disingkat GLCM memiliki beberapa properti (texture feature), contohnya Angular Second Matrix, Entropy, Contrast, dan Inverse Different Moment. Penggunaan GLCM dalam bidang pengolahan citra digital sudah cukup diakui dengan banyaknya jurnal-jurnal yang mengangkat GLCM sebagai metodenya. 
	Dalam penelitian ini, penulis menerapkan algoritma Modified K-Nearest Neighbor sebagai metode klasifikasi. MKNN adalah modifikasi dari algoritma tradisional K-Nearest Neighbor. Dengan 191 batik sebagai dataset yang kemudian dibagi menjadi training dan testing dataset dengan rasio 70:30,  penulis mengimplementasikan algoritma MKNN. Penulis juga menerapkan variasi pada data (cropping), nilai k pada algoritma MKNN, dan juga nilai alpha (&Atilde;Ž&Acirc;&plusmn;) untuk mendapatkan performa terbaik dari algoritma ini.
	Sebagai hasil dari penelitian ini, model yang diterapkan mendapatkan hasil terbaik rata-rata akurasi hingga 75,44% dengan k=9 dan alpha = 0,06 atau 0,04. Sedangkan untuk hasil terbaik pada tiap jenis batik adalah truntum dengan 85,96%, kawung dengan 75,44%, dan parang 68,42%. Cropping pada data testing juga mempengaruhi hasil dari model ini dengan penurunan yang tidak signifikan. Dari hasil tersebut juga dapat diketahui bahwa metode Modified K-Nearest Neighbor mendapatkan hasil yang lebih baik daripada metode K-Nearest Neighbor.
","Batik is one of Indonesian culture that is very popular both locally and internationally. It was proved by the recognition of batik as an international cultural heritage by UNESCO on October 2nd, 2009. Batik in Indonesia has many types, for example kawung, parang, truntum, and others. Therefore, batik should be preserved, one way to preserve it is by recognizing its pattern/types. Pattern recognition can be done with computer / machine media.
	Gray Level Co-occurrence Matrix is one method in feature extraction. Gray Level Co-occurrence Matrix (GLCM) has several properties (texture feature), for example Angular Second Matrix, Entropy, Contrast, and Inverse Different Moment. The use of GLCM in the field of digital image processing is sufficiently recognized by the number of journals that uses GLCM as its method. 
	In this research, the writer applies Modified K-Nearest Neighbor algorithm as classification method. MKNN is a modification of the traditional K-Nearest Neighbor algorithm. With 191 batik as dataset which then divided into training and testing dataset with ratio 70:30, writer implements MKNN algorithm. The writer also applies variations on the data (cropping), k values on the MKNN algorithm, and also the alpha value (&Atilde;Ž&Acirc;&plusmn;) to obtain the best performance of this algorithm. 
As a result of this study, the applied model gets the best result of an average accuracy of up to 75.44% with k = 9 and alpha = 0.06 or 0.04. While for the best results on each type of batik is truntum with 85.96%, kawung with 75.44%, and parang 68.42%. Cropping on data testing decrease the results of this model. From these results showed that the method of Modified K-Nearest Neighbor get better result than K-Nearest Neighbor method.
",,,110321,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159083,,DENY PRASETYA D,Ekstraksi Informasi Kondisi Lalu Lintas dari Media Sosial Twitter,,,"Ekstraksi Informasi, Twitter, Lalu Lintas, NER, Relation Extraction, Support Vector Machine","Guntur Budi Herwanto, M.Cs.",2,3,0,2018,1,"Twitter merupakan media sosial yang memungkinkan penggunanya mengirim berbagai informasi termasuk informasi mengenai kondisi lalu lintas di sekitarnya. Untuk memanfaatkan informasi tersebut perlu dilakukan ekstraksi informasi yang memiliki tugas dasar berupa Named Entity Recognition (NER) dan ekstraksi relasi. Tidak semua tweet relevan untuk diekstraksi informasi lalu lintasnya. Oleh karena itu diperlukan pemfilteran tweet yang akan diekstraksi informasi.
Pada penelitian ini dilakukan ekstraksi informasi kondisi lalu lintas dengan melakukan pemfilteran tweet untuk mendapat tweet yang relevan, melakukan NER untuk mengenali entitas lokasi, waktu, dan kondisi lalu lintas, serta melakukan ekstraksi relasi untuk menemukan relasi kondisi, detail, dan arah pada entitas yang telah dikenali. Ketiga fungsi tersebut diimplementasikan menggunakan algoritma machine learning yaitu Support Vector Machine ( SVM).
Hasil dari penelitian ini menunjukkan bahwa sistem ekstraksi informasi yang dibuat menggunakan algoritma SVM memiliki nilai f-measure pada komponen filter tweet sebesar 0.93, pada komponen NER sebesar 0.87, dan pada komponen ekstraksi relasi sebesar 0,83","Twitter is a social media that allows users to send various information including information about traffic conditions. To utilize the information, extraction information which has the basic task of Named Entity Recognition (NER) and relation extraction is needed. Not all tweets are relevant for extracting traffic information. Therefore it is necessary to filter tweet before the information extraction process.
In this study traffic condition information extraction is done by filtering tweets to get relevant tweets, performing NER to recognize location, times, and traffic conditions entities also extracting relations to find condition, detail, and direction relation between recognized entities. Those three tasks are implemented using Support Vector Machine (SVM) algorithm.
The result of this research shows that the information extraction system made using SVM algorithm has f-measure value of 0.93 on filtering tweet component, 0.87 on NER component, and 0.83 on relation extraction component.",,,110049,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159856,,ZATA ATIKA,ANALYSIS OF DOCUMENT CLUSTERING FOR INFORMATION TECHNOLOGY AUDIT EVIDENCE,,,"information retrieval, text clustering, lingo algorithm, stc, bisecting k- means, it audit evidence","Mardhani Riasetiawan, M.T, Dr.",2,3,0,2018,1,"Dewasa ini penggunaan teknologi terus berkembang pesat seiring berjalannya Ilmu Pengetahuan dan Teknologi informasi yang terjadi di dunia. Penggunaan teknologi juga sudah sangat luas pada praktiknya, sehingga dibutuhkan sebuah pengawasan dalam penggunaan teknologi tersebut. Pemanfaatkan IT dalam meningkatkan layanan dan membantu proses kerja sebuah perusahaan sangatlah nyata. IT Audit memegang peranan penting dalam sebuah perusahaan. Audit dibutuhkan untuk menilai apakah suatu sistem yang ada mampu membantu perusahaan untuk mencapai tujuannya secara efektif dan efisien. Hal ini melibatkan banyak dokumen yang dibutuhkan dan dihasilkan selama proses audit berlangsung. Penelitian ini mengimplementasikan metode Information Retrieval dan Document Clustering yang dapat melakukan pengelompokan dokumen hasil pencarian. Algoritma yang digunakan adalah Label Induction Group (LINGO), STC dan Bisecting K-Means dengan memanfaatkan Apache Solr untuk melakukan proses indexing dokumen. Hasil penelitian menunjukan bahwa nilai kualitas uji label klaster yang dihasilkan oleh LINGO adalah sebesar 83.5%, STC sebesar 74.6% dan Bisecting K-Means 85% untuk evaluasi hasil pencarian dan uji kualitas dari label yang dihasilkan. Berdasarkan hasil penelitian tersebut, sistem mampu memberikan hasil yang relevan untuk setiap query yang dimasukan oleh pengguna dan mampu menghasilkan kualitas label yang bermanfaat.","Nowadays, the use of technology continuous to grow rapidly as Science and Information Technology develop in a sophisticated way. Practical use of technology is already very broad, as a result surveillance on the technology usage is required. Therefore, it needs a supervision during the process. The use of technical for enhancing services in company to help human is real. That&acirc;€™s why there is an Information Technology Audit (IT Audit) and Control that plays an important role in one company. Audit process is required to assess whether an existing system is able to help the company to achieve the goals or not in an effective and efficient way. Currently, the documents that generated from an audit process amount to a very large number. This research aims to implement an Information Retrieval and document clustering that is able to grouping the documents based on the search results. The clustering algorithm that used is Label Induction Group (LINGO), Suffix Tree Clustering (STC) and Bisecting K-Means. It takes advantages of Apache Solr to do the indexing process of all document. The result of the research showed that the cluster label quality for search result clustering evaluation is 83.5% by using LINGO, 74.6% by using STC and 85% by using Bisecting K-Means algorithm. Based on this result, the system is able to grouping the document and resulting in relevant results from the query that entered by user and the cluster label quality is useful.",,,110761,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
162672,,AHMAD WIDARDI,IMPLEMENTASI PAGE DEWARPING PADA CITRA GAMBAR TEKS DENGAN MENGGUNAKAN METODE PENYISIPAN PIKSEL,,,"image processing, OpenCV, Otsu Method, page dewarping","Faizal Makhrus,S.Kom., M.Sc., Ph.D",2,3,0,2018,1,"Penelitian ini bertujuan membangun alat alternatif scanner yang menggunakan algoritma Page Dewarping. Terdapat banyak algoritma Page Dewarping yang sudah dibangun salah satu nya adalah penelitian karya Matt Zucker
(2015). Proses pengambilan gambar dilakukan dengan buku menghadap keatas dan kamera dari atas akan mengambil gambar kemudian menyimpannya pada komputer. Selanjutnya dari gambar tersebut akan diluruskan dengan algoritma Page Dewarping.
Pada awalnya gambar yang sudah diambil akan diproses dengan CLAHE dan Metode Otsu untuk binarisasinya. Setelah itu terdapat dua metode berbeda untuk meluruskan gambar yaitu menggunakan Regresi dan menggunakan Klasifikasi Spasi Baris dengan Binary Search yang dimodifikasi. Selain itu program akan ditulis dengan C++ dan menggunakan Pustaka OpenCV untuk optimasi. Nantinya kedua metode ini akan dibandingkan dari segi waktu serta hasil pelurusan gambar. Hasil akhir dari penelitian ini memperlihatkan bahwa kedua metode memiliki total waktu komputasi yang lebih cepat daripada algoritma Page Dewarping yang sudah dibuat. Namun
untuk hasil akhir dewarp kurang baik dibandingkan algoritma Page Dewarping karya Matt Zucker (2015).","This research is focused on developing an alternative device to scanner by using Page Dewarping algorithm. There are many Page Dewarping algorithm that already developed, one of them is algorithm developed by Matt Zucker (2015). Image capturing process is taken with the book is faced up and the camera from above will take the image thus saved into computer. From that saved image, it will be dewarped by Page Dewarping algorithm. First, that image will have its contrast enhanced with CLAHE and binarized by using Otsu Method. Thus there are two different methods to dewarp the image, by using Regression and using Spacing Classification with Modified Binary Search to classify. The program is written in C++ and uses OpenCV library for optimization. At last, this research will compare the result after dewarped and execution time for these two methods. This research's result shows that these methods have faster execution time than execution time of Matt Zucker (2015)'s method. However, the final result on both methods are worse than the result on Matt Zucker (2015)'s method.",,,113705,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159601,,DEBBIE NURWENDA YUDHANTARA,PENGEMBANGAN APLIKASI ISLAMIC EVENT BASED SOCIAL NETWORK UNTUK PERANGKAT ANDROID ,,,"Android, Location Based Service, Event Based Social Network, Google Maps, Cloud Firestore","Sigit Priyanta, S.Si, M.Kom., Dr.",2,3,0,2018,1,"Kesadaran masyarakat untuk menuntut ilmu agama dewasa ini semakin berkembang. Banyaknya lembaga maupun yayasan keislaman di Provinsi Yogyakarta berlomba-lomba mengadakan suatu acara keagamaan untuk menampung minat masyarakat akan ilmu syariah. Media sosial, sebagai wadah komunikasi dan berbagi antar penggunanya menjadi sarana yang cukup efektif  bagi lembaga-lembaga keislaman tersebut untuk mempublikasikan acaranya. Hanya saja, banyaknya konten yang dibagi oleh pengguna sosial media membuat informasi publikasi acara ini menjadi sulit dijangkau bagi sebagian orang.  
	Berdasarkan permasalahan diatas dibutuhkan sebuah sarana untuk mengumpulkan dan membagikan acara kajian Islam menjadi satu. Sebuah aplikasi Islamic Event Based Social Network dapat menjadi sarana yang sesuai karena konten yang tersedia hanya berfokus pada menyimpan, mencari, dan berbagi informasi kajian keislaman. Aplikasi dibangun dengan bahasa pemrograman Java, Google Maps untuk menampilkan peta, dan Cloud Firestore untuk basis datanya.
Hasil penelitian berupa aplikasi Android yang memiliki fitur pencarian acara dan dapat menampilkan daftar kajian Islam di Provinsi Yogyakarta. Dengan memanfaatkan teknologi LBS yang diimplementasikan pada Google Maps, aplikasi ini dapat menampilkan lokasi user dan lokasi acara. Teknologi LBS memungkinkan aplikasi untuk menampilkan rute menuju lokasi acara dan menemukan lokasi acara terdekat. 
","Public awareness to study religion today is growing. The number of institutions and Islamic foundations in Yogyakarta are competing to hold an Islamic event to accommodate the public interest in Islamic sciences. The social media, as a forum for communication and sharing among its users, becomes a sufficiently effective means for these Islamic institutions to publicize the event. It's just that the amount of content shared by social media users makes publication of this event difficult to reach for some people.
Based on the problem, it needs to crate a places to collect and share the Islamic event into one. An Islamic Event Based Social Network application can be an appropriate tool because the content only focuses on storing, searching, and sharing information on Islamic events. The application is built with Java, Google Maps to display maps, and Cloud Firestore is used to store information on these events.
The results of the research is an Android applications that have feature to search events and display a list of Islamic events in Yogyakarta. By leveraging LBS technology that is implemented in Google Maps, this app can display spatial data of user location and event location. This LBS technology allows the app to display routes to the event location and find the location of nearby events.
",,,110583,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
156530,,ARYA KRESNA WIJAYA,PENGELOMPOKAN DOKUMEN BERDASARKAN METADATA MENGGUNAKAN ALGORITME K-MEANS DENGAN METODE DENSITAS DAN PROBABILITAS ,,,"Dokumen, Clustering, Metadata, Algoritme K-means","Mardhani Riasetiawan, M.T., Dr",2,3,0,2018,1,"Banyaknya dokumen digital dengan berbagai jenis tipe menimbulkan permasalahan pencarian informasi pada dokumen jenis tertentu. Proses pencarian informasi tersebut dapat dipermudah dengan mengelompokkan dokumen. Clustering merupakan salah satu metode yang dapat digunakan untuk mengelompokkan dokumen. Akan tetapi, dokumen dengan jenis berbeda memiliki jenis fitur internal yang berbeda sehingga fitur internal tersebut tidak dapat digunakan sebagai fitur untuk mengelompokkan dokumen. Metadata sebagai fitur eksternal yang dimiliki oleh setiap jenis dokumen dapat digunakan sebagai fitur alternatif untuk mengelompokkan dokumen dengan jenis yang berbeda.
Pada penelitian ini proses clustering dilakukan menggunakan algoritme K-means dengan metode densitas dan probabilitas pada proses pemilihan pusat klaster awal. Dokumen yang digunakan pada penelitian ini adalah dokumen pdf, docx, jpg, dan png. Hasil penelitian menunjukkan bahwa proses clustering dengan menggunakan metode probabilitas memiliki performa yang lebih baik dibandingkan dengan metode densitas. Hal tersebut ditunjukkan dari rata-rata nilai SSE metode probabilitas yang lebih rendah dibandingkan metode densitas yaitu sebanyak 2,9951, sedangkan nilai rata-rata SSE dengan metode densitas sebanyak 3,7509. Selain itu, dokumen docx dan jpg cenderung mengelompok ke dalam klaster yang sama dikarenakan memiliki kesamaan atribut metadata yang banyak, sedangkan dokumen pdf dan png mengelompok membentuk klaster yang berbeda. 
","The number of digital documents with various types causes problems of information retrieval in a particular type of document. The information retrieval process can be made easier by grouping documents. Clustering is one method that can be used to group documents. However, documents with different types have different types of internal features so that they can not be used as a feature for grouping documents. Metadata as an external feature owned by each document type can be used as an alternative feature to group documents with different types.
In this research, the process of clustering is done using K-means algorithm with density and probability methods in the process of selecting the initial cluster centers. The documents that used in this research are pdf, docx, jpg, and png documents. The results showed that the clustering process using the probability method has better performance than the density method. This is shown by the average SSE value of the lower probability method than the density method of 2.9951, whereas the average value of SSE with the density method is 3.7509. In addition, docx and jpg documents tend to group into the same cluster because they have many common metadata attributes, whereas pdf and png documents tend to form different clusters.
",,,107594,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159347,,CHRISTIAN ADE YANUAR,HISTOGRAM OF ORIENTED GRADIENTS DAN LANDMARK PADA PENGENALAN EKSPRESI WAJAH,,,"Facial Expression Recognition, Histogram of Oriented Gradient, Landmark, Support Vector Machines.","M. Edi Wibowo, S.Kom., M.Kom., Ph.D. ",2,3,0,2018,1,"Pengenalan ekspresi wajah merupakan salah satu kemampuan komputer dalam melakukan interaksi dengan manusia. Salah satu manfaat yang bisa diambil pada pengenalan ekspresi wajah adalah untuk melakukan deteksi emosi pada peserta wawancara. Deteksi emosi ini berperan penting dalam menentukan respon terhadap permasalahan yang diberikan.
Pada pengenalan ekspresi wajah dibutuhkan suatu data yang dapat menampilkan ekspresi wajah. Dalam penelitian ini data menggunakan dataset CK+ dan FER2013. Dataset CK+ dan FER2013 merupakan dataset yang dirancang untuk melakukan pengenalan ekspresi wajah. Pada preprocessing dalam penelitian ini, dilakukan deteksi wajah untuk mengambil wajah bagian depan. Ekspresi wajah dapat diketahui dari perubahan posisi mata, alis, hidung, dan mulut. Untuk mendapatkan nilai perubahan tersebut digunakan feature extraction. Pada penelitian ini feature extraction yang digunakan adalah histogram of oriented gradient dan landmark. Kemudian hasil dari penggunaan feature extraction tersebut digunakan dalam melakukan klasifikasi dengan support vector machines.
Hasil akurasi terbaik yang didapat pada penelitian ini ketika menggunakan penggabungan kedua feature extraction diatas. Hasil akurasi yang didapatkan pada dataset CK+ sebesar 92,68%. Kemudian untuk dataset FER2013 memiliki akurasi sebesar 53,68%.","Facial expression recognition is one of the computer's ability to interact with humans. One of the benefits that can be taken on facial expression recognition is to perform emotion detection on the interviewees. This emotion detection plays an important role in determining the response to the given problem.
In facial expression recognition required data that can display facial expressions. In this research data use CK+ dan FER2013 dataset. The CK+ and FER2013 datasets are datasets designed to perform facial expressions. In preprocessing in this research, face detection is done to take the frontal face. Facial expressions can be known from changes in eye position, eyebrows, nose, and mouth. To get the value of the change is used feature extraction. In this study feature extraction used is histogram of oriented gradient and landmark. Then the result of using feature extraction is used in classification with support vector machines.
The best accuracy results obtained in this study when using the combination of both feature extraction above. Accuracy results obtained in the CK+ dataset of 92.68%. Then for the dataset FER2013 has an accuracy of 53.68%.",,,110295,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
154741,,DEWI RAHAYUNINGTIAS,SISTEM PENDUKUNG KEPUTUSAN MENGGUNAKAN ANALYTICAL HIERARCHY PROCESSDAN TECHNIQUE FOR ORDER PREFERENCE BY SIMILARITY TO IDEAL SOLUTION UNTUK PEMILIHAN FURNITURE,,,"Sistem Pendukung Keputusan, AHP, TOPSIS, furniture / Decision Support System, AHP, TOPSIS, furniture","Drs. Retantyo Wardoyo, M. Sc., Ph. D.",2,3,0,2018,1,"Furniture merupakan salah satu kebutuhan dalam setiap rumah. Dalam
penentuan furniture, kriteria yang harus dipilih oleh konsumen dipengaruhi oleh
banyak faktor, diantaranya adalah jenis kayu, harga, design, finishing, ukuran, dan
aksesoris. Penelitian ini bertujuan untuk membangun suatu sistem pendukung
keputusan (SPK) yang dapat membantu konsumen dalam pemilihan furniture yang
memenuhi kriteria.
SPK yang dikembangkan dalam penelitian ini menggunakan metode AHP
(Analytical Hierarchy Process) dan TOPSIS (Technique for Order Preference by
Similarity to Ideal Solution). Metode AHP digunakan untuk menentukan bobot
parameter menurut pengambilan keputusan, sedangkan metode TOPSIS digunakan
untuk menentukan peringkat alternatif yang didasarkan pada konsep dimana alternatif
terpilih yang baik tidak hanya memiliki jarak terpendek dari solusi ideal positif,
namun juga memiliki jarak terpanjang dari solusi ideal negatif.
Hasil dari penelitian ini adalah menunjukan suatu sistem yang digunakan
untuk pemilihan furniture berdasarkan parameter-parameter yang telah ditetapkan.
Hasil pengujian menunjukkan bahwa, sistem dapat memberikan urutan alternatif
furniture yang dapat digunakan sebagai rekomendasi bagi pengambil keputusan
dalam memilihfurniture.","Furniture is one of the needs in every home. In the determination of furniture,
the criteria that must be chosen by the consumer is influenced by many factors,
including the type of wood, price, design, finishing, size, and accessories. This study
aims to build a decision support system (DSS) that can assist consumers in the
selection of furniture that meets the criteria.
The development of DSS in this case study is using AHP (Analytical
Hierarchy Process) and TOPSIS (Technique for Order Preference by Similarity to
Ideal Solution). AHP method is used to determine the parameter quality according to
the decision making when the TOPSIS metdhod is used to determine the alternative
rank based on a concept where the a well-choosen alternative is not only has the
shortest distance from the ideal positive solution, but also has the longest distance
from the ideal negative solution.
The results of this case study is to show that a system used for the selection of
furniture based on predefined parameters. The test results show that the system can
provide alternativeorder furniture that can be used as a recommendation for decision
makers in choosing furniture.",,,105795,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159351,,ANDIMAS MUHAMMAD B,Analysis of Clustering in Java Program,,,"data mining, cluster analysis, k-prototypes clustering, program comprehension, Java","Suprapto, Drs., M.Kom., Dr.",2,3,0,2018,1,"Program Comprehension adalah proses kognitif yang penting dalam pengembangan perangkat lunak, proses ini memiliki banyak potensi pengaplikasian seperti reverse engineering dan program collaboration. Namun, program comprehension telah dikenal sebagai aktivitas yang memakan waktu dan sangat sulit dicapai tanpa pengetahuan mengenai domain program.
Penelitian ini mencoba menerapkan metode clustering menggunakan k-prototypes untuk memfasilitasi pemahaman program pada tingkat methods. Penelitian ini menggunakan dua aplikasi sebagai studi kasus: Ghost - Professional Blogging App and Protein &Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12; an atypical Dribbble App. Hasil pengelompokan kemudian dianalisis untuk mendapatkan sifat pembuatan methods dalam program Java.
Evaluasi pengelompokan dari kedua studi kasus menunjukkan bahwa dengan menggunakan Silhouette Measure k = 2 adalah hasil terbaik semestara dengan menggunakan Sum of Squared Errors k = 6 memiliki hasil validitas klaster terbaik. Meskipun beberapa kluster telah mengungkapkan kesamaan perilaku, beberapa yang lain belum membentuk perilaku yang sama.","Program comprehension is an important cognitive process in software development, it has many potential applications like reverse engineering and program collaboration. However, program comprehension has been known as time-consuming activity and very hard to accomplish without the knowledge of the program domain.
This research tries to apply a clustering method using k-prototypes to facilitate program comprehension on the method level. This research uses two application as case studies: Ghost - Professional Blogging App and Protein &Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12; an atypical Dribbble App. The clustering result then analyzed to get the nature of method creation in a Java program.
The clustering evaluation from both case studies shows that using Silhouette Measure k = 2 is the best result while using Sum of Squared Errors k = 6 has the best cluster validity result. Although some of the clusters have revealed the similarities of behaviors, others have not formed a certain similar behavior.",,,110322,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158843,,MOCHAMAD AFDAL WAHYU,"PERBANDINGAN ENKRIPSI CITRA DIGITAL DENGAN MENGGUNAKAN ALGORITMA AES, RSA, DAN CHAOS ",,," Enkripsi, Perbandingan kecepatan, Analisis Statistik, AES, RSA, Chaos","Janoe Hendarto, Drs., M.Kom",2,3,0,2018,1,"Seiring dengan perkembangan teknologi maka kebutuhan kerahasiaan pertukaran informasi semakin meningkat. Terdapat berbagai macam cara untuk menjaga kerahasiaan data informasi. Salah satu cara menjaga informasi tersebut dapat menggunakan enkripsi. Enkripsi dapat dibagi menjadi dua jenis berdasarkan pembagian kuncinya, di antaranya adalah simetris dan asimetris. Pemilihan algoritma enkripsi tidaklah mudah. Setiap enkripsi memiliki kelebihan dan kekurangan masing-masing. Advanced Encryption Standard (AES) merupakan algoritma enkripsi menggunakan kunci simetris begitu juga dengan Chaos. Berbeda dengan RSA karena menggunakan kunci bertipe asimetris. Penelitian ini bertujuan untuk menganalisis perbandingan enkripsi AES, RSA, dan Chaos dalam hal kecepatan komputasi enkripsi dan dekripsi, analisis statistik dan perhitungan kesalahan. Berdasarkan hasil penelitian didapatkan hasil AES berada pada peringkat pertama dalam proses enkripsi yang disusul oleh RSA kemudian Chaos. Pada proses dekripsi urutan tercepat adalah AES yang disusul oleh Chaos kemudian RSA. Ketiga algoritma dapat dikatakan bebas dalam serangan analisis statistik dengan menggunakan analisis histogram dan korelasi koefisien. Pada proses enkripsi RSA terdapat kekurangan yaitu tidak dapat dilakukan pemetaan beberapa nilai enkripsi ke dalam citra karena hasil enkripsi melebihi kapasitas ukuran citra.","Along with the development of technology then the need for confidentiality of information exchange is increasing. There are various ways to maintain the confidentiality of information data. One way to preserve the information can be using encryption. Encryption can be divided into two types based on key, there are symmetric and asymmetric. Selection of encryption algorithms is not easy. Each encryption has its own advantages and disadvantages. Advanced Encryption Standard (AES) is an encryption algorithm using symmetric keys as well as Chaos. Unlike RSA because it uses asymmetric type keys. This study aims to analyze the comparison of AES, RSA, and Chaos encryption in terms of computation speed of encryption and decryption, statistical analysis and error calculation. Based on the research results obtained AES is ranked first in the encryption process followed by RSA and then Chaos. In the fastest decryption process is AES followed by Chaos then RSA. The three algorithms can be said to be free in statistical analysis by using histogram analysis and correlation coefficients. In RSA encryption there are drawbacks in encryption process that the result of encryption value cannot be mapped into the image because the encryption results exceed the capacity of image size. ",,,109848,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159870,,LISA ALMIRA,Penggunaan Algoritma Genetika sebagai Seleksi Fitur dalam Pengenalan Ucapan pada Orang Penderita Dysarthria,,,"Algoritma Genetika, Speech Recognition, Dysarthria","Afiahayati, S. Kom., M. Kom., Ph. D.",2,3,0,2018,1,"Dysarthria merupakan suatu gangguan berbicara. Ciri-cirinya antara lain ucapan yang menyatu, serta bermasalah dalam mengontrol kekerasan, ritme, dan pitch. Karena hal ini, ucapan orang dysarthria sulit dimengerti. Maka dari itu, dibangunlah speech recognition yang digunakan untuk mengenali suara penderita dysarthria. Akan tetapi, banyaknya fitur yang besar menyebabkan waktu komputasi yang lama.
Algoritma genetika merupakan salah satu metode yang dapat digunakan untuk menyeleksi fitur. Algoritma ini diterapkan pada proses klasifikasi, sehingga waktu komputasi dapat lebih cepat.
Pada penelitian ini, algoritma klasifikasi yang digunakan yaitu Support Vector Machine (SVM). Model kromosom algoritma genetika direpresentasikan dengan biner, di mana nilai 1 menunjukkan fitur yang terpilih, sedangkan nilai 0 menunjukkan fitur yang tidak dipilih. Banyaknya eksperimen yang dilakukan yaitu sebanyak 50. Individu yang memiliki akurasi terbaik, yaitu 89,338 persen, diperoleh pada eksperimen dengan jumlah populasi 100, jumlah generasi 50, probabilitas crossover 0,8 dan probabilitas mutasi 0,05. Banyaknya fitur terseleksi yang digunakan yaitu sekitar 50 persen dari banyaknya fitur awal. Perbandingan akurasi klasifikasi data uji antara tanpa seleksi fitur dengan menggunakan seleksi fitur yaitu 90 persen dan 87,857 persen. Sedangkan waktu komputasi pembentukan model dan prediksi data lebih cepat dengan seleksi fitur.","Dysarthria is a speech disorder. Symptoms are united speech, also problematic in arranging loudness, rhythm, and pitch. Because of those things, speech of people with dysarthria is hard to understand. Therefore, speech recognition was built to recognise the speech of people with dysarthria. But, the number of features lead to long computation time.
Genetic algorithm is a method which can be used to selecting features. This algorithm implemented at classification process, so the computation time can be faster.
In this research, Support Vector Machine (SVM) was being used as classification algorithm. Cromosome of genetic algorithm repesented as binary, which value 1 means selected feature, value 0 means unselected feature. The total of experiments are 50. Individual that has the best accuracy score, which is 89.338 percent, was obtained from experiment with number of population is 100, number of generation is 50, crossover probability is 0.8 and mutation probability is 0.05. Number of feature being used is 50 percent of the full features. The comparison of accuracy scores between classification of test datas with feature selection and without feature selection is 87.875 percent and 90 percent. The computation time is faster with feature selection than without feature selection.",,,110912,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159104,,PRIMA SUSILA K,PERBANDINGAN KINERJA WEB SERVER APACHE DAN NODE.JS PADA PLATFORM RASPBERRY PI,,,"Webserver, Node.js, IoT, Raspberry Pi, Benchmark","Andi Dharmawan, S. Si, M.Cs, Dr.",2,3,0,2018,1,"Perkembangan teknologi internet yang semakin pesat menjadi faktor berbagai perangkat elektronik semakin mengarah ke mobilitas yang tinggi. Hal tersebut mendorong terciptanya IoT, yaitu terhubungnya berbagai perangkat ke dalam suatu sistem melalui jaringan internet yang dapat saling berkomunikasi. Dalam implementasi IoT, banyak digunakan Single Board Computer (SBC) karena kepraktisan dan kemampuannya. Salah satu SBC yang populer digunakan adalah Raspberry Pi. Contoh implementasi penerapan IoT pada Raspberry Pi adalah sistem kendali atau monitor yang terhubung dengan sensor. Sistem tersebut menggunakan Raspberry Pi sebagai web server yang melayani antarmuka pengguna. Dibutuhkan web server yang tepat untuk digunakan pada sistem tersebut sehingga kinerja sistem menjadi optimal.
Penelitian ini bertujuan untuk membangun webserver yang optimal pada Raspberry Pi menggunakan Node.Js, yang dibandingkan kinerjanya dengan Apache yang umum digunakan. Pengujian dilakukan dengan menggunakan Siege untuk simulasi akses dengan parameter requests rate dan throughput, serta Dstat untuk perekaman sumberdaya dengan parameter penggunaan cpu dan memory. Dilakukan tiga pengujian yaitu pengujian kinerja, pemberian beban, dan penggunaan sumberdaya.
Dari hasil pengujian kinerja dan pemberian beban, didapatkan nilai throughput dan requests rate Node.Js lebih tinggi dibandingkan dengan Apache dalam melayani halaman web statis. Sedangkan pada pengujian penggunaan sumberdaya, Node.Js lebih efisien dibanding Apache dalam penggunaan sumberdaya. Hal tersebut dikarenakan perbedaan arsitektur dari kedua webserver tersebut.","The rapid development of internet technology is becoming a factor of various electronic devices increasingly leading to high mobility. This encourages the creation of IoT, which connects various devices into a system through the Internet that can communicate with each other. In the implementation of IoT, Single Board Computer (SBC) is widely used because of its practicality and ability. One of the popular SBC used is Raspberry Pi. An example of implementing IoT on Raspberry Pi is a control or monitor system connected to the sensor. The system uses Raspberry Pi as a web server that serves the user interface. It takes the right web server to use on the system so that the system performance becomes optimal.
This research aims to build an optimal webserver on Raspberry Pi using Node.Js, which compared for performance with Apache that&acirc;€™s commonly used. Testing is done by using Siege for access simulation with parameter of requests rate and throughput, and Dstat for resource recording with parameter of cpu and memory usage. Three tests were performed: performance testing, load assignment, and resource usage.
From the results of performance testing and load assignment, throughput and requests rate value of Node.Js are higher than Apache in serving static web pages. While on the usage of resources testing, Node.Js is more efficient than Apache in resource usage. This is caused by the difference in architecture of both webserver.",,,110065,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
160130,,AZIZ MUSTIKA AJI,EKSTRAKSI LOKASI DAN TOPIK PADA BERITA ONLINE BERBAHASA INDONESIA,,,"location extraction, topic extraction, NER, HMM, TF*PDF","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2018,1,"Seiring dengan perkembangan teknologi internet yang ada saat ini, internet telah mengubah cara manusia dalam membaca berita. Dari berita yang hanya tersedia di dalam media cetak dan elektronik, saat ini berita sudah tersedia dalam bentuk online. Pencarian suatu berita berdasarkan lokasi dan topik menjadi hal yang sangat penting sejak penyebaran berita melalui internet terjadi sangat pesat.
Pada penelitian ini dibangun sistem ekstraksi informasi khususnya lokasi dan topik dari suatu teks berita online. Ekstraksi lokasi dilakukan dengan menggunakan pengenalan entitas bernama (named entity recognition) dengan metode Hidden Markov Model (HMM). Sedangkan metode ekstraksi topik yang digunakan adalah algoritma term frequency and proportional document frequency (TF*PDF). Hasil dari pembobotan istilah ini kemudian digunakan untuk mengambil kesimpulan topik apa yang sedang tren.
Hasil dari penelitian ini adalah sebuah sistem ekstraksi lokasi dan topik berita online dalam suatu periode waktu. Evaluasi ekstraksi lokasi untuk data latih dilakukan dengan menggunakan algoritma k-fold cross-validation dengan menunjukkan hasil akurasi 0,914. Namun untuk presisi, recall, dan F1-score belum bisa dikatakan baik, yaitu secara berurutan bernilai 0,530, 0,439, dan 0,468. Hasil dari ekstraksi topik yang didapatkan adalah sebuah daftar vektor unit dari istilah-istilah yang diidentifikasi memiliki bobot tertinggi.","Along with the rapid development of internet technologies nowadays, ways to find information have shifted significantly. Specially, internet has changed how people access and read news. Print and electronic media used to be the only means of communicating information, but in the digital era today information can also be accessed via online resources. Most notably, since the last fast advancement of the internet as a medium of spreading information, the ability to search and retrieve information based on what happens and where something happens has become very important.
In this research, an information extraction system was developed to filter online information efficiently and quickly based on the location of an occurence and the trending topic in a certain range of time. The approach to design and develop this classification model followed the procedures of the Hidden Markov Model (HMM). The results of the classification model were used to do the process of labeling and were expected to be able to accurately take a short summary of the location of an occurence. Meanwhile, the method of term weighting was used to extract the mostly-discussed topic from online news articles in a certain period of time using the algorithm Term Frequency and Proportional Document Frequency (TF*PDF). The results of term weighting were analyzed to reach a conclusion about a particular trending topic in a particular range of time.
Finally, the development of an extraction system that was expected to be able to filter online information based on the location of an occurence and the trending topic in a certain period of time was successfully accomplished. Location extraction of the training data was evaluated using the algorithm K-fold cross- validation that resulted in an accuracy value of 0,914. However, the precision, recall, and F1-score cannot be categorized as good because they only reached the values of 0,530, 0,439, and 0,468 respectively. Meanwhile, topic extraction resulted in a list of unit vectors of terms that were identified with highest weights.",,,111047,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
153992,,JOKO PRAKOSO,PERBANDINGAN PERFORMANSI SNORT DAN SURICATA SEBAGAI SISTEM DETEKSI INTRUSI PADA RASPBERRY PI,,,"Raspberry Pi, IDS, Snort, Suricata, Pytbull/Raspberry Pi, IDS, Snort, Suricata, Pytbull","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2018,1,"Perkembangan teknologi saat ini telah mendorong banyak pihak untuk
memiliki fasilitas internet dan menjadikannya sebagai bagian dalam kehidupan
sehari-hari. Hal ini memungkinkan siapapun yang terkoneksi dalam suatu jaringan,
ikut serta dalam menciptakan kerentanan terhadap sistem keamanan jaringan
tersebut. Salah satu cara untuk meningkatkan keamanan jaringan adalah dengan
menggunakan sistem deteksi intrusi seperti Snort dan Suricata.
Pada penelitian ini dilakukan pengujian sistem deteksi intrusi menggunakan
Snort dan Suricata pada Raspberry Pi dengan melakukan tiga jenis pengujian yaitu
pengujian performansi terhadap jumlah load rules IDS, pengujian performansi
terhadap trafik jaringan, dan pengujian performansi terhadap load rules disertai
trafik jaringan dan percobaan serangan menggunakan Pytbull.
Hasil pengujian performansi Raspberry Pi terhadap load rules IDS
menunjukkan bahwa Snort menggunakan lebih sedikit CPU dan RAM daripada
Suricata. Hasil pengujian performansi Raspbery Pi terhadap trafik jaringan juga
menunjukkan Snort menggunakan lebih sedikit CPU dan RAM daripada Suricata.
Hasil pengujian performansi terhadap load rules disertai trafik jaringan dan
percobaan serangan, menunjukkan bahwa Snort mengalami penurunan alert dan
drop packet yang lebih banyak daripada yang dialami Suricata. Hasil ini akan
berdampak pada keakuratan IDS, dan evaluasi terhadap keakuratan IDS tersebut
perlu diselidiki lebih lanjut.
","The development of technology today has encouraged many parties to have
internet facilities and make it a part in everyday life. This allows anyone connected
to a network, participating in creating vulnerabilities to the network's security
system. One way to improve network security is to use intrusion detection systems
like Snort and Suricata.
In this research, intrusion detection system was tested using Snort and
Suricata on Raspberry Pi by performing three types of testing, namely performance
testing of IDS load rules, performance testing of network traffic, and performance
testing of load rules along with network traffic and attack experiments using
Pytbull.
The results of Raspberry Pi performance testing on the IDS load rules show
that Snort uses less CPU and RAM than Suricata. The results of Raspbery Pi
performance testing on network traffic also shows that Snort uses less CPU and
RAM than Suricata. The results of performance testing of load rules along with
network traffic and attack attempts, indicate that Snort experienced a decrease in
alerts and drop packets more than Suricata experienced. These results will have an
impact on the accuracy of the IDS, and the evaluation of the IDS accuracy needs
further investigation
",,,105126,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
166793,,NAUFAL HAIDAR M,STUDY COMPARISON CLOUD NETWORK PERFORMANCE USING SSL VPN AND SSL VPN WITH FIREWALLS,,,"VPN, SSL VPN, Cloud, Throughput, Firewall, Latency","Medi S.Kom, M.Kom",2,3,0,2018,1,"Cloud computing memiliki peran penting dan berdampak besar dalam dunia teknologi informasi, salah satu layanan dalam cloud computing yang sering digunakan adalah layanan berbasis software as a service (SaaS) atau cloud storage. Tetapi cloud computing memiliki isu akan keamanan dan rawan akan serangan. Penggunaan VPN dan firewall menjadi solusi bagi keamanan cloud computing akan tetapi implementasinya dapat mempengaruhi throughput dan latency sehingga perlu dilakukan pengukuran terhadap kinerja jaringan cloud computing.

Dalam Penelitian ini, penulis mencoba melakukan pengukuran dan perbandingan ketika mengakses layanan cloud tanpa menggunakan VPN dengan yang menggunakan 2 jenis implementasi SSL VPN yaitu SSL portal VPN dan SSL tunnel VPN dan dengan yang menggunakan layanan VPN dan NAT Firewall terhadap nilai jumlah paket data, bytes dan bits, throughput dan latency. Proses pengukuran dilakukan dengan melihat dan menghitung nilai throughput dan latency pada saat mengunduh file dari layanan cloud dengan menggunakan aplikasi wireshark.

Setelah dilakukan pengukuran dapat diketahui bahwa baik SSL portal maupun tunnel VPN secara keseluruhan parameter cenderung menunjukkan hasil yang lebih tinggi dibandingkan yang tanpa menggunakan VPN. Begitu juga pengukuran yang menggunakan SSL VPN dengan NAT firewall menunjukkan hasil yang lebih tinggi akan tetapi sedikit lebih rendah dibandingkan yang tanpa menggunakan NAT firewall. Secara keseluruhan, firewall tidak memberikan pengaruh signifikan terhadap network performance ketika digunakan dengan VPN.","Cloud computing has an important role and has a big impact in the world of information technology, one of the services in cloud computing that is often used is services based on software as a service (SaaS) or cloud storage. But cloud
computing has the issue of security and is prone to attacks.

 The use of VPNs and firewalls is a solution for cloud computing security, but its implementation can affect throughput and latency, so it is necessary to measure the performance of cloud computing networks.
In this study, the author tries to make measurements and comparisons when accessing cloud services without using a VPN by using 2 types of SSL VPN implementations, namely SSL VPN portals and SSL tunnel VPNs and by using VPN
services and NAT Firewall against the value of data packets, bytes, and bits, throughput and latency. The measurement process is done by looking at and calculating throughput and latency values when downloading files from cloud services using the Wireshark application.

After measurement, it can be seen that both SSL portals and tunnel VPNs overall parameters tend to show higher results than those without using VPN. Likewise, measurements that use SSL VPN with NAT firewall show higher results but are slightly lower than those without using a NAT firewall. Overall, the firewall does not have a significant influence on network performance when
used with VPN.",,,118278,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159628,,IRFAN MAULANA KAMIL,NALAN KARAKTER TULISAN TANGAN MANUSIA DENGAN NEURAL NETWORK DAN KLASIFIKASI DATA,,,"Pembelajaran mesin, pengenalan tulisan tangan, klasifikasi data, machine learning, handwritting recognition, data classification","Faizal Makhruz, S.Kom, M.Sc, Ph.D",2,3,0,2018,1,"Penelitian ini dilakukan untuk mencari tahu pengaruh dari klasifikasi data dalam
proses pengenalan data tulisan tangan manusia. Sebelum data gambar tulisan
tangan dilakukan proses pengenalan, data tersebut diklasifikasikan menjadi 4 kelas.
Data yang telah terklasifikasi kemudian dilakukan proses pengenalan dengan menggunakan
metode pengenalan yang terpisah untuk masing-masing kelas. Asumsi awalnya
adalah dengan mengurangi jumlah varian data dalam satu proses pengenalan, akan
meningkatkan akurasi pengenalan per kelas, sehingga meningkatkan juga akurasi
umumnya.

Percobaan ini dilakukan dengan metode pengenalan backpropagation neural
network dan metode klasifikasi baru yang diusulkan. Metode klasifikasi ini menggunakan
perhitungan pixel aktif dalam sebuah gambar tulisan tangan untuk kemudian
diklasifikasikan. Model ini dicoba untuk mengenali dataset EMNIST yang berisi data
gambar tulisan tangan (0-9, A-Z, dan a-z). Untuk melihat pengaruh dari klasifikasi
data, maka model pengenalan tanpa klasifikasi juga dibuat dalam penelitian ini. Kedua
model pengenalan yang dibuat dirancang dengan arsitektur dan pengaturan yang
sama agar menciptakan perbandingan yang setara.

Hasil akhir penelitian ini membuktikan bahwa klasifikasi data dapat mempengaruhi
akurasi pengenalan data. Model pengenalan dengan klasifikasi data menghasilkan
akurasi umum tertinggi mencapai 75% sedangkan model pengenalan tanpa
klasifikasi menghasilkan akurasi tertinggi 64%. Berdasarkan hasil tersebut, model
pengenalan tulisan tangan dengan klasifikasi data lebih unggul kurang lebih 10%
dalam segi akurasi. Dalam segi waktu pelatihan, model baru ini tentunya lebih membutuhkan
waktu lama dikarenakan lebih banyaknya neural network yang digunakan.

Kata Kunci : Pembelajaran mesin, pengenalan tulisan tangan, klasifikasi data.","This research was conducted to find out the influence of data classification in
the process of human handwritting data recognition. Before the process of recognition,
the data is classified into 4 classes. The classified data then processed by using a
separate recognition method for each group. The initial assumption is that by reducing
the number of data variants in an recognition process, it will increase the accuracy
of recognition per class, thus increasing general accuracy as well.

This experiment was conducted with backpropagation neural network method
and proposed new classification method. This classification method uses active pixel
calculations in a handwritten characters image to be classified. This model is attempted
to recognize EMNIST datasets containing handwritten image data (0-9, AZ,
and a-z). To see the effect of data classification, an unclassified data recognition
model was also made in this experiment. Both of the recognition models are designed
with the same architecture to create equal comparisons.

The final result of this experiment proves that the data classification can affect
the accuracy of the data recognition. The recognition model with data classification
yielded the highest general accuracy of 75% while the unclassified recognition model
yielded the highest accuracy of 64%. Based on these results, handwriting recognition
model with data classification is more or less 10% better in terms of accuracy. In
terms of training time, this new model is certainly more time-consuming due to the
more neural network used.

Keywords : Machine learning, handwritting recognition, data classification.",,,110632,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158605,,ADHI MAULANA ARIAWAN,Prediksi Hasil Pertandingan Dota 2 Menggunakan Jaringan Syaraf Tiruan,,,"Backpropagation, e-Sport, The International, Turnamen","Faizah, S.Kom., M.Kom.",2,3,0,2018,1,"Mengolah data dengan menggunakan algoritma Jaringan Syaraf Tiruan (JST) merupakan salah satu tantangan tersendiri untuk dapat memprediksi turnamen e-Sport dengan nilai akurasi yang tinggi. Beberapa metode telah dikembangkan sebelumnya untuk memprediksi turnamen, namun metode-metode tersebut sebagian besar hanya menggunakan fitur tunggal yang ternyata masih kurang efektif.  Kondisi ini merupakan salah satu masalah yang harus diatasi dalam pengembangan JST. Penelitian ini dilakukan sebagai usaha untuk mendapatkan prediksi kemenangan dari turnamen seakurat mungkin dengan cara membuat sistem prediksi turnamen Dota 2 The International menggunakan JST Backpropagation.
	Proses pelatihan dan pengujian menggunakan dataset dari turnamen-turnamen sebelumnya yang terdiri dari 122 neuron input dan 2 neuron output, 499 sampel pertandingan pada tahap pelatihan, dan 25 sampel pada tahap pengujian. 
Tujuan pengujian sistem ini dilakukan untuk mengukur akurasi prediksi dari hasil disetiap pertandingan. Hasil pengujian yang menunjukkan akurasi terbaik berada pada nilai learning rate 0,01, dan jumlah neuron pada hidden layer 1000 neuron. Sistem ini dapat memprediksi hasil pertandingan dengan akurasi sebesar 65,86%.","Processing data using Artificial Neural Network (ANN) is one of the major challenges to predict e-Sport tournaments with high accuracy. Several methods have been previously developed to predict the tournaments but those methods mostly use only a single feature which is still not very effective. This situation is one of the problems that must be addressed in the development of ANN. This research was conducted in an effort to predict Dota 2 tournament The International as accurately as possible by using ANN Backpropagation. 
The training and testing process uses datasets from previous tournaments consisting of 122 units of input and 1 unit of output. The study used 499 sample matches on training stage, and 25 samples on testing stage. 
System testing is performed to measure the accuracy of the predicted outcome of each game. Test results that show the best accuracy are at the value of 0.01 learning rate, and 1000 neurons inside the hidden layer. This system can predict match results with an accuracy of 65,86%.",,,109534,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
154766,,SITI RINZHANI GHUSTY PERTIWI,PERBANDINGAN METODE K-NEAREST NEIGHBOR DAN SUPPORT VECTOR MACHINE DALAM ANALISIS SENTIMEN TWITTER TERHADAP STASIUN TELEVISI BERITA INDONESIA,,,"Analisis Sentimen, Twitter, TF-IDF, Support Vector Machine (SVM), K-Nearest Neighbor (K-NN), K-fold Cross Validation","Anny Kartika Sari, S.Si, M.Sc., Ph. D",2,3,0,2018,1,"Konten berita merupakan salah satu topik yang paling digemari warganet, dengan persentase sebesar 96,4% atau berjumlah 127,9 juta pengakses. Stasiun televisi berita adalah salah satu penyaji konten berita terbesar di Twitter. Masyarakat diharapkan dapat memilih penyaji konten berita yang kredibel dan menghindari yang memiliki image buruk. Salah satu tolak ukur untuk mengetahui image suatu televisi berita adalah melihat opini yang diberikan masyarakat terhadap stasiun televisi berita tersebut. Namun karena Twitter tidak dapat secara otomatis memberikan kesimpulan sentimen masyarakat terhadap stasiun televisi berita yang ada, maka perlu dilakukan analisis sentimen.

Penelitian ini akan membangun model analisis sentimen Twitter pada stasiun televisi berita Indonesia populer, yaitu Metro TV, Kompas TV, dan TV One, serta membandingkan nilai akurasi akhir dan waktu proses dari metode klasifikasi yang dipakai untuk mencari tahu metode mana yang memiliki performa yang paling baik. Model analisis sentimen dibangun menggunakan metode pembobotan TF-IDF dengan dua metode klasifikasi, yaitu Support Vector Machine dan K-Nearest Neighbor. Evaluasi model dilakukan dengan metode 5-fold cross validation dan 10-fold cross validation.

Metode Support Vector Machine memiliki tingkat akurasi yang lebih tinggi dibandingkan dengan metode K-Nearest Neighbor, yaitu sebesar 76,9% pada pengujian tanpa k-fold cross validation dan 76,8% pada pengujian dengan 5-fold cross validation dan 10-fold cross validation. Metode K-Nearest Neighbor memiliki waktu proses yang lebih cepat dibandingkan dengan metode Support Vector Machine, yaitu dalam kisaran 0,04687 detik pada pengujian tanpa k-fold cross validation dan dalam kisaran 0,1562 detik untuk 5-fold cross validation dan rentang 0,28 hingga 0,3 detik untuk 10-fold cross validation.","News content is one of the most popular topics for netizens with 96,4% or 127,9 million accessors. News TV station is one of the biggest providers of news content on Twitter. People are expected to pick credible news content providers and avoid the bad ones. One of the tools to find out the image of news TV stations is to see opinions from people toward them. But since Twitter can't automatically conclude the sentiments for these news TV stations, sentiment analysis is needed to determine the sentiments from people toward these news TV stations.

This research build a Twitter sentiment analysis model on Indonesia's popular news TV stations, Metro TV, Kompas TV, and TV One. The final accuracy score and running time will be compared to find which classification methods do their job the best. This research use TF-IDF method for term-weighting and use two kinds of classification methods, Support Vector Machine dan K-Nearest Neighbor. Model evaluation is conducted using 5-fold cross validation dan 10-fold cross validation.

Support Vector Machine has better performance compared to K-Nearest Neighbor. Testing without k-fold cross validation generates highest accuracy score 76,9%, while testing with k-fold cross validation generates highest accuracy score 76,8% with 5-fold and 10-fold cross validation method. K-Nearest Neighbor method has faster processing time compared to Support Vector Machine. Processing time without k-fold cross validation is in 0,04687 seconds range while testing with k-fold cross validation took around 0,1562 seconds for 5-fold cross validation method and in range of 0,28 to 0,3 seconds for 10-fold cross validation.",,,105805,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158094,,PRAWIRA JALU NINDITA,CONVERSATIONAL AGENT BERBAHASA INDONESIA PADA SISTEM PAKAR BERBASIS ATURAN UNTUK DIAGNOSIS PENYAKIT PARU-PARU,,,"conversational agent,natural language processing,pattern matching,sistem pakar,forward chaining,kecerdasan buatan","Aina Musdholifah, S.Kom, M.Kom, Ph.D.",2,3,0,2018,1,"Dalam bidang kesehatan, interaksi langsung dengan pakar merupakan metode yang terbaik untuk mendapatkan informasi kesehatan, namun terdapat beberapa masalah seperti terbatasnya waktu pakar untuk melayani tiap pasien.
Conversational agent merupakan teknologi dalam bidang kecerdasan buatan yang memungkinkan interaksi antara mesin dengan manusia menggunakan bahasa alami.
Kombinasi conversational agent dengan sistem pakar dapat memberikan akses langsung dan non-linier ke pengetahuan pakar dan memiliki potensi untuk menjangkau porsi yang lebih besar dari populasi, menghasilkan dampak yang lebih besar.
Pada tugas akhir ini dikembangkan conversational agent berbasis teks berbahasa Indonesia untuk sistem pakar diagnosis penyakit paru-paru dengan pendekatan pattern matching untuk mengekstrak input dari pengguna dan sistem pakar rule-based dengan metode inferensi forward-chaining untuk menghasilkan diagnosa. Diperoleh hasil dari pendekatan pattern matching untuk menangkap keluhan-keluhan pengguna dari 160 data tanya jawab pasien dan dokter mencapai lebih dari 80%.","In healthcare, direct interaction with experts are the gold standard for acquiring health information, but there are some problems like the lack of time for experts to tend for every patient properly.
Conversational agent is a technology in artificial intelligence that enables machine and human interaction through natural language. The combination of conversational agent and expert system can give users direct access and non-linear knowledge of the experts and has the potential to reach bigger portion of the population, giving bigger impact in the process.
In this research, text-based conversational agent for pulmonary disease expert system with pattern matching for extracting input from users and rule-based expert system with forward-chaining inference was developed. The result is as follows: pattern matching for extracting users complains from 160 data sets of question and answer between patients and doctors reached more than 80% accuracy.",,,109029,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
154767,,KURNIAWAN DWI SEPTIAN,ANALISA PERFORMA PADA CLUSTER RASPBERRY PI DENGAN IMPLEMENTASI MPICH DAN APACHE HADOOP,,,"Raspberry Pi, Cluster Computing, MapReduce, MPICH, Hadoop, Inverted Index","Arif Nurwidyantoro, S.Kom., M.Cs",2,3,0,2018,1,"Semakin berkembangnya teknologi juga mendorong semakin tingginya kebutuhan komputasi. Dari sana muncul suatu paradigma yang bernama komputer cluster. Kendala yang dihadapi adalah mahalnya komponen yang diperlukan untuk membangunnya. Kehadiran Raspberry Pi yang murah dan hemat daya seakan menjawab kendala untuk pembangunan komputer cluster. Oleh karena itu pada penelitian ini dicoba untuk membangun komputer cluster menggunakan Raspberry Pi.
Dari beberapa framework komputer cluster yang cukup populer dan sering digunakan, terdapat dua framework yang sering dibandingkan, yaitu MPICH dan Apache Hadoop. Pada penelitian ini ingin dibangun komputer cluster menggunakan kedua framework tersebut di atas Raspberry PI versi 2 dan membandingkannya dengan cara menjalankan program sederhana yaitu Inverted Index.
Dari hasil penelitian yang telah dilakukan dengan menggunakan parameter penggunaan CPU, memory, cache dan waktu eksekusi pada besaran file serta jumlah node yang berbeda, didapatkan kesimpulan bahwa Apache Hadoop tidak terlalu cocok untuk diimplementasikan sebagai framework komputer cluster dengan Raspberry Pi versi 2 pada kasus Inverted Index. Hal itu dikarenakan kebutuhan memory dari Apache Hadoop yang cukup tinggi, sedangkan spesifikasi Raspberry Pi versi 2 masih cukup terbatas.","The development of technology also encourages the increasing demand for computing power. From there emerge a paradigm called cluster computing. Constraints faced is the expensive component needed to build it. The presence of Raspberry Pi as a cheap and power efficient computer answers the obstacles for the construction of cluster computers. Therefore the author tries to build a cluster computer using Raspberry Pi. Of the several popular and often used cluster computer clusters, there are two frameworks that are often compared which is MPICH and Apache Hadoop. This research wants to build a cluster computer using both framework above Raspberry Pi version 2 and compare it with running a simple program called is Inverted Index.
From the results of research that has been done by using the parameters of CPU usage, memory, cache and execution time on the amount of files and the number of different nodes, it is concluded that Apache Hadoop is not very suitable to be implemented as a cluster computer framework with Raspberry Pi version 2 on Inverted Index . That's because the memory needs of Apache Hadoop is quite high, while the specification Raspberry Pi version 2 is still quite limited.",,,105809,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
155539,,FELIX GIOVANNI VIRGO,Analisis Sentimen Dan Deteksi Buzzer Dalam Prediksi Pilkada Dki Jakarta 2017,,,"Twitter, prediksi pilkada, analisis sentimen, deteksi buzzer","Isna Alfi Bustoni, S.T., M.Eng.",2,3,0,2018,1,"Twitter adalah layanan microblogging yang memiliki lebih dari 500 juta pesan setiap harinya. Beberapa penelitian memanfaatkan Twitter untuk memantau reaksi orang-orang dalam aktivitas politik, seperti debat dan kampanye. Penelitian-penelitian tersebut mengklaim bahwa dengan menggunakan Twitter sebagai sumber data, prediksi pemilu dapat dilakukan. Penelitian ini menyajikan sebuah pendekatan untuk memprediksi hasil Pilkada DKI Jakarta 2017 dan menganalisis pengaruh tweet yang dihasilkan oleh buzzer dalam prediksi pilkada tersebut. Pertama-tama, data di Twitter dikumpulkan selama masa kampanye. Kedua, deteksi buzzer otomatis dilakukan pada data Twitter untuk menghapus tweet yang dihasilkan oleh bot komputer, pengguna yang dibayar, serta pengguna fanatik yang biasanya akan menjadi noise dalam data. Ketiga, analisis sentimen dilakukan untuk menentukan polaritas sentimen untuk masing-masing tweet yang terkait dengan masing-masing kandidat. Dan akhirnya, untuk memprediksi hasil pilkada, jumlah tweet yang mendukung masing-masing kandidat sebelum dan sesudah dilakukan proses deteksi buzzer dibandingkan untuk mendapatkan persentase suara dukungan antar kandidat.
Percobaan ini menunjukkan bahwa penggunaan hanya sentimen positif dikombinasikan dengan teknik deteksi buzzer mampu menghasilkan prediksi terbaik dengan tingkat kesalahan 0,91%, lebih baik dibandingkan hasil prediksi yang dilakukan oleh beberapa lembaga survei independen. Penggunaan hanya sentimen positif dalam prediksi pilkada menunjukkan hasil terbaik dengan tingkat kesalahan rata-rata 1,98%. Dari penelitian ini ditemukan bahwa deteksi buzzer dapat mengurangi tingkat kesalahan hasil prediksi pilkada dengan rata-rata penurunan sebesar 0,99%. Selain itu, ditemukan juga bahwa buzzer cenderung menghasilkan tweet yang mendukung kandidat pilihan mereka lebih banyak dari tweet yang menjelekkan lawan mereka.","Twitter is a microblogging service that has more than 500 million messages on a daily basis. Many studies have been utilizing Twitter to monitor people reactions in political activities, such as debates and campaigns. The studies claim that by using Twitter as the main resource, an election prediction can be made. This research presents an approach for predicting the results of 2017 Jakarta Gubernatorial Election and analyzing the influence of tweets generated by buzzers in the election prediction. Buzzer detection was performed to remove those tweets generated by buzzers that usually become noise in the data. Sentiment analysis was performed to assign sentiment polarity for each tweet related to each candidates. To predict the election results, the number of tweets that support each candidate before and after buzzer detection were compared to get the percentage of supporting votes among candidates.
The experiment shows that the use of positive-sentiment-only combined with buzzer detection technique was capable of producing the best prediction with an error rate of 0.91%, which is better than the prediction results published by several independent survey institutions. The use of positive-sentiment-only in the election prediction showed the best result with an average error rate of 1.98%. From this research it was found that buzzer detection can reduce the error rate of election prediction result with the average decrease of 0.99%. Moreover, it was also found that the buzzers tend to produce tweets that support the the candidate of their choice more than tweets that vilify their opponent.",,,106614,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159636,,SITI HARDIYANTI,PERBANDINGAN DISTANCE BASED SIMILARITY MEASURE PADA ALGORITMA RABIN KARP UNTUK MENGHITUNG KEMIRIPAN TEKS,,,"Dice Coefficient, Cosine Similarity, Jaccard Coefficient, Rabin Karp","Anifuddin Azis, S.Si. M Kom.",2,3,0,2018,1,"Metode untuk mengukur kemiripan teks dapat digunakan untuk pengelompokan dokumen, penilaian tes objektif, dan deteksi plagiarisme. Salah satu metode untuk mendeteksi kemiripan teks adalah string matching. Metode string matching yang digunakan pada penelitian ini adalah Rabin Karp.
Tahap pengukuran similarity teks pada Algoritma Rabin Karp
menggunakan distance based similarity measure. Penelitian ini membandingkan beberapa metode distance based similarity measure yang diimplementasikan bersama dengan algoritma Rabin-Karp, yaitu Dice coefficient , Cosine similarity, dan Jaccard coefficient.
Hasil penelitian menunjukkan bahwa dari ketiga distance based similarity measure yang digunakan, Cosine Similarity memiliki akurasi paling tinggi yaitu sebesar 83,2% dengan k-gram = 2.","Methods for measuring text similarity can be use for document clustering, short answer grading, and plagiarism detection. One of method for detecting text similarity is string matching. The current string matching method is Rabin Karp.
The measurement stage of text similarity in Rabin Karp Algorithm uses distance based similarity measure. This research uses some distance based similarity measure which is implemented along with Rabin-Karp algorithm, that is Dice coefficient, Cosine similarity, and Jaccard coefficient.
The results showed that from three different distance based similarity measure, Cosine Similarity was the highest accuracy of 83.2% with k-gram = 2.",,,110595,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
155545,,IDA NOVINDASARI,Klasifikasi Jawaban Pertanyaan Uraian Menggunakan Convolutional Neural Network,,,"Klasifikasi Teks, Jaringan Saraf Konvolusi, Koreksi Jawaban Pertanyaan Uraian, Vektor GloVe","Mardhani Riasetiawan, M.T, Dr.",2,3,0,2018,1,"Pemberian tipe pertanyaan uraian dalam evaluasi pembelajaran dianggap lebih efektif daripada tipe pertanyaan yang lain. Akan tetapi, proses koreksi dari jawaban pertanyaan uraian membutuhkan waktu yang lama. Oleh karena itu, diperlukan sistem koreksi jawaban pertanyaan uraian menggunakan klasifikasi teks sehingga proses koreksi dapat dilakukan lebih cepat. Penelitian ini memfokuskan  penggunaan representasi vektor GloVe dan metode Convolutional Neural Network untuk melakukan klasifikasi jawaban pertanyaan uraian berdasarkan label yang telah diberikan. Dataset yang digunakan berasal dari Kementerian Pendidikan dan Kebudayaan yang berjumlah 2408 jawaban. Beberapa variasi dari parameter model digunakan untuk mengetahui pola dari model apabila diterapkan pada permasalahan yang diangkat serta dataset yang digunakan. Akurasi yang dihasilkan dari model dengan evaluasi 5-fold cross validation adalah sebesar 89,36%. ","While considered to be more effective method to measure student's performance, evaluating description question's answer from many students is a tiresome task. Hence, such automatic system that is based on text classification need to be developed in order to return the evaluation result to students in shorter time.  This research focused on utilization of GloVe word vector representation and Convolutional Neural Network to classify student's answer on description question based on given solution. Study was done on a set of students' answer from Indonesian Ministry of Culture and Education, 2408 answer in total. Variation on several hyper parameter is done to conclude the model's behaviour on this kind of problem and dataset. Our final model scores 89,36\% accuracy on 5 fold cross-validation.",,,106645,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
165530,,FERDINAND WINSTEIN,Pemodelan Petir Berdasarkan Dielectric Breakdown Model dengan Metode Interpolasi Fraktal,,,"Lightning modeling, fractal interpolation, video processing","Janoe Hendarto, Drs., M. Kom",2,3,0,2018,1,"Petir adalah fenomena alam yang sering digunakan pada dunia film hingga sekarang. Penyisipan petir pada video bukan perkara yang mudah. Permasalahan yang muncul yakni risiko pemeran tersambar petir ketika proses pengambilan gambar ataupun kamera yang mahal untuk mengambil video petir asli. Hal ini mendorong peneliti komputer grafis untuk memodelkan petir agar dapat menyelesaikan permasalahan tersebut. Fraktal adalah metode grafika komputer yang sering digunakan untuk memodelkan obyek alam dengan tingkat ketelitian yang tinggi. Salah satu metode pemodelan fraktal yang dapat menghasilkan grafik kontinu seperti petir adalah interpolasi fraktal. 
Penelitian ini bertujuan untuk melakukan pemodelan petir yang realistis dengan metode utama interpolasi fraktal. Selain itu, petir yang dimodelkan dapat disisipkan sebagai animasi pada suatu video.
Berdasarkan hasil pengujian yang dilakukan, berhasil dikembangkan suatu pemodelan petir dengan metode interpolasi fraktal dimana atraktor tercapai pada iterasi keempat dengan nilai dimensi fraktal D = 1,4. Model yang dikembangkan mampu menyisipkan petir yang dimodelkan sebagai animasi pada suatu video.","Lightning is a natural phenomenon that is often used in the movie until now. Lightning insertion on video is not an easy matter. Some problems emerge such as the risk of the actor(s) got struck by lightning when the process of filming. Other problem is the camera which can take the original lightning video is too expensive. Thus prompted computer graphics researchers to model lightning to solve the problem. Fractal is a computer graphics method that is often used to model natural objects with high degree of accuracy. Fractal methods that can model continuous line like lightning is fractal interpolation.
This research aims to model realistic lightning using fractal interpolation. Also, the modeled lightning can be used as an animation on a video.
Based on the experiment, A lightning modeling using fractal interpolation where the atractor is reached on the fourth iteration with fractal dimension D = 1,4 is successfully developed. The developed model also capable to insert lightning as an animation on a video.",,,116635,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167070,,GEIYA WIHANADIEPTHA,Personalization on News Search Engine Using Google plus User Profile Data and Elasticsearch,,,"Personalization, Search Engine, Elasticsearch, Google+, Profiling","Mardhani Riasetiawan, M.T., Dr",2,3,0,2018,1,"Personalisasi sudah terbukti menjadi metode yang berguna dalam membantu pengguna dalam menemukan barang dan dokumen yang relevan. Semakin banyak media digital mengimplementasikan beberapa bentuk personalisasi untuk membantu pengguna menemukan berita yang berhubungan. Walaupun banyak pekerjaan dalam bidang personalisasi, mesin pencarian hanya dapat di personalisasi hingga tingkatan tertentu. 
Media sosial juga sudah menjadi pelantar yang digunakan secara luas yang sudah menjadi identitas di dalam dunia virtual. Dalam penelitian ini kita menyelidiki atas kemungkinan untuk membangun personalisasi mesin pencarian yang mendekati waktu nyata dengan data profil pengguna Google+ sebagai sumber personalisasi menggunakan Elasticsearch. Penulis mengumpulkan data artikel berita dari detik.com dan okezone.com. Mesin pencarian menggunakan Elasticsearch, beberapa sub bidang dibuat untuk analyzer yang berbeda, salah satunya adalah Indonesian analyzer yang stem dan menghapus stopwords dari artikel berita. Sub bidang lainnya menggunakan search analyzer yang menerapkan synonym graph filter yang membuat kita dapat menggunakan synonym beberapa kata pada search time. 
Sistem lalu di uji oleh 10 pengguna berbada untuk mengukur presisi. Dalam penelitian ini dapat disimpulkan bahwa membuat personalisasi mesin pencarian menggunakan Elasticsearch dengan data profil pengguna Google+ adalah memungkinkan dan dapat menjadi data awal profil pengguna yang baik untuk personalisasi lebih lanjut. Kinerja dari personalisasi mesin pencarian sudah terukur dan mendapatkan nilai presisi adalah 0,79.","Personalization has proven to be a useful method in helping users find relevant items and documents. More and more digital media implement some form of personalization to help users find related news. Even though there are a lot of work in the field of personalization, search engines are only personalized to a limited degree. Social media are also a widely used platform that has become an identity in the virtual world. 
 In this research we investigate on the possibility of building a near real-time personalized search engine with Google+ user profile data as the source of personalization using Elasticsearch. The author gather news articles data from detik.com and okezone.com. Search engine are using Elasticsearch, multiple sub fields are created for different analyzer, one is Indonesian analyzer which stem and remove stopwords from the news articles. Other sub field are using search analyzer that apply synonym graph filter that allow us to use multiple words synonym at search time. 
The system then tested by 10 different users to measure the precision. From this research are concluded that creating a personalized search engine using Elasticsearch with Google+ user profile data are possible and could be a great starting user profile data for further personalization. The performance of the personalized search engine has measured and have score with precision are 0,79.",,,118224,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157856,,SASKIA KAMILA,FORMALISASI PERATURAN PERUNDANG-UNDANGAN DAN PENERAPANNYA,,,"Peraturan perundang-undangan, formalisasi, Prolog","Drs. Bambang Nurcahyo Prastowo, M.Sc.",2,3,0,2018,1,"Ketidakselarasan atau disharmonisasi peraturan perundang-undangan dapat
mengakibatkan bermacam permasalahan dalam penerapan hukum. Perbedaan interpretasi
peraturan adalah salah satu masalah yang dapat muncul, dari perbedaan
interpretasi dapat terjadi perbedaan penerapan peraturan yang ada, sehingga dapat
mengakibatkan inkonsistensi dalam penegakan hukum. Perbedaan dalam interpretasi
peraturan perlu diminimalisir untuk meningkatkan objektifitas dalam pengambilan
keputusan, sehingga diperlukan formalisasi peraturan.
Prolog dipilih untuk menulis bentuk formal dari peraturan perundang-undangan.
Dengan demikian hasil formalisasi dapat langsung dicobakan dengan Prolog interpreter.
Untuk membentuk sistem pada Prolog diusulkan sebuah teknik formalisasi
peraturan yang dicobakan pada penelitian ini. Formalisasi dilakukan terhadap studi
kasus dan peraturan-peraturan yang digunakan. Pengujian dilakukan dengan membandingkan
keputusan hakim pada studi kasus dengan keluaran pada Prolog.
Dari implementasi teknik formalisasi ditemukan beberapa poin untuk pengembangan
sistem serta teknik formalisasi. Dari formalisasi lima kasus hukum hak cipta
yang digunakan ditemukan adanya penerapan pasal peraturan perundang-undangan
yang berbeda untuk kasus serupa, ditemukan pula pada kasus perdata hukuman yang
diberikan tidak berdasarkan peraturan undang-undang hak cipta yang dilanggar. Hal
ini dapat dijadikan subjek untuk pengembangan dan penelitian lebih lanjut.","Discrepancy or disharmony in laws and regulations could cause various problems
in the application of law. Difference in rules interpretation is one of the problem
that could arise, since it could cause difference in the application of existing rules
which may result in inconsistencies in law enforcement. Difference in rules interpretation
needs to be minimized to improve objectivity in decision making, therefore
rules formalization is needed.
Prolog is chosen to write the formalized form of the laws and regulations, thus
the formalization results will be able to be tested directly using Prolog interpreter. To
form the system on Prolog, the writer proposed a laws and regulation formalization
technique that will be tested in this research. Formalization is conducted on case
studies and rules used in the cases. The test is done by comparing decisions made by
judge in the case studies with the output on Prolog.
There are several points for further development of the system and formalization
technique found from implementation of the technique. From the formalization
of five cases of copyright law used as case studies, it is discovered that there are different
application of articles of laws and regulations for similar cases. The writer
also discovered that on some civil cases, the judge sentenced a penalty that differs
from the rules of the violated copyright law. These discovery can be subject to further
development and research.",,,108791,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158112,,RIZKI LATIFAH HARVIANI,ANALISIS LEXICAL-SYNTACTIC FEATURE UNTUK DETEKSI CYBERBULLYING PADA TWEET BAHASA INDONESIA,,,"text mining, cyberbullying, Lexical-Syntactic Feature, Na&Atilde;&macr;ve Bayes Classifier","Dr. Azhari, M.T.",2,3,0,2018,1,"Kemudahan penyebarluasan informasi dan kebebasan berpendapat pada era digital seringkali tidak dimanfaatkan secara bijak oleh para penggunanya. Hal tersebut memicu terjadinya cyberbullying pada media sosial seperti Twitter. Berdasarkan fakta mengenai kasus-kasus cyberbullying yang terjadi di Indonesia, diperlukan suatu langkah antisipasi dan pencegahan dengan menggunakan suatu model komputasi untuk melakukan deteksi cyberbullying di media sosial, dalam hal ini adalah Twitter. 
Penelitian ini menggunakan metode Lexical-Syntactic Feature untuk melakukan analisis terhadap kemunculan lexical feature dan syntactic feature terkait cyberbullying pada tweet Bahasa Indonesia. Metode Na&Atilde;&macr;ve Bayes Classifier digunakan untuk mengetahui performa metode Lexical-Syntactic Feature dalam melakukan klasifikasi tweet bullying dan tweet normal. 
Hasil pengujian memperlihatkan bahwa metode Lexical-Syntactic Feature dapat melakukan deteksi cyberbullying pada tweet Bahasa Indonesia dengan akurasi sebesar 82.89%. Selain itu, metode Lexical-Syntactic Feature yang ditambahkan sebagai fitur klasifikasi dalam Na&Atilde;&macr;ve Bayes Classifier mampu meningkatkan akurasi sebesar 3.48% dibandingkan penggunaan Na&Atilde;&macr;ve Bayes Classifier tanpa penambahan Lexical-Syntactic Feature. 

","In this digital era, the ease of disseminating information and freedom of expression is often not used wisely by the society. That condition triggers cyberbullying on social media such as Twitter. Based on the fact about cyberbullying cases in Indonesia, it takes an anticipation and prevention step by using a computational model to detect cyberbullying on social media, in this case is on Twitter.
This study uses Lexical-Syntactic Feature method to analyze the presence of  lexical feature and syntactic feature that have related with cyberbullying on Indonesian tweet. Na&Atilde;&macr;ve Bayes Clasisifer method is used to determine the performance of Lexical-Syntactic Feature method in classifying bullying tweets and normal tweets. 
The experimental results shows that the Lexical-Syntactic Feature method can be used to detect cyberbullying on Indonesian tweet with an accuracy of 82.89%. Moreover, the addition of the Lexical-Syntactic Feature method as a classification feature in Na&Atilde;&macr;ve Bayes Classifier training model improves the accuracy of 3.48% over the Na&Atilde;&macr;ve Bayes Classifier training model without the addition of Lexical-Syntactic Feature. 

",,,109038,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167841,,E. ADITYA RADYA W,AUTOMATIC LICENSE PLATE RECOGNITION USING THE YOLO OBJECT DETECTION ,,,"You Only Look Once, Convolution Neural Network, Detection ","Wahyono, Ph.D",2,3,0,2018,1,"You Only Look Once juga dikenal sebagai YOLO adalah algoritma Jaringan Syaraf Konvolusi yang mampu mendeteksi objek dengan melewatkan gambar yang melalui CNN sekali dan memperoleh output kotak batas semua objek dalam gambar. Ini membuat YOLO mampu dijalankan secara real time. Aplikasi waktu nyata mendeteksi plat nomor kendaraan yang lewat.
Dalam penelitian ini, tujuannya adalah untuk mengoptimalkan dan membuat model YOLO yang dapat menghasilkan akurasi tinggi sementara mampu dijalankan secara real time. Menyesuaikan model YOLO, dengan mengoptimalkan dan mengevaluasi parameter hiper seperti jumlah lapisan, filter dan bahkan fungsi penyatuan dan aktivasi yang digunakan.
","You Only Look Once also known as YOLO is a Convolution Neural Network algorithm that is able to detect objects by passing the image thought the CNN once and obtain an output of the boundary boxes of all the objects within the image. This makes the YOLO to be capable of being run in real time. A real time application is detecting license plate of vehicles passing by.   
	In this research, the goal is to optimize and create a YOLO model that can produce high accuracy while being capable to be run in real time. Adjusting the YOLO model, by optimizing and evaluating the hyper parameters such as the number of layers, filters and even the pooling and activation functions used. 
",,,119118,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159396,,NURMAMISPA HANANI,Sistem Pendukung Keputusan Kelompok Berbasis Metode AHP dan WSM untuk Penentuan Kampung Wisata dengan Akreditasi Terbaik,,,"Kampung Wisata, Akreditasi, Sistem Pendukung Keputusan Kelompok, AHP, WSM/ Tourist Village, Accreditation, Group Decision Support System, AHP, WSM",Anifuddin Azis M.Kom.,2,3,0,2018,1,"Dalam upaya pengembangan kampung wisata sebagai salah satu varian baru daya tarik wisata, Dinas Pariwisata Kota Yogyakarta mengadakan akreditasi pada beberapa kampung wisata di Kota Yogyakarta. Dinas Pariwisata Kota Yogyakarta selanjutnya bekerjasama dengan Lembaga Sertifikasi Usaha (LSU) Pariwisata untuk membentuk tim evaluator guna melaksanakan penilaian berdasarkan aspek penilaian akreditasi yang telah distandardisasi. Keterbatasan tim evaluator adalah belum adanya model penentuan bobot aspek penilaian serta model perankingan kampung wisata berdasarkan preferensi banyak ahli. Adapun keterbatasan lain terkait belum adanya teknologi pendukung keputusan kelompok untuk membantu proses akreditasi kampung wisata.

Sistem Pendukung Keputusan Kelompok sebagai solusi permasalahan yang dihadapi dalam pelaksanaan akreditasi kampung wisata dikembangkan pada penelitian dengan menggunakan metode Analytical Hierarchy Process (AHP) dan Weighted Sum Model (WSM). Metode AHP digunakan untuk penentuan bobot masing-masing kriteria dan subkriteria penilaian kampung wisata, kemudian rata-rata geometrik digunakan untuk mengagregasi bobot kriteria dan subkriteria kelompok. Sedangkan, metode WSM diterapkan dalam pembobotan alternatif kampung wisata untuk menentukan urutan peringkat kampung wisata. Metode-metode selanjutnya diimplementasikan pada sistem berbasis web agar dapat diakses secara fleksibel di berbagai perangkat.

Hasil penelitian menunjukkan bahwa metode AHP dan WSM dapat digunakan dalam sistem pendukung keputusan kelompok untuk menentukan kampung wisata dengan akreditasi terbaik dengan memberikan perankingan kampung wisata. Berdasarkan perhitungan bobot kriteria dan subkriteria kelompok dengan metode AHP dan rata-rata geometrik didapatkan bobot kelompok untuk masing-masing subkriteria. Kemudian, berdasarkan perhitungan metode WSM, Kampung Wisata Dipowinatan menjadi kampung wisata dengan akreditasi terbaik. Adapun sistem yang dihasilkan bersifat dinamis dengan dapat menyesuaikan perubahan kriteria, subkriteria dan alternatif kampung wisata yang digunakan dalam perhitungan.
","In an effort to develop a tourist village as one of the new variant of tourist attraction, Dinas Pariwisata Kota Yogyakarta holds accreditation in some tourist village in Yogyakarta. Dinas Pariwisata Kota Yogyakarta then cooperates with Lembaga Sertifikasi Usaha (LSU) Pariwisata to form the evaluator team to carry out the assessment based on the accreditation assessment aspect which has been standardized. The limitations of the evaluator team are the absence of a model of weighting the assessment aspect and the ranking model of the kampung wisata based on the preferences of many experts. The other limitations related to the absence of group decision support technology to assist the process of accreditation of the village tour.

Group Decision Support System as a solution to the problems encountered in the implementation of tourist village accreditation was developed in the research using Analytical Hierarchy Process (AHP) and Weighted Sum Model (WSM) method. The AHP method is used to determine the weight of each criterion and subcriteria for the assessment of a tourist village, then geometric averages are used to aggregate the weight of group criteria and subcriteria. Meanwhile, the WSM method is applied in the weighting of alternative kampung wisata to determine the ranking ranking of the tourist village. Further methods are implemented on a web-based system to be flexibly accessible across devices.

The results show that AHP and WSM methods can be used in group decision support systems to determine the best accredited tourist village by providing tourist village ranking. Based on the weighting criteria and subcriteria of the group with AHP method and geometric mean, subcriteria of tourist attraction on product criteria, subcriteria of kampung wisata on service criteria and subcriteria of human resources on management criteria have higher weight weight than other subcriteria. Then, based on the calculation of WSM method, Dipowinatan Tourism Village becomes the best accredited tourist village. The resulting system is dynamic by being able to adjust the change criteria, subcriteria and alternative kampung wisata used in the calculation.
",,,110347,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158632,,MUHAMMAD AMIEN I,USER ACCESS PREDICTION USING CURSOR MOVEMENT TO REDUCE WEB PAGE LOADING TIME,,,"web prefetching, pergerakan kursor, waktu tunggu halaman web","Drs. Retantyo Wardoyo M.Sc., Ph.D.",2,3,0,2018,1,"Pengunjung website sering meninggalkan halaman jika waktu yang dihabiskan menunggu loading halaman web terlalu lama. Salah satu cara mengurangi waktu tunggu dapat dilakukan dengan memprediksi tautan mana yang akan diklik oleh pengguna lalu men-prefetch-nya di belakang proses. Meski demikian, tidak terdapat laporan pada penelitian sebelumnya tentang performa pada proses ini. Tujuan dari penelitian ini adalah untuk mengevaluasi performa dan total waktu yang dapat disimpan dari prediksi dan prefetching dengan menggunakan pergerakan cursor.
Metode yang digunakan pada eksperimen adalah membuat sebuah lingkaran disekitar cursor dan metode yang lain adalah mengobservasi sebuah garis lurus yang dibuat berdasarkan pergerakan cursor yang cepat dan memperhatikan apabila lingkaran dan garis tersebut bersinggungan dengan tautan. Jendela browser yang tersembunyi dibuka di belakang proses untuk mengalokasiakan halaman web yang terprediksi. Sebuah alat untuk memonitor performa disiapkan guna mengukur penggunaan CPU dan memori selama proses dan sebuah skrip terpasang untuk menangkap total waktu yang dapat disimpan dari proses
Hasil menunjukkan bahwa memprediksi akses pengguna dengan menggunakan pergerakan cursor dapat mengurangi waktu tunggu halanan web yang bergantung pada jumlah prediksi di belakang proses. Satu hal lagi adalah bahwa terdapat peningkatan dan penurunan yang signifikan pada penggunaan CPU dan memori selama proses prefetching. Meski demikian, prediksi akses user menggunakan metode yang telah dijelaskan memberikan nilai presisi yang rendah dimana metode yang digunakan memprediksi banyak tautan yang tidak dibutuhkan dan tidak akan diakses oleh pengguna.
","Web visitors often leave the page if the amount of time user have to spend in the page loading process is too long. One attempt in reducing latency is done by predicting which link will be clicked and prefetch it in the background. However, there has been no report from previous research about the performance during this process. The goal of this work is to evaluate the performance and the amount of time saved from prefetching prediction using cursor movement.
The method used in the experiment is to create a circle around the cursor and the other one is to observe a straight line created from a fast cursor movement and track if the circle and line intersect with any link elements. New hidden windows will be opened in the background to prefetch predicted web pages. A performance monitor is set up to measure the CPU usage and memory and an additional script is installed to capture the amount of time saved from prefetching process.
The results show that predicting user access using cursor movement could save time depend on how many predictions are downloading in the background. Another thing is that there is a significant increase and decrease in CPU usage and memory during prefetching process. However, predicting user next access using the defined method output a low value of precision which means that the methods used predict many unnecessary links that will not be accessed by user.
",,,109549,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159656,,LISA NURFAUZIYYA,DETEKSI SUBSEQUENCE OUTLIER PADA DATA TIME SERIES BERBASIS K-MEANS CLUSTERING MENGGUNAKAN EUCLIDEAN DISTANCE DAN MAHALANOBIS DISTANCE,,,"Time Series, Deteksi outlier, Clustering, K-Means, Mahalanobis Distance","Edi Winarko, Drs. M.Sc., Ph.D",2,3,0,2018,1,"Perkembangan teknologi semakin cepat, berdampak pada bertambahnya jumlah data yang mengakibatkan ledakan data di berbagai bidang khususnya data time series. Hal tersebut tidak lepas dari alasan bahwa pada saat ini banyak proses seperti industri, stasiun pemantauan meteorologi atau pasar saham, menghasilkan data time series yang relevan secara terus menerus. Kondisi ini mendorong adanya usaha untuk
mengolah serta menganalisis data menjadi sebuah informasi yang berguna. Data mining merupakan proses ekstraksi informasi tersembunyi dari kumpulan data yang besar untuk mendapatkan suatu informasi. Ada beberapa metode pada data mining seperti klasifikasi, clustering, prediksi, serta analisis outlier. Metode analisis outlier merupakan metode yang digunakan untuk mengetahui pola yang tidak sesuai dengan perilaku normal pada data. Outlier sendiri merupakan penyimpangan atau keanehan yang terjadi pada suatu data. Adanya outlier dapat menyebabkan perubahan hasil analisis data secara drastis. Oleh karena ini deteksi outlier bertujuan untuk mengindentifikasi suatu pola yang tidak sesuai. Sehingga perlu adanya keputusan yang harus dilakukan ketika outlier terdeteksi. Clustering merupakan salah satu metode yang dapat digunakan untuk melakukan deteksi oulier data time series.
Pada penelitian ini digunakan algoritma K-Means dengan metode Euclidean Distance dan Mahalanobis Distance pada proses pengukuran jarak. Dari hasil pengujian proses deteksi outlier, metode Mahalanobis Distance menghasilkan nilai akurasi dan sensitivitas yang lebih baik dibandingkan dengan Euclidean Distance ketika jumlah cluster semakin banyak dan ukuran subsequence semakin kecil. Dengan karakteristik dataset time series yaitu nilai varians yang dimiliki bervariasi, maka metode Mahalanobis Distance lebih tepat karena mempertimbangkan varians dari data.","Technological developments increasingly affect the rapid increase in the amount of data that resulted in the explosion of data in various fields, especially time series data. This can not be separated from the reason that at present many processes such as industry, meteorological monitoring station or stock market, produces time series data that are relevant continuously. This condition encourages the effort to process and analyze the data into a useful information. Data mining is the process of extracting hidden information from large data sets to obtain information. There are several methods in data mining such as classification, clustering, prediction, and outlier analysis. Outlier analysis method is a method used to find patterns that are not in accordance with normal behavior on the data. Outlier itself is a deviation or peculiarities that occur in a data. The presence of an outlier can lead to drastic changes in the results of data analysis. Therefore this outlier detection aims to identify an unsuitable pattern. So the need for a decision to be made when the outlier is detected. Clustering is one method that can be used to detect oulier time series data.
In this research used K-Means algorithm with Euclidean Distance and Mahalanobis Distance method on distance measurement process. From the results of outlier detection test process, Mahalanobis Distance method yielded better accuracy and sensitivity value compared to Euclidean Distance when the number of cluster is more and the size of subsequence is smaller. With the characteristic of the time series dataset that is variant value owned, the Mahalanobis Distance method is more appropriate because considering the variance of the data.",,,110572,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
156077,,PRABOWO WAHYU SUDARNO,SISTEM PENDUKUNG KEPUTUSAN MULTIKRITERIA MENGGUNAKAN METODE PROMETHEE UNTUK MONITORING DAERAN RAWAN DEMAM BERDARAH (STUDI KASUS : KOTA YOGYAKARTA),,,"DBD (Demam Berdarah Dengue),Sistem Pendukung Keputusan ,Multicriteria Decision Making Android, PROMETHEE, Firebase, google maps, Perangkingan.","Guntur Budi Herwanto, S.Kom.,M.Cs",2,3,0,2018,1,"Demam berdarah merupakan penyakit rutin tahunan yang berada di daerah Indonesia. Hal ini disebabkan oleh beberapa faktor antara lain iklim, faktor lingkungan dan faktor kependudukan. Maka perlu dibuat suatu pemetaan untuk mengidentifikasi daerah yang paling rawan untuk terjadinya wabah penyakit demam berdarah. Pemetaan memerlukan beberapa data dan metode pengambilan keputusan. Maka dari itu perlu integrasi menggunakan firebase, Google Maps API dan metode pengambilan keputusan multikriteria. Salah satu metode yang dapat digunakan untuk perangkingan suatu data mutikriteria adalah metode PROMETHEE.
	Penelitian ini berusaha membangun sistem pendukung keputusan untuk perangkingan pemetaan wilayah rawan DBD di Kota Yogyakarta menggunakan metode PROMETHEE.  Dalam pengembangannya diperlukan pemrosesan data dari sumber untuk memberikan masukkan yang tepat ke sistem.  Aplikasi yang dikembangkan berbasiskan android dimana pengambil keputusan dapat melakukan pemuatan data kriteria dan juga mengatur variabel preferensi yang digunakan untuk dilakukan pemrosesan data menggunakan dengan metode PROMETHEE.
	Hasil keluaran yang diperoleh merupakan rekomendasi berbentuk nilai olahan, peringkat daerah dan visualisasi pemetaan daerah yang rawan terkena wabah DBD. Keluaran ini akan dapat membantu pengambil keputusan untuk mengetahui daerah mana yang paling rawan maupun tidak paling rawan terkena wabah DBD. Dalam penelitian ini didapatkan hasil kriteria lingkungan dan kriteria kependudukan lebih berpengaruh dibandingkan dengan kriteria iklim, dikarenakan ilkim dalam lingkup Kota Yogyakarta relatif sama. Untuk akurasi ketepatan hasil rekomendasi aplikasi bergantung pada konfigurasi variabel promethee yang digunakan.
","Dengue fever is an annual routine disease in Indonesia. This is caused by several factors such as climate, environmental factors and population factors. A mapping is needed to identify the most vulnerable areas for dengue outbreaks. Mapping requires some data and decision-making methods. Next, this system needs integration between firebase, Google Maps API and multicriteria decision-making methods. One method that can be used to rank a data mutikriteria is the PROMETHEE method.

	This research tries to build decision support system for ranking mapping of dengue fever prone areas in Yogyakarta city using PROMETHEE method. In its development required the processing of data from the source to provide proper insert to the system. Applications developed based on android where decision makers can set the data criteria and also set the variable preference used for data processing using PROMETHEE method.

	The results obtained are recommendations in the form of processed values, regional ratings and visualization of mapping areas prone to hit by DHF outbreaks. This output will help decision makers to know which areas are most vulnerable or not most vulnerable to DHF outbreaks. In this study, the result of environmental criteria and population criteria is more influential than climate criteria, because the climate criteria in the scope of Yogyakarta City is relatively the same. For the accuracy of the recommendation result of the application depends on the configuration of the promethee variable used.
",,,107139,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
161970,,VALERY CINDY CLAUDIA,KLASIFIKASI BERITA ONLINE BERDASARKAN JUDUL DAN LEAD MENGGUNAKAN ALGORITMA SUPPORT VECTOR MACHINE,,," klasifikasi berita, support vector machine, berita online "," I Gede Mujiyatna, S.Kom, M.Kom",2,3,0,2018,1,"Seiring berjalannya waktu, informasi yang berupa dokumen, teks atau artikel semakin banyak dihasilkan dan dibutuhkan oleh seluruh kalangan masyarakat. Karena perkembangan teknologi, informasi disampaikan menggunakan sistem berbasis web secara update yang biasa disebut berita online. Berita online yang jumlahnya semakin banyak perlu diklasifikasikan agar mempermudah permbaca untuk menemukan berita pada kategori yang lebih spesifik. Penelitian ini mengklasifikasikan berita olahraga berdasarkan 11 sub kategori yaitu atletik, basket, bulutangkis, formula 1, moto GP, renang, sepakbola, tenis, tinju, olahraga lain, dan non olahraga. 
Klasifikasi berita online berkategori olahraga pada penelitian ini  dilakukan dengan algoritma support vector machine. Judul dan lead berita menjadi bagian spesifik dari berita yang diproses karena sudah mengandung inti dari berita itu sendiri tanpa harus memproses bagian yang lain. Penelitian diawali dengan proses scraping untuk pengumpulan data, labeling, preprocessing, seleksi fitur dengan pembobotan TF-IDF, kemudian klasifikasi dan evaluasi menggunakan Kfold cross validation. 
Klasifikasi berita online berkategori olahraga dengan memproses judul dan lead dari 1117 berita menghasilkan nilai rata-rata akurasi, presisi, recall dan fmeasure sebesar 94,3%, 95,3%, 94,3% dan 94,4% menggunakan 10-fold cross validation dan algoritma support vector machine saat jumlah fitur maksimum sebesar 2000. 
 ","Over time, information in the form of documents, text or articles more and more generated and required by the whole society. Due to technological developments, information is delivered using an updated web-based system commonly called online news. An increasing number of online news needs to be classified to make it easier for readers to find news in more specific categories. This study classifies sports news based on 11 sub categories: athletics, basketball, badminton, formula 1, moto GP, swimming, football, tennis, boxing, other sports, and non sports. 
 
 The classification of sports news online in this research is done with support vector machine algorithm. Title and news leads become a specific part of the news processed because it already contains the core of the news itself without having to process the other part. The research begins with a scraping process for data collection, labeling, preprocessing, feature selection with TF-IDF weighting, then classification and evaluation using K-fold cross validation. 
 
 The classification of sports online news by processing titles and leads from 1117 news resulted in average values of accuracy, precision, recall and f-measure of 94.3%, 95.3%, 94.3% and 94.4% using 10-fold cross validation and the support vector machine algorithm when the maximum number of features is 2000. ",,,112929,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
165819,,MUHAMMAD AMIRUL MUKMININ,KLASIFIKASI FILM BERDASARKAN KATA KUNCI ALUR MENGGUNAKAN MULTI-LABEL K-NEAREST NEIGHBOR,,,"klasifikasi film,klasifikasi multilabel,Multilabel k-Nearest Neighbor,kata kunci alur","Guntur Budi Herwanto, S.Kom., M.Cs.",2,3,0,2018,1,"Besarnya kuantitas film yang dirilis setiap tahun di seluruh dunia memberikan permasalahan baru dalam mengkategorikan film berdasarkan genrenya. Kategori ini bisa diklasifikasikan secara otomastis menggunakan metode yang terdapat dalam data mining. Dikarenakan suatu film bisa mengandung lebih dari satu genre, maka permasalahan klasifikasi film merupakan klasifikasi multilabel. Meskipun telah banyak penelitian sebelumnya yang bergantung pada berbagai atribur dalam film seperti: ringkasan alur, naskah, poster, dan cuplikan, penelitian ini mempertimbangkan penggunaan atribut lain yang belum pernah digunakan yaitu kata kunci alur.  Model klasifikasi film dibuat menggunakan metode Multilabel k-Nearest Neighbor yang didasarkan atas kata kunci alur. Metode ini melatih model dengan cara menghitung probabilitas prior dan probabilitas posterior pada setiap genre untuk kemudian digunakan untuk memprediksi genre film yang belum diketahui genre-nya.

Dua parameter yang dipakai dalam model klasifikasi ini adalah nilai k dalam nearest neighbors (nn) dan jenis perhitungan jarak dalam mengidentifikasi nn. Nilai k berkisar antara 4 sampai 60 dan jenis perhitungan jarak yaitu cosine dan euclidean. Model klasifikasi yang dibuat berhasil memberikan nilai f-measure micro terbaik sebesar 67,16 persen. Nilai ini merupakan kisaran nilai terbaik yang bisa didapatkan oleh model klasifikasi genre film manapun dengan keluaran multilabel sampai saat ini. Didapat bahwa model menghasilkan kinerja terbaik ketika menggunakan nilai k antara 20 sampai 30 dengan perhitungan jarak euclidean.","The large number of movies that released every year around the world provides a new problem for categorizing movies based on their genre. This category can be classified automatically by using the method in data mining. Because of a movie can contain more than one genre, so the problem of movie classification is a multilabel problem. Although there have been many studies used in various attribute films such as: plot summary, manuscript, poster and trailer, this study uses other attributes that have never been used that is plot keyword. The classification film model is made using Multilabel k-Nearest Neighbor method that use plot keyword as the data input. This method trains the model by calculating prior probabilities and posterior probabilities in each genre to predict the genre of unknown film.

Two parameters used in this model are the value of k of the nearest neighbors (nn) and the type of distance calculation in identifying nn. The value of k stands between 4 to 60 and the type of distance calculation is cosine and euclidean. The created classification model successfully gives the best f-measure micro value of 67.16 persentage. This value is a highest number that can be obtained by genre film classification with output multilabel until now. It was found that the model produces the best performance when using a value of k between 20 to 30 with the euclidean distance measurement.",,,116976,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
166331,,YOHANES LAUDA A,KLASIFIKASI PERTANYAAN PADA SISTEM TANYA JAWAB BERBAHASA INDONESIA MENGGUNAKAN NAIVE BAYES CLASSIFIER,,,"Sistem Tanya Jawab, Naive Bayes Classifier, Klasifikasi Pertanyaan","Idham Ananta Timur, S.T., M.Kom.",2,3,0,2018,1,"Penelitian tentang sistem tanya jawab hingga saat ini banyak didominasi oleh Bahasa Inggris. Untuk Bahasa Indonesia, bidang sistem tanya jawab masih jarang dieksplor meskipun Bahasa Indonesia adalah bahasa resmi yang digunakan oleh lebih dari 250 juta orang. Penelitian mengenai sistem tanya jawab berbahasa Indonesia dimulai sejak tahun 2005 dengan rata-rata 2 penelitian per tahun hingga tahun 2017. Penelitian-penelitian tersebut mengambil berbagai pendekatan seperti analisis semantik, case based reasoning, dan machine learning. 
Sistem tanya jawab berbahasa Indonesia pada penelitian ini menggunakan pendekatan machine learning. Metode pendekatan machine learning yang diambil adalah naive bayes classifier (NBC). Pendekatan ini diambil karena memiliki nilai akurasi yang tinggi dalam klasifikasi teks merujuk  pada penelitian sebelumnya. Data yang digunakan dalam penelitian ini adalah pertanyaan yang dikumpulkan melalui platform Google From dengan domain jaringan komputer. Data dibagi menjadi data latih dan data uji dengan proporsi 50% sebagai data latih dan 50% sebagai data uji.
Hasil pengujian menunjukkan bahwa untuk evaluasi pelatihan sistem dengan stemming menghasilkan akurasi jawaban sebesar 100% dan tanpa stemming menghasilkan akurasi jawaban sebesar 96,43%. Kemudian untuk hasil pengujian sistem dengan pertanyaan baru menunjukkan akurasi jawaban sebesar 92,86 persen saat menggunakan stemming dan menghasilkan akurasi sebesar 85,71% ketika tidak menggunakan stemming.
","Research on question answering system has been done mainly in English. For Indonesian, it is rarely explored although Indonesian is official language which used by more than 250 million people. Research on Indonesian question answering system began since 2005 with approximately 2 research every year. That research using many approach such as semantic analysis, case based reasoning, and machine learning.
Indonesian question answering system which developed on this research is using machine learning approach. Machine learning method which used is naive bayes classifier. This approachment have good accuracy based on previous research on text classification. Data which used on this research are gathered from Google Form platform with computer network domain. Data is divided into train data and test data with proportion 50% of train data and 50% of test data.
Evaluation result indicate that systems training work well with 100% accuracy  for system by stemming and 96,43% accuracy for system without  stemming. Then evaluation result for new question yield 92,86% accuracy for system by stemming and 85,71% for system without stemming.",,,117518,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159165,,RAHMAT NUR AZZIS,ANALISIS PENGARUH BANYAKNYA ROUTER DALAM KINERJA SELF-HEALING JARINGAN SENSOR NIRKABEL BERBASIS ZIGBEE,,,"Jaringan Sensor Nirkabel, ZigBee, Self-Healing, Kualitas Layanan","Ahmad Ashari, Drs., M.I Kom. Dr. Techn",2,3,0,2018,1,"Pesatnnya perkembangan teknologi membuat segala sesuatu dimunngkinkan untuk dilakukan secara otomatis atau sedikit melibatkan manusia. Teknologi yang digunakan juga berkembang untuk memenuhi kondisi tersebut. Pada perkembangan teknologi jaringan, sensor nirkabel menjadi salah satu kemajuan yang akan banyak digunakan di masa depan. Salah satu perusahaan yang mengembangkan jaringan sensor nirkabel (JSN) ini adalah Aliansi ZigBee, dengan nama perangkatnya adalah ZigBee. Pada JSN berbasis ZigBee, protokol network layer-nya mempunyai fungsi self-healing. Dengan adanya fungsi self-healing, sebuah simpul dapat mengalihkan rute transmisi dari router yang mengalami kegagalan ke router lain. Kinerja self-healing ini bisa terjadi jika menggunakan topologi mesh dan konfigurasi simpul-simpul yang sesuai.
Pada penelitian ini, dilakukan pengujian adakah pengaruh banyaknya router yang tersedia terhadap kualitas jaringan dan kinerja dari self-healing JSN berbasis ZigBee. Penelitian ini menggunakan 3 skenario untuk membandingkan hasil dari tiap skenario yang berbeda jumlah router-nya dengan jumlah end-device sama yaitu 20 end-device. Pada skenario 1 router yang digunakan berjumlah 12, skenario 2 berjumlah 4 router dan skenario 3 berjumlah 8 router. Setiap skenario disimulasikan mengalami kegagalan sebuah router untuk memicu terpanggilnya fungsi self-healing. Dari hasil simulasi, didapat bahwa skenario 1 dengan router yang jumlahnya lebih banyak dari skenario lain mempunyai kinerja self-healing lebih baik, yaitu mencapai 100% dibandingkan skenario 2 dan 3 walaupun rerata kecepatan gabung tercepat terjadi di skenario 3 yaitu 7 detik.","The rapid development of technology makes things can do automatically or without human. The technology used develop to fullfy the condition. On the development of network technology, wireless sensors become one of the means to be in the future. One company that developed this wireless sensor network (WSN) is the ZigBee Alliance, with the device name is ZigBee. In ZigBee-based WSN, its network protocol layer has a self-healing function. With self-healing, A node can change the route of a router connected to another router. This self-healing can occur when using mesh topology and configuration of appropriate nodes.
In this study, there was a test the effect of many routers in network quality and performance of ZigBee-based self-healing WSN. This study used three scenarios to compare the results of each different number of routers with 20 end devices. In scenario 1 the router is use 12, the 2nd scenario is use 4 routers and 3rd scenario is use 8 routers. Each scenario simulates a router to handle the resumption of self-healing functions. From the simulation result, it is found that scenario 1 with many routers than other scenario has better for self-healing performance, that is equal to 100% compared to another scenarios, although average rejoin times faster is 3rd scenario that is 7 seconds.",,,110130,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159683,,ASTUTI PURWANINGSIH,PERBANDINGAN METODE SUPPORT VECTOR MACHINE DAN K-NEAREST NEIGHBORS UNTUK KLASIFIKASI DATA SENSOR GERAK MANUSIA,,,"Identifikasi gerak manusia, support vector machine, k-nearest neighbors, seleksi fitur","Isna Alfi Bustoni, S.T., M.Eng",2,3,0,2018,1,"Di tengah berkembangnya produktivitas manusia, terdapat 18,2% populasi dunia mengalami disabilitas dan 2,45% populasi Indonesia. Para penyandang disabilitas memerlukan pengawasan menggunakan sistem pengenalan gerak aktivitas manusia yang dibaca dari sensor. Proses pembacaan data dari sensor menjadi sesuatu yang mudah dibaca merupakan sebuah tantangan tersendiri bagi sistem tersebut. Diperlukan adanya algoritma klasifikasi yang memiliki performa tinggi untuk menghasilkan hasil klasifikasi yang akurat.
Penelitian ini memfokuskan penggunaan algoritma SVM dan k-NN sebagai metode klasifikasi data sensor gerak manusia. Sebagai variabel kontrolnya, data dilakukan pra-pemrosesan berupa seleksi fitur dan tidak. Dataset yang digunakan berasal dari situs resmi UCI Machine Learning Repository yang berjumlah 10.929 baris dan 561 variabel. Akurasi yang dihasilkan dari model klasifikasi SVM pada data tanpa seleksi fitur adalah 0,962 dan pada data dengan seleksi fitur adalah 0,948. Sedangkan akurasi yang dihasilkan dari model klasifikasi k-NN pada data tanpa seleksi fitur adalah 0,891 dan pada data dengan seleksi fitur adalah 0.887. Dari hasil pengujian tersebut, dapat dikatakan bahwa metode klasifikasi SVM memiliki performa yang lebih baik untuk melakukan klasifikasi pada data sensor gerak manusia tanpa dilakukan pra-pemrosesan berupa seleksi fitur.","Among the development of human productivity, there are 18.2% of the world's disability population and 2.45% of Indonesia's population. Persons with disabilities require supervision using a system of human activity recognition, which is read by sensor. The process of reading the data from the sensor into something easy to read is something challenging for the system. A high-performing classification algorithm is required to produce accurate classification results.
This study focuses on the use of SVM and k-NN algorithms as the method of classification of human activity recognition. As the control variable, data is pre-processed using feature selection and not. The dataset used is downloaded from the official UCI Machine Learning Repository website, which covers 10,929 lines and 561 variables. The accuracy&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&macr;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&iquest;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&frac12;s result from the SVM model in the non-feature selection data is 0.962 and the data with feature selection is 0.948. While the accuracy generated from the k-NN classification model on non-feature selection data is 0.891 and the data with feature selection is 0.887. From the evaluation, it can be concluded that the SVM classification method has better performance to configure the data sensor without feature selection pra-processing.",,,110626,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132292,,HANIFIA DYONIPUTRI,Dysarthric Speech Classification Using Convolutional Neural Network and Support Vector Machine,,,"Dysarthric Speech, Convolutional Neural Network, Support Vector Machine, Classification","Afiahayati, M. Cs, Ph.D",2,3,0,2018,1,"INTISARI
KLASIFIKASI PENGUCAPAN PENDERITA DYSARTHRIA MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK DAN SUPPORT VECTOR MACHINE
Hanifia Dyoniputri
13/344197/PA/15143
	Dysarthria adalah suatu gangguan neurologis yang membuat orang kehilangan kemampuannya untuk berartikulasi dengan benar. Untuk membantu penderita Dysarthria dalam berkomunikasi dengan orang normal, ASR (Automatic Speech Recognition) tengah diteliti dan dikembangkan. Salah satu basis tahapan membangun suatu ASR adalah proses klasifikasi dan prediksi.
	CNN adalah metode yang terkenal dalam mengenali pola, sedangkan SVM adalah metode klasifikasi yang banyak diacu keunggulannya. Penelitian ini menggabungkan CNN dengan SVM dengan mengganti Softmax classifier pada layer paling akhir CNN dengan SVM. Untuk hasil yang maksimal, dilakukan tuning Hyperparameters seperti Kernel Size, Feature Map, Hidden Unit dan Epoch, dan diimplementasikan menggunakan bahasa Python. 
	Penelitian ini berhasil membuktikan bahwa CNN dengan SVM lebih baik dari CNN dengan Softmax, dengan rata-rata peningkatan akurasi 8%.
","ABSTRACT
DYSARTHRIC SPEECH CLASSIFICATION USING CONVOLUTIONAL NEURAL NETWORK AND SUPPORT VECTOR MACHINE
Hanifia Dyoniputri
13/344197/PA/15143
	Dysarthria is one of neurological disorder that makes people loose their ability to articulate properly. To establish better communication between Dysarthric speakers with normal speakers, ASR (Automatic Speech Recognition) has been researched and developed. One of basic phase to build ASR is a classification process.
	CNN (Convolutional Neural Network) is a well-known method for recognizing pattern, while SVM is a classification method that has been referred for its exelence. This research combined CNN with SVM by replacing Softmax Classifier to SVM. To maximize the result, tuning hyperparameters such as Kernel Size, Feature map, Hidden Unit and Epoch, is conducted and implemented using Python language.
		This research proved that CNN with SVM gave better result compared to CNN with Softmax, with average of accuracy at 8%.
",,,83276,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159430,,ISMA MISDALENI TIARASANI,PENGENALAN ENTITAS BERNAMA PADA ARTIKEL BERITA BERBAHASA INDONESIA MENGGUNAKAN METODE HIDDEN MARKOV MODEL DAN RULE BASED,,,"Named Entity Recognition, Hidden Markov Model, Rule Based","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2018,1,"Proses pengenalan entitas bernama ini dilakukan untuk teks berbahasa Indonesia. Pengenalan entitas bernama merupakan suatu proses pemberian label entitas pada token dalam kalimat. Hal yang menjadi tantangan dalam melakukan pengenalan entitas bernama yaitu sulitnya dalam melakukan analisis secara manual karena menurut Ekbal dan Bandyopadhyay (2007), hal tersebut termasuk dalam open class of expressions yang berarti bahwa ada banyak variasi yang tak terbatas dan ekspresi baru yang akan terus berkembang.
Metode penelitian ini dilakukan dengan menggabungkan Hidden Markov Model dan Rule Based dengan harapan mendapatkan nilai performa sistem yang baik. Sistem dikembangkan dengan memperhatikan fitur kata pada Hidden Markov Model, sedangkan pada Rule Based dengan memperhatikan fitur kelas kata, kontekstual, dan morfologi kata. 
Pada proses pengujian, data yang digunakan yaitu data corpus penelitian Fachri (2014). Hasil evaluasi dari performa sistem dengan menggunakan metode Hidden Markov Model pada observasi kata tersebut menghasilkan nilai precision 18,3% dan akurasi 30,6%. Pada penggabungan metode Hidden Markov Model dan Rule Based dapat menghasilkan nilai precision 63,2% dan akurasi 88,4%. Berdasarkan hasil pengujian tersebut dapat disimpulkan bahwa penggabungan metode Hidden Markov Model dan Rule Based dapat meningkatkan nilai precision 44,9%, dan akurasi 57,8% dibandingkan dengan hanya menggunakan metode Hidden Markov Model.","A named entity recognition process is performed for Indonesian text. The Named Entity Recognition is a process of labeling entities on token in a sentence.The challenge of Named Entity Recognition is that such expressions are hard to analysis manually because in Ekbal and Bandyopadhyay (2007), it is included in the open class of expressions which means that there are an infinite variety and new expressions are constantly being invented. 
This research method is done by combining Hidden Markov Model and Rule Based in hopes of getting good system performance value. This system was developed with attention to the word feature of the Hidden Markov Model, while in Rule Based with respect to word class, contextual, and word morphology features.
In the testing process, the data used is  research corpus data Fachri (2014). The result of system performance evaluation using Hidden Markov Model on word observation resulted precision value 18,3% and accuracy 30,6%. In the merger of Hidden Markov Model and Rule Based method can provide value of precision 63.2% and accuracy 88.4%. Based on the test results, it can be concluded that the merger of Hidden Markov Model and Rule Based method can increase the value of precision 44,9%, and accuracy 57,8%, when compare with only Hidden Markov Model method.
",,,110418,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
161734,,ANGGER DILLAH K L I,STUDI PERBANDINGAN PUSH NOTIFICATION PADA APLIKASI WEB DAN APLIKASI ANDROID,,,"push notification, firebase cloud messaging, payload, user experience, web application, Android","Bambang Nurcahyo Prastowo, Drs., M.Sc.",2,3,0,2018,1,"Push notification telah digunakan secara masif di berbagai perangkat termasuk desktop. Pengembang juga telah menggunakan berbagai layanan notifikasi yang disediakan oleh berbagai penyedia salah satunya google firebase cloud messaging. Namun pengembangan notifikasi pada aplikasi web cenderung lebih sedikit dibandingkan dengan aplikasi native. 
Penelitian ini dilakukan dengan membangun aplikasi pada web dan native kemudian membandingkan waktu proses dalam pengiriman notifikasi dari keduanya. Dalam praktiknya, besar data divariasi hingga mencapai batas dari kapasitas yang dimiliki oleh masing-masing aplikasi. Disamping itu, pengujian dilakukan hingga 100 kali pengiriman notifikasi secara beruntun untuk mengamati adanya penundaan pengiriman, hubungan antara besar variasi data dengan waktu pengiriman, serta berapa lama rata-rata waktu pemrosesan yang dibutuhkan oleh masing-masing aplikasi baik web maupun Android dalam satu kali pengiriman.
Hasil dari penelitian menunjukkan platform Android dan mobile web (chrome Android) mampu menangani 10 baris notifikasi secara langsung, sedangkan desktop web (chrome) hanya dapat menangani 3 baris notifikasi. Notifikasi diterima oleh setiap perangkat dengan mekanisme antrian first in first out. Payload maksimal pada Android mencapai 3810 karakter sedangkan aplikasi web mencapai 3695. Aplikasi web memiliki performa yang memenuhi kriteria sebagai aplikasi berbasis push notification.","Push notification has been used massively in several devices including desktop PC by the web application. Developers also use several services of push notifications, which one is firebase cloud messaging owned by Google. But, the developing of push notification on web applications tend to a little bit less compared with native applications.
This research built with developing a web application and a native Android application. And then comparing the processing time in sending notifications both of web and Android platform. In the experiment, the data payload varied by their size until reaching the limit of the payload's capacity. Besides, this research was done until 100 experiments in sequence for knowing the possibility of delay time, the relation between payload size with send time, and also the mean of running executions time that needed for once of sending the message from the both applications.
This experiment shows an Android platform and mobile web (using chrome mobile for Android) can handle until 10 rows of notification at the same time, then desktop web (using chrome) only handle until 3 rows of notification. When the notifications were handled by a device, it will make a queue using first in first out mechanism. The maximum payload was handled by Android application reach until 3810 characters and 3695 characters for the web application. Performance of web application fulfils the criteria as a push notification based application.",,,112678,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159434,,M MIFTAHUDDIN GHOFUR,STOCHASTIC GRADIENT DESCENT - MULTI LAYER PERCEPTRON UNTUK PERAMALAN NILAI TUKAR MATA UANG RUPIAH TERHADAP DOLAR AMERIKA,,,"SGD, MLP, Peralaman, Nilai Tukar Mata Uang","Agus Sihabuddin, S.Si, M.Kom, Dr",2,3,0,2018,1,"Nilai tukar mata uang suatu negara merupakan indikator penting dalam
perekonomian suatu negara. Nilai tukar yang dapat berubah sewaktu-waktu dapat
memperngaruhi  pengambilan  keputusan  dalam  suatu  negara.  Maka  dari  itu
dibutuhkan solusi untuk meramal nilai tukar dimasa mendatang. Salah satu cara
untuk memprediksi nilai tukar dimasa mendatang adalah dengan menggunakan
jaringan saraf tiruan yaitu Multi Layer Perceptron(MLP). 
MLP merupakan arsitektur JST yang mempunyai satu atau lebih  layer
tersembunyi. Pada penelitian ini, perubahan bobot akan menggunakan algoritma
Stochastic  Gradient  Descent  (SGD).   SGD  berperan  dalam  menemukan  nilai
bobot yang memerikan nilai keluaran terbaik. Penelitian ini menggunakan data
nilai tukar mata uang Rupiah terhadap Dolar Amerika. 
Hasil dari penelitian ini menunjukan model SGD-MLP dapat digunakan
untuk memprediksi nilai tukar mata uang dan hasil yang didapatkan baik. Model
yang paling optimal yaitu dengan learning rate 0.01, masukan  sliding window3
dan jumlah  hidden  unitadalah 1. Hasil penelitian ditunjukan dengan nilai untuk
kerja RMSE, MAE, MAPE dan Dstat dengan masing-masing sebesar 61.992,
43.889, 0.330 dan 61.667%. Dengan demikian model ini telah memenuhi standar
industri untuk model peramalan nilai tukar karena nilai Dstat sudah diatas 60%","The exchange rate of a country's currency is an important indicator in a
country's economy. Changes that may change at any time may affect a country's
decision-making. Therefore a solution is needed to forecast future exchange rates.
How to predict future exchange rates is to use the artificial neural network of
Multi-Layer Perceptron (MLP). 
MLP is an ANN architecture that has one or more hidden layers. In this
study, Stochastic Gradient Descent(SGD) algorithm is the algorithm which used
to  hange the  weight. SGD plays  a  role in  finding the value  of  weights that
characterize the best output. This research uses Rupiah exchange rate data to US
Dollar. 
The results of this study show the SGD-MLP model can be used to predict
the exchange rate of currency and the results obtained well. The most optimal
model is with learning rate 0.01, input of sliding window is 3 and the number of
hidden unit is 1. The research results are shown with values for RMSE, MAE,
MAPE, and Dstat work with 61.992, 43.889, 0.330 and 61.667%, respectively.
Thus this model meets industry standard for forecasting model of exchange rate
because the Dstat value is above 60%.",,,110382,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157389,,KOMANG ARY TEBUANA,IMPLEMENTASI MODIFIKASI ENHANCED CONFIX STRIPPING STEMMER UNTUK BAHASA JAWA KUNO (KAWI),,,"Stemming, Bahasa Kawi, Bahasa Jawa Kuno, Enhanced Confix Stripping, Porter","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2018,1,"Pemrosesan digital dalam mencari informasi pada dokumen teks berbahasa Kawi (Jawa Kuno) memerlukan filtering dengan menggunakan teknik dalam sistem temu balik informasi yaitu Stemming. Penggunaan stemming bertujuan untuk mencari kata dasar yang ada pada sebuah term atau kata berimbuhan dengan mengekstraksi kata dari imbuhan yang melekat pada kata tersebut. 
Stemmer Bahasa Kawi yang dibangun dalam penelitian ini menggunakan algoritma Enhanced Confix Stripping. Algoritma berbasis kamus tersebut diuji pula performanya apabila tanpa pengecekan kamus dan dengan beberapa metode pengecualian aturan pemotongan imbuhan. Selain itu, dibandingkan pula dengan stemmer yang dibangun dengan algoritma Porter. Obyek penelitian yang digunakan adalah kata-kata dalam Bahasa Kawi pada kitab Sarasamuscaya yang berjumlah 860 kata. Data uji diproses dengan menerapkan aturan pembentukan kata berimbuhan (wangun) dalam Bahasa Kawi tanpa aturan sandhi pada masing-masing algoritma stemming.
Keluaran dari stemmer yaitu berupa kata dasar hasil pemrosesan pemotongan imbuhan. Dari penelitian ini, didapatkan bahwa waktu proses dan akurasi bergantung pada metode yang digunakan. Enhanced Confix Stripping memperoleh nilai akurasi yang tertinggi, sedangkan Enhanced Confix Stripping tanpa kamus dengan waktu proses yang paling lambat dan terjadi penurunan akurasi. Dengan dilakukan pengecualian pemotongan pada imbuhan tertentu mampu mendapatkan peningkatan performa pada algoritma ECS tanpa kamus dan Porter untuk Bahasa Kawi.
","Digital processing in searching information on Kawi's (Old Javanese) language document required filtering by using technique in information retrieval that is Stemming. Stemming used to find the root word that exists in a term or word with affixes by extracting words from affixes attached to the word.
The Kawi language stemmer built in this study uses Enhanced Confix Stripping algorithm. This dictionary-basef algorithm also tested for performance if without dictionary checking and with some exception method of affix cutting rule. In addition, this stemmer compared to stemmer build with Porter algorithm. The research object is words with Kawi language in Sarasamuscaya book which amounted to 860 words. The test data is processed by applying the formation (wangun) rules on each stemming algorithm.
The output of stemmer is the root word that the result of affix cutting process. From this research, it is found that process time and accuracy depend on the method used. Enhanced Confix Stripping receives the highest accuracy value, while Enhanced Confix Stripping without dictionary with the slowest processing time and decreased accuracy. With the exception of cutting at certain affixes it is able to get a performance improvement on the stemming algorithm without the dictionary for Kawi Language.
",,,108362,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159187,,AKBAR NAFISA JA'FAR,TOPIC MODELING OF APP REVIEW IN GOOGLE PLAY BASED ON LATENT DIRICHLET ALLOCATION,,," Latent Dirichlet Allocation, topic modeling, topic chain, Jensen-Shannon Divergence, sliding window, Facebook, Google Play","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2018,1,"Beberapa tahun terakhir, aplikasi di Google Play berkembang dengan pesat. Hal ini membantu pengembang aplikasi untuk mendapatkan feedback dari pengguna aplikasi mereka melalui review. Review pada aplikasi berbeda dari review produk lainya. Review pada aplikasi yang populer melebihi kemampuan manusia untuk menyimpulkan sebuah informasi utama di dalamnya. Selebih itu, setiap aplikasi memiliki beberapa pembaharuan pada fitur mereka, sehingga review dapat bervariasi dari waktu ke waktu. Untuk itu dibutuhkan identifikasi topik dari review aplikasi untuk menemukan informasi apa saja yang sering dibahas dalam review pengguna.
Dalam penelitian ini, penulis mencoba menganalisis topik dari review pengguna aplikasi pada Google Play dengan pemodelan topik  Latent Dirichlet Allocation (LDA). Selain itu penulis juga menganalisis hasil topik dari LDA menggunakan Jensen-Shannon divergence dan Sliding Window untuk membantu memahami apa yang dikeluhkan pengguna dari waktu ke waktu. Penelitian ini menggunakan data review dari aplikasi Facebook pada Google Play mulai dari tanggal 3 Desember 2017 sampai 13 Maret 2018.
Berdasarkan penelitian tersebut, pemodelan topik LDA  terbukti mampu untuk mengidentifikasi komplain, bug atau kesalahan pada aplikasi. Hasil eksperimen LDA telah diuji kemudahanya untuk diinterpretasi oleh manusia melalui Word Intrusion Task. Kesimpulan yang didapatkan adalah topic yang dihasilkan LDA dapat diinterpretasi manusia dengan cukup baik. Selain itu, hasil dari analisis topik menggunakan Jensen-Shannon divergence dan Sliding Window berhasil mengidentifikasi masalah jangka panjang, masalah jangka pendek dan pergeseran topik.
","In recent years, apps on Google Play are growing rapidly. It also help app developers to get feedback from their app users through reviews. Review on apps is different from other product reviews. Review on popular applications exceeds the human ability to identify key information therein. Furthermore, each app has several updates on their features, so reviews can vary over time. This requires to identify topics from app reviews to find what information is discussed in the reviews.
In this study, the author tries to analyze the topic of the app reviews on Google Play with Latent Dirichlet Allocation (LDA) topic modeling. Moreover, the authors also analyze the results of the topic of LDA using Jensen-Shannon divergence and Sliding Window to help understand what users complain from time to time. This study use the review data from the Facebook app on Google Play from December 3, 2017 to March 13, 2018.
Based on this research, the LDA topic modeling has proved to identify complaints, bugs or errors in the application. The results of LDA have been tested for its ease to be interpreted by humans through the Word Intrusion Task. The conclusion is the topic produced by LDA can be interpreted human quite well. Moreover, the results of topic analysis using Jensen-Shannon divergence and Sliding Window successfully identified long-term issues, short-term issues and topic shifts.
",,,110182,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
163796,,NOORMAN ARTA WICAKSONO,SISTEM PAKAR UNTUK IDENTIFIKASI PENENTUAN STATUS KESUBURAN HEWAN TERNAK SAPI BETINA DENGAN METODE DEMPSTER SHAFER THEORY ,,,"Expert system, forward chaining, Dempster Shafer Theory, artificial insemination.","Anifuddin Azis, S.Si., M.Sc",2,3,0,2018,1,"Inseminasi buatan menjadi salah satu teknologi dalam reproduksi hewan ternak sapi pada proses perkembangbiakan secara efektif  tanpa harus melibatkan pejantan dalam proses perkawinan. Proses pelaksanaan inseminasi buatan memerlukan waktu yang tepat agar diperoleh hasil kebuntingan yang cepat, sehingga diperlukan identifikasi yang tepat pula. Teknologi identifikasi secara sistem dapat dilakukan secara cepat untuk membantu dalam identifikasi kesuburan hewan ternak sapi betina yang seharusnya dilakukan oleh seorang ahli fisiologi dan reproduksi hewan ternak. 
Sistem pakar adalah sistem komputasional yang meniru kepakaran seseorang dalam suatu bidang spesifik. Pengembangan sistem pada penelitian ini adalah sistem pakar untuk identifikasi kesuburan hewan ternak sapi betina. Sistem ini dibuat dengan metode forward chaining sebagai mesin inferensi dan metode Dempster Shafer Theory digunakan sebagai perhitungan keyakinan untuk masalah ketidakpastian. Sistem dibuat dengan dua jenis user, yaitu pakar fisiologi dan reproduksi hewan ternak dan peternak. Pakar dapat melakukan update pada informasi yang digunakan untuk proses inferensi, sementara peternak hanya dapat melakukan identifikasi. 
Hasil dari penelitian ini menunjukkan bahwa sistem dapat memberikan keputusan identifikasi berupa hasil kategori kesuburan hewan ternak sapi betina dan tingkat kepercayaan. Pengujian data lapangan berupa seluruh input gejala hewan ternak sapi betina terhadap output sistem didapatkan nilai akurasi yang sesuai keputusan pakar sebesar 88%.
","The artificial insemination is one of technologies of cattle reproduction during the effective breeding without involving males of the mating process. In addition, the right time for implementing the artificial insemination is required to get an effective pregnancy result, so that proper identification is needed. Identification technology can be done quickly to assist the fertility identification of female cattle that should be carried out by a physiologist and reproductive female cattle.
An expert system is a computational system which imitate the expertise of a person in the specific field. The system is developed in this study is an expert system to identify the fertility of female cattle. This system is made with the forward chaining method as an inference engine and the Dempster Shafer Theory method is used as confidence calculation to ensure uncertainty problems. The system is made with two types of users, namely experts and breeder. The experts can update the information which used in the inference process, while the breeder can only do identification process.
The expected results of this research is to indicate that the system can provide identification decisions in the form of the fertility category of female cattle and the level of trust. The system is tested by matching all inputs symptoms of female cattle to the system output obtained by comparing identification results from an expert show accuracy 88%.",,,114888,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
166613,,NIKOLAUS ALDO HALIM,PREDIKSI NILAI TUKAR MATA UANG DOLLAR AMERIKA DAN YUAN CHINA TERHADAP RUPIAH MENGGUNAKAN MULTIPLE REGRESSION BERBASIS ALGORITMA GENETIKA MENGGUNAKAN SUKU BUNGA,,,"Genetic Algorithm, Linear Regression, Currency, Prediction,  Algoritma Genetika, Regresi Linear, Mata Uang, Prediksi ","Wahyono, Ph.D",2,3,0,2018,1,"Multiple regression meriupakan sebuah metode numerik yang dapat digunakan untuk melakukan prediksi terhadap sebuah nilai dari sebuah variabel yang bergantung terhadap beberapa variabel lainnya. Pada kasus ini, variabel yang bergantung merupakan nilai tukar mata uang Dollar Amerika dan Yuan China terhadap Rupiah, dan variabel yang mempengaruhinya merupakan tanggal, dan nilai suku bunga. Penelitian ini akan menghitung nilai akurasi sistem prediksi nilai tukar mata uang asing terhadap rupiah menggunakan algoritma genetika menggunakan metode multiple regression. Variabel yang akan digunakan dalam penelitian ini antara lain adalah data nilai tukar mata uang terhadap rupiah pada masa lampau, dan juga nilai suku bunga sebagai faktor eksternal selain data itu sendiri. Penelitian ini akan menggunakan nilai kesalahan Mean Absolute Error (MAE) dan Minimum Squared Error (MSE) dalam penghitungan akurasi sistem yang dibuat. ","Multiple regression is a numerical method that could be used to predict a value from a variable based on the value of two or more variables. In this research, variable predicted is the exchange rates of the United States Dollar and Chinese Yuan to Indonesian Rupiah, and the other variables are the date and the interest rates. This research will calculate the accuracy of the foreign currency to the rupiah prediction system created with genetic algorithm using linear regression. The variables used in this system are the past currency value and the currency interest rate as an external factor besides the past currency value. This research will use Mean Absolute Error (MSE) and Minimum Squared Error (MSE) as the error values to calculate the accuracy of the system created.",,,118372,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159702,,MH RUSHAN FAIZAL J,PENGEMBANGAN CHATBOT DAN WEB SERVICE UNTUK PENCARIAN INFORMASI TIKET PESAWAT SESUAI PREFERENSI PENGGUNA,,,"chatbot, web service, telegram, usability, tiket, API","Khabib Mustofa, S.Si., M.Kom., Dr.tech.",2,3,0,2018,1,"Pembelian tiket pesawat secara online sudah menjadi hal umum Namun dalam proses pencarian ini sering dialami kendala yaitu kerumitan dalam mencari tiket sesuai preferensi pengguna. Preferensi tersebut meliputi waktu, transit, harga dan pilihan maskapai. Pengguna harus menyaring puluhan hingga ratusan tiket yang tersedia secara manual. Padahal proses ini bisa dilakukan dengan lebih cepat jika dilakukan oleh komputer. Maka dibangunlah sistem yang bisa mengakomodir kebutuhan tersebut, sistem berupa chatbot. Chatbot dipilih untuk memudahkan pengguna untuk menggunakan dan berintenteraksi. Chatbot ini dapat memahami masukan dalam bahasa indonesia dan memberikan tiket sesuai preferensi harga, waktu, transit dan maskapai. Setelah dilakukan pengujian, chatbot ini memiliki akurasi sebesar 75% dan nilai SUS (System Usability Scale) di atas rata-rata, yaitu 72.","Buying online airlines tickets is already a common way. However, in this searching process there always a problem, a difficulties to search the ticket as user preferences. This preferences includes time, transit, price, and the airlines. The user should filter a dozens even a hundreds available tickets manually.. Meanwhile, this process can be quickly done by using computer. Therefore, there is a system called chatbot that is built to accomodate the needs. Chatbot is choosen to make the user easily uses and interacts. This chatbot understands bahasa indonesia and can give the ticket that based on user preferences, such as suitable to the price, time, transit place, and airlines preference. After doing a testing, this chatbot have 75% accuracy and the SUS (System Usability Scale) value is above average, 72.",,,110655,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159707,,IBRAHIM ABDILLAH LUBIS,"PERBANDINGAN LRU LINKED-LIST, LRU HEAP DAN KOMBINASI LRU HEAP DAN GDSF SEBAGAI CACHE REPLACEMENT POLICY DI SQUID PROXY SERVER",,,"proxy server, squid, cache, jaringan komputer, cache replacement / proxy server, squid, cache, computer network, cache replacement","Andi Dharmawan, S.Si., M.Cs., Dr. / Anny Kartika Sari, S.Si., M.Kom., Ph.D;Medi, Drs. M.Kom.",2,3,0,2018,1,"Perkembangan teknologi informasi berkembang dengan pesat di saat ini terutama internet. konten internet yang semakin banyak menyebabkan traffic jaringan semakin padat dan membutuhkan bandwidth yang lebih besar pula. Proxy caching memegang peran penting untuk menangani web traffic untuk mengurangi pengunaan internet secara langsung di jaringan komputer terhadap permintaan pengguna. Namun, penggunaan cache replacement policy LRU pada beberapa pustaka belum optimal untuk melakukan proses pergantian data cache yang tersimpan pada local cache. Maka perlu cache replacement policy selain LRU yang tepat digunakan pada proxy server untuk proses pergantian cache data secara optimal. 
Penelitian ini bertujuan untuk membandingkan LRU linked list, LRU heap dan kombinasi algoritma LRU heap dan GDSF untuk memperoleh cache replacement policy yang optimal digunakan. Pengujian dilakukan dengan menjalankan akses website yang telah ditentukan dengan hasil pengujian ditentukan dari log yang dihasilkan oleh squid proxy server. Parameter yang digunakan adalah request hit ratio dan byte hit ratio.
Berdasarkan hasil pengujian, didapatkan bahwa berdasarkan parameter request hit ratio, algoritma LRU linked list menghasilkan nilai lebih baik. Sedangkan pada parameter byte hit ratio, modifikasi algoritma LRU heap dan GDSF lebih baik walaupun nilai yang dihasilkan fluktuatif.","The development of information technology is growing rapidly at this time especially the internet. The increase of internet content causes more crowded network traffic and requires larger bandwidth as well. Proxy caching contains an important role to handle web traffic to reduce the usage of the internet directly on the network computer against user demand. However, the LRU as cache replacement policy for some references is not yet optimum for data cache replacement process stored in local cache. Then the need for a replacement policy cache other than LRU that correctly used on the proxy server for optimal data cache replacement.
This research aimed to compare LRU linked list, LRU heap and the combination of LRU heap and GDSF algorithm to obtain optimum cache replacement policy. Testing is done by running access to the specified website with the test results determined from the log generated by the squid proxy server. The parameters used are request hit ratio and byte hit ratio.
Based on the test results, in the request hit ratio parameter, rate value of LRU Linked List algorithm is better. while the byte hit ratio parameter, modification of LRU heap and GDSF algorithm is better although the value is fluctuated.",,,110670,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
167135,,YANT PARAMA MULYA,KLASTERING DATA TWITTER KEJAHATAN PENCURIAN MENGGUNAKAN METODE k-MODES,,,"k-modes, silhouette coefficient, kejahatan pencurian, twitter","Dr. Azhari SN, M.T.",2,3,0,2018,1,"Survei merupakan metode pengumpulan data yang umum digunakan
dalam proses penelitian. Namun, apabila data yang dikumpulkan berupa kejahatan
pencurian di Indonesia, akan memakan banyak biaya dan waktu yang lama.
Pengumpulan data akan lebih cepat dan murah jika dilakukan dengan crawling
dan menggunakan text mining untuk memperoleh atribut yang berkaitan dengan
pencurian. Selain itu, untuk memperoleh informasi yang maksimal metode
klastering harus disesuaikan dengan bentuk data dari hasil text mining .
Penelitian ini menggunakan data kategorik. Data didapat dengan
memanfaatkan REST API twitter untuk mencari tweet yang berkaitan dengan
kejadian pencurian di Indonesia. Data melalui tahap preproses untuk mendapatkan
4 atribut kategori yaitu kategori waktu dengan 4 sub kategori, hari dengan 7 sub
kategori, objek pencurian dengan 54 sub kategori, dan lokasi dengan 34 sub
kategori. Metode klastering yang digunakan adalah metode k-modes. Metode kmodes merupakan metode modifikasi dari k-mean yang khusus menangani data
kategorik. Selanjutnya, metode silhouette coefficient digunakan untuk
menentukan jumlah klaster yang optimal.
Hasil yang diperoleh dari penelitian ini adalah k-klaster optimal ketika k
bernilai 6 dengan data input sejumlah 3265. Jumlah k tersebut ditentukan dari
nilai akhir silhouette coefficient terbaik dengan nilai 0,1175 dan nilai silhouette
coefficient perklaster yang bernilai positif. Selanjutnya, jumlah iterasi yang
dilakukan dalam proses klastering k-modes sebanyak 3 kali.","Surveys are data collection methods commonly used in the research
process. However, if the data collected in the form of theft crimes in Indonesia, it
will take a lot of time and money. Data collection will be faster and cheaper if
done by crawling and using text mining to obtain attributes related to theft. In
addition, to obtain maximum information, the clustering method must be adjusted
to the data form from the results of text mining.
This study uses categorical data. Data is obtained by utilizing the twitter
REST API to search for tweets related to theft in Indonesia. Data through the
preprocessing stage to get 4 category attributes, namely the time category with 4
sub-categories, days with 7 sub-categories, object theft with 54 sub-categories,
and locations with 34 sub-categories. The clastering analysis used is the k-modes
method. The k-modes method is a modified method of k-mean that specifically
handles categorical data. Then the silhouette coefficient method is used to
determine the optimal number of clusters.
The results obtained from this study are optimal k-cluster when k is 6
with input data of 3265. The number of k is determined from the final value of the
best silhouette coefficient with a value of 0.1175 and the value of the silhouette
coefficient of the cluster is positive. Furthermore, the number of iterations
performed in the clustering process k-modes 3 times.",,,118371,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158950,,FONDA KRISTA N,Algoritme Genetika untuk Mensintesis Respon pada Chatbot AIML Berbasis Generasi,,,"Algoritme genetika, chatbot, retrieval-based chatbot, generation-based chatbot, AIML, entropy, word error rate","Anifuddin Azis, S.Si, M.Kom.",2,3,0,2018,1,"Chatbot adalah program yang dapat mensimulasikan percakapan manusia dalam bahasa alami. Berdasarkan cara suatu respon dihasilkan, ada 2 jenis chatbot, retrieval-based dan generation-based chatbots. Sistem retrieval-based hanya semata-mata memperoleh respon dari basis pengetahuan yang ada, sehingga hanya dapat menghasilkan respon yang umum atau telah ditentukan sebelumnya. Sebaliknya, sistem generation-based memodelkan bagaimana respon dapat dihasilkan untuk mempelajari fitur alami dari basis pengetahuan yang diberikan, sehingga dapat mensintesis teks-teks baru melalui konteks yang belum teramati sebelumnya.
Dalam penelitian ini disajikan suatu generation-based chatbot kombinasi dari komponen retrieval-based dan komponen generation-based. Komponen retrieval-based dikembangkan dengan arsitektur AIML. Sedangkan, komponen generation-based adalah rutin algoritme genetika yang secara generatif dapat mentransformasi teks yang diberikan. Chatbot menghasilkan beberapa respon sesuai spesifikasi AIML sebagai populasi awal dari rutin algoritme genetika. Respon-respon tersebut kemudian dievolusi selama beberapa generasi dengan hanya menggunakan operator mutasi untuk membangkitkan respon-respon baru yang berbeda dari nilai awalnya. Sistem mengevaluasi setiap respon tersebut berdasarkan nilai entropy dan memilih yang tertinggi sebagai respon akhir.
Sistem diuji untuk setiap peningkatan parameter probabilitas mutasi dan generasi maksimum terhadap nilai entropy dan word error rate (WER). Nilai entropy meningkat sebanding terhadap nilai-nilai parameter dengan nilai rata-rata maksimum tercapai 10.81 untuk probabilitas mutasi 5% dan generasi maksimum 20, sebaliknya standar deviasi yang menurun menandakan entropy akan memuncak pada suatu nilai maksimum. Sementara itu, WER meningkat dengan nilai rata-rata maksimum 67.5% untuk probabilitas mutasi 5% dan 20 generasi maksimum, beberapa pengujian bahkan mencapai WER 100%. Hasil pengujian menunjukkan bahwa sistem dapat menghasilkan respon-respon yang lebih random dan lebih beragam dengan setiap mutasi dan generasi yang dilakukan.","Chatbot is a program that could simulate human conversation in natural language. Based on how responses are delivered, there are 2 kinds of chatbots, retrieval-based and generation-based chatbots. Retrieval-based system solely derive its response from an existing knowledge base, as such it would only produce predefined or generic responses. Whereas generation-based system models how the response could be generated to learn the natural feature of a given knowledge base, thus is able to synthesize new texts from unseen contexts.
This paper presents a generation-based chatbot system that is a combination of both retrieval-based and generation-based components. The retrieval-based component is developed using AIML architecture. On the other hand, the generation-based component is a genetic algorithm routine that generatively transforms given texts. The chatbot generates some responses using AIML specification based on a given input and treats them as initial population for the genetic algorithm routine. The responses then evolved several times through generations using only mutation operator to generate new responses that varied from its initial value. The system then evaluate each responses by its entropy and choose the highest one as the final response.
The system was tested on increasing mutation probability and maximum generation parameters to evaluate the responses generated based on the value of entropy and word error rate (WER). The entropy shows steady increase, proportionate to the parameters, with the maximum average attained being 10.81 for mutation probability of 5% and maximum generation of 20, while it's standard deviation declines, signifying that the entropy would be capped at a maximum value. Meanwhile, WER tends to get higher as the parameters went up with the maximum average being 67.5% for 5% mutation probability and 20 maximum generation, moreover, some of the test sample even reach word error rate of 100% which particularly create an entirely new sentence. These result show that the system is able to generate more random and more diverse responses with every mutation and every generation occured.",,,109932,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159462,,ALI MAHFUD,PENGGUNAAN ALGORITMA K-MEANS UNTUK CLUSTERING PRODUK PADA TOKO ONLINE DI INDONESIA,,,"e-commerce, clustering, k-means, scrapping, website, toko online","Azhari SN, Drs., M.T., Dr",2,3,0,2018,1,"E-commerce akhir - akhir ini menjadi cara baru untuk masyarakat berbelanja di Indonesia. Kemudahan dalam hal akses dan pembayaran membuat platform ini menjadi ramai. Hal ini diikuti dengan jumlah penjual yang semakin banyak membangun situs-situs web e-commerce nya sendiri. Salah satu komoditas yang laris di pasar e-commerce adalah komoditas fashion. Di Indonesia terdapat puluhan bahkan ratusan toko yang menjual produk fashion. Hal tersebut membuat pembeli yang ingin berbelanja online memiliki banyak pilihan, namun di sisi lain membuat pembeli harus memilih dari toko mana mereka akan membeli barang yang sesuai dengan kriteria dalam hal harga. 
Dalam penelitian ini dilakukan pengelompokan antar toko online dengan membandingkan harga produk-produk yang dijual. Sebelumnya data-data produk tersebut dikumpulkan dengan menggunakan teknik web scrapping. Pengelompokan produk-produk tersebut menggunakan metode clustering k-means. Penilaian hasil clustering menggunakan metode silhouette coefficient menunjukkan angka berkisar antara 0.52 dan 0.61 yang berarti bahwa hasil clustering dapat dikatakan sukses. Dilakukan 3 kali clustering pada penelitian ini. Proses clustering pertama pada semua produk memperlihatkan adanya toko yang lebih mahal dan toko yang lebih murah. Clustering kedua memperlihatkan bahwa toko tidak selalu lebih murah / lebih mahal pada setiap kategori. Clustering ketiga dapat membagi 3 klaster toko dengan kategori murah dan mahal masing-masing.


","E-commerce has recently become a new trend by Indonesian to shopping and purchasing product. Easiness of access and payment method make this e-commerce platform getting more rapidly grow. Including, number of sellers that build a website to sell their product is also increasing. One of the best-selling commodity in e-commerce is fashion. In Indonesia there are tens even hundreds online store that selling fashion product. Therefore, buyers have many choices where he purchases products. But in the other way it makes buyers giving extra effort to choose and decide online store that sell products meet his criteria in terms of price. 
In this research, we will be grouping between online stores by comparing the price of products that they sell. Before that process is conducted, products data had been gathered using web scrapping technique. The method that is used to group the products is k-means clustering. The assessment of clustering results using the silhouette coefficient method shows the numbers ranging from 0.52 and 0.61 which means that the clustering results can be said to be succeed. In this research, 3 clustering process had been commited. The first clustering process on all products shows the existence of more expensive stores and cheaper stores. The second clustering shows that stores are not always cheaper or more expensive in each category. The third clustering able to create clusters of stores with their own cheap and expensive category.
",,,110437,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157934,,MUHAMMAD NAUFAL ABIYYU,WEB USAGE MINING UNTUK ANALISIS PROFIL NAVIGASI PENGUNJUNG WEBSITE (STUDI KASUS: WEBSITE KARIR.COM),,,"Web mining, Web usage mining, Navigation profile, K-means, FP-Growth","Arif Nurwidyantoro, S.Kom., M.Cs.",2,3,0,2018,1,"Untuk menarik pengunjung, administrator web memerlukan informasi terkait dengan minat pengunjung web. Informasi  terkait minat tersebut bisa didapatkan dari pola navigasi pengunjung. Web usage mining merupakan salah satu metode yang dapat digunakan untuk mendapatkan pola navigasi tersebut. Penelitian ini menggunakan web usage mining untuk mendapatkan profil navigasi pengunjung pada portal lowongan kerja karir.com.
Penelitian ini terdiri dari beberapa tahap. Tahap pertama diawali dengan mengidentifikasi session pada data access log. Langkah selanjutnya yaitu menghitung nilai frekuensi dan interest untuk setiap session. Nilai tersebut digunakan sebagai masukan untuk melakukan clustering dengan algoritma k-means. Setelah terbentuk cluster, dilanjutkan dengan mencari pola akses pengguna pada setiap cluster dengan menggunakan algoritma FP-Growth.
Pada tahap pengujian, dilakukan dengan membandingkan rule yang dihasilkan dengan proses clustering dengan rule yang dihasilkan tanpa menggunakan proses clustering. Perbandingan dilakukan dengan melihat jumlah rules yang terbentuk, rata-rata dan standar deviasi nilai confidence, lift dan conviction. Rules terbaik diperoleh melalui proses clustering dengan menggunakan nilai interest dengan rata-rata nilai confidence = 0,470890, lift = 68,51497 dan conviction = 2,695677 untuk minimum support 0.5% dan nilai rata-rata confidence=0.430818, lift = 29,63833 dan conviction = 2,039723 untuk minimum support 1.0%.
","The web administrator needs related information about the interest of web visitors. That information usually captured from user navigation patterns. One method that can be use it to get user navigation pattern is web usage mining. This research uses web usage mining to get user navigation pattern on karir.com job portal.
This research consists of several stages. The first stage begins with identifying the session in the data access log. The next step calculates the frequency value and interest for each data session.  The value is used as input for K-means clustering algorithm. The next step is to find user access pattern for each cluster using the FP-Growth algorithm.
The testing step is done by comparing the rule of the clustering process with the resulting rule without using the clustering process. A comparison is made by looking at the number of rules, average and standard deviation of confidence, lift, and conviction. The best result is obtained through clustering process by using feature cluster in the form of interest value with the average confidence = 0,470890, lift = 68,51497 and conviction = 2,695677for minimum support 0,5% and average value confidence=0.430818, lift = 29,63833 and conviction = 2,039723 for minimum support 1%.
",,,108871,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
166382,,BILY MUHAMAD FACHRI,Recurrent Neural Network untuk Pendeskripsian Citra Otomatis Berdasarkan Anotasi Gambar,,,"Pendeskripsian Citra Otomatis, Recurrent Neural Network, Long Short-Term Memory, Natural Language Generation","Isna Alfi Bustoni, M.Eng",2,3,0,2018,1,"Penelitian tentang pendeskripsian citra otomatis sudah pernah dilakukan. Pendekatan umum yang dilakukan adalah dengan mendeteksi objek menggunakan teknik computer vision, kemudian dilanjutkan dengan natural language processing untuk mengasilkan kalimat dari objek terdeteksi. Akan tetapi belum banyak penelitian yang hanya berfokus pada proses pembuatan kalimat deskripsi. Penelitian ini berfokus membuat sebuah model pendeskripsian citra otomatis berdasarkan anotasi gambar untuk mengetahui performa algoritma NLP tanpa adanya galat pada saat pendeteksian objek. Recurrent Neural Network LSTM digunakan sebagai model pembuat kalimat deskripsi pada penelitian ini. Pada tahap pengujian, diberikan tiga skenario dengan dan tanpa adanya kalimat konteks. Masing-masing skenario terdiri enam variasi parameter. Hasil kalimat deskripsi otomatis dievaluasi menggunakan ROUGE-1 dan BLEU. Berdasarkan hasil pengujian, nilai ROUGE tertinggi yaitu 0,7219 diperoleh dari model dengan 128 unit tersembunyi dan 64 masukan vektor, dan model lain dengan 256 unit tersembunyi dan 32 masukan vektor menghasilkan nilai BLEU tertinggi yaitu 0,9036.","Research about automatic image description has been done with several different approaches. One of the common approaches is detecting object using computer vision technique, then followed by natural language processing to generate the novel description sentence from an image. However, there is no experiment that only focused on the process of generating sentence. This study focused on constructing an automatic image description model based on image anotation to evaluate the performance of NLP algorithm without any error from the image detection process. Reccurent Neural Network LSTM was used in this study to generate sentence based on image anotation. In the experiment, given three scenarios with or without context sentence. Each scenario consists of six different parameter configurations. The description sentence made by the model is evaluated by ROUGE-1 and BLEU. The result of the experiment shows that the highest ROUGE score is 0,7219 obtained by model with 128 hidden unit and 64 dimension of vector inputs, while another model with 256 hidden unit and 32 demension of vector inputs obtain the highest BLEU score with 0,9036.",,,117519,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
159471,,NATASHA C SANTOSA,ADAPTIVE NEURO-FUZZY INFERENCE SYSTEM FOR PRELIMINARY DIAGNOSIS OF BREAST CANCER,,,"Kanker payudara, neuro-fuzzy, pelatihan, diagnosa, ekstraksi fitur, pengolahan citra ","Retantyo Wardoyo, Drs., M.Sc., Ph.D",2,3,0,2018,1,"Kanker payudara merupakan salah satu penyebab kematian terbesar di dunia. Beberapa penelitian tentang penyakit ini telah dilaksanakan dan beberapa faktor penyebab telah diajukan, namun sebenarnya penyebab utama kanker payudara masih belum diketahui hingga hari ini. Cara terbaik untuk mengurangi angka kematian yang disebabkan oleh kanker payudara adalah dengan memberi pasien-pasien penderita penyakit ini tindakan awal, dan untuk melakukan itu, deteksi dini kanker dipercaya akan dapat sangat membantu. Salah satu cara untuk mendeteksi kanker dalam payudara adalah dengan cara mengidentifikasi benjolan yang berada di dalam hasil mamografi. 
Di penelitian ini, penulis mencoba untuk mendesain sebuah metode yang menggabungkan antara logika fuzzy dan jaringan syaraf buatan untuk mendiagnosa awal benjolan-benjolan yang berada di dalam citra mamogram. Data mammogram yang digunakan dalam penelitian ini sebelumnya diterapkan beberapa tahapan pengolahan citra seperti penghilangan noise, pengurangan latar belakang, transformasi orientasi, dan penghilangan otot pektoral.  Ekstraksi fitur juga dilaksanakan di tahap berikutnya dengan tujuan untuk mendapatkan variabel input untuk proses pelatihan sistem Adaptive Neuro Fuzzy. Dari penelitian ini, disimpulkan bahwa model dapat mendiagnosa citra mamogram cenderung secara  akurat. 

","Breast cancer is one of the leading causes of death all around the world. There have been many researches done to this disease and some factors have been proposed, however the main cause of breast cancer is still unknown up to this day. The best way to decrease the number of deaths by breast cancer is by giving patients early treatment, and to do that, an early detection of the cancer itself is believed to be able to make a big difference. One way to detect cancer within the breast is by identifying lumps that exist within the mammogram results. 
In this research, the writer attempts to design a method that combines fuzzy logics and artificial neural network to preliminary diagnose the lumps within the mammograms. The mammograms which are used go through several image processing steps such as noise removal, background subtraction, orientation transformation, and pectoral muscle removal. Features extraction is also done as the next step in order to gather input variables for the Adaptive Neuro Fuzzy Inference System's training and testing process. It is concluded that the model can diagnose the breast mammograms rather accurately. 
",,,110452,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
156146,,SENSA GUDYA SAUMA SYAHRA,IMPLEMENTASI ATTENTIVE RECURRENT NEURAL NETWORK DALAM PEMBUATAN HEADLINE DENGAN PENDEKATAN ABSTRAKTIF,,,"Attention, RNN, summarization, abstractive, headline, Rouge","Azhari SN, Drs., M. T., Dr.",2,3,0,2018,1,"Pertumbuhan informasi yang sangat pesat menyebabkan kebutuhan untuk menemukan informasi yang relevan dengan cepat. Pencarian informasi yang relevan dari judul berita sudah dilakukan oleh banyak orang. Namun, hasilnya tidak selalu sesuai harapan. Hal ini karena headline tidak selalu merepresentasikan informasi dengan baik. Oleh karena itu, dibutuhkan pembangunan headline yang baik dalam merepresentasikan informasi berita. 

Penelitian ini membangun model summarization untuk menghasilkan headline berita yang baik. Penelitian ini berfokus pada pengembangan model summarization abstraktif dengan menggunakan kombinasi RNN dan mekanisme attention. Data yang digunakan dalam penelitian ini adalah berita dalam bahasa Indonesia dan diambil dari detik.com dengan total 17.862. Untuk mendapatkan model terbaik, pengujian dilakukan dengan memvariasikan nilai parameter. Kualitas ringkasan diukur dengan menggunakan ROUGE-N. 

Berdasarkan pengujian dan pengukuran yang dilakukan, disimpulkan bahwa model summarization terbaik adalah model dengan 128 unit tersembunyi, 32 masukan vektor, dan ukuran beam 1. Model ini menghasilkan ringkasan utama dengan nilai rata-rata F-score yaitu 0,27022.","The very rapid growth of information leads to the need to find the relevant information quickly. Searching the relevant information from title of the news has already been done by many people. However, the result isn't always as they as expect. This is because headlines do not always represent information well. Therefore, it is required a good headline construction representing information of the news. 

This study constructed a summarization model to produce a good news headline. It focuses on the development of abstractive summarization model using the combination of RNN and attention mechanism. The data used in this study is the news in Indonesian language and was taken from detik.com with the total of 17.862. In order to obtain the best model, the testing was performed by variating of parameters values. The quality of summary was measured by means of ROUGE-N. 

Based on the testing and the measurement performed, it was concluded that the best summarization model was one with 128 hidden units, 32 vector inputs, and beam size 1. This model produced a headline summary with average value of F-Score was 0,27022.",,,107206,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157938,,NITA KINANTI,Sinkronisasi Data Rekam Medis pada Aplikasi Rekam Medis Elektronik Berbasis Progressive Web Apps,,,"progressive web apps, rekam medis, Indexeddb, Cache, AJAX, service worker, JSON, PostgreSQL, cloud, sinkronisasi","Medi, Drs., M.Kom",2,3,0,2018,1,"Pengisian rekam medis merupakan salah satu kegiatan wajib di rumah sakit, puskesmas, maupun klinik pribadi. Tuntutan untuk dapat mengisi data rekam medis dimanapun dan dalam kondisi tidak ada koneksi internet semakin tidak terelakkan. Teknologi progressive web apps, yaitu service worker, memungkinkan aplikasi web dapat berjalan secara offline dan menyimpan data input pengguna pada basis data lokal.
Aplikasi web dibuat dengan menggunakan bahasa pemrograman Python dan framework Django untuk kemudian dijalankan pada platform cloud Heroku dengan menggunakan basis data PostgreSQL. Jika tidak ada koneksi internet, data pengguna akan tersimpan pada Indexeddb. Ketika pengguna kembali online, sinkronisasi akan berjalan dengan mengunggah data dari Indexeddb menuju PostgreSQL. Sinkronisasi dilakukan dengan terlebih dahulu mengubah data ke dalam format JSON untuk kemudian dikirim dengan menggunakan AJAX.
Hasil implementasi dari penelitian ini adalah suatu aplikasi rekam medis berbasis progressive web apps yang dapat menyimpan data rekam medis pada Indexeddb secara offline dan melakukan sinkronisasi data rekam medis antara Indexeddb client dengan PostgreSQL server cloud dengan rata-rata tingkat sinkronisasi sebesar 100% saat terdapat koneksi internet.","Writing medical records is one of the obligatory activities in hospitals, health centers, and private clinics. The need to be able to add medical record data anywhere with no internet connection began to rise. There is a solution for this problem; service worker. It is a technology from progressive web apps that allows web applications to run offline and store user input data in a local database.
The web application is built using Python programming language and Django framework. Cloud platform Heroku is used to run the web application online with PostgreSQL as the database. If there is no connection, user&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s data will be stored in Indexeddb. When the user is back online, the synchronization process will begin by converting data in Indexeedb to JSON and uploading data to PostgreSQL on the server using AJAX.
The result of this research is a electronic medical record based on progressive web apps that can store medical record data in Indexeddb when user is offline and synchronize medical record data from Indexeddb to PostgreSQL with average of 100% synchronization rate when the user is back online.",,,108877,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
163062,,RUBILA DWI ADAWIYAH ,IMPLEMENTATION OF FUZZY C-MEANS CLUSTERING AND PROXIMITY-IMPACT-POPULARITY ON USER-BASED COLLABORATIVE FILTERING,,,"Collaborative Filtering, Recommender System, Sparsity, Cold Users, Fuzzy C-Means","Arif Nurwidyantoro, S.Kom, M.Cs",2,3,0,2018,1,"Memory-based Collaborative Filtering (CF) adalah metode sistem rekomendasi yang banyak digunakan karena implementasinya yang mudah. Namun, memory-based CF memiliki permasalahan sparsity, skalabilitas, dan cold-start. Masalah-masalah ini mempengaruhi performa sistem rekomendasi. Penelitian ini mencoba untuk mengatasi skalabilitas dan masalah sparsity pada salah satu tipe memory-based CF yaitu user-based CF dengan mengimplementasikan Fuzzy C-Means (FCM) dan mengatasi masalah cold-start dengan menggunakan Proximity-Impact-Popularity (PIP) sebagai perhitungan similaritas. Ada dua metode defuzzifikasi yang digunakan yaitu Best Cluster dan All Cluster defuzzification. Dataset yang digunakan yaitu MovieLens dataset dengan tingkat sparsity 93,69%.
Hasil dari penelitian ini adalah PIP menghasilkan akurasi dan coverage yang lebih tinggi (MAE = 0,7734 dan coverage = 74,32%) daripada korelasi Pearson (MAE = 0,849588 dan coverage = 47%). Namun, skalabilitas PIP sangat buruk (throughput = 4239,113 rec/sec). Implementasi FCM pada kedua metode defuzzifikasi tersebut meningkatkan skalabilitas sistem dengan throughput yang lebih tinggi (throughput FCM Best Cluster defuzzification=7386,41 rec/sec, throughput FCM All Cluster defuzzification=15552.1 rec/sec). Dalam peningkatan jumlah cold-user (pengguna baru), FCM dengan kedua metode defuzzifikasi juga menghasilkan akurasi dan coverage yang lebih tinggi.
","Memory-based Collaborative Filtering (CF) is a widely used recommender system method due to its easy implementation. However, it suffers from sparsity, scalability, and cold start problems. These problems influence the performance of the recommender system. This research attempted to overcome scalability and sparsity problem on one type of memory-based CF which is user-based CF by implementing Fuzzy C-Means (FCM) clustering and to overcome cold-start problem by using Proximity-Impact-Popularity (PIP) as similarity measure. There are two defuzzification methods namely Best Cluster and All Cluster defuzzification. The system is implemented in MovieLens dataset with sparsity level 93,69 %. 
The result is PIP gave higher accuracy and higher coverage (MAE = 0,7734 and coverage = 74.32%) than Pearson correlation (MAE = 0.849588 and coverage = 47%). However, it still suffers very poor scalability (throughput = 4239,113 rec/sec). The implementation of FCM both defuzzification methods improve the scalability of the system with higher throughput (throughput FCM Best Cluster defuzzification = 7386,41 rec/sec, throughput FCM All Cluster defuzzification = 15552,1 rec/sec). In increasing number of cold users, FCM with both defuzzification methods also result in higher accuracy and coverage.
",,,114087,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
157950,,EMMANOEL P P HASTONO,MODEL CHECKING FOR PARAMETRIC DISCRETE-TIME MARKOV CHAINS,,,"model checking, numerical model checking, statistical model checking, parametric discrete-time Markov chains, IPv4 Zeroconf Protocol, Crowds Protocol","Dr.-Ing. Mhd. Reza M. I. Pulungan, S.Si., M.Sc.",2,3,0,2018,1,"Pengecekan model sistem stokastik, seperti parametric discrete-time Markov chain, dilakukan dengan metode numerik atau statistik. Tujuan dari penelitian ini adalah untuk membandingkan kedua metode perihal waktu eksekusi dan akurasi dan presisi, dan untuk menyarankan penggunaan satu metode di atas yang lain untuk kondisi tertentu.
Pendekatan numerik dan statistik untuk parametric discrete-time Markov chain akan dibandingkan dengan menggunakan pemeriksa model PRISM, dengan IPv4 Zeroconf Protocol dan Crowds Protocol sebagai contoh model parametrik. Program Python dirancang untuk menjalankan beberapa perintah PRISM, mengumpulkan hasilnya, dan menghasilkan data untuk analisis. Pendekatan numerik akan menghasilkan persamaan probabilitas untuk properti dalam hal variabel parametric discrete-time Markov chain, yang kemudian digunakan untuk menghitung nilai probabilitas untuk variabel. Nilai-nilai tersebut kemudian dibandingkan dengan nilai yang dihasilkan dari pengecekan model statistik untuk menganalisis akurasi, presisi, dan kinerja. Hasil pendekatan statistik dihasilkan menggunakan selang kepercayaan.
Hasil eksperimen menunjukkan bahwa metode numerik memiliki waktu eksekusi yang lebih cepat dibandingkan dengan metode statistik untuk verifikasi IPv4 Zeroconf Protocol, yang dianggap sebagai sistem kecil dan sederhana. Namun, untuk Crowds Protocol, model yang besar dan kompleks, waktu yang dibutuhkan untuk pengecekan model numerik meningkat secara eksponensial, membuatnya tidak efektif dalam waktu dan memori. Selain itu, membandingkan hasil statistik dengan yang numerik, akurasi untuk hasil statistik ditentukan menjadi rendah untuk nilai probabilitas mendekati 0, dan ketepatan untuk hasil statistik juga ditentukan menjadi rendah untuk nilai probabilitas mendekati 0.","Model checking of stochastic systems, including parametric discrete-time Markov chains, is done with either numerical or statistical methods. The purpose of this research is to perform comparison of the two model checking methods in terms of execution time and accuracy and precision, and to suggest the use of one method over the other for certain conditions.
The numerical and statistical approaches for parametric discrete-time Markov chains will be compared using PRISM model checker, with IPv4 Zeroconf Protocol and Crowds Protocol as the parametric model examples. A Python program is designed to run multiple PRISM commands, collect the results, and generate data for analysis. The numerical approach will generate a probability equation for a property in terms of the parametric variables of the Markov chain, which is then used to calculate probability values for the variables. The values are then compared with the resulting values from statistical model checking to analyze the accuracy, precision, and performance. The statistical approach outcomes are generated using confidence interval.
The results of the experiments show that the numerical method has a faster execution time compared to the statistical one for verification IPv4 Zeroconf Protocol, considered a small and simple system. However, for Crowds Protocol, a large and complex model, the time taken for numerical model checking exponentially increases, rendering it ineffective in terms of time and memory. In addition, comparing statistical results to numerical ones, the accuracy for statistical results are determined to be low for probability values near 0, and the precision for statistical results are also determined to be low for probability values near 0.",,,108909,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111876,,ILHAM SULAKSONO,Analisa Kinerja Algoritma Job Scheduling pada YARN Cluster,,,"Hadoop, YARN, Job Scheduling, Cluster Computer, Analisa kinerja","Dr. Mardhani Riasetiawan, M.T.",2,3,0,2017,1,"Algoritma Job Scheduling pada cluster sangat mempengaruhi kinerja dari suatu sistem cluster. Dalam Hadoop cluster, framework yang berfungsi untuk job scheduling terdapat dalam YARN. YARN memiliki algoritma job scheduling berupa fair dan capacity scheduling. Fair scheduling melakukan pembagian resource secara merata untuk tiap job. Sedangkan capacity scheduling melakukan pembagian resource berdasarkan pada kapasitas queue yang dikonfigurasi.
Penelitian ini melakukan analisa pada kinerja dari job scheduling yang terdapat dalam YARN. Analisa dilakukan dengan mengobservasi hasil benchmarking dengan menggunakan HiBench. Parameter yang dijadikan sebagai penilaian kinerja adalah waiting time, execution time, serta CPU dan memory fairness. Pengujian dilakukan 3 kali skenario, dimana skenario 1 dengan menggunakan tipe dan ukuran workload yang berbeda serta skenario 2 dan 3 menggunakan konfigurasi yang berbeda. 
Penelitian ini menghasilkan kinerja dan hubungan antara konfigurasi dengan kinerja yang dihasilkan untuk tiap job scheduling yang dianalisis. Penelitian ini mendapatkan hasil bahwa kinerja dari fair scheduling lebih optimal daripada capacity scheduling dengan berdasarkan pada execution time rata-rata 231 detik untuk fair scheduling dan 252 detik untuk capacity pada skenario 1 menggunakan &quot;Huge&quot;benchmark. Dengan menggunakan 3 workload bersamaan, parameter konfigurasi dari fair scheduling yang berhubungan dengan kinerja secara signifikan adalah assignmultiple. Konfigurasi assignmultiple memiliki nilai korelasi koefisien dengan execution time sebesar 0,847782. Capacity scheduling memiliki konfigurasi dengan nilai korelasi yang signifikan, yaitu capacity dan maxcapacity. Nilai korelasi untuk execution time dengan capacity queue &quot;a&quot; adalah -0,9742. Sedangkan, untuk queue maxcapacity adalah -0,43275.
","Algorithm of Job Scheduling on cluster greatly affects the performance of a cluster system. In Hadoop cluster, framework for job scheduling is contained in the YARN. YARN has a job scheduling algorithm, fair and capacity scheduling. Fair scheduling algorithm allocating cluster resource evenly to each job. While capacity scheduling algorithm allocating cluster resource based on the capacity of the queue that is configured.
This research analyzing the performance of the job scheduling that contained in YARN. The analysis is done by observing the results of benchmarking by using HiBench. The parameters that serve as a metrics is waiting time, execution time, CPU and memory fairness. The testing was done with 3 scenario, where scenario 1 using different type and size of workload and scenario 2 and 3 with different configuration.
This research resulted in performance and the relationship between configurations and performance for each job scheduling. This research have result that performance from fair scheduling more optimal than capacity scheduling with average of waiting time 231 second for fair scheduling and 252 second for capacity using &Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&quot;Huge&quot; benchmark first scenario. The configuration of the fair scheduling that affect performance in a significant way is assignmultiple with correlation coefficient value for waiting time is 0.847782. While capacity scheduling is queue capacity and maxcapacity. The value of correlation coefficient for execution time with capacity queue &quot;a&quot; -0.9742. And, correlation for queue maxcapacity with execution time is -0.43275.
",,,62664,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
131077,,KRISOSTOMUS NOVA RAHMANTO,PROSES DATA PRESERVATION DI LINGKUNGAN BIG DATA,,,"data preservation, big data, data archiving, long-term data, Open Archival Information System","Dr.Mardhani Riasetiawan,M.T.",2,3,0,2017,1,"Data preservation adalah kemampuan untuk memastikan data digital yang disimpan sekarang dapat dibaca dan diintepretasikan sampai puluhan atau ratusan tahun (Factor et al. dalam Viana et al, 2014). Di tengah jumlah data yang dipreservasi bertumbuh semakin besar, maka diperlukan strategi tertentu untuk mempreservasi data. Salah satu strategi yang sesuai dengan permasalahan tersebut adalah membawa proses data preservation ke dalam lingkungan big data. Proses data preservation diatur dalam sebuah standar ISO (International Standard Organization) bernomor 14721:2012 atau sering disebut dengan Open Archival Information System (OAIS).
Penelitian ini mengusulkan kerangka kerja proses data preservation pada lingkungan big data dengan menggunakan komponen utama OAIS yakni Ingest, Archival Storage, Preservation Planning, Access, dan Data Management. 
Berdasarkan hasil pengujian yang telah dilakukan, usulan kerangka kerja proses data preservation berhasil menjalankan empat dari lima komponen utama OAIS dalam lingkungan big data, yakni Archival Storage, Preservation Planning, Data Management, dan Access. ","
Data preservation deals with ensuring that digital data stored today can be read and interpreted tens or hundreds of years from now (Factor et al. at Viana et al, 2014). As the data to be preserved increasing, bring data preservation process into big data environment is required. An ISO 14721:2012 standardizes data preservation process, called Open Archival Information System (OAIS).
This research proposing a framework for data preservation process in big data architecture by using OAIS main components: Ingest, Archival Storage, Preservation Planning, Access, and Data Management.
Based on the results, four out of five OAIS main components work in the proposed framework in big data environment, which is Archival Storage, Preservation Planning, Data Management, and Access.",,,82054,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114185,,NINDYA ANGGITA AZIS,IMPLEMENTASI ALGORITMA GENETIKA UNTUK OPTIMASI PENJADWALAN PERKULIAHAN DI D-3 KOMSI SEKOLAH VOKASI UGM,,,"penjadwalan, kuliah, asisten, optimasi, algoritma genetika, course scheduling, lecture, asisstant, optimization, genetic algorithm","Faizah, S.Kom., M.Kom.",2,3,0,2017,1,"Proses penjadwalan perkuliahan merupakan agenda penting yang dilakukan secara rutin setiap semester. Hal yang sama juga terjadi di lingkungan Program Studi D-3 KOMSI Sekolah Vokasi UGM. Selama ini, jadwal yang digunakan, dihasilkan melalui 2 tahap, yaitu proses penjadwalan perkuliahan, kemudian proses penempatan asisten praktikum. Dengan adanya 2 tahap proses penjadwalan yang dilakukan, tentunya memberi pengaruh terhadap lamanya waktu yang dibutuhkan untuk menghasilkan jadwal.
Salah satu cara untuk mereduksi waktu yang dibutuhkan untuk melakukan penjadwalan perkuliahan dan penempatan asisten adalah dengan menggabungkannya dalam satu tahapan algoritma genetika. Dengan penyajian data (solusi) yang tepat, algoritma genetika diharapkan mampu mengarahkan sistem untukmenghasilkan solusi, dalam waktu yang lebih singkat. Penelitian ini diawali dengan membuat desain solusi yang direpresentasikan dalam model representasi bilangan bulat. Kemudian menerapkan metode seleksi ranking (Rank-based selection) untuk memilih pasangan parent. Langkah selanjutnya adalah menerapkan metode multiple-cut-point crossover, random mutation dan metode Steady State dengan Elitism pada tahapan seleksi survivor. Tahap-tahap algoritma genetika ini akan diulang hingga kondisi dari stopping criteria terpenuhi.
Penelitian ini mengembangkan sebuah program berbasis desktop yang mampu melakukan proses penjadwalan perkuliahan dan penempatan asisten dalam satu tahapan algoritma genetika. Selain ditemukannya solusi optimal, lamanya waktu pengujian juga menjadi objek perhatian pada penelitian ini. Keberhasilan proses pencarian solusi bergantung terhadap parameter yang digunakan. Berikut ini adalah parameter optimal, atau parameter yang dapat menghasilkan solusi dalam waktu yang lebih singkat : Pc = 70%, Pm = 9,5%, dan popsize = 20 individu. Setelah melakukan pengujian tahap kedua, dapat diketahui bahwa 2 pengujian yang dapat menghasilkan jadwal yang memenuhi 5 tingkat prioritas batasan, adalah pengujian dengan menggunakan 50 kelas dan 100 kelas. Sementara pengujian dengan menggunakan 152 kelas, program mampu menghasilkan jadwal yang memenuhi 4 tingkat prioritas batasan.
","Course scheduling is an important task to do when a new semester begins in an educational institution, like Program Studi D-3 KOMSI Sekolah Vokasi UGM. Course scheduling used in Program Studi D-3 KOMSI Sekolah Vokasi UGM has been generated through two phases, scheduling of course only and scheduling of asisstant of practical work class. Two phases of course scheduling which have been doing, absolutely took more times.
One of alternatives that can be implemented to reduce times took by doing those two phases is merging them into one phase. An optimum solution can be found briefly with the suitable data representation. The process of genetic algorithms in this study will be started by designing a solution in an integer-representation-model. Then, the following steps are implementing Rank-based selection to gather parents will be crossovered, implementing multiple-cut-point crossover, implementing random mutation method, and implementing Steady State with Elitism as an approach in survivor selection phase. Those genetic algorithm steps will be executed repeatedly until the conditioned of stopping criteria is fulfilled.
This study develops a desktop program which is able to perform scheduling of course only and scheduling of asisstant of practical work class in a single run. Not only the ability to find the best solution which become the main focus in this study, but also the running time needed too. The success of solution-searching-process, depends on the value of parameter given. The combination of parameter which is able to get the best result in a fewer running time are : Pc = 70%, Pm = 9,5%, and popsize = 20 chromosomes. After doing the second phase experiment, it can be concluded that two experiment which successfully found a schedule fulfilling 5 level priority of constraints are the experiment using 50 classes and the experiment using 100 classes. While another experiment only successfully found a solution fulfilling 4 level of constraints defined.
",,,65035,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116746,,NATANAEL EVAN TJANDRA,EVALUASI AKSESIBILITAS MENGGUNAKAN PENDEKATAN ANALISA A NEW QUALITY IN USE MODEL PADA APLIKASI M-BANKING,,,"Aksesibilitas, A new Quality in Use Model, Mobile banking","Faizah, S.Kom., M.Kom",2,3,0,2017,1,"Perkembangan mobile banking saat ini semakin meningkat. Oleh karena itu, penting untuk memastikan bahwa aplikasi mobile banking mudah diakses oleh semua orang termasuk mereka yang memiliki keterbatasan pada penglihatan serta memperhatikan orang berusia lanjut yang mulai mengalami penurunan kemampuan fisik. Evaluasi aksesibilitas menjadi isu yang penting saat ini, namun pada kenyataanya evaluasi aksesibilitas sering terabaikan dalam proses pengembangan aplikasi berbasis perangkat mobile dikarenakan kurangnya informasi mengenai  cara evaluasi aksesibilitas.
Pada penelitian ini dilakukan evaluasi aksesibilitas dengan panduan WCAG 2.0 serta usability testing dan dilakukan perbaikan dengan nenmbuat prototype perbaikan aplikasi mobile banking, perbaikan ini mengacu pada hasil usability testing terhadap 3 aplikasi perbankan yang sudah ada. Selanjutnya digunakan pendekatan A New Quality in Use Model untuk menilai apakah rancangan perbaikan aplikasi perbankan sudah lebih baik dibandingkan aplikasi yang sudah ada
Dengan mengevaluasi aksesibilitas dan melakukan perbaikan terhadap desain aplikasi mobile terjadi peningkatan rata-rata nilai pada pendekatan A New Quality in Use Model,  dan rata-rata nilai pada rancangan perbaikan ini sudah mendekati rekomendasi yang disarankan pada teori ini.
","The development of mobile banking is increasing. Therefore, it is important to ensure that mobile banking are easily accessible to people with limited vision and attention of people with age-related limitations. The evaluation of mobile application&acirc;€™s accessibility is being important, but many developers forget to bring this issues during the development process because the lack of information .
	 This research do the accessibility evaluation using WCAG 2.0 guidance and usability testing as the tool for the evaluation, then prototyping of banking application improvements was made, this improvement based on the evaluation result of 3 application usability. A New Quality in Use Model approach is used to assess whether the new application is better than the existing application
	By evaluating accessibility and improving the design it shows the increasing score of A New Quality is Use Model approach and the prototype&acirc;€™s score approachs the reccomendation.",,,67618,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116747,,MUHAMMAD QADRI,KLASIFIKASI KOMENTAR POSITIF DAN NEGATIF PADA AKUN INSTAGRAM PUBLIK FIGUR INDONESIA,,,"klasifikasi, instagram, naive bayes,complement naive bayes","Arif Nurwidyantoro, S.Kom., M.Cs.",2,3,0,2017,1,"Pada media sosial Instagram para publik figur senang berbagi aktivitas kesehariannya kepada para penggemarnya lewat foto dan video. Foto ataupun video dari publik figur biasanya banyak mendapat perhatian dari penggemar publik figur tersebut. Mereka menunjukan perhatiannya dengan memberikan komentar di foto atau video publik figur yang mereka sukai. Unggahan dari seorang publik figur biasanya memiliki banyak komentar, komentar yang diberikan tidak hanya komentar positif tapi bisa juga komentar negatif.

Penelitian ini menggunakan metode Supervised Learning yaitu algoritma Naive Bayes dan Complement Naive Bayes untuk melakukan klasifikasi komentar positif dan negatif pada akun Instagram publik figur Indonesia. Complement Naive Bayes dipilih karena data set komentar tidak seimbang untuk masing-masing kelas komentar. Kalimat komentar diekstraksi dari setiap akun Instagram, kemudian diproses menggunakan beberapa tahap preprocessing dan digunakan sebagai model dalam kedua algoritma. Hasil pemodelan tersebut digunakan untuk mengklasifikasikan data komentar yang belum mempunyai label.

Penelitian ini menunjukan Naive Bayes memiliki performa lebih baik dibandingkan dengan Complement Naive Bayes untuk data set Balance dengan akurasi tertinggi mencapai 82,18%, sedangkan Complement Naive Bayes memiliki performa lebih baik dibandingkan dengan Naive Bayes untuk data set Imbalance dengan akurasi tertinggi mencapai 83,50%. Hasil pemodelan Complement Naive Bayes digunakan untuk pengklasifikasian komentar yang belum diklasifikasikan. Hasil klasifikasi komentar menunjukan jumlah komentar positif lebih banyak dibandingkan dengan komentar negatif. 
","Public figures tend to share their daily activities with their fans using photos and videos in Instagram. Their followers usually show their thought by commenting on those photos and videos. There are many comments in every post, not only positive comments but also negative comments.

This research use Supervised learning approaches: Naive Bayes and Complement Naive Bayes, to classify positive and negative comments in public figure&acirc;€™s Instagram accounts. These comments were extracted, labeled, and preprocessed to be used as features for the model of the algorithms. The model then used to classify comments that are not labeled yet.

This research showed that Naive Bayes performs better than Complement Naive Bayes for Balanced data set with maximum accuracy 82,18%. Meanwhile, Compelement Naive bayes perform better for Imbalanced data set with maximum accuracy 83,50%. The model then used as the classifier for other comments, showed that positive comments occur more frequently that negative comments.
",,,67626,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129294,,NURLIA RAHMA DHANI,SISTEM PAKAR MENGGUNAKAN METODE CERTAINTY FACTOR UNTUK DIAGNOSIS PENYAKIT NEURODEGENERATIF,,,"sistem pakar, forward chaining, certainty factor, neurodegeneratif","Retantyo Wardoyo, Drs., M.Sc, Ph.D ",2,3,0,2017,1,"Neurodegeneratif merupakan suatu kondisi patologis pada sel saraf dimana sel saraf tersebut mengalami kehilangan struktur atau fungsi sebenarnya secara progresif. Seiring dengan semakin banyaknya penderita penyakit neurodegeneratif dan terbatasnya jumlah pakar yang ada, dibutuhkan sebuah media yaitu sistem pakar untuk membantu melakukan diagnosis penyakit.
Sistem pakar dapat digunakan untuk menghasilkan sebuah kesimpulan yang menggunakan pengetahuan dari seorang pakar. Sistem yang dibangun dalam penelitian ini adalah sistem pakar untuk mendiagnosis penyakit neurodegeneratif. Sistem dibangun menggunakan mesin inferensi forward chaining dan metode certainty factor. Metode certainty factor digunakan untuk memberikan nilai kepastian terhadap pengetahuan yang tidak memiliki nilai ukur. Terdapat dua pengguna yang dapat menggunakan sistem ini, yaitu pakar dan paramedis. Pakar dapat mengelola pengetahuan yang ada dalam sistem. Sedangkan paramedis dapat melakukan konsultasi penyakit berdasarkan gejala yang dialami oleh pasien.
Data masukan berupa gejala yang dialami oleh pasien beserta nilai certainty factor setiap gejala. Berapapun gejala masukan dari paramedis akan dapat menghasilkan sebuah kesimpulan. Hasil diagnosis penyakit terdiri dari nama penyakit, keterangan penyakit, saran penanganan serta tingkat kepastian pasien mengalami penyakit tersebut.","Neurodegenerative is a pathological condition in the nerve cells which the cells itself lost its function and structure progressively. Along with threatening increasement of neurodegenerative disease patients also a limited number of expert, an expert system is needed to help making diagnosing.
The system that build on this research is an expert system for diagnosing neurodegenerative disease. The system is implementing forward chaining inference machine and certainty factor method. The certainty factor method used to provide assurance against knowledge that has no measuring value. There are two users that can use this expert system, experts and paramedics. An expert can modify knowledge base that used for inference process. Paramedics can use the system for disease consultation based on patient&acirc;€™s symptomps.
The input data are symptomps that suffered by the patient with certainty factor value for each symptomps. Regardless of the input entered by paramedics will produce the output of a conclusion. Output data shown are disease name, explanation of the disease, suggestion for the disease and certainty factor value for the disease.",,,80197,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114191,,GOLDI RILLO PANGAYOM,MODEL PERAMALAN LONG SHORT-TERM MEMORY RECURRENT NEURAL NETWORK UNTUK PERAMALAN NILAI TUKAR MATA UANG DOLAR AMERIKA-RUPIAH INDONESIA,,,"peramalan, nilai tukar mata uang, long short-term meory, recurrent neural network, sliding window, Dstat","Dr. Agus Sihabuddin, S.Si., M.Kom.",2,3,0,2017,1,"Nilai tukar mata uang merupakan hal yang penting dalam ekonomi global. Nilai tukar mata uang berubah tiap waktu dengan nilai yang fluktuatif. Dapat memprediksi nilai tukar pada masa mendatang sangat bermanfaat khususnya dalam bidang ekonomi. 
Peramalan dapat menggunakan pendekatan analisis teknis atau menggunakan data historis untuk meramalkan nilai mendatang. Jaringan saraf tiruan (JST) digunakan untuk melakukan analisis teknis. Terdapat kasus terjadinya vanishing atau exploding gradient saat pelatihan yang terkadang muncul saat menerapkan recurrent neural network (RNN). Salah satu algoritma yang dapat digunakan untuk mengatasi masalah ini adalah long short-term memory recurrent neural network (LSTM RNN). LSTM dapat menangani masalah tersebut karena memiliki gate yang menentukan nilai yang akan masuk dan keluar dari neuron LSTM.
Penelitian ini menggunakan data nilai tukar harian USD/IDR dari 2 Januari 2014 hingga 31 Maret 2017 dengan 60% sebagai data latih, 15% sebagai data validasi, dan 25% sebagai data uji. Untuk mengetahui akurasi peramalan, Dstat, MAE, RMSE, dan MAPE digunakan sebagai metrik evaluasi. 
Dari hasil penelitian, diperoleh arsitektur untuk peramalan univariat paling baik adalah arsitektur 5-3-1 memperoleh nilai rata-rata Dstat sebesar 64,36% dengan Dstat terbaik sebesar 68,14%, MAE 48,84, RMSE 65,87, dan MAPE 0,37% sedangkan untuk peramalan multivariabel dengan menambahkan mata uang asing (USD/SAR) adalah arsitektur 2-1-1. Hasil akurasi yang dicapai adalah rata-rata Dstat sebesar 66,72% dengan hasil paling baik adalah sebesar 69,61%, rata-rata RMSE sebesar 64,14 dengan RMSE paling kecil 63,64, rata-rata MAE sebesar 48,23 dengan MAE paling kecil 47,93, dan MAPE sebesar 0,36%.
","Currency exchange rates are important in the global economy. Currency exchange rates change every time with a fluctuating value. Doing a prediction of values is very useful, especially in the economic field.
Technical analysis or using historical values can be used to predict future values which artificial neural network (ANN) is capable of doing it. There are some cases of vanishing or exploding gradients during training that sometimes occur when applying recurrent neural network (RNN). One of the algorithms that can be used to solve this problem is the long short-term memory recurrent neural network (LSTM RNN). LSTM can handle the problem because it has gates that determines the values that enters or leaves (as output) the LSTM neuron.
This research uses USD/IDR daily exchange rates data from January 2nd 2014 until March 31st 2017 with 60% as training data, 15% as validation data, and 25% as testing data. To know the accuracy of forecasting, Dstat, MAE, RMSE, and MAPE are used as evaluation metrics.
This research gives results that 5-3-1 architecture produces the best result for univarite forecasting which results the average Dstat score equals to 64.36% with the best Dstat is 68.14%, 48.84 in MAE, 65.87 in RMSE, and 0.37% in MAPE. For multivariable forecasting, by adding USD/SAR currency gets the best result by using 2-1-1 architecture resulting 66.72% of the average Dstat with the best result is 69.61%, average of RMSE equals to 64.14 with best RMSE of 63.64, average of MAE equals to 48.23 with the best MAE 47.93, and MAPE by 0.36%.",,,64954,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
153878,,RIZKI NUR WAKIDAH,MANAJEMEN PROYEK PADA ANDROID DENGAN MEMANFAATKAN GROUP FACEBOOK DAN COLLABTIVE,,,"Collaborative Sostware, manajemen Proyek, Collabtive, Android, Group Faceboook/ Collaborative Software, project management, Android, Collabtive, Facebook Group","Y. Suyanto, Drs., M.Kom., Dr",2,3,0,2017,1,"Collaborative Software adalah perangkat lunak komputer yang dirancang untuk membantu orang yang terlibat dalam suatu tugas bersama agar mencapai tujuannya. Collabtive menyediakan platform berbasis web untuk membawa proses manajemen proyek dan dokumentasi online. Collabtive hanya menyediakan sistem chat sebagai sarana komunikasi antar user. Belum ada sarana komunikasi yang dapat digunakan untuk tempat berdiskusi atau penyampaian informasi, contohnya undangan rapat, pengumuman, atau berita.
Pada penelitian ini dibangun sistem untuk mengembangkan manajemen proyek dengan memanfaatkan group Facebook dan Android. Facebook menyediakan API bernama Facebook Graph API yang memungkinkan pengembangan aplikasi dengan memanfaatkan layanan dari Facebook. Melakukan integrasi antara Collabtive dan group Facebook dengan memanfaatkan Facebook Javascript SDK dan Facebook PHP SDK. Penambahan fitur-fitur Facebook seperti posting video, tombol like disetiap post serta posting teks, gambar serta link pada penelitian sebelumnya diharapkan supaya meningkatkan kinerja tim proyek  dengan menjamin kolaborasi dan kerja sama tim melalui sistem yang dibangun terutama dari segi sarana komunikasi.
Sistem ini memberikan layanan group Facebook yang sudah terintegrasi ke dalam Collabtive namun dapat dinikmati pada platform Android. Tersedia fitur posting video yang dapat melakukan upload video dan akan ditampilkan juga pada Collabtive dengan dilengkapi tombol like pada setiap post, selain itu sistem juga dilengkapi dengan fitur notifikasi. Sehingga dapat meningkatkan kinerja aktivitas proyek dengan menjamin sebuah kolaborasi dan kerja sama tim melalui sebuah sistem yang dibangun. 
","Collaborative Software is a computer software designed to help people involved in a common task to achieve its goals. The Collabtive provides a web-based platform to bring the process of project management and online documentation. Collabtive only provides a chat system as a means of communication between users. There is no means of communication that can be used for the place of discussion or delivery of information, for example meeting invitations, announcements, or news.
In this study built a system to develop project management using the group Facebook and Android. Facebook provides an API called Facebook Graph API that allows the development of applications by utilizing services from Facebook. Integrate between Collabtive and Facebook groups by utilizing the Facebook Javascript SDK and Facebook PHP SDK. The addition of Facebook features such as video postings, like buttons in every post as well as text posts, images and links in previous research are expected to improve the performance of the project team by ensuring collaboration and teamwork through systems built primarily in terms of means of communication.
This system provides Facebook group services that are already integrated into Collabtive but can be enjoyed on the Android platform. Available video post feature that can upload video and will also be displayed on Collabtive with buttons like on each post, in addition to the system also comes with a notification feature. So as to improve the performance of project activities by ensuring a collaboration and teamwork through a system built.
",,,104956,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113686,,AGUS TRIYONO,Deep Belief Network untuk Peramalan Nilai Tukar Mata Uang,,,"Deep Belief Network, Time Series Prediction, Generative Model",Dr. Agus Sihabuddin,2,3,0,2017,1,"Fluktuasi nilai tukar mata uang asing menjadi fenomena yang diamati oleh banyak aktor pasar finansial. Teknik pengambilan keputusan yang digunakan oleh pelaku pasar mata uang asing adalah dengan melakukan peramalan. Teknik peramalan dapat menggunakan data masa lalu untuk memprediksi nilai yang akan datang. Nilai tukar mata uang asing adalah permasalahan yang nonlinear atau mudah berubah. Jaringan syaraf tiruan merupakan salah satu metode nonlinear yang dapat memetakan data yang bersifat tidak linear. Jaringan syaraf tiruan mempunyai permasalahan inisialisasi awal yang tidak baik, sehingga solusi yang didapat tidak optimal. 
Pada penelitian ini metode yang digunakan untuk meningkatkan kemampuan JST adalah menggunakan model Deep Belief Network. Dimana terdapat proses pre-training untuk inisialisasi awal jaringan. Kemudian dilanjutkan dengan proses Fine-Tuning untuk memperbaharui bobot dengan target data. DBN dibuat dengan 3 lapis RBM dan satu lapis output regresi. Stochastic Gradient Descent digunakan sebagai optimasi dalam, learning rate dalam SGD dilakukan Time-schedule Learning Rate dengan penggunaan Decay. 
Eksperimen pada mata uang IDR/USD menghasilkan Directional Accuracy sebesar 69,56522%. Penggunaan Decay dalam pengurangan learning rate meningkatkan akurasi DBN dengan konsisten pada 4 pengujian RMSE, MAE, MAPE, DA. Diperoleh hasil lain bahwa Pre-Training berpengaruh terhadap pencarian solusi optimal dalam Deep Neural Network. ","The fluctuations of foreign exchange rates are a phenomenon observed by many financial market actors. The decision-making technique used by foreign currency market participants is to forecast. Forecasting techniques can use past data to predict future values. Foreign currency exchange rates are nonlinear or volatile. Artificial neural network is one nonlinear method that can map data that is not linear. Artificial neural networks have initial initialization problems are not good, so the solution obtained is not optimal. 
In this study the method used to improve the ability of ANN is to use Deep Belief Network model. Where there is a pre-training process for initial network initialization. Then proceed with the Fine-Tuning process to update the weights with the target data. DBN is created with 3 layers of RBM and one regression output layer. Stochastic Gradient Descent is used as an optimization in, learning rate in SGD is done Time-schedule Learning Rate with Decay usage. 
Experiments on the IDR / USD currency resulted in Directional Accuracy of 69.56522%. The use of Decay in the reduction of learning rate improves consistent DBN accuracy across 4 RMSE, MAE, MAPE, DA tests. Another result is obtained that Pre-Training has an effect on the search for optimal solutions in Deep Neural Network.",,,64450,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107287,,AMELIA NURSANTI,Pencarian Frequent Itemset Berbasis MapReduce Pada Platform Cascading,,,"Frequent Itemset, MapReduce, Cascading, Hadoop, Amazon product co-purchasing network metadata","Nur Rokhman, S.Si., M.Kom., Dr. ",2,3,0,2017,1,"Implementasi suatu algoritma yang berjalan secara paralel dan terdistribusi telah menjadi objek penelitian yang sangat diminati pada dekade terakhir ini. Hal ini dilakukan untuk menangani data berskala besar. Salah satu model pemrograman yang digunakan dalam penanganan data besar adalah MapReduce menggunakan Hadoop framework. 
Frequent itemset merupakan item yang paling sering muncul di dalam dataset. Proses pencarian frequent itemset memerlukan komputasi yang besar sehingga tidak efisien. Pencarian frequent itemset menjadi bermasalah jika diimplementasikan pada data berskala besar.
Dalam penelitian ini diimplementasikan pencarian frequent itemset berbasis MapReduce pada platform Cascading. Penulisan kode program menggunakan bahasa pemrograman Java menggunakan Cascading API  yang berjalan diatas kerangka kerja Hadoop. Cascading digunakan untuk menyederhanakan kompleksitas pemrograman MapReduce. Penelitian ini menggunakan dataset Amazon product co-purchasing network metadata.
Hasil dari penelitian menujukkan bahwa pencarian frequent itemset berbasis MapReduce lebih cepat dibandingkan dengan pencarian frequent itemset menggunakan metode non MapReduce pada data berskala besar.","Implementation of an algorithm that runs in parallel and distributed has become an interest research object in recent decades. It is used to handle large scale data. One of programming model that used for handling big data is MapReduce using Hadoop Framework
Frequent itemsets are items that most frequently appear in the dataset. Frequent itemset finding process requires massive computing and become inefficient. Implementation of finding frequent itemset becomes problematic if it is implemented on a large scale data.
	In this research, process of finding frequent itemset based on MapReduce using Cascading has been implemented. Source code is written in Java programming languange using Cascading API that runs on top of Hadoop Framework. Cascading is used to simplify the complexity of MapReduce. Dataset that used  in this research is Amazon product co-purchasing network metadata.
The result from research showed that implementation of finding frequent itemset based on MapReduce is faster than non MapReduce method for finding frequent itemset on big data.",,,57883,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113692,,MOCHAMAD BAGUS P,ALGORITMA A* YANG DIIMPLEMENTASIKAN KE DALAM PENGEMBANGAN GAME 2D PLATFORMER YANG BERJALAN PADA REPRESENTASI RUANG GRID GRAPH,,,"A* Algorithm, Android, Game 2D, Pathfinding, Grid graph, Unity 5 ","Agus Sihabuddin, Dr",2,3,0,2017,1,"Smartphone sebagai sebuah terobosan dalam teknologi informasi dan banyak diminati oleh berbagai pihak dengan rentang usia yang luas. Salah satu fitur yang diminati ialah pada bagian game. Kebutuhan game setiap tahunnya semakin meningkat dengan semakin pesatnya perkembangan pada bidang teknologi, karena itu game dituntut untuk menambah suatu fitur yang bisa menarik minat pengguna untuk memainkannya.
Pada penelitian ini, game berbasis android akan dikembangkan dengan karakter yang hanya dapat bergerak 2 dimensi dan mempunyai fitur pencarian jalan. Algoritma pencarian jalan digunakan algoritma A* dengan heuristic chebyshev distance, yaitu heuristic dengan kemampuan untuk berjalan pada 8 arah yang berbeda. Algoritma A* akan berjalan di atas suatu representasi ruang yang mempunyai waktu paling sedikit untuk sampai ke tempat yang dituju.
Berdasarkan hasil pengujian, aplikasi game telah dapat berjalan sesuai dengan rancangan model use case diagram, class diagram, dan antarmuka.  Hasil pengujian tiap representasi ruang didapatkan dari simulasi game pada game engine dengan ditempatkannya objek gerak dan objek target pada koordinat tertentu. Berdasarkan hasil yang didapatkan dari simulasi pengujian, representasi ruang grid graph 8 arah dipakai pada game karena mempunyai hasil waktu paling sedikit dari representasi ruang grid graph 4 arah dan 6 arah.
","Smartphone as a breakthrough in technology and high in demand by various parties with a wide range of age. One of many features of interest is the game feature. Gaming needs annually is increasing with the rapid development of the gaming industry; therefore, a game is required to add some feature that can attract users to play it.
In this research, an android based game will be developed with characters that can only move two-dimensional and has a pathfinding feature. A* is used as pathfinding algorithm with Chebyshev distance for heuristics, with an ability to run in 8 different directions. A* algorithm will run on a spatial representation that has the least time to get to the destination.
Based on test results, the game application has been worked in accordance with the design of the use case diagram design, class diagram design, and interface design. The test results of each space representation obtained from game simulation in a game engine with a target object and a moving object placed on specific coordinates. Based on the results of the simulation tests, 8-way grid graph space representation used in the game because it has the least amount of time than 4-way and 6-way grid graph space representation.
",,,64611,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116252,,CAROLUS GAZA NINDRA TAMA,Sistem Operasi untuk Pemrosesan Big Data dengan berbasis Centos 7,,,"big data, sistem operasi, remastering, linux, kernel, Centos 7","Mardhani Riasetiawan, M.T., Dr",2,3,0,2017,1,"Big data adalah kondisi dimana model penyimpanan basis data konvensional tidak dapat lagi menanggulangi data dengan jumlah yang besar. Big data memiliki karakteristik lima V, yaitu Volume, Velocity, Variety, Value, dan Veracity. Beberapa tahun terakhir ini, sudah terdapat banyak sekali framework-framework yang dapat digunakan untuk pemrosesan big data. Framework tersebut dapat diinstal dalam sistem operasi yang telah tersedia saat ini, seperti Centos dan Ubuntu, tetapi sistem operasi tersebut masih belum optimal untuk pemrosesan big data.
Penelitian ini bertujuan untuk membuat suatu sistem operasi yang memiliki kemampuan untuk pemrosesan big data. Pada penelitian ini, dilakukan modifikasi kernel linux untuk meningkatkan performa pada pemrosesan big data dan juga penggabungan beberapa framework big data ke dalam sebuah sistem operasi. Metode yang digunakan untuk pembuatan sistem operasi ini adalah metode remastering dengan menggunakan Centos 7 sebagai sistem operasi dasar. Sistem operasi hasil remastering ini diberi nama Gamabox OS.
Berdasarkan hasil pengujian yang telah dilakukan, kernel Gamabox OS mendapatkan peningkatan performa 12% lebih baik dibandingkan dengan kernel Centos 7. Selain itu, framework big data dari Gamabox OS memiliki kinerja yang lebih efisien dan memiliki waktu eksekusi yang lebih kecil daripada HDP. Setelah diuji dalam klaster dengan dua node, performa dapat meningkat sebesar 32 % pada pengujian HPL dan sebesar 49% pada pengujian TeraSot dibandingkan dengan satu node.","Big data is a condition where a conventional database storage model can not cope with large amounts of data. Big Data has the five V characteristic, namely Volume, Velocity, Variety, Value, and Veracity. Five V shows a problem and challenge which faced in big data. In recent years, there have been many frameworks that can be used for big data processing. The framework can be installed in currently avaliable operating system, such as Ubuntu and Debian, but the operating system still not optimal for big data processing.
This research aims to create an operating system that has an ability to process big data. In this research, I modified the linux kernel to improve performance on big data processing and also merge some big data frameworks into an operating system. The method that be used to make this operating system is remastering by using Centos 7 as the base operating system. Remastered operating system is named Gamabox OS.
Based on the results of tests that have been done, the Gamabox OS kernel get 12% better performance improvement compared with Centos 7 kernel. In addition, big data framework from Gamabox OS has more efficient performance and less execution time than HDP. After being tested in a cluster, the performance can increase 32% in HPL testing and 49% in TeraSort testing compared with one node.",,,67029,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111135,,JASON KURNIAWAN,METAGENOMIC ASSEMBLY MENGGUNAKAN RANDOM FOREST,,,"Metagenomic Assembly, Supervised Learning, Random Forest, klasifikasi","Afiahayati, S.Kom, M.Kom, Ph.D",2,3,0,2017,1,"Kemajuan teknologi telah banyak mempengaruhi perkembangan ilmu biologi molekular modern dan genetika. Dengan teknologi Next Generation Sequencing(NGS), Whole Genome Sequencing (WGS) atau pengurutan keseluruhan genom dari suatu organisme dapat dilakukan secara masif dan paralel dengan waktu yang lebih cepat dan murah. Hal ini juga menyebabkan pertumbuhan data genom yang besar sehingga membutuhkan metode komputasi untuk mengolahnya. Data yang berasal dari teknologi NGS merupakan potongan-potongan genom yang saling tumpang tindih (overlap) atau biasa disebut dengan reads. Potonganpotongan tersebut perlu disusun kembali menjadi sequence yang lebih panjang, proses
penyusunan ini disebut dengan assembly. Pada data genom yang diambil dari suatu lingkungan atau komunitas, informasi genom tidak hanya berasal dari satu spesies. Dibutuhkan metode tertentu untuk melakukan assembly pada data metagenome yang biasa disebut dengan metagenomic assembly. 

Pada penelitian ini, dilakukan pengembangan kembali tool metagenomic assembly MetaVelvet-SL di mana Random Forest digunakan sebagai algoritma machine learning yang digunakan pada modul Supervised Learning. Pada penelitian
ini dilakukan pembandingan nilai akurasi dan F-measure pada klasifikasi kandidat chimeric node dengan penelitian sebelumnya yang menggunakan Support Vector
Machines. Pada penelitian ini juga dilakukan pembandingan hasil assembly dengan menghitung skor N-50 dan Nm50. Dari hasil eksperimen yang dilakukan, Random Forest terbukti mengungguli performa Support Vector Machines pada kasus
klasifikasi chimeric node pada 4 level taksonomi, yakni Ordo, Family, Genus, dan Species. Nilai akurasi dan F-measure yang dihasilkan tidak memberikan pengaruh yang signifikan terhadap skor N-50. Pada level taksonomi Family dan Species skor Nm50 Random Forest mampu menggungguli Support Vector Machines.","Advances in technology have influenced the development of modern molecular biology and genetics. With technology Next Generation Sequencing (NGS), Whole Genome Sequencing (WGS) or sequencing the entire genome of an organism can be done in parallel with the massive and faster time and cost. It also led to the growth of large genomic data that require computational methods to process it. The data coming from NGS technologies are pieces of the overlapping genome or
commonly called the reads. The pieces need to be reassembled into longer sequence, the preparation process is called assembly. At the genomic data extracted from a
environtment or community, genome information not only reserved from one species. It takes a certain method to do assembly in the metagenome data which is called the
metagenomic assembly.

In this study, we conducted redevelopment tool metagenomic assembly MetaVelvet-SL where Random Forest is the machine learning algorithm that used on the Supervised Learning module . In this research, benchmarking classification accuracy and F-measure values  in the candidate node chimeric previous research which uses Support Vector Machines. In this study, we also conducted benchmarking results of the assembly by calculating a score of N-50 and Nm50. From the results
of the conducted experiments, Random Forest proved to surpass the performance of Support Vector Machines in case the classification node chimeric at 4 level taxonomy: Order, Family, Genus and Species. The accuracy value is not give a
significant impact on the resulted N-50 score. On the taxonomy level of the Family and Species, the Nm50 scores of Random Forest are able to outperform Support Vector Machines.",,,61832,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113964,,LUCGU QOLFIERA MUHAMMAD,Deteksi Gambar Porno Menggunakan Image Zoning dan Rough Set Analysis,,,"Deteksi Gambar Porno, Rough Set Analysis, Image Zoning","Arif Nurwidyantoro, S.Kom., M.Cs",2,3,0,2017,1,"Perkembangan internet dewasa ini membuat beranekaragam informasi mudah untuk didapatkan. Mulai dari informasi yang postif maupun informasi yang negatif seperti pornografi. Pornografi saat ini mudah sekali untuk masuk kedalam kehidupan kita dalam berbagai media, mulai dari gambar hingga video. Maka dari itu dibutuhkan sebuah metode untuk melakukan klasifikasi pornografi.
Pada penelitian ini dikembangkan sebuah metode klasifikasi pornografi menggunakan teknik image zoning dan rough set analysis. Deteksi pornografi yang dilakukan menggunakan fitur-fitur yang diekstraksi dari gambar seperti face percentage dan skin percentage. Metode ini mampu untuk mendeteksi pornografi dalam bentuk gambar.
Berdasarkan hasil pengujian metode ini mampu melakukan klasifikasi dengan tingkat keakuratan sebesar 84,7%. Namun, berdasarkan pengujian melakukan klasifikasi menggunakan rough set analysis tanpa menggunakan image zoning memiliki tingkat akurasi 0.2% lebih baik dibandingkan melakukan deteksi menggunakan image zoning.","Rapid grow of internet development cause many information easier to get either positive or negative information such as pornography. Pornography nowdays is very easy to influence our life in any type of media like image or video. Therefore a method is needed to classify the pornography.
In this research, a classification method was develop using image zoning and rough set analysis. Porn detection used some feature that extracted from the picture such as face percentage and skin percentage. This method can classify pornography images.
Based on experiment, this method can classify image with accuracy about 84,7%. However, the experiment result was better when not using the image zoning technique. It&acirc;€™s about 0,2% better than classify using image zoning.",,,64707,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114221,,HAIKAL HAWARI PUTRA,Sistem Pakar Untuk Diagnosis Penyakit Usus Menggunakan Metode Certainty Factor,,,"sistem pakar, usus, forward chaining, certainty factor, expert system, intestines","Retantyo Wardoyo, Drs., M.Sc., Ph.D",2,3,0,2017,1,"Makanan atau minuman yang tidak bersih serta kandungan bahan baku berbahaya yang berlebihan dapat menimbulkan berbagai macam penyakit, salah satunya penyakit pada pencernaan, terutama pada bagian usus. Seiring dengan meningkatnya ancaman penderita penyakit usus dan terbatasnya jumlah pakar yang ada, dibutuhkan sistem pakar untuk membantu melakukan diagnosis awal penyakit.
Sistem pakar adalah salah satu cabang dari Artificial Intelligence (AI) yang membuat penggunaan secara luas knowledge yang khusus untuk penyelesaian masalah tingkat manusia yang pakar. Sistem yang dibangun pada penelitian ini adalah sistem pakar untuk diagnosis penyakit usus. Sistem yang dibangun menggunakan mesin inferensi forward chaining dan metode certainty factor. Pengguna sistem ini terbagi menjadi dua, yaitu pakar dan paramedis. Pakar dapat melakukan penambahan atau modifikasi pengetahuan yang digunakan untuk proses inferensi. Paramedis dapat melakukan konsultasi penyakit berdasarkan gejala yang dialami oleh pasien.
Proses pengujian dilakukan dengan mencocokkan seluruh data input dari user dengan data output dari sistem. Data input berupa gejala-gejala dan nilai CF untuk setiap gejala yang dialami oleh pasien. Data output berupa nama penyakit, keterangan penyakit, dan perhitungan nilai CF untuk hasil diagnosis penyakit.","Foods or drinks that made with low hygiene and excessive amount of dangerous basic material can cause various kinds of disease, one of them is intestinal disease. Along with threatening increasement of intestinal disease patients and a limited number of expert, an expert system is needed to help making early diagnosing of intestinal disesase.
Expert system is a branch of Artificial Intelligence (AI) that uses a specific knowledge to solve a problem in expert level. The system that built on this research is expert system for diagnosing intestinal disesase. The system is implementing forward chaining inference machine and certainty factor method. The user of this system is divided into two types : experts and paramedics. An expert can add or modify the knowledge that used for inference process. Paramedics can use the system for disease consultation based with patient symptopms. 
The examining process is done by matching the entire input data from user with output data from the system. The input data are the symptomps that suffered by the patient with certainty factor value for each symptomps. The output data that shown are disease name, disease explanation, and certainty factor value for the diagnosed disease.",,,64971,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132658,,FAIZ NURALIM,ANALISIS KINERJA JARINGAN WIFI PADA FREKUENSI 5GHZ,,,"wireless, wi-fi, access point, bandwidth, throughput, latency, jitter, packet loss, QoS, 5 GHz","Drs. Dr.techn. Ahmad Ashari, M.Kom.",2,3,0,2017,1,"Di era digital dan teknologi maju ini, dibutuhkan jaringan Wi-Fi dengan transfer data yang besar. Wi-Fi frekuensi 5 GHz menawarkan transfer data jauh lebih besar dan spektrum yang lebih lebar dibandingkan dengan Wi-Fi frekuensi 2.4 GHz.
Penelitian ini bertujuan untuk membandingkan kinerja Wi-Fi frekuensi 5 GHz dengan 2.4 GHz. Selain itu, penelitian ini juga bertujuan untuk menyelidiki dan mencari hubungan antara kekuatan sinyal yang  diterima dengan QoS pada jaringan IEEE 802.11AC seperti throughput, latency, jitter, dan packet loss melalui pengukuran propagasi radio. Pengukuran dengan propagasi radio secara langsung di lapangan dilakukan karena lebih akurat daripada simulasi komputer atau pemodelan matematis, di mana karakteristik propagasi radio itu kompleks dan tidak dapat ditebak. Hasil dari penelitian ini adalah hubungan kekuatan sinyal dan QoS pada Wi-Fi frekuensi 5 GHz dan 2.4 GHz. Sehingga dapat digunakan sebagai bahan pertimbangan untuk penggunaan Wi-Fi frekuensi 5 GHz.
","In this era of digital and advanced technology, Wi-Fi networks are required with large data transfers. 5 GHz Wi-Fi offers much larger data transfer and wider spectrum compared to 2.4 GHz Wi-Fi.
This research aims to compare the performance of 5 GHz Wi-Fi with 2.4 GHz Wi-Fi. In addition, this research also aims to investigate and find the relationship between received signal strength on IEEE 802.11AC networks QoS in terms throughput, latency, jitter, and packet loss through extensive radio propagation measurements. Direct propagation measurement is done because it is difficult to derive real and accurate signal strength and QoS parameters using analytical modeling and computer simulation. The results of this research are the relationship of signal strength and QoS on 5 GHz and 2.4 GHz Wi-Fi. So it can be used as a consideration for the use of 5 GHz Wi-Fi.
",,,83649,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112946,,SAIFULHAQ MAYYAZI,JARINGAN SYARAF TIRUAN UNTUK MEMPREDIKSI NILAI SISWA SMA MENGGUNAKAN BACKPROPAGATION DENGAN METODE INISIALISASI BOBOT AWAL,,,"nilai, jaringan syaraf tiruan, backpropagation,  metode inisialisasi bobot awal","Azhari SN, Drs., MT., Dr",2,3,0,2017,1,"Nilai mata pelajaran merupakan salah satu indikator pemahaman materi dan kesuksesan bagi siswa. Nilai seorang siswa dapat diprediksi dengan data nilai siswa selama masa studi di semester sebelumnya. Kemampuan untuk memprediksi nilai siswa dapat dijadikan tolak ukur, sehingga bila nilai hasil kurang memuaskan, siswa dapat meningkatkan intensitas belajar agar mendapat hasil memuaskan. Maka diperlukan sistem yang dapat melakukan prediksi nilai siswa untuk membantu siswa dalam proses pembelajaran.
Backpropagation adalah salah satu algoritma jaringan syaraf tiruan yang pada dasarnya menggunakan perubahan bobot dari epoch sebelumnya. Backpropagation merupakan salah satu jaringan syaraf tiruan yang memakan waktu lama dalam mendapat bobot ideal terutama pada jumlah data besar. Maka diperlukan metode untuk mempercepat proses pencarian bobot ideal. Metode yang digunakan dalam penelitian ini adalah metode inisialisasi bobot awal oleh Kim dan Ra (1991). Nilai bobot awal biasanya memakai suatu nilai yang kecil mendekati nol, tetapi dengan metode ini bisa didapat nilai bobot awal yang lebih ideal. Dengan metode ini jaringan syaraf tiruan dapat bekerja lebih optimal dan meningkatkan nilai akurasi dalam memprediksi nilai siswa.
Untuk memprediksi nilai siswa ini dibutuhkan data sampel untuk digunakan dalam melatih jaringan syaraf tiruan dan melakukan pengujian. Data ini didapatkan dari SMA Batik 1 Surakarta.
Dari pengujian diperoleh hasil bahwa backpropagation tanpa menggunakan metode inisialisasi bobot awal dapat memprediksi nilai siswa dengan nilai akurasi sebesar 68,95%. Sedangkan dengan menggunakan metode inisialisasi bobot awal dapat memprediksi dengan nilai akurasi sebesar 70,17%.","Test score is one of many indicators for a student's competence and proof that the student understands the subject tested. The outcome of student's score can be predicted using student's scores in his study time on highschool. The ability to predict student's test score can be used as a parameter, so students can study more if the result's deemed unsatisfying. A system is needed to be able to predict the outcome of student's score to help students better their study.
Artificial neural network with backpropagation can be used to predict student's score, but it takes a long time for the neural network to achieve an ideal weight especially with large data size. Initial weight in artificial network usually uses a small number around zero, but using a method to calculate the value of initial weight can improve neural network's performance and accuracy.
In order to predict the outcome, a sample data from student's scores is needed to train the neural network. The sample data is obtained from SMA Batik 1 Surakarta.
The result of this research shows that backpropagation without weight initialization method can predict student's score outcome with 68,95% accuracy. Using weight initialization method, the neural network can predict student's score outcome with 70,17% accuracy.",,,63675,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113202,,AHMAD ALIM AKHSAN,ANALISIS DAN PERANCANGAN INTERAKSI CHATBOT REMINDER DENGAN USER-CENTERED DESIGN,,,"interaksi, chatbot, user-centered design, user experience / interaction, chatbot, user-centered design, user experience","Faizah, S.Kom, M.Kom",2,3,0,2017,1,"Keberadaan teknologi yang tidak dipersiapkan untuk menghadapi interaksi dengan pengguna akan membuat user experience menjadi buruk. Masih banyak ditemukan chatbot reminder yang tidak dapat memenuhi harapan pengguna. Pendekatan user-centered design dapat membantu dalam mengatasi masalah yang berhubungan dengan pengguna.

Pada penelitian ini digunakan metode Google Design Sprint sebagai pendekatan user-centered design dalam tahapan analisis hingga pengujian. Interaksi merupakan masalah utama yang dianalisis dan diharapkan dapat meningkatkan user experience. Rekomendasi rancangan dan prototype chatbot baru mengacu pada hasil usability testing empat chatbot terpilih. Pengujian dilakukan dengan melakukan usability testing pada prototype chatbot baru. Sebanyak lima partisipan yang berbeda digunakan setiap chatbot pada saat usability testing. Hasil usability testing chatbot baru kemudian dibandingkan dengan chatbot lama.

Diperoleh skor SUS (System Usability Scale) chatbot baru sebesar 70,5. Skor tersebut meningkat sebanyak 18,4 secara rata-rata dari chatbot lama. Berdasarkan data yang diperoleh saat usability testing dan skor SUS dapat diambil kesimpulan bahwa interaksi dan pemilihan antarmuka yang tepat dapat meningkatkan user experience pada chatbot.","The presence of technology that is not prepared to face user interaction could worsen the user experience. There are still many chatbots reminder that can not fulfill the users' expectations. User-centered design approach can help to overcome the problems associated with the user.

In this research, Google Design Sprint as a user-centered design approach was used in the analysis until testing phase. Interaction was the main problem to be analyzed and was expected to improve the user experience. Design recommendation and prototype for a new chatbot refers to the result of usability testing of four previous chatbots. Testing was done by doing usability testing of a new chatbot. A total of five different participants were used each chatbot during usability testing. The results of usability testing of the new chatbot prototype were compared to the previous chatbots.

The result was the new chatbot has a SUS (System Usability Scale) score of 70.5. The score increase of 18.4 from previous chatbots on average. Based on data obtained from usability testing and SUS score, it was concluded that interaction and appropriate selection of interfaces could improve the user experience of chatbot. ",,,64072,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111160,,PRETTY M LOUIS,SISTEM PAKAR BERBASIS ATURAN SEBAGAI ALAT BANTU DIAGNOSIS SKIZOFRENIA,,,"expert system, schizophrenia, rule based, forward chaining, certainty factor","Sri Mulyana, Drs., M.Kom",2,3,0,2017,1,"Skizofrenia adalah gangguan pervasif yang memengaruhi lingkup yang luas
dari proses psikologis mencakup kognisi, afek, dan perilaku. Skizofrenia ditandai dengan
kelakuan sosial yang abnormal dan ketidakmampuan untuk membedakan mana
yang nyata dan tidak nyata. Masyarakat Indonesia masih mengaitkan gejala-gejala
yang ditampakkan oleh penderita gangguan Skizofrenia dengan hal-hal magis. Seiring
dengan tingginya jumlah penderita gangguan Skizofrenia dan terbatasnya jumlah
pakar yang ada, dibutuhkan sebuah sistem yang dapat digunakan untuk membantu
mendiagnosis Skizofrenia.
Sistem pakar adalah sistem yang bertindak seperti seorang pakar dalam suatu
bidang. Pada penelitian ini, sistem pakar diimplementasikan sebagai alat bantu diagnosis
Skizofrenia. Sistem ini dibuat berbasis aturan dengan metode inferensi forward
chaining serta menggunakan metode certainty factor untuk menyelesaikan masalah
ketidakpastian. Sistem dibangun untuk digunakan oleh dua jenis pengguna, yaitu
pakar dan paramedis. Pakar dapat memperbarui pengetahuan yang digunakan pada
proses inferensi, sementara paramedis hanya dapat melakukan diagnosis terhadap
pasien.
Proses pengujian dilakukan dengan mencocokkan hasil yang diberikan sistem
setelah memasukkan input dengan perhitungan manual dan diagnosis pakar. Input
sistem ini berupa gejala-gejala yang diamati dari pasien serta tingkat kepercayaan
masing-masing gejala tersebut. Output yang dihasilkan sistem ini berupa jenis gangguan
serta keterangannya, saran penanganan gangguan, gejala yang diberikan oleh
pengguna dan tingkat kepercayaannya, serta tingkat kepercayaan dari hasil diagnosis.
Pengujian dilakukan sebanyak lima kali.","Schizophrenia is a pervasive disorder which affects the broad scope of the
psychological processes including cognition, affect, and behavior. Schizophrenia is
characterized by abnormal social behavior and the inability to distinguish between
what is real and unreal. Many Indonesians still associate the symptoms displayed by
patients with Schizophrenia disorder with supernatural things. Along with the high
number of people with Schizophrenia disorders and the limited number of experts
who exist, a system which could be used to help diagnose Schizophrenia is needed.
Expert system is a system which acts like an expert in a field. In this study,
the expert system is implemented as a tool for the diagnosis of Schizophrenia. The
system is developed with rule-based inference with forward chaining method and
using certainty factor to solve the uncertainty problem. The system is developed
to be used by two types of users, i.e. experts and paramedics. Experts can update
the knowledge used in the process of inference, while paramedics can only make a
diagnosis of the patient.
The testing process is done by comparing the results given by the system after
user giving the input with manual calculations and expert diagnosis. The input of
this system is in the form of the observed symptoms of the patient as well as the
confidence level of each of these symptoms. The output produced by this system
is the type of disorder as well as the explanation, disorder treatment, the symptoms
given by the user and the level of confidence, and the confidence level of diagnosis.
The testing was done five times.",,,61920,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112440,,HARRYSON BARINGIN SAMOSIR ,Analisis Keamanan Website Dengan Protokol Keamanan SSL Terhadap Serangan Packet Sniffing,,,"Protokol Keamanan, SSL, Informasi, Serangan Ilegal, Paket Data, Ettercap, Driftnet, ARP Poisoning, Keamanan, Sertifikat SSL ","Yohanes Suyanto Dr., M. I. Kom.; I Gede Mujiyatna S. Kom., M. Kom.; Medi Drs., M. Kom.",2,3,0,2017,1,"Perkembangan penggunaan internet yang semakin lama semakin canggih memungkinkan penggunanya untuk dapat melakukan pertukaran informasi hanya dengan menekan tombol saja. Informasi juga bermacam-macam dan pastinya ada informasi yang bersifat sensitif dan tak ingin orang lain mengetahuinya. Namun melakukan transmisi melalui internet memiliki kelemahan dibagian keamanannya dan merupakan masalah utama. Protokol Kemanan SSL merupakan salah satu usaha yang dilakukan untuk mencegah terjadinya serangan-serangan ilegal yang dapat mengekspos informasi dari pengguna. Ketika Komputer mengirim data melalui jaringan, data tersebut akan ditrasmisi kedalam bentuk paket-paket data. Sniffing adalah satu cara yang dapat digunakan untuk mencuri informasi dari paket-paket data yang mengalir tadi. Informasi yang diperoleh dari paket data tersebut pun macam-macam ada yang berupa password, id username, e-mail, atau informasi sensitif lainnya. Pada penelitian ini dilakukan 2 tahap uji coba pencurian paket data yang pertama hasilnya berupa username dan password sebuah akun, uji coba kedua hasilnya berupa gambar. Aplikasi yang digunakan adalah Ettercap untuk data berupa teks dan Driftnet untuk data berupa gambar. Serangan yang dilakukan terhadap protokol keamanan SSL adalah dengan ARP Poisoning. ARP Poisoning adalah sebuah cara dimana penyerang membangun sebuah koneksi antara client dengan server dan menjadikan dirinya sebagai jembatan yang membuat penyerang dapat melihat atau memonitor segala aktivitas client dengan server. Dari hasil pengujian diperoleh bahwa Protokol Kemanan SSL dalam penelitian ini belum sudah menjamin 100% keamanan pengguna dan website yang menggunakannya. Namun soal kemanan kembali lagi kepada pengguna pada saat mengakses internet karena metode yang digunakan pada penelitian ini bertujuan untuk mengelabui pengguna terbukti bahwa username/password dan gambar terekam ketika target pengguna mengakses sebuah website dengan menghiraukan policy dari sertifikat SSL. Hal ini dapat membahayakan privasi dari pengguna dan kenyamanan mengakses internet, sehingga sangat diperlukan peningkatan sistem keamanan lebih lanjut untuk mencegah serangan ilegal yang dapat mengganggu pengguna.","The use of the Internet at this time is growing rapidly allow user to be able to exchange information by simply pressing buttons. There is many type of information and definitely there is some sensitive information that we need to keep it safe from outsiders. But transmitting data via internet has flaws, the most major problem that we need to face is the security system. Protocol Security SSL is one of many way to keep user safe. When computer sends data over the network, the data is transmitting into parts called packet data. Sniffing is one of many way to steal information inside packet data. Information that can be obtained from Sniffing are password, username, e-mail, or other sensitive information. This research conducted two phase of trial the first trial yield username and password for website account, the second trial yield images. Applications that used in the research are Ettercap and Driftnet, Ettercap for text data and driftnet for images data. The attack done by using a technique called ARP Poisoning. ARP Poisoning is a technique where attacker build a connection between the
client and server connections and establish itself as the connection that makes the attacker can view or spy all the activities between the client and the server which allow attacker to look at the contents inside packet data that flow from client to server. From the test results showed that the security protocol SSL network is still lacking and can not guarantee 100% that user and website are safe. Proved
that username / password and the image data being recorded when a user accessing a multiple of websites. This may threaten the privacy of users and the convenience of accessing the internet, so it is necessary to increase the security system to prevent further attacks by Packet Sniffer or any other illegal activity. Moreover the level of SSL security depends on the classification of SSL itself. SSL is divides into 5 level called class. Each class has a different security level and different purposes.",,,63499,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132410,,IKHSAN PERMADI KUSUMAH,SISTEM REKOMENDASI KULINER MENGGUNAKAN METODE ANALYTIC HIERARCHY PROCESS BERDASARKAN ANALISIS SENTIMEN TWITTER,,,"sistem rekomendasi kuliner, analisis sentimen, analytic hierarchy process, naive bayes, twitter, foursquare","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2017,1,"	Semakin banyaknya tempat makan di Yogyakarta terkadang membuat pecinta kuliner kesulitan memilih lokasi yang tepat untuk dikunjungi. Di sisi lain dengan berkembangnya teknologi, makin banyak situs pencarian tempat makan, tetapi tidak dapat merekomendasikan pilihan mana yang sesuai dengan keinginan pecinta kuliner. Review dan komentar di internet juga belum tentu sesuai dengan keinginan karena selera setiap orang berbeda-beda.

	Dalam penelitian ini dibuat sistem rekomendasi kuliner di Yogyakarta berdasarkan empat kriteria, yaitu harga, jarak, keramaian, dan review. Kriteria harga, jarak, dan keramaian diperoleh langsung maupun dengan menghitung data dari Foursquare dan Twitter, sedangkan kriteria review dibangun berdasarkan analisis sentimen tweet yang berkorelasi dengan tempat makan di Yogyakarta. Selain empat kriteria tersebut, pengguna juga dapat menambahkan kriteria lainnya. Sistem rekomendasi yang digunakan adalah metode Analytic Hierarchy Process (AHP) sedangkan analisis sentimen menggunakan metode Naive Bayes.

	Data tempat makan di Yogyakarta dikumpulkan menggunakan Foursquare API, kemudian digunakan Twitter API untuk mendapatkan tweet yang berhubungan dengan tempat makan tersebut. Proses analisis sentimen terhadap tweet menghasilkan akurasi rata-rata 94,21% setelah diuji sebanyak 10 kali menggunakan metode k-fold cross validation. Sistem yang dibuat kemudian diujikan kepada 10 pengambil keputusan dan semuanya menyimpulkan bahwa sistem dapat memberikan rekomendasi sesuai harapan.
","	The increasing number of restaurants in Yogyakarta sometimes causes confusion to choose the perfect place to eat. While the technology development makes it easier to look for restaurants through the website, it is still lacking in recommendation system. Reviews and comments on the internet might not be suitable for another because everyone has different preferences.
	This research creates a culinary recommendation system in Yogyakarta based on four criteria, namely price, distance, crowd, and review. The value criteria for price, distance, and crowd are obtained directly or by calculating data from Foursquare and Twitter, while the review criteria is built based on tweet sentiment analysis that correlates with restaurants in Yogyakarta. The recommendation system used is Analytic Hierarchy Process (AHP) method while sentiment analysis using Naive Bayes method.
	Restaurants data in Yogyakarta are collected by using Foursquare API, then tweets related to the restaurants are collected by using Twitter API. The process of sentiment analysis on tweets yields an average accuracy of 94.21% after being tested 10 times using the k-fold cross validation. The system is tested to 10 decision makers and all conclude that the result suits their preferences.",,,83441,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113722,,MUHAMMAD RIFQI F P D,EVALUASI EMPIRIS DARI GATED UNIT RECURRENT NEURAL NETWORK SEBAGAI METODE UNTUK VOICE ACTIVITY DETECTION,,,"voice activity detection, recurrent neural network, gated recurrent unit, long short-term memory, deep learning","Anifudin Azis, S.Si., M.Kom",2,3,0,2017,1,"Voice activity detection (VAD), juga disebut sebagai speech activity detection, adalah proses pengidentifikasian secara otomatis, pada bagian kapan sinyal suara mengandung speech dan yang tidak mengandung speech. Semakin tinggi tingkatan noise dari suatu sinyal suara pengucapan, performa dari metode VAD-pun semakin
buruk. Permasalahan tersebut menjadi objek penelitian dari VAD, dan salah satu metode yang memberikan performa yang baik ialah Recurrent Neural Network (RNN). Seperti yang sudah dilakukan pada penelitian sebelumnya, RNN dengan menggunakan Long Short-Term Memory (LSTM) mampu menunjukkan performa yang lebih baik dari metode state-of-the-art berbasis statistik. Performa dari LSTM-RNN juga pernah dievaluasi dengan model deep learning lain seperti Convolutional Neural Network (CNN) dan Deep Neural Network (DNN), dan menunjukkan performa yang lebih unggul dibandingkan dengan model deep learning tersebut. Selain LSTM-RNN, terdapat mekanisme gating lain dari LSTM-RNN, yaitu GRU-RNN. Penggunaan GRU-RNN belum pernah dibahas dan diimplementasikan pada VAD. Penelitian ini membahas tentang evaluasi performa secara empiris dari arsitektur RNN dengan mekanisme gating yaitu GRU-RNN dan LSTM-RNN sebagai metode untuk VAD.

GRU-RNN dievaluasi dan dibandingkan dengan LSTM-RNN dan tanh-RNN pada suara pengucapan bersih dan suara pengucapan dengan noise pada SNR 5 dB, 0 dB, dan -5 dB, menggunakan metrik Area Under-ROC-Curve (AUC) untuk deteksi per-frame. Hasil dari eksperimen menunjukkan bahwa GRU-RNN mampu mengungguli LSTM-RNN dan tanh-RNN pada suara pengucapan bersih, suara yang pekat oleh noise pada SNR 0 dB, dan -5 dB, dengan AUC 0.9684, 0.9142, dan 0.8504. Didapat bahwa GRU-RNN mampu mengungguli LSTM-RNN dan tanh-RNN pada suara pengucapan dengan noise tinggi, sehingga GRU-RNN dapat digunakan sebagai salah satu metode VAD.","Voice activity detection (VAD) also refered to as speech activity detection, is an automatic identification process, on which part a speech signal contains speech and which does not contain speech. The higher the noise level of a speech sound signal, the performance of the VAD method is worse. That problem becomes the research object of VAD, and one of the methods that gives good performance is Recurrent Neural Network (RNN). As has been done in previous studies, RNN using Long Short-Term Memory (LSTM) is able to show a better performance than the state-of-the-art statistic based methods. The performance of LSTM-RNN has also been evaluated with other deep learning models such as the Convolutional Neural Network (CNN) and Deep Neural Network (DNN), and shows superior performance compared to the other deep learning model. In addition to LSTM-RNN, there is another gating mechanism from LSTM-RNN, called GRU-RNN. The use of GRU-RNN has not been discussed and implemented in VAD. This experiment discusses the empirical performance evaluation of the RNN architecture with gating mechanism of GRU-RNN and LSTM-RNN as a method for VAD. 

GRU-RNN is evaluated and compared with LSTM-RNN and tanh-RNN on clean speech and noise robust speech with SNR 5 dB, 0 dB, and -5 dB, using Area Under-ROC-Curve (AUC) for detection on each frame. The result of the experiments show that GRU-RNN is able to outperform LSTM-RNN and tanh-RNN in clean speech, noise robust speech on SNR 0 dB, and 5 dB with AUC 0.9684, 0.9142, and 0.8504 respectively. It is found that GRU-RNN is able to outperform LSTM-RNN and tanh-RNN on noise robust speech, so GRU-RNN can be used as one of VAD method.",,,64483,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129595,,MUHAMMAD NU`MAN,SISTEM REKOMENDASI PARIWISATA BERBASIS ANDROID DENGAN METODE SMARTER (SIMPLE MULTI-ATTRIBUTE RECOMMENDATION TECHNIQUE EXPLOITING RANK),,,"Sistem Rekomendasi, SMARTER, Utility-based, Android, SQLite","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2017,1,"Sistem informasi dibangun untuk mempermudah pengguna dalam memperoleh informasi, tidak terkecuali dibidang pariwisata. Banyaknya informasi yang ada, membuat pengguna menjadi kesulitan dalam  menentukan tujuan wisata sehingga perlu sistem yang dapat memberikan rekomendasi destinasi wisata. Sistem dibangun berbasis android seiring dengan lingkungan masyarakat yang mulai berkembang mengikuti kemajuan teknologi menjadi mobile-friendly environment.

Banyak metode yang dapat digunakan dalam perhitungan rekomendasi, salah satunya SMARTER (Simple Multi-Attribute Recommendation Technique Exploiting Rank), yaitu metode yang menggunakan beberapa atribut sebagai parameter untuk dijadikan pertimbangan dalam menentukan pilihan alternatif. Alternatif-alternatif yang ada, akan diseleksi berdasarkan atribut yang telah ditentukan. Implementasi sistem menggunakan basis android dengan bahasa pemrograman Java untuk pemrosesan data dan XML untuk tampilan antarmuka (User Interface). Penyimpanan data dilakukan dengan menggunakan SQLite yang terintegrasi dengan Android Studio sebagai media untuk membangun sistem.

Hasil penelitian ini berupa aplikasi mobile yang dapat memberikan rekomendasi wisata berdasarkan perhitungan dengan metode SMARTER dan dapat diimplementasikan pada berbagai jenis perangkat dengan  merk dan versi android yang berbeda dengan minimal versi 4.1.0 atau Jelly Bean. Aplikasi memiliki tingkat kemanfaatan yang cukup baik pada aspek memudahkan user dalam memperoleh rekomendasi dengan score 7,556 dan pencapaian tertinggi dari sistem terdapat pada aspek mempermudah user dalam menentukan destinasi wisata dengan score 7,833 dan pencapaian terendah dari sistem terdapat pada aspek tampilan antarmuka dengan score 5,278.","Information systems are built to ease users to acquire information, including information in tourism sector. Excessive informations available will complicate users in making decisions of their travel destinations therefore a system needed to provide users with travel destination recommendation (to ease users to choose their travel destinations according to their needs). The system is built on Android  to follow the suit of society that began to grow following the advancement of technology into a mobile-friendly environment.

Many methods that can be used in recommendation system, such as SMARTER (Simple Multi-Attribute Recommendation Technique Exploiting Rank) which is a method that use few attributes as parameters to be considered in determining alternative options. Existing options will be selected based on specified attributes. Implementation of the system using Android base with JAVA programming language for data processing and XML for user interface. Data storage is held by using SQLite that integrated with Android Studio as development environment to build the system.
 
The results of this research is a mobile application that can give recommendations about tourist destinations based on calculations using SMARTER method and can be implemented on different types of devices with different brands and android versions with a minimum version 4.1.0 or Jelly Bean. Applications have a good level of usefulness on the aspect of facilitating the user to get recommendations with score 7.556, and the highest achievement of the system contained on the aspect of facilitating the user to decide the tourist destinations with score 7.833 and the lowest achievement of the system is in the user interface with score 5.278.
",,,80495,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132678,,LISTIYANI WULANDARI,RANCANG BANGUN WEB SERVICE KETERSEDIAAN KAMAR KOSONG PADA RUMAH SAKIT MENGGUNAKAN XML DAN JSON,,,"web service, rumah sakit, XML, JSON / web service, hospital, XML, JSON","Moh. Edi Wibowo, S.Kom., M.Kom., Ph. D",2,3,0,2017,1,"Saat ini penggunaan web service pada teknologi-teknologi dalam beberapa sistem terditribusi semakin beragam. Salah satunya untuk membuat rancang bangun web service  Ketersediaan kamar kosong dirumah sakit. Dalam keadaan yang sangat mendesak, masyarakat menuntut untuk mendapatkan informasi secara cepat mengenai ketersediaan kamar kosong dan spesifikasinya. Akan tetapi, belum ada
aplikasi rancang bangun web service yang memuat informasi tentang ketersediaan kamar kosong, termasuk spesifikasi harganya dan pemesanan kamar secara langsung.
Pada penelitian ini dikembangkan aplikasi rancang bangun web service ketersediaan kamar kosong yang memuat informasi tentang rumah sakit sebagai tempat rujukan rawat inap. Aplikasi ini memiliki fitur berupa pesan kamar, melihat data nama dokter praktek, serta mencari informasi kamar yang diinginkan. Aplikasi ini menggunakan perbandingan waktu antara XML dan JSON. Sistem ini dibangun berbasiskan web service menggunakan bahasa pemrogramman PHP yang menggunakan Adobe Dreamweaver serta Java Script untuk pengelolaan databasenya.
Sistem telah selesai dibangun sesuai dengan perancangan. Dari hasil pengujian telah dilakukan beberapa percobaan fitur yang memuaskan bagi para pengguna, karena sangat berguna bagi banyak masyarakat.","Recently the applications of web service on the technologies in some distributed systems are increasingly diverse. One of the applications is in the making of a web service design of the availability of empty inpatient rooms in hospitals. In
urgent desperate moment, people are required to seek for instant information on the availability of empty rooms also its price specifications. However, there has not been
a web service design application that contains information about inpatient rooms, whether its availability, specifications prices, and direct booking system.
In this study, the writer developed a web service design application to check the availability of empty rooms which contains the information about the hospitals as an inpatient referral facility. This application includes some features such as room booking, the list of doctors or specialist, and room specification. This application uses the time comparison between XML and JSON. This system was built based on web service using PHP programming language that uses Adobe Dreamweaver and Java Script for its database management.
The system has been completed and run successfully corresponding with the design. From the results of the experiments, the features were rated as satisfactory by
the users because it is considered to be very useful for many people.",2018-01-19 00:00:00,53,83658,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113734,,EKKI RINALDI,Opinion Mining pada Komentar Video YouTube menggunakan Support Vector Machine,,,"opinion mining,youtube,support vector machine,bahasa Indonesia","Aina Musdholifah, S.Kom., M.Kom., Ph.D.",2,3,0,2017,1,"Penelitian ini melakukan opinion mining komentar berbahasa Indonesia dari sebuah video Youtube mengenai resensi smartphone sehingga dapat diketahui sentimen dan tipe komentar terhadap video menggunakan algoritma Support Vector Machine. Metode pendekatan yang dilakukan terdiri dari pendekatan bag-of-words (FVEC) dan pendekatan chunking (STRUCT). Penelitian diujikan pada tiga kategori, kategori SENTIMENT, kategori TYPE, dan kategori ALL. Ada empat jenis fungsi kernel yang diujikan yaitu linear, polynomial derajat 2, polynomial derajat 3, dan RBF. Pengujian dibagi menjadi dua tahap, tahap pertama dilakukan untuk menentukan fungsi kernel terbaik, dan tahap kedua dilakukan untuk menentukan metode pendekatan yang paling baik (FVEC atau STRUCT).
	Fungsi kernel linear pada SVM untuk opinion mining kategori SENTIMENT, TYPE, dan ALL mempunyai kinerja terbaik dibandingkan fungsi kernel polynomial derajat 2, polynomial derajat 3, dan RBF. Pendekatan FVEC pada SVM fungsi kernel linear lebih baik dibandingkan pendekatan STRUCT. Secara keseluruhan, berdasarkan percobaan metode SVM linear FVEC mampu mengklasifikasikan komentar video youtube dengan akurasi 81,77% untuk kategori SENTIMENT, 69,67% untuk kategori TYPE, dan 62,76% untuk kategori ALL.","This research performs opinion mining on Indonesian smartphone review comment from YouTube video using Support Vector Machine. The approach consisting of the bag-of-word method (FVEC) and chunking method (STRUCT). There are three categories tested, SENTIMENT category, TYPE category, and ALL category. There is four kernel function tested, linear, polynomial degree 2, polynomial degree 3, and RBF. Testing is divided into two stages, the first stage is performed to determine the best kernel function, and the second stage is to determine the best approach method (FVEC or STRUCT).
	Linear kernel function SVM for opinion mining on SENTIMENT, TYPE, and ALL category perform better than polynomial and RBF kernel function. FVEC approach method has proven better result than STRUCT approach. Overall, based on the experimental SVM linear method FVEC was able to classify YouTube video comments with 81,77% accuracy for SENTIMENT category, 69,67% for TYPE category, and 62,76% for ALL category.",,,64588,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
108615,,YOHANES ADI DHARMAWAN,ALGORITMA GLOBAL-BEST HARMONY SEARCH TERMODIFIKASI PADA PERMASALAHAN ALOKASI RUANG KERJA,,,"metaheuristik, optimasi kombinatorial, algoritma global-best harmony search, office space allocation, algoritma greedy","Aina Musdholifah, S.Kom., M.Kom., Ph.D.",2,3,0,2017,1,"Permasalahan alokasi ruang kerja merupakan permasalahan yang sering dijumpai pada proses organisasi sebuah institusi. Penyelesaian permasalahan tersebut memberikan dampak pada efisiensi penggunaan sumber daya (terutama ruang) dan sinergi antar-anggota. Akan tetapi permasalahan alokasi ruang kerja merupakan sebuah permasalahan optimasi kombinatorial yang memiliki kompleksitas eksponensial, pencarian solusinya yang optimal merupakan tugas yang sulit. Metaheuristik merupakan salah satu pendekatan untuk menyelesaikan permasalahan tersebut, terdapat sebuah metaheuristik yang telah dikembangkan dalam dua dekade ini, yaitu algoritma Harmony Search (HS).
Pada penelitian ini, akan dikembangkan suatu modifikasi dari sebuah varian algoritma HS, yaitu algoritma Global-best HS, untuk menyelesaikan permasalahan alokasi ruang kerja. Modifikasi tersebut meliputi penggunaan metode inisialisasi peckish untuk pembentukan populasi awal, memperkuat penggunaan konsep swarm intelligence dari algoritma Global-best HS, kustomisasi prosedur-prosedur random consideration dan pitch adjustment sehingga sesuai dengan permasalahan alokasi ruang kerja, dan penggunaan nilai parameter pencarian yang dinamis, mengacu dari algoritma Improved HS.
Algoritma yang dikembangkan akan diujikan pada 7 problem dataset permasalahan alokasi ruang kerja dari dunia nyata. Hasil pengujian menunjukkan algoritma memberikan hasil yang lebih baik daripada hasil penelitian sebelumnya pada 3 problem dataset, hasil yang sama dengan hasil penelitian sebelumnya pada 1 problem dataset, dan 3 problem dataset lainnya dengan hasil lebih buruk, namun dengan selisih yang tidak terlalu besar. Selain itu, ditunjukkan bahwa nilai parameter HMS yang optimal berbeda-beda pada setiap problem dataset dan dipengaruhi oleh tingkat kompleksitas dari problem dataset.
","Office space allocation is one of several problems which emerged from an institution s organisational tasks. Solving this problem would yield in more resource-efficient (especially space) institution and giving synergies among the institution's members. However office space allocation problem is a combinatorial optimisation problem which requires exponential computation in finding the optimal solution, tackling this problem would be a challenging task. One approach to solve this problem is metaheuristic, in the last two decades a new metaheuristic was proposed, and still being a research interest by scientists to this date, which is named Harmony Search Algorithm (HSA).
In this research a new modification of HSA is proposed, derived from the Global-best HSA, to tackle the office space allocation problem. The modification includes introducing peckish initialisation method for initial population, enhancing the Global-best HSA s swarm intelligence concept, customising basic HSA s random consideration and pitch adjustment routinues to meet up with the problem's nature, and incorporating the concept of dynamic parameter value, adopted from Improved HSA.
The proposed algorithm will be tested on 7 datasets from real world problems. The test results show that the proposed method outperforms earlier researches test results on 3 datasets, agrees on 1 dataset s test result, and is outperformed on 3 datasets, only by narrow margins. Furthermore, the results show that the optimal value of HMS parameter is varied for each dataset, depending on the dataset s complexity.
",,,59265,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
108106,,HIDAYAT MUKMIN,ANALISIS KINERJA APLIKASI HTTP PROXY DENGAN PEMECAHAN KONEKSI,,,"proxy, HTTP, uji kinerja","Triyogatama Wahyu Widodo, S.Kom., M.Kom.",2,3,0,2017,1,"Salah satu cara untuk meningkatkan kecepatan pada HTTP adalah dengan melakukan pemecahan koneksi pada HTTP. Hal ini diimplementasikan dengan membuat HTTP proxy yang akan membuat koneksi tambahan untuk suatu request pada protokol HTTP. Penelitian ini melakukan pengujian kinerja terhadap proxy HTTP tersebut dengan melakukan pengunduhan file berukuran besar dan diamati peningkatan kecepatan antara tanpa koneksi tambahan dan dengan koneksi tambahan. Pemakaian memori juga akan diamati untuk setiap koneksi tambahan. Hasil pengujian menunjukkan bahwa kenaikan kecepatan yang signifikan terjadi pada penambahan dua koneksi dan konsumsi memori maksimal dapat mencapai 150 % dari ukuran file yang diunduh. Konsumsi memori bergantung pada ukuran file dan jumlah koneksi tambahan yang digunakan. Selain itu, ukuran file maksimal yang dapat dilakukan pemecahan koneksi adalah 2 GB.",One of the ways to increasing speed in HTTP is using connection splitting for HTTP. The system implemented with HTTP proxy and will create additional connection. The experiment is analyse performance of HTTP proxy by downloading big files using connection splitting method then comparing the result without connection splitting and monitoring memory usage. The experiment have results that increasing speed happen significantly after using two additional connection and also increasing memory usage as much as number of additional connection and file size. Memory usage up to 150 % from original file size. The maximal file size to do connection splitting is 2 GB.,,,58680,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
108876,,MUHAMMAD NURUL HAKIM,RANCANG BANGUN APLIKASI PENCATAT KEUANGAN   D EXPENSE MANAGER&Atilde;&cent; BERBASIS ANDROID,,,"Android, Pencatat Keuangan, SQLite, Aplikasi D Expense Manager","Agus Sihabuddin, Dr",2,3,0,2017,1,"Keuangan merupakan salah satu elemen yang penting untuk kelangsungan hidup seseorang. Untuk mengatur keuangan dengan teratur, maka diperlukan literasi finansial yang baik. Namun, tidak sedikit individu dengan tingkat literasi finansial yang rendah dikarenakan berbagai faktor yang mempengaruhinya. Adapun seiring dengan perkembangan teknologi informasi dan komunikasi, aliran informasi menjadi lebih cepat untuk mencapai pengguna sehingga mempermudah terjadinya proses globalisasi. Dibalik hal tersebut muncul dampak perilaku konsumtif bagi sebagian orang, terutama kawula muda. Oleh karena itu, dibuat sebuah aplikasi pencatat keuangan untuk membantu mencatat aliran finansial sehari-hari yang dilakukan penggunanya.
Pada penelitian ini dibangun aplikasi D Expense Manager yang berbasis Android agar dapat membantu pengguna mengatur keuangannya. Aplikasi dibangun menggunakan metode prototyping. Kebutuhan dari aplikasi ini didapatkan langsung dari berbagai kalangan pengguna melalui survey. Pengembangan aplikasi ini dimulai dari tahap perancangan UML, antar muka, database, pengkodean hingga pengujian aplikasi dan peluncuran pada Google Play Store.
Aplikasi akan menyimpan data transaksi yang telah dimasukkan oleh pengguna pada SQLite database sehingga aplikasi ini memiliki ukuran yang cukup kecil. Aplikasi yang dibangun juga memberikan fitur reminder untuk mengingatkan pengguna terhadap suatu data transaksi, serta fitur history yang lebih informatif dengan adanya penampil data sesuai dengan kategori tertentu. Berdasarkan hasil pengujian, seluruh fungsionalitas yang disebutkan sebelumnya dapat berjalan dengan baik sesuai dengan rancangan yang dibuat. Selain itu aplikasi juga telah berhasil diuji coba pada beberapa perangkat Android yang berbeda dan dapat menjalankan fitur aplikasi ini dengan baik.
","The Financial aspect is important part in daily life. Good financial literacy required to manage finances regularly. But, there still few people with poor level of financial literacy due to various factors that influencing it. Flow of information become quickly to reach the user as development of information and communication technology, thus simplifying the globalization process. Therefore an impact of consumptive behavior is created for some people. Hence, an expense recorder application to help recording its user daily financial flow is created.
In this research, an application named D Expense Manager which is based on Android  is created. The application was built using prototyping methods. Application requirements is obtained directly from various circles of users via surveys. Development process of this application started from UML design phase, interfaces, databases, coding, application testing and application launch on the Google Play Store.
 The application will store transaction data that have been inputted by the user on a SQLite database, so this application is quite small on its size. The application that have been built also gives reminder feature to alert the user for a specific transaction data, and history feature that more informative with data viewer according to specific categories. Based on test results, the entire functionality that have been mentioned above could already be fulfilled by the application. In addition, the application has been successfully tested on several different Android devices and could run the application&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s feature well.
",,,59560,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114254,,M ERSAN RAMADHAN,IDENTIFIKASI TWEET BANJIR DI JAKARTA PADA DATA TWITTER MENGGUNAKAN ALGORITMA MULTINOMIAL NAIVE BAYES DAN SUPPORT VECTOR MACHINE,,,"Banjir, Multinomial Naive Bayes, Support Vector Machine","Moh Edi Wibowo, S.Kom, M.Kom., Ph.",2,3,0,2017,1,"Twitter merupakan salah satu media sosial yang umum digunakan oleh masyarakat. Masyarakat menuliskan informasi tentang banjir melalui media twitter dalam bentuk tweet. Beberapa tweet yang dituliskan tidak menunjukkan secara nyata bencana banjir yang sedang terjadi. Terdapat tweet tentang banjir menunjukkan tweet yang bukan kategori bencana, sebagai contoh banjir hadiah. Padahal informasi bencana banjir secara nyata dan cepat dibutuhkan untuk mengantisipasi banjir yang berkelanjutan. Oleh karena itu diperlukan sistem yang secara otomatis mengidentifikasi secara otomatis tweet banjir termasuk dalam kategori bencana atau banjir dalam arti lain.
Pada penelitian ini proses identifikasi tweet banjir menggunakan algoritma Multinomial Naive Bayes dan Support Vector Machine. Data yang digunakan merupakan tweet yang mengandung kata banjir yang berlokasi di DKI Jakarta. Data tweet banjir diidentifikasikan menjadi 2 kategori yaitu bencana dan bukan bencana. Jumlah tweet sebanyak 7789 sebagai data training dan 2327 sebagai data testing yang diambil pada 12 November 2014 sampai 30 Januari 2015.
Pengujian dilakukan dengan membandingkan algoritma Multinomial Naive Bayes dan Support Vector Machine. Pengujian juga dilakukan dengan menggunakan metode K-fold cross validation dengan jumlah iterasi 10 kali. Dari hasil pengujian didapatkan nilai akurasi terbesar dengan menggunakan algorima Support Vector Machine  dengan rata-rata akurasi 78.9%
","Twitter is one of the most common media sosial used by the society which is effective and fast in reporting the current situation, such as flood in DKI Jakarta. Flood tweets will be automatically identified so that it can provide information whether it is flooded or not flooded in certain area. The use of Twitter as a source of information regarding flood is potentially helpful to anticipate the occurrence of long-standing flood. That is why a system is needed to automatically categorized a flood containing tweets as flood as an event or other meaning
The identification process of flood tweets used in this research is Multinomial Naive Bayes and Support Vector Machine algorhytms. The data used are the tweets containing the word &quot;banjir&quot; located in DKI Jakarta. The data is identified into 3 categories, namely flooded, not flooded and unknown. Total tweets of 7789 is used as data training and 2327 tweets is used as data testing taken from November 12, 2014 - January 25, 2015.
The test is done by comparing Multinomial Naive Bayes dan Support Vector Machine algorithms. The test is also done by using K-Fold Cross Validation method with 10 times iteration. The test result shows that the greatest accuracy obtained by using Support Vector Machine algorithms with average accuracy of 78.9%.
",,,65000,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110671,,DANIS PRABANDANA,IMPLEMENTASI IMAGE PROCESSING PADA SISTEM PENYALAAN LAMPU OTOMATIS DENGAN RASPBERRY PI,,," Kata kunci : image processing, Raspberry Pi, histogram of oriented gradient, contour detection, background subtraction","Lukman Heryawan, S.T.,M.T.",2,3,0,2017,1,"Di dalam sebuah rumah, kita sering menemukan lampu menyala walaupun tidak digunakan. Penghuni rumah sering kali membiarkan lampu di dalam rumah menyala karena malas untuk mematikan, lupa, atau karena sedang terburu-buru. Oleh karena itu dibutuhkan suatu sistem yang dapat membuat lampu tersebut dapat menyala dan mati secara otomatis sesuai kebutuhan.
Pada penelitian ini, image processing akan diimplementasikan pada sistem penyalaan lampu otomatis dengan Raspberry Pi. Sistem ini dibuat menggunakan kamera yang dipasang pada Raspberry Pi untuk mendeteksi ada atau tidak adanya manusia di dalam suatu ruangan, yang kondisi tersebut diperlukan untuk menentukan menyala dan matinya lampu. Program dibuat menggunakan teknik background subtraction, histogram of oriented gradient, dan contour detection. Teknik background subtraction digunakan dengan background model ditentukan pada awal pengambilan video. Teknik histogram of oriented gradient diterapkan pada setiap frame yang diambil. Pada program, ditentukan nilai-nilai yang digunakan untuk membedakan antara manusia dan bukan manusia. Ketika ada manusia terdeteksi, teknik contour detection akan dijalankan, teknik ini dapat mendeteksi perubahan kontur dari video yang diambil, perubahan kontur yang dimaksud ialah gerak manusia.
Setelah sistem dibuat, dapat disimpulkan bahwa sistem dapat berjalan dengan baik. Pada proses deteksi manusia digunakan variabel nilai sliding window (4,4), nilai padding (16,16), sedangkan pada proses deteksi gerak manusia digunakan variabel nilai alpha (0,3), nilai threshold 5, dan nilai min_area 100 sehingga sistem dapat membuat lampu menyala ketika terdapat manusia di dalam ruangan dan lampu mati ketika tidak terdapat manusia di dalam ruangan.
","In a house, we often find lights that not being used is left on by residents. Residents usually left the lights on because for laziness to turning it off, forget to turning it off, or because they was in a hurry. Therefore we need a system that can make the lightss on and off automatically as needed. 
In this research, image processing will be implemented on light automation systems using Raspberry Pi. The system is made using a camera mounted on a Raspberry Pi to detect the presence or absence of a human in a room, which is the necessary condition for determining of the on-off lights. Programs created using background subtraction, a histogram of oriented gradient and contour detection technique. Background subtraction technique used by the background model specified at the beginning of the video capture. Histogram of oriented gradient applied to each frame captured. In the program, the specified values are used to differentiate between human and nonhuman. When there is a detectable human, contour detection technique is executed, this technique can detect changes in the contour of the captured video, which meant changes in the contour is human movement.
Once the system is created, it can be concluded that the system can run well. In the process of human detection is used sliding window value (4.4), the value of the padding (16.16), while in the process of motion detection is used alpha value (0.3), the threshold value of 5, and the value of min_area 100 so that the system can turn the lights on when there is a person in the room and turn the lights off when there are no person in the room.
",2017-04-10 00:00:00,53,61390,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107345,,RATRI ABDATUSH S,ANALISIS KINERJA INDEXING DAN SEARCHING MENGGUNAKAN PENGATURAN SHARD DAN STRUKTUR INDEKS PADA ELASTIC SEARCH,,,"shard, struktur indeks, indexing, searching, Elastic Search","Mardhani Riasetiawan, M.T.",2,3,0,2017,1,"Proses indexing dokumen digunakan untuk keperluan pencarian dokumen atau publikasi ilmiah. Publikasi ilmiah berupa dokumen skripsi bagian abstrak membutuhkan proses indexing dalam pengelolaannya. Kinerja proses indexing berupa ukuran indeks dan kecepatan waktu pencarian dipengaruhi oleh banyak faktor, yaitu arsitektur sistem, struktur indeks, spesifikasi mesin dan karakteristik data. 
Penelitian ini mengeksplorasi peran shard dan struktur indeks terhadap kinerja proses indexing dan searching pada Elastic Search. Elastic Search merupakan mesin pencari open source terdistribusi yang mengakomodasi kebutuhan penyimpanan data besar dan kecepatan pencarian dengan performa yang baik. Tahapan yang dilakukan adalah akuisisi data, ekstraksi data menggunakan Apache Tika dan pengaturan struktur indeks, pengaturan jumlah shard pada single node serta pengujian indexing dan searching. Pengaturan struktur indeks memisahkan konten dokumen menjadi field judul, nama penulis, nim, abstrak dan kata kunci. Pengaturan jumlah shard menggunakan 1 shard, 5 shard, 10 shard dan 15 shard. 
Hasil pengujian menunjukkan bahwa pengaturan jumlah shard dan pengaturan struktur indeks berpengaruh pada kinerja indexing dan searching pada Elastic Search berdasarkan parameter ukuran indeks dan waktu pencarian. Ukuran indeks yang dihasilkan berbanding lurus dengan jumlah shard yang digunakan. Pengaturan struktur indeks membuat ukuran indeks menjadi lebih kecil. Waktu pencarian berada pada titik optimal pada penggunaan 15 shard pada ekstraksi data yang struktur indeksnya telah diatur.","Indexing documents used for searching documents or scientific publications. Abstract sections of Scientific publications require indexing process management. Index size and search times of indexing performance is influenced by many factors, including system architecture, the index structures, the engine specifications and the data characteristics.
This research explores the role of index structure and shards influence indexing and searching performance in Elastic Search. Elastic Search is a open source search engine that accommodates the needs of distributed big data storage and search speed with good performance. Methods being taken is the data acquisition, data extraction using Apache Tika, setting the index structure, setting the number of shard on a single node, indexing and searching. The configuration of the index structure separate content into the fields that contain of document title, author name, nim, abstract and keywords. Setting the amount of shard using 1 shard, 5 shards, 10 shards and 15 shards.
The results show that the number of shard and configuration of the index structure affected the performance indexing based on the parameter of the index size and search times. The configuration of the index structure makes the index size become smaller. The search times get optimum value when using 15 shard and data extraction that has been configured.",,,57991,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114001,,TYAS NUUR KHOOLISH,GATED RECURRENT UNIT - RECURRENT NEURAL NETWORK UNTUK PERAMALAN NILAI TUKAR MATA UANG RUPIAH TERHADAP DOLAR AMERIKA,,,"GRU, RNN, Peramalan Nilai Tukar, USD-IDR","Agus Sihabuddin, S.Si., M.Kom., Dr",2,3,0,2017,1,"Nilai tukar mata uang yang fluktuatif mempengaruhi kebijakan di bidang ekonomi karena merupakan salah satu faktor pengambilan keputusan. Maka dari itu dibutuhkan sebuah model peramalan untuk memprediksi nilai tukar sehingga fluktuasi nilai tukar mata uang dapat diperkirakan. Salah satu model tersebut adalah Recurrent Neural Network (RNN). RNN kemudian berkembang dengan adanya tambahan unit gating seperti Gated Recurrent Unit (GRU-RNN) yang bertujuan untuk meminimalisasi terjadinya vanishing gradient seperti yang terjadi pada RNN konvensional (tanh-RNN). 
Pada penelitian ini, digunakan model GRU-RNN untuk meramalkan nilai tukar mata uang Rupiah terhadap Dollar Amerika (USD-IDR). Model dilatih dan dievaluasi dengan data nilai tukar Rupiah terhadap Dolar Amerika periode 20 Oktober 2014-31 Maret 2017. Arsitekur GRU-RNN yang optimal adalah dengan jumlah masukan lag 3 dan 1 hidden layer dengan jumlah hidden unit sebanyak 4.  
Hasil penelitian ini menujukkan model GRU-RNN memiliki nilai unjuk kerja RMSE, MAE, MAPE, dan Dstat yang masing-masing sebesar 62.82839, 45.62825, 0.342715, dan 69.67%. Nilai unjuk kerja Dstat yang melebihi 60% menunjukkan bahwa model GRU-RNN telah memenuhi standar industri untuk model peramalan nilai tukar. Pada penelitian ini, GRU-RNN juga telah terbukti mengungguli tanh-RNN dengan memiliki nilai unjuk kerja RMSE, MAE, dan MAPE yang lebih rendah, serta memiliki nilai Dstat yang lebih tinggi.","A fluctuating currency exchange influences economic policy because it is one of the factors of decision making. Therefore it takes a forecasting model to predict the exchange rate so that currency exchange rate fluctuations can be estimated. One such model is the Recurrent Neural Network (RNN). RNN then developed with additional gating units such as Gated Recurrent Unit (GRU-RNN) which aims to minimize the occurrence of vanishing gradient as happened in conventional RNN (tanh-RNN).
In this research, GRU-RNN model was used to predict exchange rate of Rupiah to US Dollar (USD-IDR). Model was trained and evaluated with Rupiah exchange rate data against US Dollar for the period of October 20, 2014 - March 31, 2017. The optimal GRU-RNN architectures are the number of lag 3 and 1 hidden layer inputs with 4 hidden units.
The result of this research shows that GRU-RNN model has performance value of RMSE, MAE, MAPE, and Dstat which are 62.82839, 45.62825, 0.342715, and 69.67%, respectively. Dstat performance values exceeding 60% indicate that the GRU-RNN model meets industry standards for exchange rate forecasting models. In this study, GRU-RNN has also been shown to outperform the tanh-RNN with lower RMSE, MAE, and MAPE performance values, and higher Dstate values.",,,64755,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111955,,DWI PRASETYO ADI NUGROHO,Segmentasi Lajur Jalan Menggunakan Deconvolutional Neural Network,,,"Jaringan Saraf Konvolusi, Segmentasi Semantik, Segmentasi Lajur Jalan, Sistem Peringatan Perubahan Lajur","Moh. Edi Wibowo, Ph.D",2,3,0,2017,1,"Sistem peringatan perubahan lajur atau Lane Departure Warning (LDW) yang kini telah banyak disematkan di kendaraan-kendaraan modern terbukti dapat mengurangi tingkat kecelakaan yang diakibatkan kebiasaan mengemudi yang buruk sebesar 11-23% (Kusano &amp; Gabler, 2015). Meskipun demikian, tingkat kesuksesan suatu sistem LDW sangat bergantung pada seberapa baik sistem ini dapat mengenali dan melakukan segmentasi lajur yang aman untuk digunakan. Seiring dengan berkembangnya metode deep learning, sistem pengenalan lajur yang dahulu banyak menggunakan sensor LIDAR yang mahal kini telah digantikan dengan pengenalan citra jalan yang direkam dari kamera digital yang relatif lebih terjangkau. 

Sebagian besar penelitian mengenai segmentasi lajur hanya memperhatikan tingkat akurasi segmentasi. Sementara itu, dalam implementasi nyata suatu sistem LDW, lama waktu eksekusi juga menjadi hal penting. Penelitian ini berfokuspada pengaplikasian metode deconvolutional neural network untuk menyelesaikan permasalahan pengenalan dan segmentasi lajur jalan dengan akurat secara real-time. Model dilatih sehingga dapat menghasilkan feature map dari citra masukkan yang dapat merepresentasikan segmentasi lajur jalan. Penggunaan lapisan konvolusi dan lapisan pooling dalam membangun feature map ini akan menghasilkan feature map dengan ukuran spasial lebih kecil. Operasi dekonvolusi dan unpooling kemudian dikenakan pada feature map ini untuk mengembalikan ukuran spasialnya ke ukuran asal citra masukkan.

Metode yang digunakan pada penelitian ini dapat melakukan segmentasi lajur jalan dengan tingkat akurasi sebesar 98,38% dan dapat melakukan inferensi satu citra masukan dalam waktu 28 milidetik. Hal ini memungkinkan terpenuhinya pengoperasian real-time yang merupakan hal penting dari dari suatu sistem LDW.","Lane departure warning (LDW) system attached to modern vehicles is responsible for lowering car accident caused by inappropriate lane changing behaviour by 11-23% according to Kusano &amp; Gabler (2015). However the success of LDW system depends on how well it define and segment the drivable ego lane. As the development of deep learning methods, the expensive LIDAR guided system is now replaced by analysis of digital images captured by low-cost camera. 

Numerous method has been applied to address this problem. However, most approach only focusing on achieving segmentation accuracy, while in the real implementation of LDW, computational time is also an importance metric. This research focuses on utilizing deconvolutional neural network to generate accurate road lane segmentation in a realtime fashion . Feature maps from the input image is learned to form a representation. The use of convolution and pooling layer to build the feature map resulting in spatially smaller feature map. Deconvolution and unpooling layer then applied to the feature map to reconstruct it back to its input size. 

The method used in this research resulting a 98.38% pixel level accuracy and able to predict a single input frame in 28 ms, enabling realtime prediction which is essential for a LDW system.",,,64058,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
130901,,CARFIN,PREDIKSI NILAI TUKAR DOLAR AMERIKA TERHADAP RUPIAH DENGAN MENGGUNAKAN FUNGSI TRANSFORMASI DAN ALGORITMA GENETIKA,,,"prediksi, nilai USD/IDR, algoritma genetika, regresi linear, fungsi transformasi.","Faizal Makhrus, S.Kom., M.Sc., Ph.D.",2,3,0,2017,1,"Perdagangan valuta asing merupakan salah satu instrumen investasi dan perdagangan yang ramai diminati oleh para investor dan pengusaha. Pengamatan dan ketelitian dalam memperhatikan perubahan nilai tukar mata uang atau kurs merupakan suatu hal yang dasar untuk melakukan perdagangan valas.
Dalam penelitian ini, dilakukan pencarian parameter terbaik dalam memprediksi kurs. Data kurs akan ditransformasikan lalu dikelompokkan dan setiap kelompok data akan di-clustering berdasarkan range koefisien regresi. Selanjutnya algoritma genetika digunakan untuk mencari solusi optimal pada setiap cluster kelompok data.
Dalam penelitian ini terdapat 2 buah jenis data input yaitu data training dan data uji. Data training menggunakan data kurs Dolar Amerika terhadap Rupiah dari tahun 2000-2015 sedangkan data uji menggunakan data pada tahun 2016 (januari hingga desember). Parameter terbaik hasil dari penelitian ini menggunakan lebar subset data sebesar 3 dan range gradien sebesar 20. Hasil rata-rata error yang didapat sebesar 35.8554 rupiah. Akurasi hasil penelitian ini adalah 99,747%.","Forex is one of the investment and trading instruments that attract investors and entrepreneurs nowday. Observation and thoroughness in observing changes in currency exchange rate is a basic thing to do foreign exchange trading.
In this research found the best parameter for exchange rate prediction. Exchange rate data are transformed and then divided into several segment. Then every segment of data is clustered based on regression coefficient. Lastly, genetic algorithm is used to find the optimal solution for every data cluster.
In this research there are 2 kind of data, one for training and one for testing. Training data is USD/IDR exchange rate from 2000 to 2015 meanwhile testing data is USD/IDR exchange rate in 2016 (january to december). The best parameter which is resulted from this research by subset size is 3 and the gradient range is 20. The average error of prediction is resulted 35.8554 rupiah. Accuracy of this reseach is 99,747%.",,,81914,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129626,,MEGA SAKTI KEMILAU,SISTEM PAKAR UNTUK MENDIAGNOSIS PENYAKIT KULIT PADA ANJING MENGGUNAKAN METODE FORWARD CHAINING DAN CERTAINTY FACTOR,,,"sistem pakar, forward chaining, certainty factor, penyakit kulit pada anjing","Drs. Sri Mulyana, M.Kom.",2,3,0,2017,1,"Kulit merupakan organ tubuh yang terletak paling luar yang berfungsi sebagai pelindung tubuh sehingga kulit mudah mengalami penyakit. Penyakit kulit pada anjing merupakan penyakit yang paling sering ditemukan oleh para ahli hewan. Dampak penyakit kulit pada anjing dapat mengganggu secara estetika, infeksi bahkan kematian apabila dibiarkan secara terus-menerus. Karena terbatasnya jumlah dokter hewan yang ada di Indonesia dibandingkan dengan jumlah kasus penyakit kulit pada anjing, maka dibutuhkan sebuah alat bantu yang dapat membantu dalam mendiagnosis penyakit kulit pada anjing. Sistem pakar dapat digunakan untuk membantu mendiagnosis penyakit kulit pada anjing. 
Sistem pakar dapat menyimpan pengetahuan, penalaran serta kemampuan seorang pakar dalam menyelesaikan masalah. Sistem pakar yang dibangun dalam penelitian ini adalah sistem pakar untuk mendiagnosis penyakit kulit pada anjing. Sistem ini dibangun dengan metode forward chaining sebagai metode penalaran dan metode certainty factor untuk masalah ketidakpastiannya. Pakar dapat memperbarui pengetahuan yang ada pada sistem, sedangkan paramedis veteriner hanya dapat melakukan konsultasi penyakit kulit anjing pada sistem ini.
Proses pengujian pada sistem dilakukan dengan melakukan perbandingan antara input terhadap output dari sistem. Input pada sistem ini berupa gejala-gejala yang dialami oleh pasien anjing beserta tingkat keyakinan dari masing-masing gejala. Output pada sistem ini berupa jenis penyakit kulit yang mungkin terjangkit oleh anjing dengan menampilkan penjelasan penyakit, cara penanganan serta nilai keyakinan diagnosis dari penyakit tersebut. Pengujian juga melakukan perbandingan hasil diagnosis penyakit yang dilakukan oleh sistem dengan yang dilakukan oleh pakar. Pengujian sistem dilakukan dan dihasilkan nilai akurasi sebesar 92 persen.","Skin is the outer organs of the body that serves as a barrier so that skin is susceptible to disease. Dog skin disease is the most frequent and the most frustated disease that commonly encountered by veterinarians. The impact of dog skin disease is very detrimental to the owner of the dog because it can interfere directly and continuously. The number of veterinarians in Indonesia is very small compared to the number of dog skin disease cases, so it needs a tool that can help in diagnosing dog skin diseases. Therefore, it needs an expert system that can be used to help diagnose dog skin diseases.
Expert system is a system that have ability to store an expert's knowledge and reasoning also have the ability to solve problems like an expert does. This system is developed with forward chaining method as a reasoning method and certainty factor method for uncertainty problem. Expert can update existing knowledge on system, whereas veterinary paramedics can only consult dog skin disease on this system.
Testing process on system is done by doing a comparison between inputs to outputs of the system. The input on this system is symptoms on dog patients also the confidence level of each symptom. The output on this system is some of dog skin disease that may be infected by the dog with showing the explanation of the disease, how to handle it also the value of confidence level of the disease. The test also performs a comparison of disease diagnose results that performed by system and performed by expert. Testing on system is done and the value of accuracy is 92 percent.
",,,80527,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114268,,MOHAMMAD MAULIADI,ANALISIS SENTIMEN TERHADAP PELAYANAN PUBLIK DI JAKARTA DENGAN DATA TWITTER MENGGUNAKAN ALGORITMA NAIVE BAYES,,,"analisis sentimen,pelayanan, twitter, naive bayes","Janoe Hendarto, Drs., M.I.Kom.",2,3,0,2017,1,"Saat ini dunia telah memasuki era dimana masyarakat dapat secara bebas menyuarakan pendapat mereka di berbagai media, salah satunya melalui media sosial. Twitter merupakan salah satu media sosial yang umum digunakan oleh masyarakat, termasuk masyarakat yang berada di DKI Jakarta. Masyarakat memanfaatkan media sosial Twitter untuk menyampaikan opini-opini tentang pelayanan publik yang didapatkan dari pemerintah. Opini tersebut dapat dimanfaatkan sebagai bahan evaluasi untuk mengetahui penilaian pelayanan publik apakah positif atau negatif. Oleh karena itu, dibutuhkan analisis sentimen yang dapat menilai sentimen dari opini yang disampaikan masyarakat.
Analisis sentimen adalah proses memahami, mengekstrak, dan mengolah data tekstual secara otomatis untuk mendapatkan informasi. Pada penelitian ini, proses analisis sentimen menggunakan algoritma Naive Bayes. Data yang digunakan adalah tweet masyarakat DKI Jakarta yang di-post dengan me-mention beberapa akun pemerintah DKI Jakarta. Data diambil sebanyak 1400 tweet sebagai data latih. Data uji yang digunakan untuk proses pengujian berjumlah 1000 data tweet. Data diambil mulai dari Maret 2016 sampai Maret 2017.
Pengujian dilakukan dengan membandingkan 3 variasi algoritma Naive Bayes, yaitu Binarized Multinomial Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes. Pengujian dilakukan menggunakan metode 10-Fold Cross Validation dengan iterasi sebanyak 10 kali. Dari hasil pengujian didapatkan akurasi terbesar yang dihasilkan dengan menggunakan algoritma Bernoulli Naive Bayes dengan rata-rata akurasi sebesar 83.2%.
","Today the world has entered an era where people can freely voice their opinions in various media, one of them through social media. Twitter is one of the social media commonly used by the society, including the people who are in DKI Jakarta. Twitter is used as a platform to express opinions about public services obtained from the government. Such an opinion can be used as an evaluation material to know whether public service assessments are positive or negative. Therefore, a sentiment analysis can be used to assess the sentiments of public opinion.
Sentiment analysis is the process of understanding, extracting and processing textual data for information automatically. The process of sentiment analysis method used in this research is Naive Bayes algorithm. The data used are the tweets of DKI Jakarta society which are post by mentioning some government accounts of DKI Jakarta. The data taken are 1400 as data training. Data testing used for the testing process amounted to 1000 tweets. The data taken from March 2016 to March 2017.
The research is done by comparing 3 variations of Naive Bayes algorithm which are Binarized Multinomial Naive Bayes, Multinomial Naive Bayes and Bernoulli Naive Bayes. The test is done by using K-Fold Cross Validation method with 10 iterations. The result shown the greatest accuracy obtained by using Bernoulli Naive Bayes algorithm with average accuracy of 83.2%.
",,,64994,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
131935,,ADIYANTI DWIASTUTI ARUMSARI,Analisis Sentimen pada Tweet Indonesia Menggunakan Recurrent Convolutional Neural Network,,,"analisis sentimen, klasifikasi, deep learning, recurrent convolutional neural network, twitter","Drs. Edi Winarko, M.Sc., Ph.D",2,3,0,2017,1,"Media sosial merupakan sarana yang banyak digunakan pada masyarakat sekarang. Salah satunya yaitu Twitter, sering digunakan oleh para penggunanya untuk mengemukakan pendapat, memberikan komentar, respon, atau review mengenai suatu hal. Dari kegunaan tersebut, tweet pada Twitter dapat dimanfaatkan untuk melakukan analisis sentimen, yaitu menganalisis apakah tweet memiliki sentimen positif, negatif, atau netral.
Penelitian ini menggunakan metode deep learning yaitu recurrent convolutional neural network (RCNN) untuk klasifikasi sentimennya. RCNN merupakan penggabungan dari recurrent neural network (RNN) dan convolutional neural network (CNN). Data yang digunakan adalah tweet dengan bahasa Indonesia yang didapatkan dari Twitter API dengan kueri emoticon sebagai label kelas sentimen positif dan negatif serta menggunakan akun media nasional untuk label kelas sentimen netral. Data akan mengalami praproses dan konversi teks menjadi vektor dengan menggunakan word2vec.
Hasil dari pengujian memperlihatkan bahwa metode RCNN memberikan hasil yang lebih baik jika dibandingkan dengan metode naive bayes, CNN, dan RNN. Metode RCNN menghasilkan akurasi sebesar 63,5%, presisi sebesar 62%, recall sebesar 64%, dan f-measure sebesar 59%.","Social media is widely used in today's society. Twitter is one of the social media that is mostly used by its users to express opinions, give comments, responses, or review of a matter. Sentiment analysis can be performed on Twitter's tweets, in which it analyzes whether tweets have positive, negative, or neutral sentiment.
This research used a deep learning method that is recurrent convolutional neural network (RCNN) for classifying sentiments. RCNN is a combination of recurrent neural network (RNN) and convolutional neural network (CNN). The data sets used are tweets in Indonesian, obtained from Twitter API with emoticon as the query for positive and negative sentiment, also from national media accounts for the neutral sentiment. The data will be pre-processed and converted into vector using word2vec.
The results of the tests show that RCNN gives better results when compared to naive bayes, CNN, and RNN. The RCNN method yielded 63.5% for the accuracy, 62% precision score, 64% recall score, and 59% f-measure score.",,,82910,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110687,,HAMIDAH TRI HANDAYANI,Deteksi Kejahatan Perbankan menggunakan Self-organizing Map,,,"kejahatan perbankan, self-organizing map, rule-based classification, akurasi, sensitivitas","Dr. Azhari, M.T.",2,3,0,2017,1,"Deteksi kejahatan perbankan merupakan suatu hal yang sangat penting untuk dilakukan bagi institusi-institusi finansial. Banyak metode-metode supervised dan unsupervised yang telah dikembangkan untuk deteksi kejahatan transaksi perbankan, dengan masing-masing kelebihan dan kekurangannya. 
Pada penelitian ini dikembangkan model algoritma menggunakan self-organizing map dan rule-based classification untuk melakukan deteksi terhadap transaksi kejahatan perbankan. Self-organizing map digunakan untuk membentuk klaster dari transaksi perbankan yang dianalisis dan selanjutnya dilakukan klasifikasi dari hasil klaster yang telah terbentuk. Untuk itu diperlukan rule-based classification untuk mengklasifikasi klaster apakah termasuk kejahatan perbankan atau tidak. Tahapan yang dilakukan dalam penelitian ini yaitu prapemrosesan dataset perbankan, dilanjutkan dengan klasterisasi menggunakan self-organizing map, dan dilakukan klasifikasi menggunakan rule-based classification.
Hasil dari penelitian ini, model algoritma menggunakan self-organizing map dan rule-based classification berhasil diimplementasikan untuk melakukan deteksi kejahatan perbankan. Diperoleh akurasi sebesar 92,091 % dan sensitivitas sebesar 97,143 %.","Banking fraud detection is great importance to financial institutions. Many supervised and unsupervised method have been developed to detect banking fraud, with each strength and limitations. 
In this research, self-organizing map and rule-based classification are developed to detect  the banking fraud tansaction. Self-organizing map is used to form a cluster of banking transactions and then the cluster have to be classified as fraud or not. It required rule-based classification to classify whether the cluster including banking fraud or not. The steps being taken in this research are do preprocessing of banking dataset, followed by clustering using a self-organizing map, and carried out the classification using a rule-based classification.
The results of this research is the model algorithm uses self-organizing map and rule-based classification successfully implemented to commit banking fraud detection. Obtained accuracy of 92,091 % and sensitivity of 97,143 %.",2017-04-10 00:00:00,53,61389,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129632,,GRANDIS SANTIKA YOGA FATHONA,SISTEM APLIKASI PENGHITUNG PEMBAGIAN HARTA WARIS SESUAI HUKUM ISLAM BERBASIS WEB,,,"ahli waris, website, ilmu faro'id,  hukum Islam,  harta waris, inheritance, Islamic law, web based application system, Islamic inheritance, distribution","Suprapto, Drs., M.I.Kom, Dr.",2,3,0,2017,1,"Semua hak kepemilikan harta bagi setiap individu dan pemindahan kepemilikan harta bagi seseorang sesudah meninggal kepada ahli waris dari seluruh kerabatnya telah ditentukan tata cara dan ketentuannya secara terperinci secara adil di dalam hukum Islam. Permasalahan pembagian harta waris menjadi hal yang sensitif yang kerap menimbulkan konflik dan perselisihan di masyarakat khususnya bagi masyarakat yang beragama Islam di Indonesia. 

Semakin populernya aplikasi berbasis website, pada penelitian ini dibangun sistem aplikasi berbasis website yang memungkinkan pengguna untuk melakukan perhitungan dan pembagian waris sesuai dengan hukum Islam (ilmu faro'id). Dengan mempertimbangkan efisiensi kebutuhan dan daya guna website yang dapat berjalan di berbagai sistem operasi, tidak membutuhkan banyak resource bagi pengguna dan mendukung berbagai device, sistem aplikasi ini dapat digunakan tanpa dibatasi ruang dan waktu.

Berdasarkan hasil pengujian sistem yang telah dilakukan, fungsi-fungsi yang disebutkan dapat berjalan dengan baik. Sistem aplikasi dapat menyajikan bagian harta waris untuk 25 golongan ahli waris dan dapat menyelesaikan perhitungan pembagian harta waris sesuai hukum Islam dan kaidah ilmu faro'id.","The Islamic law of inheritance is the most important branch of Islamic law by providing rigid and clear-cut-rules of inheritance. Inheritance law in Islam has very complicated calculations makes most Islamic people cannot apply this law.

The popularity growth of web based application system had risen an idea to make an application to calculate the distribution of inheritance according to Islamic inheritance laws that running on that platform. Considering the positive aspects, web application are much flexible which can be operated anytime, anywhere, and users can access the system through the uniform environment.

Based on the test results, the functionality of application can be run properly according to system design and the functions mentioned. The application system can make accurate calculation to the distribution of inheritance according to Islamic inheritance laws.
",,,80528,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
130401,,KURNIA RAMADANI,PERANCANGAN HIGH FIDELITY PROTOTYPE POLISIKITA DENGAN PENDEKATAN USER-CENTERED DESIGN DAN METODE ANALITYCAL HIERARCHY PROCESS,,,"polisikita, user-centered design, usability testing, AHP, prototyping.","Guntur Budi Herwanto S.Kom., M.Cs",2,3,0,2017,1,"Prototype adalah model dari produk yang digunakan untuk memberikan konsep atau gambaran dari ide yang akan dibuat. Proses perancangan prototype perlu mempertimbangkan kebutuhan dari calon pengguna. Aplikasi polisikita merupakan salah satu aplikasi yang dalam proses perancangan dari segi fitur dan fungsi aplikasi ditentukan oleh kepolisian daerah yogakarta dan belum melibatkan pengguna dalam proses perancangannya. Untuk memecahkan masalah yang ada, penelitian ini menggunakan pendekatan user centered design dan metode Analytic Hierarchy Process (AHP).
Pada penelitian ini menggunakan metode google design sprint sebagai langkah langkah yang digunakan dalam merancang prototype polisikita. Hasil analisis terhadap aplikasi polisikita versi lama didapatkan dari pelaksanaan usability testing. Hasil usability testing menjadi acuan dalam membuat tiga rancangan prototype polisikita versi baru. Pemilihan rancangan terbaik dibantu oleh sistem pendukung keputusan yakni menggunakan Analytic Hierarchy Process (AHP). Proses usability testing melibatkan delapan partisipan. Hasil usability testing antara polisikita versi baru dan polisikita versi lama akan dibandingkan.
Hasil perhitungan AHP menunjukkan bahwa rancangan terbaik memperoleh rata rata usability score sebesar 7.364 dengan nilai prioritas sebesar 0.719911. Jumlah tap minimum pada polisikita versi baru lebih sedikit 20.83% dari polisikita versi lama. usability score polisikita versi baru mengalami peningkatan sebesar 50,43% dari polisikita versi lama. Berdasarkan data yang diperoleh dari perbandingan usability testing dan penilaian user melalui usability score, dapat diambil kesimpulan bahawa perancangan dan pemilihan prototype dengan pendekatan terhadap pengguna dapat menghasilkan rancangan yang lebih baik.
","Prototype is a model of the product used to provide the concept or description of the design to be made. Prototype design process needs to consider the user needs. Polisikita one of application that in the process of designing in terms of features and application functions determined by the regional Police of Yogyakarta and has not involved users in the design process. To solve the existing problem, this research uses user-centered design approach and Analytic Hierarchy Process (AHP) method.
In this study using google design sprint method as a step that is used in designing prototype polisikita. The results of the analysis of the old version of polisikita application obtained from the implementation of usability testing. The results of usability testing become the reference in making three new polisikita prototype designs. The best design selection is assisted by a decision support system that uses Analytic Hierarchy Process (AHP). The usability testing process involved eight participants. The result of usability testing between the new and polisikta versions of the new version will be compared.
The results of AHP calculation shows that the best design obtained an average usability score of 7,364 with a priority value of 0.719911. The number of minimum taps on polisikita of the new version is 20.83% less than the old version of the polisikita. the usability score of the new version of polisikita has increased by 50.43% from the old version of the polisikita. Based on the data obtained from the comparison of usability testing and user assessment through usability score, it can be concluded that the design and selection of prototype with approach to the user can produce a better design.
",,,81324,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114274,,RINTO ACHMAD SULCHANANTO,PENGUJIAN SIMILARITAS BERITA MENGGUNAKAN METODE RABIN KARP,,,"algoritma Rabin-Karp, similaritas berita, stemming, dice coefficient","Janoe Hendarto, Drs., M.I.Kom",2,3,0,2017,1,"Perkembangan peminat berita berbasis online didorong oleh dua hal, yang
pertama adalah semakin mudahnya masyarakat Indonesia untuk memperoleh
internet terutama secara broadband wireless. Yang kedua adalah peningkatan
jumlah pengguna smartphone di Indonesia. Dengan bertambahnya peminat berita
online, maka bermunculan situs berita dan informasi yang tidak mempedulikan
kualitas isi berita. Sehingga diperlukan sistem yang dapat digunakan tolok ukur
untuk menentukan similaritas berita. Metode yang dapat digunakan adalah
menggunakan Rabin-Karp. Rabin-Karp unggul dalam pencarian string dengan
pola yang panjang.
Metode Rabin-Karp dalam sistem ini memiliki langkah-langkah
preprosesing, penghitungan k-gram menggunakan proses parsing, hashing serta
menghitung similiaritas menggunakan dice coefficient. Dalam preprosesing
dilakukan stemming menggunakan Algoritma Nazief-Adriani. Pada sistem ini
dilakukan modifikasi yakni ketika dokumen akan di parsing akan dilakukan
pengurutan kata terlebih dahulu dan sistem memiliki nilai k-gram awal 4. Hasil
yang didapatkan dari penelitian ini adalah, sistem dapat menentukan tingkat
similaritas berita dengan rata-rata tingkat kesalahan 6,13%.","The development of online-based news enthusiasts is driven by two things,
the first is the easier the Indonesia people to obtain internet especially using
broadband wireless. The second is the increasing number of smartphone users in
Indonesia. The presence of smartphones make it easier for users to get news
information anywhere. With the addition of online news enthusiasts, the
emergence of news and information sites that do not care about the quality of
news content. Many news sites often just copy pastes from other sites. So we need
a system that can be used benchmarks to determine the news similarity. The
method that can be used is using Rabin-Karp. Rabin-Karp excels in the search
string with a long pattern.
The Rabin-Karp method in this system has preprocessing, hashing and
calculating similiarity steps using dice coeficient. In preprocessing is done with
stemming using Nazief-Adriani Algorithm. There is modifications in this system
parsing process, there are words sorting and the system has an initial k-gram
value 4. The results obtained from this research is, the system can determine the
level of news similarity with the average error rate of 6,13%.",,,65014,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
128613,,DHENDA RIZKY P.,DESIGN AND DEVELOPMENT OF DIGITAL AND REMOTE DESCRIPTIVE VITAL SIGN EXAMINATION SYSTEM BY INTEGRATING THE USE OF TELEGRAM INSTANT MESSAGING,,,"Telemedicine System, Telegram, DBMS, SMTP","Lukman Heryawan, S.T., M.T.",2,3,0,2017,1,"Selama satu dekade terakhir, sistem telemedis menjadi topik hangat untuk dikembangkan dan diterapkan di dunia perawatan kesehatan. Kehadiran sistem telemedis bertujuan untuk membantu dan memfasilitasi pekerjaan praktisi medis.
Penelitian saat ini dalam sistem telemedis berfokus terutama pada masalah jaringan, distribusi gambar, rekam medis pasien, dan lain-lain. Namun, dalam membantu mempercepat waktu pemeriksaan masih jarang ditemukan pada penelitian sistem telemedis saat ini.
Penelitian ini menerapkan sistem telemedis untuk memeriksa tanda vital menggunakan aplikasi server web lokal yang terintegrasi dengan pesan instan Telegram. Telegram itu sendiri digunakan oleh dokter yang memiliki fungsi untuk menerima data tanda vital pasien yang telah diperiksa oleh perawat atau penjaga. Data yang telah diperoleh dari pasien akan otomatis tersimpan ke dalam database server web lokal dan database jarak jauh. Data tanda vital akan dikirim melalui
Simple Mail Transfer Protocol (SMTP), dimana data akan dikirim ke email sistem dan akan diambil oleh aplikasi pihak ketiga Zapier untuk menampilkan data di Telegram dari dokter.
Dari penelitian ini disimpulkan bahwa pengiriman data tanda vital dari perawat atau perawat ke dokter melalui email SMTP dapat mempercepat waktu pengiriman data secara jarak jauh dan efisien.","Over the last decade,  telemedicine system became a  hot topic to be developed and implemented in the world of healthcare.  The presence of telemedicine system aims to assist and facilitate the work of medical practitioners.
Current research in telemedicine system focuses mainly in network issues,  image distribution,  electronic patient record,  etc. However,  in  helping  to  speed-up 
examination time are rarely to be found in current telemedicine system research. 
This research implements a telemedicine system to examine vital sign using a local web server application which integrated with Telegram instant messaging. Telegram itself is used by the doctor that has a function to receive the vital sign data of the patient that has been examined by the nurse or caretaker. The data that has been obtained from the patient will be automatically stored into local web server database and remote database. Vital sign data will be sent through the 
Simple Mail Transfer Protocol (SMTP), which the data will be sent to the email of the system and will be retrieved by third-party application Zapier to display the data in Telegram of the doctor.
From this research concludes that sending the vital sign data from nurse or caretaker to doctor through SMTP email can fasten the time for delivering the data remotely and efficiently. ",,,79453,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114797,,ARIF RAKHMAT N,STUDI PERBANDINGAN SISTEM KLASIFIKASI ARITMIA JANTUNG DENGAN JARINGAN SYARAF TIRUAN PROPAGASI BALIK DAN SUPPORT VECTOR MACHINE (SVM),,,"aritmia jantung, klasifikasi, pengenalan pola, jaringan syaraf tiruan, propagasi balik, support vector machine","Sri Mulyana, Drs., M.Kom",2,3,0,2017,1,"Aritmia Jantung adalah istilah medis untuk gangguan pada jantung yaitu suatu keadaan dimana impuls listrik yang mengkoordinasikan denyut jantung seseorang tidak berfungsi dengan baik. Aritmia Jantung dapat dikenali melalui pembacaan Elektrokardiogram (EKG) yaitu grafik pola gelombang yang dihasilkan oleh alat perekam aktivitas kelistrikan jantung Elektrokardiograf dalam waktu tertentu. Pengenalan aritmia jantung dari gelombang EKG dengan akurat, membutuhkan ilmu dan pengalaman dalam mengamati pola gelombang EKG yang dimiliki oleh seorang pakar jantung. Permasalahannya adalah keberadaan pakar jantung sangatlah minim. Dalam permasalahan kurangnya tenaga ahli jantung, solusi yang ditawarkan adalah dengan menerapkan sistem kecerdasan buatan yang mampu mengenali pola gelombang EKG dan mengklasifikasikannya ke dalam jenis aritmia jantung yang sesuai. 
Pada penelitian ini dilakukan pengujian sistem klasifikasi aritmia jantung dengan metode Jaringan Syaraf Tiruan Propagasi Balik (JST-PB) dan Support Vector Machine (SVM). Sistem klasifikasi dibangun dengan memanfaatkan library WEKA (Waikato Environment for Knowledge Analysis). Validasi model dilakukan dengan cross validasi k-fold dengan nilai k = 10. Sistem klasifikasi aritmia jantung dengan JST-PB diperoleh akurasi sebesar 78,55% dan dengan SVM diperoleh akurasi sebesar 89,68%. Sistem klasifikasi aritmia jantung dengan SVM memberikan performa yang lebih baik.","Cardiac Arrhythmias is the medical term for the condition of heart failure in which electrical impulses that coordinate the heartbeat is not working properly. Cardiac arrhythmias can be identified by reading electrocardiogram (ECG), a graphic wave pattern generated by the heart's electrical activity recording tool (Electrocardiograph) within a certain time. An accurate identification of cardiac arrhythmias from ECG waveforms needs some knowledge and experience in observing the ECG wave pattern form a cardiologist. The problem is the presence of cardiologist that is very minimum. In the lack of cardiologist problem, the solution offered is the implementation of an artificial intelligence system that can recognize the ECG waveforms pattern and classifies them into the appropriate cardiac arrhythmia type.
This research done the evaluation of Cardiac Arrhythmias classification system with Back Propagation Neural Network (BPNN) and Support Vector Machine method. Classification system has built using WEKA (Waikato Environment for Knowledge Analysis). Model validation use k-fold with value of k = 10. Cardiac Arrhytmias with BPNN give the accuracy 78,55% and SVM give the accuracy 89,68%. SVM give more good accuracy than BNPP.",,,65542,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110702,,YOGIARTA BAGUS OKTAVIAN,Penyusunan Alat Ukur untuk Penentuan Indeks Usabilitas dari Mesin Pencari,,,"search engine, usability, Cochran test, Analytical Hierarchy Process, Objective Matrix  ","Medi,Drs.,M.Kom",2,3,0,2017,1,"Internet memiliki banyak fungsi kegunaan, salah satu contohnya ialah search engine yang sangat berkembang pesat saat ini. Faktor yang mempengaruhi banyaknya pengguna pada search engine adalah usabilitas search engine itu sendiri karena besarnya tingkat usabilitas pada suatu search engine dapat memberikan kenyamanan bagi pengguna, sehingga pengguna tetap setia menggunakan search engine tersebut. Kemampuan suatu search engine dapat diukur dengan suatu nilai standar dengan membuat indeks usabilitas search engine. Tujuan dari penelitian ini adalah menentukan atribut dan kriteria yang menjadi faktor usabilitas search engine, menentukan bobot setiap dimensi usabilitas search engine, dan membuat indeks usabilitas search engine. 
Metode yang digunakan dalam penelitian ini adalah pengujian Cochran dalam penentuan atribut penting, pembobotan dimensi, atribut, dan kriteria usabilitas diukur dengan metode Analytical Hierarchy Process, normalisasi ukuran menggunakan Objective Matrix untuk menyetarakan beberapa ukuran kriteria dengan satuan yang berbeda. Data diperoleh dari pengukuran nilai kuantitatif dan kualitatif. Pengukuran nilai kuantitatif dilakukan oleh peneliti, sedangkan pengukuran nilai kualitatif dilakukan dengan skala persepsi responden.
Hasil dari penelitian ini Google merupakan search engine terbaik dengan indeks usabilitas total sebesar 4,712. Urutan kedua yaitu Yahoo dengan indeks usabilitas total sebesar 3,712. Urutan ketiga yaitu DuckDuckGo dengan indeks usabilitas total 3,342. Kemudian disusul oleh Bing pada urutan ke empat dengan indeks usabilitas total 3,100. Dan urutan kelima diduduki oleh Ask dengan indeks usabilitas total 3,078. Berdasarkan hasil tersebut search engine Google-lah yang sangat usable dan nyaman digunakan bagi pengguna.
","The Internet has many functions of usability, one example is the search engine a very rapidly growing at this time. Factors that affect the number of users on the search engines is the usability of the search engine itself because of the magnitude of the level of usability at a search engine can provide comfort for users, so that users remain faithful to use search engine. The ability of a search engine can be measured with a default value with the index search engine usability. The purpose of this study was to determine the attributes and criteria into the search engine factors, determine the usability of the weighting of each dimension of the usability of search engines, and make the index search engine usability.
The methods used in this study is testing the Cochran in the determination of important attributes, weighting of dimensions, attributes, and criteriafor usability is measured by the method of Analytical Hierarchy Process, normalization of the size using Objective Matrix to equate some of the size criteria with different units. Data obtained from the measurement of the quantitative and qualitative value. Measurement of the quantitative values done by researchers, whereas the qualitative value of the measurement is performed with the scale of the perception of the respondent.
The results of this research Google is the best search engine with an index of usability total of 4.712. Second-order i.e. Yahoo with an index of usability total of 3.712. The third sequence i.e. DuckDuckGo with index usability total 3.342. Then followed by Bing on the fourth sequence with index usability total 3.100. And the fifth was occupied by Ask the index usability total 3.078. Based on the results of the search engine Google is a very usable and convenient to use for the user.
",,,61425,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
128633,,AMIE YULIKAWATI,Normalisasi Leksikal Pesan Twitter Berbahasa Indonesia Menggunakan Metode Phonetic Matching,,,"normalisasi leksikal, phonetic matching, soundex, levenshtein distance, skip-gram, peter norvig, natural language processing","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2017,1,"Normalisasi leksikal pesan singkat berbahasa Indonesia pada penggunaan
teknologi seperti Twitter yang memiliki ruang terbatas telah membawa dampak
terhadap penggunaan kata serta kualitas teks yang dihasilkan. Keterbatasan panjang
karakter yang dapat diinputkan pada suatu tweet menyebabkan kecenderungan
menggunakan kata tidak baku yang sengaja dipersingkat atau menggunakan koreksi
ejaan yang salah untuk mengikuti bahasa modern saat ini. Karena permasalahan ini,
diperlukan suatu sistem normalisasi leksikal pencocokan kata yang sesuai
definisinya adalah proses yang dapat mengubah atau mengoreksi kata tidak baku
menjadi kata baku sesuai Kamus Besar Bahasa Indonesia.
Salah satu pendekatan normalisasi leksikal yang dilakukan adalah
menggunakan metode phonetic matching dengan algoritma soundex, di mana
algoritma ini untuk mengurangi kesalahan pengetikan suatu kata. Pada penelitian
ini, pemrosesan menggunakan serangkaian algoritma pencocokan kata, yaitu
algoritma Levenshtein Distance, Soundex, Skip-gram, dan Peter Norvig. Penelitian
ini difokuskan pada tweet berbahasa Indonesia.
Berdasarkan pengujian yang telah dilakukan, kode soundex bahasa Inggris
menghasilkan rata-rata tingkat akurasi sebesar 74,83%, sedangkan pengujian sistem
dengan kode soundex bahasa Indonesia menghasilkan rata-rata tingkat akurasi
sebesar 70,83%. Lama waktu pemrosesan masing-masing memiliki kesamaan
waktu kira-kira 12 detik untuk setiap tweet.","Lexical normalization on Indonesian text messages in technologies such as Twitter, which has limited space, has brought an impact on word usage and the resulting text quality. The limitation on character length for a tweet caused a tendency of using non-standard words which are intentionally shortened or using non-normative spelling to follow the current trends instead. Because of this issue, a lexical normalization system is required, which corresponds to its definition as a process to change or correct non-normative words to normative ones according to Indonesia Dictionary. One of the lexical normalizations approaches done were using phonetic matching method with Soundex algorithm, where it is used to reduce the mistype of a word. In this research, processes were done using a series of word matching algorithms, such as Levenshtein Distance, Soundex, Skip-gram, and Peter Norvig. This research is focused on Indonesian Tweets. Based on the testing, in English, Soundex code resulted in an average accuracy of 74,83%, while in Indonesian, its accuracy is averaging at 70,83%. Processing time for each is similar at approximately 12 seconds per tweet.",,,79466,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129146,,RAHMAT ALFIANTO,PERBANDINGAN KINERJA MICROSOFT AZURE DAN HEROKU PADA ALGORITMA MERGE SORT DAN PERKALIAN MATRIKS,,,"cloud server, Microsoft Azure, Heroku, merge sort, perkalian matriks","Drs. Medi, M.Kom.",2,3,0,2017,1,"Cloud server berguna untuk melakukan komputasi awan dimanapun, kapanpun, dan dengan platform apapun dengan sumberdaya yang sudah disediakan oleh penyedia layanan, sehingga pengguna tidak perlu mengeluarkan banyak biaya untuk membeli server sendiri. Penyedia layanan cloud server ada banyak, dan untuk menentukan cloud server yang tercepat, perlu diadakan pengujian. Tugas akhir ini membahas tentang perbandingan kinerja dua cloud server bermodel platform as a service (PaaS), yaitu Microsoft Azure dan Heroku. 
Metode yang digunakan yaitu dengan mengimplementasikan website berisi algoritma merge sort dan perkalian matriks. Kedua algoritma dipilih karena stabil dan mudah untuk diimplementasikan. Parameter yang digunakan adalah loading time yang dibutuhkan untuk membuka halaman website yang memuat masing masing algoritma dengan jumlah data yang bervariasi.
Dari semua skema pengujian yang telah dilakukan pada penelitian ini menunjukkan bahwa Heroku membutuhkan rata-rata loading time 975,7 ms sedangkan rata-rata loading time yang dibutuhkan Microsoft Azure sebesar 1129,4 ms.

","Cloud servers are useful for cloud computing anywhere, anytime, and with any platform with resources already provided by service providers, so users do not have to spend a lot of money to buy their own servers. There are many cloud server providers, and to determine the fastest cloud servers, testing needs to be done. This final project discusses the performance comparison of two cloud servers having a platform as a service (PaaS), namely Microsoft Azure and Heroku.
The method used is to implement the website contains merge sort and matrix multiplication algorithm. Both algorithms are chosen because they are stable and easy to implement. The parameter used is the loading time required to open the website page containing each algorithm with varying amount of data.
From all the test schemes that have been done in this study shows that Heroku has loading time average 975.7 ms and Microsoft Azure has loading time average 1129,4.
",,,80067,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113022,,DICKI DAHRUROZAK,ANALISIS USER EXPERIENCE ELISA DENGAN PENDEKATAN USER-CENTERED DESIGN DAN METODE EVALUASI USABILITY TESTING,,,"user experience, eLisa, user-centered design, usability testing","Faizah, S.Kom., M.Kom",2,3,0,2017,1,"User experience eLisa masih belum sempurna. Banyak pengguna yang masih kurang paham dan kurang puas terhadap eLisa. Pendekatan user-centered design dan metode usability testing dapat membantu mengatasi masalah yang dialami pengguna.
Pada penelitian ini, user experience dianalisis untuk mencari tahu, memberi solusi, dan meningkatan user experience eLisa. Google Design Sprint digunakan sebagai langkah acuan dalam tahap analisis hingga pengujian. Hasil analisis eLisa lama didapatkan dari pelaksanaan usability testing. Rancangan serta prototype eLisa baru mengacu pada hasil usability testing eLisa lama. Pengujian dilakukan dengan melakukan usability testing terhadap prototype eLisa baru. Sebanyak lima partisipan digunakan pada setiap usability testing. Hasil usability testing prototype eLisa baru kemudian dibandingkan dengan eLisa lama.
Diperoleh tingkat kesuksesan task scenario eLisa baru sebesar 38,2% lebih tinggi dari eLisa lama. Hasil rata-rata nilai SUS (System Usability Scale) eLisa baru meningkat 59,45% lebih tinggi dari eLisa lama. Berdasarkan data yang diperoleh dari pelaksanaan usability testing dan nilai SUS, dapat diambil kesimpulan bahwa dengan pendekatan terhadap pengguna dan perancangan yang tepat dapat meningkatkan user experience eLisa.","eLisa&acirc;€™s user experience was still not perfect. Many users are still less understood less satisfied with eLisa. User-centered design approach and usability testing method can help to solve the problems experienced by users.
In this research, the user experience was analyzed to find out, provide solutions, and improve eLisa user experience. Google Design Sprint was used as a reference step in the analysis until testing phase. The results of previous eLisa analysis obtained from the usability testing implementation. The design and the prototype of the new eLisa refer to previous usability testing results. Testing was done by doing usability testing to the new eLisa prototype. A total five participants were used in each usability testing. The results of usability testing of the new eLisa prototype were compared to the previous eLisa.
The success rate of the new eLisa task scenario is 38,2% higher than the previous eLisa. The average SUS (System Usability Scale) value of the new eLisa increased 59,45% higher than the previous eLisa. Based on the data obtained from the implementation of usability testing and SUS, it can be that with the right approach to the users and the right design can improve the eLisa user experience.",,,63755,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132227,,FAISAL AHMAD SULHAN,PERBANDINGAN ALGORITMA REKOMENDASI COLLABORATIVE FILTERING ANTARA ITEM-BASED DENGAN LATENT FACTOR MODEL (STUDI KASUS: APLIKASI BOOK SHARING),,,"Collaborative Filtering, Item-Based CF, Latent Factor Model, Stochastic Gradient Descent, Alternating Least Square, Book Sharing.","Sigit Priyanta, S.Si., M.Kom., Dr",2,3,0,2017,1,"Aplikasi Book Sharing memungkinkan pengguna mengetahui koleksi buku antar penggunanya sehingga mempermudah untuk saling meminjam buku yang diinginkan dikala harga buku kurang terjangkau. Keberadaan fitur rekomendasi buku semakin mempermudah pengguna karena mampu memberikan rekomendasi-rekomendasi pribadi yang sesuai dengan selera pengguna. Dalam menghasilkan rekomendasi, Collaborative Filtering (CF) secara luas digunakan karena hasilnya yang lebih personal. Memory-Based CF mudah diterapkan dan menghasilkan rekomendasi yang cukup bagus, namun belum optimal dan tidak scalable pada skenario dunia nyata dimana dibutuhkan rekomendasi secara real-time dengan dataset yang besar sehingga Model-Based CF (Latent Factor Model) diperlukan karena lebih optimal dan scalable.
Penelitian ini mengimplementasikan Latent Factor Model (LFM) ke dalam fitur rekomendasi buku pada aplikasi Book Sharing dengan terlebih dahulu melakukan proses model selection menggunakan Grid Search 10 Fold Cross Validation (CV) terhadap tiga algoritma yakni Item-Based CF, LFM Stochastic Gradient Descent (SGD) dan LFM Alternating Least Square (ALS) untuk mendapatkan model terbaik yang kemudian diimplementasikan ke proses model prediction dan model retraining dan masing-masing diuji kembali menggunakan 10 Fold CV.
Hasil dari penelitian menunjukkan bahwa implementasi LFM SGD meningkatkan akurasi rekomendasi dibandingkan dengan Item-Based dan LFM ALS, serta penyimpangan akurasi RMSE test LFM SGD pada tahap pengujian model prediction dan model retraining cukup kecil sehingga model mampu bekerja dengan cukup baik terhadap data baru yang artinya proses model selection tidak mengalami overfitting terhadap data train.","The Book Sharing Application lets users know the collection of books between users, making it easier for users to borrow books from one another when book prices are less affordable. The existence of a book recommendation features further simplify the user, being able to provide personalized recommendations in accordance with user preferences. In generating recommendations, Collaborative Filtering (CF) is widely used because of the more personalized results. Memory-Based CF is easy to implement and produces good recommendations but is not yet optimal and also not scalable in real-world scenarios where real-time recommendations are needed with large datasets so that Model-Based CF (Latent Factor Model) is required because it is more optimized and scalable.
This research implements Latent Factor Model (LFM) into book recommendation feature in Book Sharing Application by first doing model selection process using Grid Search 10 Fold Cross Validation (CV) to three algorithms ie Item-Based CF, LFM Stochastic Gradient Descent (SGD) and LFM  Alternating Least Square (ALS) to get the best model to be implemented to model prediction and model retraining process and each tested again using 10 Fold CV.
The result of this research shows that LFM SGD implementation improves recommendation accuracy compared to Item-Based and LFM ALS, and deviation accuracy of RMSE test LFM SGD in the testing phase of model prediction and model retraining small enough so that the model able to work well enough to new data which means model selection process not overfitting the data train.",,,83388,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
115076,,ALICYA NOVITA H,ANALYSIS ON MUSIC FEATURE CLASSIFICATION USING SELF ORGANIZING MAP,,,"music features, Madmom, Librosa, self-organizing map","Mardhani Riasetiawan, M.T., Dr",2,3,0,2017,1,"        Bertambahnya penelitian yang dilakukan di dalam bidang music information retrieval membuat bertambahnya metode yang diciptakan untuk membuat klasifikasi pada genre musik menjadi semakin optimal dan akurat.
	Penelitian ini melakukan analisis pada fitur dari audio musik yang diklasifikasi berdasarkan genre nya dengan menggunakan self-organizing map. Implementasi dua library Python juga digunakan di dalam proses ekstraksi fitur musik untuk mendapatkan jumlah event dari fitur beat, CQT-chromagram, DCP-chromagram, Mel spectrogram, MFCC, spectral centroid dan tonnetz dari 500 audio musik. Hasil dari ekstraksi fitur ini selanjutnya dibagi menjadi 5 folds dan terdiri dari data latih dan data uji. Setiap fold berisi hasil dari fitur ekstraksi pada 100 audio musik, selanjutnya data-data ini diklasifikasi menjadi 11 kelas yang merepresentasikan genre dengan menggunakan X-Y fused self-organizing map. Penelitian ini mendapatkan persentase true positive rate sebesar 78,4%.
","       Along with the increase of researches performed in the field of music information retrieval, there also have been many methods invented to make the music genre classification becomes more optimal and accurate. 
       This research is performing analysis on music audio features to be classified based on its genre by using self-organizing map. The implementation of two Python libraries such as Librosa and Madmom is also used in the process of music feature extraction to get the total amount of event beat, CQT-chromagram, DCP-chromagram, Mel spectrogram, MFCC, spectral centroid and tonnetz of 500 music audio files. The result of extracted features is then divided into five folds and consists with training data and testing data. Each fold contains with the result of features extraction on 100 music audio files then these data are being classified into 11 classes which represents genres using X-Y fused self-organizing map. This research obtained true positive rate percentage with the amount of 78,4%.
",,,65756,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129670,,AGUNG R.M. ALAM,SISTEM APLIKASI PERSONAL FITNESS TRAINER BERBASIS ANDROID,,,"Android, Smartphone, Fitness, Application, System, Database, Java, Android Studio, Texttospeech, Content-based Recommendation System","Suprapto, Drs., M.I.Kom., Dr",2,3,0,2017,1,"GfK Asia merilis informasi bahwa Indonesia telah membeli 14,8 juta smartphone senilai lebih dari 3,33 milyar dolar pada tiga kuartal pertama di tahun 2013. Dengan 72 persen dari market share smarthphone tersebut adalah android. Tren yang tampaknya akan terus meningkat sesuai dengan kebutuhan konsumen. Kepopuleran android sendiri tidak terlepas dari banyaknya aplikasi yang tersedia yang bisa diunduh pengguna. Aplikasi personal fitness trainer sendiri lahir berdasarkan ide yang didapat dari kepopuleran android yang digabungkan dengan aktivitas yang juga populer yaitu fitness. Aplikasi ini mampu memberikan rekomendasi program latihan, indeks masa tubuh, instruksi gerakan dalam bentuk teks, audio dan gambar. 

Pada penelitian ini pengumpulan data dilakukan, data yang dikumpulkan berupa gambar gerakan-gerakan latihan. Kemudian sebuah database dibangun yang berisi gerakan-gerakan latihan, program latihan dan rekomendasi diet. Aplikasi bekerja dengan cara memberikan nilai untuk setiap masukan yang diisi  pengguna, kemudian aplikasi akan memberikan jadwal dan program latihan setelah menghitung nilai masukan tersebut melalui sistem rekomendasi content-based. Aplikasi ini juga menyediakan instruksi gerakan-gerakan latihan dengan tampilan visual ditambah instruksi dalam bentuk teks dan suara. Intsruksi berbentuk audio dihasilkan dengan memakai fitur android texttospeech dan instruksi melalui gambar ditampilkan dengan cara mengakses gambar ke server melalui koneksi jaringan internet.

Dari hasil pengujian, fungsi-fungsi yang disebutkan dapat berjalan dengan baik. Pilihan berbeda yang dimasukkan oleh pengguna ketika mengisi data di dalam aplikasi menghasilkan rekomendasi program latihan yang berbeda pula. Pengujian usability dilakukan dengan metode USE Question pada 14 responden yang merupakan member di sebuah gym. Pengujian tersebut menghasilkan feedback dari pengguna dengan keterangan memuaskan.
","
GfK Asia released information that Indonesia has purchased 14.8 million smartphones worth more than 3.33 billion dollars in the first three quarters of 2013. With 72 percent of the market share is android. Trends are likely to continue to increase in accordance with the needs of consumers. Android popularity can not be separated from the number of applications available that can be downloaded by the users. Personal fitness trainer application itself was born based on the idea gained from the popularity of android which is combined with a popular activity that is fitness. This application is able to provide recommendations of exercise programs, body mass index, motion instructions in the form of text, audio and images.

In this study conducted data collection such as drawing exercises. Then a database that contained exercise gestures, exercise programs and dietary recommendations was built. The application works by providing values for each user-filled input, then the app will provide the exercise schedule and program after calculating the input value through a content-based recommendation system. The app also provides instructions for exercise movements with visual display plus instruction in text and sound. Audio-shaped instructions are generated using the android texttospeech feature and instructions through images are displayed by accessing images to the server via internet connection.

The results of the testing process shows the functions mentioned can run well. Different options entered by the user when filling in the data within the application result in different exercise program recommendations. Testing usability is done by USE Question method on 14 respondents who are members in a gym. The test produces user feedback which give the app a satisfying predicate.
",,,80573,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
131213,,WILDAN AULIA HIKMAWAN,ANALISIS WEB USAGE MINING UNTUK PEMBENTUKAN PROFIL USER MENGGUNAKAN ALGORITMA CLUSTERING K-MEANS (STUDI KASUS: WEBSITE ETD.UGM.AC.ID),,,"website, profil navigasi, clustering, pemodelan topik, pattern mining","Guntur Budi Herwanto, S.Kom.,M.Cs",2,3,0,2017,1,"Perkembangan teknologi internet saat ini sangat pesat dapat dilihat dari kemudahan mendapatkan akses internet. Seiring dengan hal tersebut, terjadi peningkatan aktivitas pengguna dalam mengunjungi berbagai website. 
Etd.ugm.ac.id merupakan salah satu website di Universitas Gadjah Mada. website tersebut dipilih sebagai bahan penelitian karena website etd.ugm.ac.id cukup sering diakses oleh mahasiswa untuk mencari refrensi penelitian atau tugas akhir. Pola akses pengunjung website tersebut dapat digunakan untuk mengetahui kebiasaan seorang pengunjung dalam melakukan penelusuran dalam sebuah website. Hal tersebut dapat digunakan guna perbaikan kualitas website dan meningkatkan kepuasan pengunjung dalam mengakses website untuk menemukan informasi di dalamnya sesuai dengan kebutuhannya.
Pada penelitian ini dilakukan pengelompokan pengunjung berdasarkan log akses dan konten yang diakses, dengan menggunakan algoritma K-Means. Pengujian sistem dilakukan dengan menghitung jarak antar cluster yang terbentuk dan jarak antar anggota pada suatu cluster dengan metode silhouette coefficients. Sehingga didapatkan nilai silhouette coefficient 0,05 untuk cluster dokumen dengan 500 fitur kata dan K = 30, sedangkan untuk cluster pola akses buku oleh pengguna mendapatkan nilai silhouette coefficient  0,15 dengan jumlah cluster 14.","The development of internet technology today is very rapid can be seen from the ease of getting internet access. Along with this, there is an increase in user activity in visiting various websites.
Etd.ugm.ac.id is one of the websites at Gadjah Mada University. It was chosen as research material because website etd.ugm.ac.id quite often accessed by students to search for research reference or final project. Visitor pattern of the website can be used to know the habits of a visitor in doing a search in a website. It can be used to improve the quality of the website and increase the satisfaction of visitors in accessing the website to find information in it according to their needs.
In this study, visitors are grouped based on access logs and accessed content, using K-Means algorithm. System testing is done by calculating the distance between the clusters formed and the distance between members on a cluster with the method of silhouette coefficients. So the value of silhouette coefficient 0,05 for document cluster with 500 word feature and K = 30, whereas for cluster access pattern book by user get value of silhouette coefficient 0,15 with number of cluster 14.",,,82190,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132237,,AKHADIA CIPUTRA,IMPLEMENTASI RESTFUL WEB SERVICE UNTUK SISTEM INFORMASI PENJUALAN RESELLER TERINTEGRASI,,,"RESTful API, Web Service, Sistem Informasi Penjualan, Reseller, Integrasi","Y. Suyanto, Drs., M.Kom., Dr",2,3,0,2017,1,"Muneera Silver merupakan sebuah usaha yang bergerak dalam bidang penjualan kerajinan perak. Muneera Silver saat ini memiliki sejumlah reseller yang masih melakukan pengolahan data secara manual baik dalam pemesanan, penjualan maupun pembuatan laporan penjualan.
Pada penelitian ini dibangun sistem informasi penjualan untuk reseller yang terintegrasi dengan sistem informasi Muneera Silver. Disamping itu, untuk mengintegrasikan sistem Muneera Silver dengan sistem reseller, dibangun API web service dengan menggunakan pendekatan arsitektur REST pada sistem Muneera Silver yang digunakan untuk menyediakan layanan pemesanan dan informasi produk yang tersedia. Pada sistem reseller yang dibangun dengan mengimplementasikan API tersebut, sisi pengguna admin mampu menampilkan informasi produk dan pemesanan yang ada pada sistem informasi Muneera Silver dan mampu mengirimkan data pemesanan ke sistem informasi Muneera Silver. Sistem ini dibangun berbasiskan web menggunakan bahasa pemrograman PHP dengan framework CodeIgniter dan database MySQL. Pemodelan sistem menggunakan UML berupa use case diagram dan activity diagram serta diuji menggunakan metode pengujian black box.
Implementasi API web service pada sistem yang dibangun untuk reseller dinilai berhasil karena hasil pengujian dengan metode black box menunjukkan bahwa fungsi API web service dan fungsi utama sistem informasi penjualan untuk reseller telah bekerja dengan baik sesuai dengan kebutuhan. ","Muneera Silver is a local bussiness that sells silver craft product. Muneera Silver has a number of resellers that still doing data processing manually either in ordering, selling and make sales report.
This research built a sales information system for resellers integrated with Muneera Silver information system. In addition, to integrate the Muneera Silver system with the reseller system, a web service API is built using the REST architecture approach on Muneera Silver system that is used to provide ordering services and available product information. The reseller system that built using the API, the admin user side is able to display existing sales data information on the Muneera Silver information system and its also able to send order data to the Muneera Silver information system. This system is web based and built using PHP programming language with Codeigniter framework and MySQL database. System modeling is using UML in the form of use case diagram and activity diagram and system is tested using black box testing method.
Implementation of the web service API on reseller system is considered successful because the results of testing with black box method indicates that the function of the API web service and the main function of sales information system for resellers has been working well in accordance with the needs.",,,83245,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110223,,FARID AMIN RIDWANTO,PENDETEKSIAN JALAN PADA CITRA SATELIT MENGGUNAKAN JARINGAN SYARAF KONVOLUSIONAL,,,"road, map, convolutional neural network, satellite image  ","Moh. Edi Wibowo, S.Kom., M.Kom., Ph.D",2,3,0,2017,1,"Peta jalan banyak diperlukan untuk berbagai keperluan seperti perancangan jalur transportasi, pembuatan sistem navigasi, pembuatan sistem peringatan dini, dan lain-lain. Saat ini, peta jalan digital dibuat dengan cara menandai pixels dari sebuah citra satelit yang besar secara manual. Karena cakupan daerah yang harus dicek secara manual sangat luas, proses penandaan jalan dengan cara seperti ini membutuhkan waktu yang cukup lama. Oleh karena itu, jika pada suatu saat terjadi perubahan pada ruas jalan sedemikian sehingga membutuhkan pembaruan pada peta digital, diperlukan waktu yang lama untuk melakukan pembaruan tersebut. Oleh karena itu, diperlukan sebuah pendekatan untuk menandai pixel berisi luas jalan dari sebuah citra satelit untuk mempercepat proses pembaruan peta jalan. Pada penelitian ini, akan dilakukan pendeteksian jalan secara otomatis berdasarkan citra satelit dengan menggunakan jaringan syaraf.
Arsitektur jaringan syaraf yang digunakan pada penelitian ini adalah kombinasi antara jaringan syaraf konvolusional dan jaringan syaraf fully-connected. Pelatihan jaringan syaraf dilakukan dengan menggunakan algoritma Stochastic Gradient Descent. Pembuatan sistem dilakukan dengan bahasa Python dengan Theano sebagai library utama. Pengujian dilakukan dengan 3-fold Cross Validation.
Pada penelitian ini, diperoleh hasil bahwa jaringan syaraf konvolusional yang dikombinasikan dengan jaringan syaraf fully-connected mendeteksi jalan pada citra satelit dengan rata-rata nilai precision-recall breakeven point pada pengujian sebesar 0,5763. Selain itu, ditemukan juga bahwa ketimpangan kelas dapat mempengaruhi performa, penurunan nilai cost yang kecil dapat memperbaiki performa dengan cukup signifikan, dan penambahan nilai bobot pada fungsi cost dapat memudahkan model mendeteksi jalan.
","Road maps are vastly needed for various purposes such as transportation track designs, navigation sustems, early warning systems, etc. Nowadays, digital road maps are created by labelling pixels from large satellite images by hand. Because of the very large area needed to be checked manually, labelling by hand takes long time. Thus, if someday the map needs to be updated due to road&acirc;€™s changes, the process will take long time. Therefore, an approach needed to label pixels containing road automatically to make map updating faster. In this research, road detection is done automatically based on satellite images using neural networks.
The neural network architecture used in this research is a combination of convolutional neural network and fully-connected neural network. The training  of network is done using Stochastic Gradient Descent algorithm. The system is built mainly using Python language programming with Theano as the main library. The testing is done using 3-fold Cross Validation.
In this research, it is found that convolutional neural network that is combined with fully-connected neural network detected road on satellite image with average precision-recall breakeven point on testing 0,5763. Other than that, it is also found that class imbalance can affect model&acirc;€™s performance, little cost decrease can improve  performance quite significantly, and adding weight value on cost function can make the model easier to detect road.
",,,60889,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129171,,BUDI TRIWIBOWO YULI WIDHIYANTO,"PERBANDINGAN AKURASI METODE GATED RECURRENT UNIT, BIDIRECTIONAL GATED RECURRENT UNIT, CONVOLUTIONAL GATED RECURRENT UNIT DAN MEMORY NETWORK DALAM PENYELESAIAN PERMASALAHAN MESIN TANYA JAWAB BERBASIS PENALARAN",,,"Gated Recurrent Unit, Bidirectional GRU, Convolutional GRU, Memory Network, Permasalahan tanya jawab berbasis penalaran","Dr. Mardhani Riasetiawan SE ak, MT",2,3,0,2017,1,"Di era ini, mesin telah banyak berkomunikasi dengan kita manusia. Penggunaan chat bot untuk keperluan industri kini pun semakin marak. Perusahaanperusahaan memanfaatkan chat bot untuk melayani berbagai keperluan pelanggannya. Chat bot mampu melakukan percakapan yang cukup baik dengan manusia. Tentu dalam berbagai percakapan ini, mesin haruslah mampu menjawab berbagai pertanyaan dari manusia. Dalam perkembangannya, mesin juga dituntut untuk mampu menjawab permasalahan yang membutuhkan penalaran secara transitif. Tugas untuk menjawab berbagai pertanyaan secara otomatis dengan melibatkan penalaran transitif inilah yang kita sebut dengan permasalahan tanya jawab berbasis penalaran.

Pada Penelitian ini digunakan model Gated Recurrent Unit, Bidirectional Gated Recurrent Unit, Convolutional Gated Recurrent Unit dan Memory Network untuk menyelesaikan permasalahan tanya jawab berbasis penalaran yang berasal dari dataset Facebook babi.

Hasil penelitian menunjukkan bahwa rata-rata akurasi model Gated Recurrent Unit adalah 85.8%, Bidirectional Gated Recurrent Unit adalah 86.6%, model Convolutional Gated Recurrent Unit adalah 71.7% dan model Memory Network adalah 76.3%.","In this era, machines have communicated with humans. The use of chat bots for the industrial need is increasing now. Companies use chat bots to serve the various needs of their customers. Chat bots are able to have a pretty good conversation with humans. Of course in these various conversations, the machine must be able to answer questions from people. The machine is also required to be
able to answer questions that require transitive transparent. The task of answering these questions that needs to do transitive reasoning automatically is what we call the Reasoning Based Question Answering Problem.

In this research we use Gated Recurrent Unit model, Bidirectional Gated Recurrent Unit, Convolutional Gated Recurrent Unit and Memory Network to solve the reason based question answering machine problem that comes from facebook BaBi datasets.

The results show that the average accuracy of Gated Recurrent Unit model is 85.8%, Bidirectional Gated Recurrent Unit is 86.6%, Convolutional Gated Recurrent Unit model is 71.7% and Memory Network model is 76.3%",,,80128,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
131732,,DZAKY WIDYA PUTRA,PERAMALAN MATRIKS ASAL-TUJUAN PADA SISTEM TRANSPORTASI BUS RAPID TRANSIT MENGGUNAKAN RECURRENT NEURAL NETWORK,,,"Matriks Asal-Tujuan, Peramalan, Recurrent Neural Network, Feedforward Neural Network","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2017,1,"Dalam bidang transportasi, matriks asal-tujuan merupakan salah satu komponen utama yang penting, terutama dalam hal analisa, perancangan, dan pengelolaan jaringan transportasi umum. Peramalan jumlah penumpang juga menjadi salah satu cara yang banyak digunakan oleh penyedia layanan transportasi umum untuk mengetahui perilaku perjalanan penumpang dan untuk membantu operasional dan manajemen sistem transportasi. Kombinasi antara kedua komponen tersebut, yaitu peramalan matriks asal-tujuan, diharapkan dapat menjadi solusi bagi penyedia layanan transportasi umum untuk menjawab permasalahan yang dihadapi.

Pada penelitian ini, dilakukan pengujian hyperparameter untuk mendapatkan model peramalan terbaik, yang dapat digunakan untuk melakukan peramalan matriks asal-tujuan menggunakan recurrent neural network. Pengujian yang dilakukan adalah pengujian learning rate untuk mendapatkan kecepatan training yang optimal, pengujian batch size untuk menentukan berapa jumlah data yang harus digunakan dalam satu kali pembelajaran, pengujian hidden neuron untuk menentukan jumlah neuron pada hidden layer, dan pengujian hari peramalan untuk mengetahui berapa hari yang dibutuhkan untuk melakukan peramalan satu hari berikutnya agar mendapatkan hasil peramalan yang optimal. Model recurrent neural network yang didapatkan kemudian dibandingkan dengan feedforward neural network menggunakan evaluasi mean absolute error dan mean absolute percentage error untuk mengetahui performa kedua model dalam melakukan peramalan matriks asal-tujuan.

Pada akhir penelitian, didapatkan model recurrent neural network terbaik dengan learning rate 0,01 , batch size 32, hidden neuron 1, dan hari peramalan 7. Model recurrent neural network juga memiliki performa yang lebih baik dibandingkan dengan feedforward neural network dengan perbandingan rata-rata mean absolute error sebesar 275,5 berbanding 293,9 , dan rata-rata mean absolute percentage error sebesar 22,69 % berbanding 23,74 %. Berdasarkan uji t yang dilakukan, hasil peramalan model recurrent neural network terbukti memiliki perbedaan yang signifikan terhadap hasil peramalan model feedforward neural network.","In transportation field, the origin-destination matrix is one of the main and important components, especially in analyzing, planning, and managing a public transport network. Forecasting the number of passengers is also one way that widely used by public transportation service provider in order to know passengers travel behavior, and to assist the operational and managerial work of public transport system. The combination of those two components, which is forecasting origin-destination matrix, is expected to be a solution for public transportation service provider to solve various problems.

In this research, hyperparameter testing is performed to obtain the best forecasting model which can be used to forecast origin-destination matrix using a recurrent neural network. Various testing conducted, including learning rate testing to get the optimal training speed, batch size testing to determine how many data that should be used in one training iteration, hidden neuron testing to determine how many neurons in hidden layer, and forecasting days testing to determine how many days before that should be used in order to forecast one day after. The recurrent neural network model then compared with the feedforward neural network using mean absolute error and mean absolute percentage error evaluation to determine which model performs better when it comes to forecast the origin-destination matrix.

At the end of this research, the best recurrent neural network model obtained, with learning rate 0,01, batch size 32, hidden neuron 1, and forecasting days 7. The recurrent neural network model also performs better compared to the feedforward neural network model with an average comparison of mean absolute error is 275,5 compared to 293,9, and average comparison of mean absolute percentage error is 22,69 % compared to 23,74 %. Based on t-test performed, forecasting result of recurrent neural network model is proven to be significantly different from the forecasting result of feedforward neural network model.",,,82738,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
131482,,TAUFIK EKO CAHYONO,PENDEKATAN HYBRID UNTUK SISTEM REKOMENDASI FILM DENGAN COLLABORATIVE FILTERING DAN CONTENT-BASED,,,"sistem rekomendasi, hybrid, collaborative filtering, content-based, neigborhood, sparsity, content","Anny Kartika Sari, S.Si., M.Sc., Ph.D.",2,3,0,2017,1,"Pertumbuhan jumlah data dan keanekaragaman informasi memunculkan dilema bagi masyarakat untuk memilih informasi yang relevan. Banyak teknik pembentuk rekomendasi yang dapat digunakan untuk merekomendasikan informasi yang sesuai dengan keinginan pengguna. Collaborative filtering (CF) dan content-based (CB) merupakan contoh teknik dasar sistem rekomendasi yang populer hingga saat ini. Pada kasus pencarian film yang ingin ditonton, CF menggunakan rating dari user lain untuk membentuk rekomendasi sedangkan CB menggunakan content dari item berupa atribut film (sinopsis, keyword, genre dan sebagainya). Namun, teknik CB dan CF memiliki kelebihan dan kekurangan masing-masing. Salah satu cara untuk menangani kelemahan masing-masing teknik adalah dengan menggabungkankan keduanya (hybrid). Penggabungan teknik dasar ada yang dapat dilakukan secara parallel dan ada yang hanya bisa dilakukan secara sequential. Dalam prakteknya ada kondisi tertentu yang mana menyebabkan suatu rekomendasi berkurang kualitasnya, misalnya pada sistem rekomendasi film dengan jumlah rating yang terlalu sedikit atau informasi content dari film kurang lengkap.
	Dalam penelitian ini dilakukan berbagai macam skenario percobaan untuk mengevaluasi teknik hybrid. Teknik hybrid yang dievaluasi meliputi hybrid weighted, switching dan feature augmentation dengan kondisi di mana sparsity level dari dataset yang tersedia lebih besar dan informasi content yang diperlukan lebih lengkap. Dataset rating film yang digunakan berasal dari MovieLens, sedangkan content item (atribut film) diperoleh dari situs IMDB.
	Hasil akhir dari penelitian ini memperlihatkan bahwa teknik hybrid weighted memiliki nilai terbaik dalam hal waktu eksekusi dengan lama 1,145 detik maupun akurasi dengan MAE sebesar 0,743. Sedangkan dalam hal penanganan sparsity problem dan juga efektifitas, teknik feature augmentation memiliki hasil terbaik dengan nilai conversion rate 100%. Selain itu, penambahan content film dengan atribut lainya tidak terlalau memberikan perubahan yang signifikan.","The growing number of data and the diversity of information create a dilemma for people to choose relevant information. Many recommendation-forming techniques can be used to recommend information that suits the user's wishes. Collaborative filtering (CF) and content-based (CB) are examples of the basic recommendation techniques which is popular until now. In the case of movie that want to watch, CF uses ratings from other users to generate recommendation while CB uses movie&acirc;€™s (item&acirc;€™s) content as it&acirc;€™s attributes (synopsis, keyword, genre and so on). However, CF and CB techniques have their respective advantages and disadvantages. One way to deal the weaknesses of each technique is by combining both technique (hybrid). Combining basic techniques can be done in parallel and some can only be done in sequentially. Practically, there are certain conditions which cause the quality of recommendations to decrease such as for example on movie recommendation system with small number of rating or the content information of the item is not enough. 
	In this study, various experimental scenarios were conducted to evaluate hybrid techniques. Hybrid techniques that were evaluated are hybrid weighted, switching and feature augmentation with conditions where the sparsity level of the available dataset is greater and the required content information is more complete. The movie rating dataset used comes from MovieLens, while the content item (movie attribute) is obtained from the IMDB site.
	The end result of this study shows that hybrid weighted technique has the best value in terms of execution time with 1,145 seconds long and accuracy with 0,743 of MAE score. Meanwhile in terms of sparsity problem handling and effectiveness, feature augmentation technique get the best result with 100% of conversion rate. In addition, the addition of movie content with other attributes did not give any significant changes. ",,,82472,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
108955,,HIJRIYANI NUGROHO DWI SAPUTRI,"DETEKSI OUTLIER DENGAN METODE INTER QUARTIL RANGE (BOXPLOT) UNTUK PENEMPATAN PEGAWAI YANG MENGEFISIENKAN JARAK TEMPUH (Studi Kasus Penempatan Guru SMA, SMK, MA Kota Yogyakarta)",,,"outlier, deteksi outlier, data mining, data penempatan, guru, Inter Quartil Range, Greedy","Nur Rokhman, S.Si., M.Kom., Dr",2,3,0,2017,1,"Jarak merupakan angka yang menunjukkan seberapa jauh suatu benda
berubah posisi. Penempatan kerja bagi pegawai dalam hal ini guru, seringkali hanya
berdasarkan kebutuhan instansi terkait, tidak memandang alamat pegawai dan
seberapa efektifnya jika rumah pegawai jauh dari tempat bekerja Kondisi jarak
antara rumah guru dengan sekolah yang jauh sering menjadi alasan keterlambatan,
hal ini menyebabkan siswa menjadi tidak disiplin dan malas datang pagi ke sekolah
meskipun jam pelajaran sudah dimulai.
Outlier merupakan objek yang berbeda dibandingkan objek-objek lainnya
dalam suatu data. Pada penelitian ini akan dilakukan deteksi outlier pada data
penempatan guru SMA, SMK, dan MA kota Yogyakarta dengan menggunakan metode
Inter Quartil Range (Boxplot). Data yang terdeteksi sebagai outlier merupakan data
yang mempunyai jarak jauh dari rumah menuju sekolah dibanding dengan kumpulan
data lainya. Kemudian dilakukan pertukaran penempatan guru dengan menerapkan
konsep algoritma Greedy untuk mengfisienkan jarak tempuh guru menuju sekolah.
Hasil dari penelitian ini, metode Inter Quartil Range (Boxplot) dapat
melakukan deteksi outlier pada data penempatan guru SMA, SMK, dan MA kota
Yogyakarta. Hasil dari algoritma pertukaran penempatan guru memberikan gambaran
guru-guru yang memungkinkan untuk dilakukan reposisi guna mengefisienkan jarak
tempuh guru menuju sekolah.","Distance is a number that indicates how far away an object changes position.
Work placements for employees in this case teachers, often only based on the needs
of the relevant institutions, do not look at the address employee and how effectively
if home employees away from the workplace conditions within the home teachers
to remote school is often the reason for the delay, it causes students to become not
discipline and lazy came early to school despite hours of lessons already begun.
Outliers are objects differently than other objects in the data. This research
will be conducted on the data outlier detection placement high school teachers,
SMK, and MA Yogyakarta using the Inter Quartile Range (Boxplot). The data is
detected as a outlier is data that have a long distance from home to school compared
with other data sets. Then the exchange of the placement of teachers by applying
the concept Greedy algorithms to streamline the distance home to school teachers.
The results of this study, the method Inter Quartile Range (Boxplot) can
detect outlier in the data placement high school teacher, SMK, and MA Yogyakarta.
Results of teacher placement exchange algorithm provides an overview that allows
teachers to do repositioning for efficient mileage teachers to school.",,,59795,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132509,,APRIHADI PERDANA,IMPLEMENTASI SISTEM AUTENTIKASI CLOUD BERBASIS INFORMASI PERANGKAT,,,"Cloud, Informasi Perangkat, Autentikasi","Dr.techn. Khabib Mustofa, S.Si., M.Kom.",2,3,0,2017,1,"Teknologi cloud menjadi salah satu bagian yang berkembang pesat dari industri teknologi informasi. Kehadiran teknologi cloud dapat membantu memenuhi kebutuhan sumber daya yang tinggi. Pengguna yang mengadopsi teknologi cloud rentan terhadap masalah keamanan yaitu akses ilegal. Akses ilegal yaitu ketika orang lain mengakses cloud dengan menggunakan data yang diisikan pengguna untuk mengakses cloud seperti data alamat email dan password. Permasalahan akses ilegal dicoba untuk diselesaikan dengan penelitian ini. Penelitian ini mencegah akses ilegal ke sistem cloud dengan menggunakan data informasi perangkat. Proses autentikasi menggunakan metode Dua Faktor Autentikasi. Data informasi perangkat digunakan sebagai salah satu syarat untuk proses autentikasi pengguna selain data pengguna. Penerapan sistem autentikasi ini dilakukan pada perangkat smartphone berbasis Android dan perangkat laptop serta komputer yang berbasis Windows. Data informasi perangkat yaitu data CPU id, motherboard serial number dan BIOS serial number untuk perangkat laptop dan komputer sedangkan data IMEI untuk perangkat smartphone. Masing - masing data informasi perangkat di setiap perangkat bersifat unik. Keunikan tersebut dapat mencegah akses ilegal pada sistem cloud. Penelitian ini telah mencoba menyelesaikan permasalahan akses ilegal dengan memanfaatkan data informasi perangkat yang bersifat unik. Pengguna ilegal tidak dapat mengakses sistem cloud walaupun pengguna tersebut mengetahui data alamat email dan password yang digunakan oleh pengguna yang terautentikasi. Hal ini dikarenakan perangkat yang digunakan oleh pengguna ilegal tidak terdaftar pada daftar perangkat yang digunakan oleh pengguna yang terautentikasi untuk mengakses sistem cloud.","Cloud technology is one of the fastest growing parts of the information technology industry. The presence of cloud technology can help to fulfill high resource requirements. User who adopt cloud technology are vulnerable against security concern that is illegal access. Illegal access is when others try to get access the cloud by using data that users use to access the cloud such as email address and password. The problem of illegal access is tried to be solved by this research. This research prevents illegal access to the cloud system by using device information data. The authentication process uses the Two Factor Authentication method. Device information data is used as one of the requirements for user authentication process besides user data. Implementation of this authentication system is done on Android-based smartphones and Windows-based laptops and computers. Device information data are CPU id, serial number motherboard and BIOS serial number for laptops and computers while IMEI data for smartphone devices. Each device information data in each device is unique. Uniqueness of device information data can prevent illegal access to the cloud system. This research has tried to solve the illegal access problem by utilizing unique device information data. Illegal users can not access the cloud system even if the user knows the data of the email address and password used by the authenticated user. This is because the device used by illegal users is not listed on the list of devices used by authenticated users to access the cloud system.",,,83480,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112030,,AGRIVIAN ANDITYA,Role-Playing Games: Improving Battle Flow Using Active Time Battle System and Its Calculation for Optimal Speed,,,"battle system, active time battle, battle flow, role-playing game","Agus Sihabuddin S.Si., M.Kom.",2,3,0,2017,1,"Penelitian ini mendiskusikan mengenai penggunaan sistem Active Time Battle (ATB) untuk meningkatkan sistem pertarungan default daripada RPG Maker MV (RMMV) dengan kondisi tahap awal sebuah game Role-playing Game (RPG). Sistem pertarungan default dari RPG Maker MV adalah sistem dengan urutan aksi tokoh yang sewenang-wenang.

Tujuan dari penelitian ini adalah untuk mengetahui apakah sistem ATB akan merubah sistem pertarungan sebelumnya, apabila berubah, apakah ada perkembangan dari perubahan itu. Selain itu, untuk menemukan waktu optimal dari kecepatan pertarungan dengan mengkonfigurasi nilai tick rate dan gauge speed requirement. Implementasi dari sistem ATB tersebut dilakukan menggunakan fitur scripting dari alat pembuatan game RMMV.
	
Hasil penelitian menunjukkan bahwa setelah sistem ATB diimplementasikan, statistika kecepatan tokoh memiliki peran yang lebih baik, aliran pertarungan telah berubah dan berkembang karena lebih teratur dibandingan sistem pertarungan sebelumnya. Keteraturan ini juga ditandai dengan sebuah kotak yang terus berisi, mengindikasikan kapan tokoh akan mendapat giliran. Perhitungan kotak ini berdasarkan kecepatan tokoh/petarung.

Pengaturan terakhir sesuai dengan parameter yang dibandingkan, sehingga diketahui kecepatan optimal untuk sistem ATB ini dengan mengisi pengaturan nilai untuk tick rate adalah 1, untuk k daripada speed gauge requirement membutuhkan nilai 5000/6000 untuk full gauge dan nilai 2000/2400 untuk charge gauge, sedangkan nilai m tetap berada di 100 untuk full gauge dan 20 untuk charge gauge. (Abstrak)
","This paper discusses the use of the Active Time Battle (ATB) system to improve the default combat/battle system of RPG Maker MV within the condition of a Role-playing Game's (RPG) beginning stage. The default RPG Maker MV (RMMV) battle system is a simple single-phase one in which all battlers take their turns and commence action immediately.

The objective of this paper is to find out whether ATB system changed the battle system or not, and if it changed, what is the improvement of that change.
Finding the optimal battle speed by configuring tick rate value, gauge speed requirement. Implementation of the ATB system was done using scripting feature of the game creation tool.

The results indicate that after the ATB system was implemented, agility statistics have a better role, battle flow is changed and improved as it is even more organized because the turns are determined clearly instead of arbitrary order, an ATB/time bar is displayed as a turn indicator. The bar charges based on the agility of the battler.

Final settings conform to the parameters being compared. The optimal speed needs the calculation of tick rate of 1 and gauge speed requirement k of 5000/6000 for full gauge and 2000/2400 for charge gauge, while m works best default at 100 and 20 for full gauge and charge gauge speed requirements, respectively. (Abstract)",,,62831,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112031,,RIKA WIDIANINGTYAS,Model Identifikasi Aktivitas Merisak pada Media Sosial Menggunakan Pendekatan Analisis Sentimen dan Support Vector Machine,,,"aktivitas merisak, klasifikasi, rekayasa fitur, sentiwordnet, support vector machine","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2017,1,"Media sosial adalah medium yang popular dalam penyebaran informasi dan komunikasi melalui internet saat ini. Informasi yang diperoleh melalui sosial media sangat berguna bagi semua orang, namun juga terdapat beberapa pengguna yang menghasilkan konten yang negatif karena mengandung unsur kekerasan, ofensif dan penghinaan. Penggunaan situs jejaring sosial yang tumbuh sangat cepat mengakibatkan pengguna jejaring sosial rentan terhadap aktivitas merisak. Salah satu bentuk aktivitas merisak adalah berupa komentar yang mengandung kata-kata kasar di situs jejaring sosial yang berdampak pada psikologi penggunanya. Untuk itu dibutuhkan suatu model yang dapat mengidentifikasi aktivitas merisak yang berupa komentar kasar.
Dalam penelitian ini dilakukan pembangunan model untuk mengidentifikasi aktivitas merisak menggunakan bag of words dan rekayasa fitur berdasarkan sentimen dari teks dengan Sentiwordnet dan algoritma machine learning support vector machine. Dengan rekayasa fitur ini, dibangun 6 fitur yaitu jumlah nilai polaritas, jumlah nilai polaritas positif, jumlah nilai polaritas negatif, jumlah term positif, jumlah term negatif, dan rasio term. Performa dari model dievaluasi berdasarkan nilai akurasi menggunakan Stratified Cross Validation dengan k=10.
Hasil terbaik support vector machine didapatkan dengan menggunakan kernel linear dan nilai cost 0,2 menghasilkan akurasi sebesar 82,7%. Sedangkan fitur rekayasa yang menghasilkan akurasi terbaik adalah jumlah nilai polaritas dan jumlah nilai polaritas negatif.","Social media is a popular mediums for information sharing and communication over the internet today. The information gained from these social media can be very useful for people around the world, but sometimes there are some users generated contents are very negative as they contain abusive, offensive and insulting element. The use of social networking sites which is rapidly growing make its users vulnerable to bullying. One of the bullying form is comments containing abusive words in social networking sites that gives bad impact to the psychology of the users. Therefore, it is need a model which can identify the bullying activity in the form of abusive comments.
This research focuses on generating a model for bullying identification using bag of words and feature engineering based on sentiment of the text with Sentiwordnet and machine learning algorithm support vector machine. With this feature engineering, it was generated 6 features, they were sum of polarity score, sum of positive polarity score, sum of negative polarity score, count of positive term, count of negative term, and term ratio. The performance of the model was evaluated by accuracy score using stratified cross validation.
The best support vector machine was achieved using linear kernel and cost score 0,2 with 82,7% accuracy. Engineered features that gave the best accuracy score were sum of polarity score and sum of negative polarity score.",2017-05-26 00:00:00,53,62806,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
108193,,FANSYURI JENAR,DESIGN AND IMPLEMENTATION OF E-COMMERCE PLATFORM USING TELEGRAM BOT,,,"Telegram bot, e-commerce","Drs. Bambang Nurcahyo Prastowo, M.Sc.",2,3,0,2017,1,"Dalam beberapa tahun terakhir, internet telah membuat kapasitas lain yang disebut bisnis berbasis web e-commerce. Bisnis online umumnya dikenal sebagai perdagangan elektronik dan sekarang berubah menjadi jalan prinsip virtual dunia. Hal ini pada dasarnya menutupi semua kegiatan di web dan mendorong pelanggan untuk membeli item melalui internet. Dalam situasi yang cepat berubah dalam e-bisnis, pelanggan memiliki banyak permintaan, mereka mencari strategi pertukaran dengan cara sederhana dan berguna. Dengan mengamati dan mendobrak perilaku dan persyaratan pelanggan online, mereka mengharapkan penawaran termurah dan pelayanan tercepat. Ini adalah permintaan yang penting setiap pelanggan yang akan menggunakan e-commerce. 
Menurut Jakob Bjerre (2015) Telegram messenger menggunakan teknik enkripsi dan memiliki keamanan yang tinggi karena menyediakan keamanan lebih dari setiap aplikasi untuk keamanan, privasi, dan cepat.cepat, dan Penerima Anda mendapat data secepat Anda mengklik tombol kirim. Gratis,sumber terbuka, dan aman, membuat Telegram mudah digunakan untuk pengembang aplikasi yang ingin memodifikasi kode sumber Telegram Messenger. Dengan menggunakan Bot oleh Telegram Messenger pengembang bisa aplikasi dibangun atas Telegram dan mempersembahkan jemaat itu kepada pengguna Telegram. Oleh itu e-commerce sistem dapat menggunakan keuntungan dari Telegram bot untuk desain dan implementasi sistem e-commerce yang lebih efisien untuk pengguna.","In the recent years, the internet has made another capacity called e- commerce web based business. Online business is generally known as electronic trade and now as turn into a virtual principle road of the world. It is basically cover all activities on the web and prompts the customer to buy the item over the internet. In the quickly changing situation of e-business, customers have turned out to be additionally requesting; they are searching for strategies by which exchange should be possible in a simple and helpful way. By watching and breaking down the conduct and the prerequisites of the online customers, they expect cheapest deals and faster delivery and service. These are the essential requests of each customer who will be using e-commerce.
Telegram messenger according to Jakob Bjerre (2015) It has high security as it provides more security than any other messenger apps and the secret chat using encryption technique is an example for security and privacy, it is fast, and your recipient gets the data as quickly as you click the send button. Free, open source, and secure, makes Telegram easy to use for an application developer who wants modify the source code of Telegram messenger. By using Bot by Telegram Messenger developer can built application over Telegram and presents it to Telegram users. By that e-commerce system can use the advantage of Telegram bot to design and implementation system e-commerce that more efficient for the user.",,,58944,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129446,,AFIF IZZUL FALAKH,PENGEMBANGAN KOMPONEN PERANTARA SISTEM VISUALISASI GRAF HASIL TRANSFORMASI DATA BRT MENGGUNAKAN ALGORITMA MULTIPLEX VISIBILITY GRAPH PADA GRAPHX,,,"transformasi data, data deret waktu, graf, komponen perantara, HTTP server, GraphX, Spark","Dr. Mardhani Riasetiawan, S.E., M.T.",2,3,0,2017,1,"Visualisasi data dalam bentuk graf dapat digunakan untuk menganalisis pola data dan mengeksplorasi data untuk bisa mendapatkan pengetahuan baru. Visualisasi graf tersebut dapat diterapkan pada analisis halte Bus Rapid Transit (BRT) untuk menemukan pola penggunaan halte dari waktu ke waktu maupun untuk menemukan informasi dengan mengeksplorasi relasi antarhalte. namun karena ukuran data BRT yang besar, diperlukan mesin pengolahan graf skala besar seperti GraphX yang merupakan bagian dari mesin pengolahan big data Spark. Dengan menggunakan Spark, data BRT yang merupakan data deret waktu ditransformasikan menjadi graf lalu dimuat dalam GraphX. Kemudian data graf dalam GraphX tersebut diekspor menjadi berkas untuk dapat divisualisasikan menggunakan program visualisasi di luar lingkungan pengolahan data Spark. berdasarkan latar belakang tersebut, dalam penelitian ini diusulkan penggunaan komponen perantara berupa HTTP server yang tertanam pada kode program untuk bisa menyediakan data graf secara langsung kepada antarmuka visualisasi yang berada di luar sistem Spark. Dengan demikian, tidak diperlukan proses ekspor data graf menjadi berkas. Sehingga menyingkat proses yang diperlukan untuk memvisualisasikan graf dan memungkinkan interaksi secara dinamis antara antarmuka visualisasi dengan GraphX. Dari hasil pengujian didapatkan bahwa algoritma MVG dapat digunakan untuk mentransformasikan data BRT menjadi graf dengan beberapa langkah prapemrosesan untuk menyiapkan data BRT. Pertambahan waktu transformasi menggunakan algoritma MVG juga relatif konstan terhadap penambahan jumlah data olah dengan rata-rata pertambahan waktu transformasi sebesar 18,21 detik tiap penambahan 5.500.000 baris data olah. Kemudian didapatkan juga bahwa penyediaan data siap visualisasi menggunakan komponen perantara sedikit lebih cepat namun lebih stabil dibandingkan tanpa menggunakan komponen perantara, dengan rata-rata pertambahan waktu penyediaan data dengan komponen perantara sebesar 1 detik dan standar deviasi sebesar 1,89 dibandingkan tanpa menggunakan komponen perantara sebesar 1,27 detik dan standar deviasi sebesar 6,49.","Graph visualization can be used to analyze data pattern and to explore the data to gain new insight. This graph visualization can be implemented on Bus Rapid Transit (BRT) shelter analysis to find the dynamic usage pattern of the shelter from time to time and to gain new information by exploring the relation of each shelter. But, because the sheer size of the BRT data, a large scale graph processing engine such as GraphX which is a part of big data processing engine Spark is needed. Using Spark, BRT data which is a time series data transformed into graph and then loaded into GraphX. Then, the graph data inside GraphX is exported into files which can be used to build the visualization using external visualization program outside Spark environment. Based on that background, this research proposed the use of middleware in the form of HTTP server, which is embedded inside the program code, to serve the graph data directly to the visualization interface. Thus, no need to export the graph data into files. so that the process to visualize the graph can be shortened and enabling a dynamic interaction between visualization and GraphX. From the evaluation result, it is found that MVG algorithm can be used to transform BRT data into graph by performing some preprocessing steps to prepare the BRT data. The time needed to transform the data using MVG algorithm also shows a relatively constant increase along the icreasing amount of processed data with an average increase of 18.21 seconds for each 5,500,000 additional data rows. It is also proved that the processing time needed to serve visualization ready data using middleware is a bit faster but much stable than without the use of middleware with the average increase of processing time using middleware comes to 1 second with standard deviation of 1.89 compared to the average increase of processing time without the use of middleware which comes to 1.27 seconds with standard deviation of 6.49. ",,,80370,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113319,,CHANDRA SAHA DEWA P,SISTEM REKOMENDASI PADA E-COMMERCE BERBASIS GRAF MENGGUNAKAN ALGORITMA K-NEAREST NEIGHBOR,,,"Sistem Rekomendasi, graf, K-Nearest Neighbor, Collaborative Filtering, Content based, Hybird","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2017,1,"Semakin banyaknya informasi produk yang ada di internet menghadirkan tantangan baik pembeli maupun pebisnis online dalam lingkungan e-commerce. Pembeli sering mengalami kesulitan saat mencari produk di internet karena banyaknya produk yang dijual di internet. Selain itu, pebisnis online sering mengalami kesulitan karena memiliki data mengenai produk, pembeli, dan transaksi yang sangat banyak, sehingga menyebabkan pebisinis online mengalami kesulitan untuk mempromosikan produk yang tepat pada target pembeli tertentu.
Sistem rekomendasi dikembangkan untuk mengatasi permasalahan tersebut dengan berbagai metode seperti collaborative filtering, content-based, dan hybrid. Metode collaborative filtering menggunakan data rating pembeli, content based menggunakan konten produk seperti judul atau deskripsi, dan hybrid menggunakan keduanya sebagai dasar rekomendasi. Dengan menggunakan basis data graf, maka model sistem rekomendasi dapat dirancang dengan berbagai metode pendekatan sekaligus.
Pada penelitian ini, algoritma K-Nearest Neighbor digunakan untuk menentukan top-n rekomendasi produk untuk setiap pembeli. Hasil dari penelitian ini metode content based mengungguli metode lain karena data yang digunakan sparse, yaitu kondisi dimana jumlah rating yang diberikan pembeli relatif sedikit terhadap banyaknya produk yang tersedia pada e-commerce.","The growing number of product information available on the internet brings challenges to both customer and online businesses in the e-commerce environment. Customer often have difficulty when looking for products on the internet because of the number of products sold on the internet. In addition, online businessman often experience difficulties because they has much data about products, customers, and transactions, thus causing online Businessman have difficulty to promote the right product to a particular customer target. 
A recommendation system was developed to address those problem with various methods such as collaborative filtering, content-based, and hybrid. Collaborative filtering method uses customer's rating data, content based using product content such as title or description, and hybrid using both as the basis of the recommendation. Using a graph database,the recommendation system model can be designed with a variety of approach methods at once. 
In this research, the K-Nearest Neighbor algorithm is used to determine the top-n product recommendations for each buyer. The result of this research method content based outperforms other methods because the sparse data, that is the condition where the number of rating given by the customers is relatively little compared the number of products available in e-commerce.
",,,64114,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112552,,BAIHAKI DWI WAHYU K,PENGARUH OPERASI FINITE AUTOMATA PADA GRAMMAR SPECIFICATION JAVACC TERHADAP PERFORMA PARSER YANG DIHASILKAN,,,"kecepatan,parser,JavaCC,finite automata,lookahead","Dr., Suprapto, M.I.Kom.",2,3,0,2017,1,"Salah satu faktor yang mempengaruhi kecepatan pembacaan parser adalah proses lookahead. Proses ini digunakan untuk memilih satu dari beberapa aturan produksi yang mungkin untuk dipilih pada proses pembacaan. Proses ini dilakukan dengan melakukan pembacaan tambahan pada masukan. Dikarenakan adanya proses pembacaan tambahan, kecepatan pembacaan parser menjadi melambat.
	Salah satu bentuk grammar yang dapat digunakan pada grammar specification JavaCC adalah right-linear grammar. Bentuk ini bisa diubah ke bentuk finite automata secara ekuivalen. Bentuk finite automata sendiri dapat diubah dari non-deterministic finite automata ke bentuk deterministic finite automata. Melalui pengubahan ini, maka tidak ada lagi ambiguitas untuk memilih transisi yang tepat, sehingga jika pengubahan ini digunakan pada grammar dari parser, parser tidak memerlukan proses lookahead.
	Pada penelitian ini, dicoba untuk mengenakan operasi finite automata, yaitu pengubahan bentuk non-deterministic finite automata ke bentuk deterministic finite automata serta minimalisasi jumlah state dari deterministic finite automata, pada grammar specification. Hasil pengenaan dibandingkan dengan parser asli untuk mengetahui pengaruhnya terhadap kecepatan pembacaan. Dari perbandingan, ditunjukkan bahwa parser yang dikenakan operasi finite automata lebih lambat dibandingkan parser asli. Hal ini disebabkan karena lonjakan pada jumlah variabel dan aturan produksi serta pemecahan string. Hal ini mengakibatkan tambahan waktu yang lebih tinggi dibandingkan tambahan waktu dari lookahead yang dihilangkan.
","One of the factors that affect the speed of parser readings is the lookahead process. This process is used to select one of several production rules that may be selected in the reading process. This process is done by performing additional readings on the input. Due to an additional reading process, the parser reading speed slows down.
	One form of grammar that can be used in JavaCC's grammar specification is the right-linear grammar. This form can be converted to an equivalent form of finite automata. The shape of the finite automata itself can be changed from non-deterministic finite automata to the deterministic finite automata. Through this conversion, there is no ambiguity to choose the right transition, so if this conversion is used on the grammar of the parser, the parser does not require a lookahead process.
	In this study, it is attempted to impose finite automata operations, namely the non-deterministic finite automata conversion to the deterministic finite automata and the minimization of the number of states of deterministic finite automata, to the grammar specification. The imposition results are compared with the original parser to determine its effect on the readout speed. From the comparison, it is shown that the parser subjected to finite automata operation is slower than the original parser. This is due to a spike in the number of variables and production rules as well as breaking the string. This results in an additional time higher than the additional time of the removed lookahead.
",,,63270,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116904,,MIFTA ADDINA,APLIKASI LOCATION BASED SERVICE DAERAH RAWAN KECELAKAAN DENGAN METODE PUSH NOTIFICATION PADA SISTEM OPERASI ANDROID  (Studi Kasus Daerah Istimewa Yogyakarta),,,"Location Based Service, Android, Rawan Kecelakaan, AHP, push notification","Sigit Priyanta, S.Si., M.Kom.",2,3,0,2017,1,"Keselamatan berkendara merupakan hal yang harus diutamakan setiap pengguna transportasi baik di darat, laut maupun udara. Data Kepolisisan RI menyebutkan pada tahun 2012 terjadi 117.949 kecelakaan lalu lintas di Indonesia dengan 29.544 orang tewas karenanya. Angka kecelakaan ini meningkat 8.51% dibandingkan tahun sebelumnya. 
Kota Yogyakarta memiliki tingkat kecelakaan lalu lintas yang tinggi dengan menempati urutan kedua terbanyak sebagai penyebab kematian. Hampir seluruh Polresta Yogyakarta Unit Laka memiliki sistem atau mekanisme untuk mengarsipkan kejadian kecelakaan lalu lintas dari waktu ke waktu. Berdasarkan rekaman data inilah kemudian dianalisis untuk menentukan daerah yang rawan terhadap kecelakaan lalu lintas. Metode AHP mampu menyerdahanakan permasalahan kompleks yang tidak terstruktur, stratejik, dan dinamik untuk mengambil keputusan yang efektif atas persoalan tersebut. 
Hasil penelitian berupa aplikasi mobile pada sistem operasi Android berbasis Location Based Service yang dapat mendeteksi lokasi dengan tingkat rawan kecelakaan tinggi serta memberikan peringatan push notification berupa suara dan getar kepada pengguna kendaraan agar lebih waspada saat memasuki area tersebut. Aplikasi ini mengolah data yang diperoleh dari server website dengan menerapkan metode AHP sebagai sistem pendukung keputusan.","Safety riding is a top priority to every riders both on land, sea or air. Indonesian Police Departement's data said in 2012 there were 117,949 traffic accidents in Indonesia with 29,544 people killed by it. This accident rate increased 8.51% over the previous year.
	Yogyakarta city has a high traffic accident rate with the second largest number as the cause of death. Almost all of Yogyakarta police accident units have a system or mechanism to archive traffic accidents from time to time. Based on the recorded data, it can be analyzed to determine areas that are prone to traffic accidents. The AHP method is capable to simplify complex, unstructured, strategic and dynamic problems to make effective decisions on the issue.
	The result of this research is a mobile application on Android operating system based on Location Based Service which can detect locations with high hazardous sites levels and providing a notification alert with push notification in sound and vibration to make the user more cautious when entering the radius of the area. This application process data obtained from website server by applying AHP method as decision support system.",,,67757,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116649,,HANIEF,EKSTRAKSI INFORMASI WEBSITE RUMAH SAKIT DENGAN METODE WEB SCRAPING DAN DOMXPATH,,,"Rumah Sakit, Ekstraksi Informasi, Web Scraping, PHP, DOMXPath","Hospital, Extraction Information Web Scraping, PHP, DOMXPath",2,3,0,2017,1,"Seiring pesatnya perkembangan teknologi terkini, telah banyak instusi rumah sakit yang merasakan pentingnya penyediaan informasi dalam media elektronik, salah satunya dengan menyediakan situs web resmi rumah sakit. Tersedianya situs web resmi rumah sakit memang sudah cukup memudahkan masyarakat untuk mencari informasi layanan yang disediakan. Meski demikian, situs web resmi rumah sakit tersebut hanya menyediakan informasi dari satu rumah sakit saja.

Penelitian ini merumuskan sebuah ekstraksi informasi dari beberapa situs web resmi rumah sakit dengan menggunakan metode Web Scraping dan DOMXPath. Ekstrasi informasi difokuskan pada 2 hal, yaitu layanan poliklinik dan jadwal praktek dokter.

Dari hasil pengujian yang dilakukan, terlihat bahwa telah berhasil dibangun sebuah solusi ekstraksi informasi layanan dan jadwal praktek dokter yang terdapat dalam empat situs web resmi rumah sakit yang berada dalam provinsi DIY. Hasil pengujian juga menunjukan bahwa ekstraksi informasi dalam penelitian ini memiliki tingkat rata-rata ketepatan data 90%.","With the development of the latest technology, many hospitals have felt the importance of providing information in electronic media, one of them is by providing the hospital's official website. The hospital's official website is enough to make it easier for the public to find the information provided by hospital. However, the hospital's official website only provides information from one hospital.

This research formulates an information extraction from some of the hospital's official websites using Web Scraping method and DOMXPath. Information Extraction is focused on two things: the polyclinic service and the doctor's schedule.

From the results of the tests conducted, showed that it has successfully built a solution extraction information of service and doctor's practice schedule from four hospital's official website located in the DIY province. The test results also show that the extraction of information in this research has data accuracy with average 90%.",,,67532,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112042,,ARIF SUDIONO,PENGEMBANGAN APLIKASI BERBASIS LOKASI UNTUK PENCARIAN MASJID DI YOGYAKARTA PADA SISTEM OPERASI ANDROID,,,"Android, Compass, Location Based Service, Mosque","Moh Edi Wibowo, Ph.D.",2,3,0,2017,1,"Saat ini penggunaan Location Based Service pada smartphone Android semakin meningkat. Salah satunya untuk membuka aplikasi pencarian masjid. Banyak dari pengguna mencari masjid selain untuk menunaikan salat juga untuk keperluan lain dengan memanfaatkan fasilitas yang disediakan masjid. Akan tetapi belum ada aplikasi pencari masjid yang memuat informasi fasilitas dari masjid. 
Pada penelitian ini dikembangkan aplikasi pencarian masjid yang memuat informasi fasilitas dari masjid pada smartphone Android. Selain memuat informasi fasilitas dari masjid, aplikasi ini juga memiliki fitur berupa jadwal salat, penunjuk arah kiblat, serta tambah masjid yang memungkinkan pengguna dalam berkontribusi menambahkan masjid yang belum ada di server. Aplikasi ini juga memuat daftar masjid yang berada pada radius 1 km dari lokasi pengguna dan mengurutkannya dari jarak yang terdekat. Aplikasi ini juga dapat menggambar rute menuju masjid yang dipilih.
Sistem telah selesai dibangun dan dapat berjalan dengan baik sesuai dengan perancangan. Dari hasil pengujian pada 10 smartphone Android dari berbagai merek dan versi, semua fitur dapat berjalan dengan baik. Namun memiliki beberapa kekurangan, di antaranya fitur penunjuk arah kiblat hanya dapat berjalan pada smartphone yang dilengkapi sensor kompas dan fitur daftar masjid terkadang menunjukkan jarak yang tidak sesuai jika koneksi internet tidak stabil.","Nowadays, the use of Location Based Service on Android phones has increased. One of them is to open an application for finding a mosque. Some users besides of looking for mosque for salat, they also have purposes for utilizing the provided facilities of the mosque. But there isn't any application yet for finding a mosque which contains information about facilities of the mosque.
This research developed an application for finding mosques that contain information about facilities of the mosques on Android phones. In addition, this application also contains other features such as a prayer time, qibla direction, and add mosques that allows users to contribute in adding mosques that doesn't exist on the server. This application also contains a list of mosques within a 1 km radius of user location and sort them by shortest distance. This application can also make a route to go to the chosen mosque.
The system has been completed developed and can be run properly according to system design. Based on the test result from 10 Android phones from various brands and various versions, all features work properly. But it has some drawbacks, such as the qibla direction feature can only work on the phones which have compass sensor and the list of mosques shows the distance that sometimes inaccurate on unstable internet connection.",2017-05-29 00:00:00,53,62816,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107436,,REDHA JAUHAR RAHMADY,Implementasi User Interface dan User Experience pada Learning Management System eLisa Universitas Gadjah Mada dengan Mengguna Heuristics of Responsive Web Design dan Metode SUS (System Usability Scale),,,"User experience,user interface,responsive web design","Janoe Hendarto, Drs., M.I.Kom",2,3,0,2017,1,"Saat ini pengaksesan internet oleh perangkat bergerak semakin gencar, salah satunya untuk mengakses learning management system sebagai wadah untuk bertukar informasi. Universitas Gadjah Mada mempunyai sebuah learning management system bernama eLisa, namun sayangnya antarmuka eLisa tidak mampu beradaptasi dengan lebar layar terutama pada perangkat bergerak. User experience merupakan persepsi dan respon dari seseorang terhadap suatu produk, sistem atau jasa. Dalam pengembangannya, user interface pada sistem dirancang menggunakan user experience research. Dengan menggunakan analisis user experience, user interface dan heuristics of responsive web design, antarmuka eLisa akan dapat beradaptasi dengan lebar layar terutama pada perangkat bergerak.

Dalam penelitian ini akan dibangun sebuah antarmuka pengguna eLisa dengan menggunakan kaidah pola-pola desain dan heuristics of responsive web deisgn yang akan dapat berjalan baik pada browser perangkat desktop, tablet dan smartphone. Selain itu juga memiliki nilai kebergunaan yang baik di atas standar kebergunaan pada umumnya. Antarmuka yang dibangun mulai dari antarmuka pengguna mahasiswa hingga dosen.

Hasil pengujian menunjukkan bahwa antarmuka eLisa dapat berjalan baik pada perangkat desktop, tablet dan smarphone. Kemudian antarmuka eLisa juga memiliki nilai kebergunaan sebesar 74.5 untuk antarmuka eLisa dosen dan 82.5 untuk antarmuka eLisa mahasiswa.
","Nowdays the internet that using by mobile device become more and more, one of them is using for accessing a learning management system as a place to share informations. Universitas Gadjah Mada has a learning management system named eLisa, but sadly eLisa cannot handle the media screen espesially for mobile device mdeia screen. User experience is preseption and response from people of a product, system or service. In their development, user interface in a system built by user experience research. By using an analysis of user experience, user interface and also heuristics of responsive web design, the interface of eLisa will be adapt to media screen espesially mobile device media screen.

In this research will be build an user interface eLisa by using design patterns and heuristics of responsive web design that run well into desktop browser, tablet browser and also smartphone browser. Beside that the interface of eLisa also have a good usability above the usability standar score. Interface that  will be build start form college student interface up to lecturer interface.

The result show that the interface of eLisa run well in the desktop  browser, tablet browser and smartphone browser. And then interface of eLisa also have good usability score that is 74.5 for lecturer interface of eLisa and 82.5 for college student interface of eLisa.
",,,58066,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
127661,,M. ALFATH ISLAMI,EKSTRAKSI TREN TOPIK  UTAMA PADA PORTAL BERITA ONLINE INDONESIA MENGGUNAKAN METODE COSINE SIMILARITY,,,"Ekstraksi topik, cosine similarity, trending, portal berita Online, similaritas","I Gede Mujiyatna, S.Kom., M.Kom",2,3,0,2017,1,"Meningkatnya jumlah pengguna internet sangat berpengaruh bagi perkembangan kehidupan manusia. Menurut data yang dikeluakan oleh APJII pada tahun 2015, pengguna internet di Indonesia mengalami pertumbuhan menjadi 88,1 juta pengguna. Dalam hal pengguanan internet untuk mengakses berita, hasil penelitian menyebutkan adanya kecenderungan semakin banyak orang yang mencari berita melalui portal berita online dengan persentase pengguna mencapai angka sebesar 59,7% dan banyaknya portal berita online baru yang tumbuh. akan tetapi bagi yang hanya ingin mengetahui tren berita saja pada hari itu dari semua portal berita akan kesulitan untuk mengetahui  karena keterbatasan waktu dan lain sebagainya. 
Pada penelitian ini yaitu dilakukan ekstraksi berita dengan menggunakan metode similaritas yaitu cosine similarity untuk mencari sebuah tren dari semua berita yang diterbitkan. Dengan menggunakan data dari 10 website portal berita online dengan memanfaatkan RSS feed pada website berita yang bertujuan untuk mengambil judul dan lead dari sebuah berita . Berita yang digunakan tidak terbatas pada kategori tertentu. Pengujian dilakukan sebanyak 30 kali yaitu 3 kali pengujian dalam 1 hari maka waktu yang dibutuhkan untuk pengujian adalah 10 hari. Dari pengujian tersebut didapatkan hasil bahwa penggunaan judul berita lebi baik dalam penentuan tren topik utama berita dibandingan lead berita.","The increasing number of internet users is very influential for the development of human life. According to data released by APJII in 2015, internet users in Indonesia have grown to 88.1 million users. In terms of using the internet to access news, the results of the study mentioned the tendency of more and more people who seek news through the online news portal with the percentage of users reached a figure of 59.7%, and many new online news portals are growing. But for those who just want to know the trend of news portal will be difficult to know because of time constraints and so on. 
In this research that is extraction news by using method similaritas that is cosine similarity to look for a trend from all news published. Using data from 10 online news portal websites by utilizing RSS feeds on news onlines that aim to take the title and description of a news item. The news used is not limited to certain categories. Testing done 30 times that 3 times test in 1 day then time required for testing is 10 days. Test results obtained that the use of headlines is better than news leads in the determination of major topic trends.",,,78473,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114352,,SIKHATI NIKMAH ABADIYAH,SISTEM PAKAR DIAGNOSIS PENYAKIT GIGI DAN MULUT MENGGUNAKAN METODE FORWARD CHAINING DAN CERTAINTY FACTOR ,,,"sistem pakar, kesehatan gigi mulut,  foward chaining, certainty factor, uncertain term","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2017,1,"Gigi dan mulut merupakan organ yang berperan langsung dalam proses pencernaan pada makanan secara utuh tanpa melalui ekstrasi terlebih dahulu. Artinya secara tidak langsung kesehatan gigi dan mulut berpengaruh terhadap kesehatan organ lainnya. Perawatan kesehatan gigi dan mulut biasanya dilakukan pada sebuah klinik pelayanan kesehatan dengan bantuan dokter gigi. Akan tetapi, tidak setiap saat dokter gigi tersedia di klinik pelayanan kesehatan tersebut. Oleh karena itu perlu adanya sistem pakar untuk menangani kendala tersebut.
Sistem pakar merupakan sistem yang mengadopsi pengetahuan pakar ke dalam sistem komputer. Sistem Pakar ini disusun menggunakan metode Foward Chaining dengan penerapan aturan IF-THEN dan  metode Certainty Factor untuk menangani masalah ketidakpastian. Tingkat ketidakpastian akan diklasifikasikan berdasarkan Uncertain term. Sistem pakar ini dioperasikan oleh dokter sebagai pakar dan perawat gigi sebagai user. Sistem ini menyajikan satu set pernyataan berupa gejala yang akan dijawab oleh pasien melalui perawat gigi dan satu set aturan yang merupakan pengetahuan dari pakar yang telah dikodekan.
Pengujian sistem dilakukan dengan membandingkan kesesuaian dari seluruh input terhadap output antara data puskesmas dengan data sistem. Diperoleh akurasi sistem sebesar 100%  atas pengujian 155 data. 
","Teeth and mouth are body organs that directly involved in the whole digestive process without an extraction before. Which means the sanity of teeth and mouth indirectly effect the sanity of the other body organs. Teeth and mouth's treatment often been doing at a public clinic with an assistance from a dentist. But, dentist are not always available there to help. Therefore we need an expert system to solve this problem.
Expert system is a system that adopt the expert's knowledges then converting it to the computer system. This expert system created by using the foward chaining method by applying the IF-THEN rule and using the certainty factor to solve the uncertainty problem. The level of uncertainty will be classified based on the uncertain term. This expert system will be operated by dentist as an expert and dental hygienist as user. This system will provide one set of statement contains symptoms which will be asked to the patient by dental hygienist and one set of rule contains answer which based on knowledges coded from the expert.
Testing process in this system done by matching all input with output between data from the clinic with the system's data. we have obtained 100% of the system's accuracy by testing 155 datas.
",,,65110,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
108979,,QORINTYA PATHYA SWASTYANA,SISTEM PAKAR UNTUK PENENTUAN TINDAK PIDANA TERHADAP TUBUH SESEORANG MENGGUNAKAN METODE FORWARD CHAINING,,,"sistem pakar, tindak pidana, delik, penalaran maju, penganiayaan, kesusilaan","Retantyo Wardoyo, Drs., M.Sc., Ph.D.",2,3,0,2017,1,"Tindak pidana yang paling sering terjadi dalam masyarakat, khususnya di Indonesia adalah tindak-tindak pidana yang berobjek pada tubuh seseorang, yaitu penganiayaan dan kesusilaan. Kurangnya sosialisasi dari instansi atau lembaga terkait tentang permasalahan tersebut membuat masyarakat yang tidak mengerti hukum menjadi bingung saat terlibat dalam suatu kasus, baik sebagai tersangka maupun korban, atau hanya sekedar mencari informasi. Belum lagi pasal-pasal dalam peraturan-peraturan hukum yang mengatur tentang permasalahan tersebut tidaklah sedikit, sehingga dapat menyulitkan masyarakat dalam memilah-milah dan memahami setiap pasalnya.

Salah satu cara untuk mengatasi hal tersebut adalah dengan membangun suatu sistem pakar yang dapat digunakan untuk membantu masyarakat yang tidak paham tentang hukum, dalam menelusuri suatu tindak pidana sehingga menghasilkan pasal apa saja yang dapat dikenakan pada seorang pelaku tindak pidana, terkait dengan tindak pidana terhadap tubuh seseorang.

Sistem pakar adalah sistem yang meniru kepakaran seorang pakar dalam suatu bidang. Sistem ini dibuat dengan metode inferensi forward chaining, yaitu metode yang memulai proses penelusuran dari fakta-fakta menuju ke suatu konklusi. Materi hukum untuk sistem ini diadopsi dari KUHP dan Undang-Undang No. 35 Tahun 2014, khususnya yang mengatur tentang tindak pidana atau delik penganiayaan dan kesusilaan.","The most frequent crime that is occur in the community, especially in Indonesia is a criminal act that uses a person's body as the object, which is abuse and immorality. Lack of socialization from related institutions or agencies on these issues makes people who do not understand the law becomes confused when involved in a case, either as a suspect or a victim, or just looking for information. Not to mention the articles of the law that regulates that thing is so many, so it can be difficult for people to sort out and understand each chapter.

One way to overcome this is to build an expert system that can be used to help people who do not understand the law, in tracking down a criminal act or delict, so that people will understand the articles to be imposed on perpetrators of a criminal act, relating to a delict against a person's body.

Expert system is a system that mimics the expertise of an expert in the field. This system was designed with forward chaining as an inference method, a method which starts the process of fact-finding led to the conclusion. Legal materials for this system was adopted from KUHP and Undang-Undang No. 35 Tahun 2014, particularly regarding criminal act or delict of abuse and immorality.",,,59655,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111028,,YULIAWAN RIZKA S,KLASIFIKASI KOMENTAR SPAM DAN ANALISIS SPAMMER PADA INSTAGRAM,,,"klasifikasi, clustering, spam, spammer, instagram, naive bayes, complement naive bayes, k-means","Arif Nurwidyantoro, S.Kom.,M.Cs.",2,3,0,2017,1,"Perkembangan dunia media sosial memicu perkembangan spam yang tinggi. Permasalahan spam disebabkan oleh spammer, yaitu pengguna yang cenderung lebih banyak melakukan spam bila dibandingkan dengan seluruh kiriman yang pengguna tersebut miliki.
Penelitian ini menggunakan pendekatan Supervised Learning yaitu algoritma Naive Bayes dan Complement Naive Bayes untuk melakukan klasifikasi spam pada komentar di Instagram. Pada setiap algoritma di atas, kalimat komentar dipecah menggunakan tokenisasi untuk dijadikan pemodelan dan digunakan untuk melakukan klasifikasi. Kemudian algoritma clustering digunakan untuk mengetahui persebaran spam dan spammer berdasarkan hasil klasifikasi.
Penelitian ini menunjukkan hasil klasifikasi menggunakan Complement Naive Bayes dapat mencapai nilai Precision 98,86% pada kelas bukan spam dan 81,47% pada kelas spam, nilai Recall 81,23% pada kelas bukan spam dan 98,88% pada kelas spam, nilai Accuracy 89,26% dan nilai F-Measure 89,18%. Hasil parameter pengujian menunjukkan bahwa Complement Naive Bayes cocok digunakan untuk penelitian ini, yaitu dimana jumlah masing-masing data berbeda. Clustering spammer dilakukan menggunakan k-Means dengan 2 cluster yaitu spammer dan bukan spammer. Clustering menghasilkan nilai silhoutte coefficient dapat mencapai nilai 0,930 yang berarti hasil clustering memiliki hasil yang memiliki kepadatan yang tinggi dan keterpisahan cluster yang cukup jauh dari cluster lainnya.","The development of social media lead to the development of spam. This spam problems caused by spammers, which is users who have tendency to spam when compared with the rest of the posts that users have.
This research used Supervised Learning approach which is Naive Bayes and Complement Naive Bayes algorithm to classify spam in Instagram comment. In the algorithm used, comment sentences is tokenized and used as models for classifications.Clustering approach then used to determine the distribution of spam and spammers in Instagram comment using classification results.
This study indicate the classification results using the Complement Naive Bayes can reach a value of Precision 98.86% in non spam class and 81.47% in spam class, Recall value of 81.23% in non spam class and 98.88% in spam class, Accuracy value is 89.26% and F-Measure value is 89.18%. This result shows that Complement Naive Bayes is suitable to used in this study, in which the respective number of data is different. Spammer clustering performed using k-Means using 2 clusters, spammer and non spammer. Clustering producing silhoutte coefficient value that can reach 0.930 which means this clustering have the densest cluster while at the same time every cluster is separated from each other cluster.",,,61721,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110006,,PATRICK WENDY K,KLASIFIKASI MUTU BUAH TOMAT BERDASARKAN FITUR TEKSTUR DAN LUAS AREA CACAT MENGGUNAKAN DECISION TREE,,,"klasifikasi mutu buah tomat, pengolahan citra digital, decision tree","Moh Edi Wibowo, S.Kom, M.Kom, Ph.D",2,3,0,2017,1,"Sortasi mutu buah tomat merupakan proses yang penting dalam kegiatan pasca panen. Selama ini petani melakukan sortasi buah tomat secara manual. Sortasi yang dilakukan secara manual dirasa kurang praktis. Diperlukan sebuah program yang dapat membantu petani dalam melakukan sortasi mutu buah tomat secara otomatis. Dalam penelitian ini dilakukan klasifikasi mutu buah tomat berdasarkan fitur tekstur dan dan luas area cacat menggunakan decision tree.
Sistem menerima input berupa citra buah tomat. Selanjutnya program melakukan preprocessing, segmentasi dan ekstraksi fitur. Pada tahap preprocessing dilakukan cropping dan resize citra tomat. Segmentasi dilakukan untuk membuat mask citra tomat. Fitur yang digunakan adalah fitur tekstur, RGB dan luas area cacat. Fitur tekstur terdiri dari contrast, energy dan homogeneity. Fitur tekstur diperoleh dengan cara membentuk matriks GLCM. Nilai RGB merupakan perbandingan nilai red, green dan blue pada setiap pasang citra tomat. Luas area cacat merupakan besarnya cacat pada permukaan buah tomat. Fitur luas area cacat diperoleh dengan cara melakukan operasi sobel pada citra untuk memperoleh semua edge pada citra. Setelah fitur diperoleh, dilakukan klasifikasi menggunakan decision tree.
Digunakan 250 pasang citra tomat varietas Ratna untuk dataset. Penentuan kelas buah tomat mengacu pada SNI 01-3162-1992. Terdapat 4 kelas buah tomat yang digunakan yaitu mutu 1, mutu 2, luar mutu 1 dan luar mutu 2. Diperoleh akurasi terbaik sebesar 86.8% menggunakan luma grayscale untuk ekstraksi fitur. Ukuran citra terbaik yang digunakan adalah 360x320 piksel.","The tomato quality sorting process is an important process in post-harvest activities. So far, farmers do tomato quality sorting manually. The manual sorting processis not practically effective. A system that can help farmers to classify the quality of tomato is needed. In this research, tomato quality is classified based on texture features and defect area by using decision tree.
The system receives input data in the form of tomato images. After that, the program performs preprocessing, segmentation and features extraction. In preprocessing, cropping and resizing are done to the tomato images. Segmentation is done to make the mask of tomato images. The features used in the program are texture features, RGB and defect area. Texture consists of contrast , energy and homogeneity. Texture are obtained by forming GLCM matrices. RGB is the ratio of red, green and blue on a pair of tomato images. Defect area is the size of defects on the tomato surface. Defect area is obtained using Sobel operator in the image to obtain edges.
Based on the features that have been obtained, classification is done by using decision tree. Dataset used in this research is 250 pairs of Ratna tomato images. Tomato is classified based on SNI 01-3162-1992. There are four classes of tomato used in this research, i.e. mutu 1, mutu 2, luar mutu 1 and luar mutu 2. The best accuracy is 86.8% using luma grayscale for features extraction. Best image size is 360x320 pixels.",,,60650,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110774,,ACHMAD JEIHAN P,ALGORITMA FLOW FIELD A STAR UNTUK MELAKUKAN PATHFINDING PADA PERMAINAN REAL-TIME STARTEGY,,,"pathfinding,flow field,real-time strategy games","Faizal Makhrus, Ph.D",2,3,0,2017,1,"Pada permainan real-time strategy, proses pathfinding dilakukan untuk
sekumpulan objek seperti prajurit yang bergerak dari source yang sama dan menuju
ke target yang sama. Algoritma A* adalah algoritma pathfinding yang digunakan
dalam permainan real-time startegy. Algoritma ini tidak dapat diskala secara efektif
karena harus melakukan komputasi untuk setiap objek yang akan bergerak sehingga
banyak komputasi dan memory digunakan.
Pada penelitian ini, dikembangkan sebuah algoritma pathfinding Flow Field
A* yang merupakan modifikasi dari algoritma A*. Algoritma ini mampu memproses
pencarian path untuk sekumpulan objek yang memiliki source dan target sama.
Selain itu, dengan mengubah output algoritma menjadi flow field, permasalahan
collision antar objek juga dapat diatasi.
Berdasarkan hasil pengujian, algoritma Flow Field A* memiliki running time
10 kali lebih cepat dari algoritma Flow Field Bruteforce dan 5 kali lebih cepat dari
algoritma A*. Selain itu, penggunaan memory algoritma Flow Field A* 20 kali lebih
sedikit dari algoritma A*.","In real-time strategy video games, pathfinding processes are done for a group
of objects like soldiers that move from the same source to the same target. A* is a
pathfinding algorithm used in this games. This algorithm tend to scale poorly because
it must compute a path for each moving object, so that many computation and memory
are being used.
In this research, a new algorithm called Flow Field A* was developed by
modifying A* algorithm. This algorithm was able to do pathfinding process for a
group of objects that has the same source and target. In addition, by changing the
output of pathfinding algorithm into a flow field, collision problem that occurs can be
solved.
Based on the experiments, the running time of Flow Field A* algorithm was
10 times faster than Flow Field Bruteforce and 5 times faster than A* algorithm. In
addition, memory usage in Flow Field A* algorithm was 20 times fewer than A*.",2017-04-12 00:00:00,53,61480,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112822,,I NYOMAN PRAYANA TRISNA,EKSTRAKSI KATA KUNCI PADA DOKUMEN TUNGGAL DENGAN PHRASE CHUNKING,,,"Ekstraksi kata kunci, Phrase Chunking, pola frase, frekuensi kandidat","Arif Nurwidyantoro, S.Kom., M.Cs",2,3,0,2017,1,"Kata kunci pada suatu dokumen membantu proses komputasi di dalam
dokumen tersebut. Namun, pembentukan kata kunci secara manual membutuhkan
pemahaman lebih, waktu yang banyak, dan sering terdapat unsur subjektivitas
dalam pemilihan kata kunci. Dengan ekstraksi kata kunci secara otomatis, maka
kata kunci dapat dibentuk dengan cepat dan objektif. Salah satu metode yang dapat
digunakan adalah Phrase Chunking.
Metode Phrase Chunking membutuhkan pola frase yang menjadi acuan
dalam ekstraksi kandidat kata kunci, sehingga sebelum proses ekstraksi dibutuhkan
pembelajaran terhadap pola frase yang sering muncul dalam kata kunci. Setelah
kandidat berhasil diekstrak, maka kandidat diseleksi sesuai skenario-skenario yang
berdasarkan teks acuan (abstrak atau badan teks), pola frase, dan bentuk
pengurutan. Selain itu, pada penelitian ini dicoba untuk mereduksi kandidat yang
diekstrak. Metode Phrase Chunking pada penelitian ini juga dibandingkan dengan
metode TextRank.
Hasil penelitian ini memberi nilai f-measure yang lebih baik apabila
dibandingkan dengan metode TextRank. Penelitian ini menghasilkan nilai f-measure
tertinggi ketika teks yang digunakan adalah abstrak, dengan reduksi kandidat dan
pola frase yang terdiri dari dua atau tiga kata, dan pengurutan mengunakan
frekuensi kandidat. Hasil dari penelitian ini juga memperlihatkan bahwa ekstraksi
di abstrak lebih baik dibandingkan badan teks. Selain itu, kandidat yang baik
dihasilkan ketika kata kunci yang diekstrak berupa frase dua atau tiga kata. Bentuk
pengurutan terbaik ketika kandidat diurutkan berdasarkan hasil kali frekuensi
kandidat dengan bobot pola frase. Namun, percobaan menggunakan reduksi
kandidat ternyata tidak menghasilkan nilai f-measure yang lebih baik.","Keywords in a document help the computing process in that document.
However, manual assigment of keywords requires more effort to understand, take
more time, and there is often an element of subjectivity in the selection of keywords.
With automatic keyword extraction, keywords can be formed quickly and
objectively. One of the methods that can be used is Phrase Chunking.
Phrase Chunking method requires phrase patterns that become the reference
in extraction of keyword candidates, so before the extraction process, learning
phrase patterns that often appear in the keyword is needed. After the candidate has
been successfully extracted, candidates are selected according to scenarios based
on reference text, phrase patterns, and sorting. In addition, this study attempted to
reduce the extracted candidate. Phrase Chunking method in this study also
compared with TextRank method.
The results of this study gave a better f-measure value when compared with
the TextRank method. This study yielded the highest f-measure value when the text
used was abstraction, with candidate reduction and phrase pattern consisting of two
or three words, with sequencing using candidate frequency. The results of this study
also show that the extraction in abstraction is better than the main body of the text.
In addition, a good candidate is generated when the extracted is just a phrase
consisting of two or three words. The best sorting form when the candidate is sorted
by the frequency of the candidate with the weight of the phrase pattern. However,
experiments using candidate reduction did not result in a better f-measure value.",,,63572,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107706,,LAZUARDY KUSUMA S,DESIGN AND IMPLEMENTATION E-LEARNING SYSTEM USING RENPY BASED VISUAL NOVEL,,,"Game, Visual Novel, RenPy, eLearning, Education","Bambang Nurcahyo Prastowo, Drs., M.Sc. ",2,3,0,2017,1,"Permainan serius adalah alat yang menjanjikan untuk mengajar interaktif dan pelatihan. Permainan serius berlaku untuk berbagai skenario mulai dari pendidikan sekolah untuk pelatihan profesional. Desain game serius berbeda dari merancang permainan tradisional karena ada tujuan belajar yang terlibat. Perpaduan sempurna dari pembelajaran dan hiburan bersama dengan hasil yang menjanjikan adalah pekerjaan menantang. Penelitian ini menyajikan pelaksanaan dan prototipe dari sebuah platform eLearning menampilkan gamifikasi. Tujuan dari kombinasi ini adalah untuk menyediakan alat dengan mempelajari materi dengan layanan berbasis gamifikasi.
Visual novel adalah salah satu jenis game yang populer di Jepang. Dalam penelitian ini, peneliti mencoba menerapkan visual novel sebagai permainan yang serius untuk pendidikan sekolah dasar. Untuk membuat, visual novel ini menggunakan Ren'Py, yang diharapkan dapat digunakan dalam berbagai jenis media. Untuk mendapatkan bahan sumber dari EBook Sekolah di Indonesia, yang akan menjadi isi dari teks dan gambar pada e-book akan berubah menjadi &quot;visual novel&quot;. Dengan menambahkan beberapa statis gambar, animasi, dan suara untuk membuatnya lebih menarik dan menjadi permainan digital.
Visual novel Jenis permainan yang dibuat dengan bahan-bahan yang bersumber dari BSE dengan standar pendidikan Indonesia, dapat meningkatkan minat belajar para siswa dan meningkatkan kualitas pendidikan dengan meningkatkan keragaman media pendidikan. Kemudian pada visual novel berisi konten sesuai untuk bahan ajar sekolah di Indonesia. Game ini sudah tersedia pada alur material yang dapat diikuti oleh pengguna dan dapat menentukan alur material sebagai keinginan pengguna. Visual novel ini juga akan dimainkan pada beberapa komputer dan platform smartphone.","Serious Game is a promising tool for interactive teaching and training. Serious games apply to a wide variety of scenarios ranging from school education to training of professionals. The serious game design is different from traditional game designing as there is a learning objective involved. The perfect blend of learning and entertainment along with promising result is a challenging task. This research present the implementation and prototyping of an eLearning platform featuring gamification. The purpose of this combination is to provide the tool with learning the material with gamification based services.
The visual novel is one type of game that is popular in Japan. In this research, the researchers tried to implement visual novel as a serious game for primary school education. For making, this visual novel uses Ren&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;Py, which is expected to be used in a variety of media platforms. To obtain source materials from EBook School in Indonesia, which will be the content of text and images on the eBook will be transformed into a visual novel. By adding some static images, animations, and sound to make it more attractive and become a digital game.
The visual novel type games made with materials sourced from BSE with Indonesian education standards, may increase interest in student performance and improve the quality of education by increasing the diversity of educational media. Then on the visual novel contains content suitable for school teaching materials in Indonesia. This game is already available on the flow of material that can be followed by users and can define the material flow as the user desires. This visual novel will also be played on some computers and smartphone platforms.",,,58282,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114618,,DIRHAN SETIAWAN UMASUGI,Perancangan Aplikasi Rekap Laporan Keuangan (Studi Kasus pada Sistem Keuangan Pemerintah Kabupaten Buru),,,"Rekap, Keuangan.","Lukman Heryawan, S.T.,M.T",2,3,0,2017,1,"	Rekap laporan keuangan adalah suatu ringkasan isi atau ikhtisar pada akhir laporan atau akhir hitungan. Proses rekap laporan keuangan sangat besar manfaatnya  karena merupakan salah satu alat untuk melaksanakan kegiatan-kegiatan dalam perencanaan, pengendalian, pengawasan dan pengambilan keputusan.
	Perancangan aplikasi rekap laporan keuangan yang mengambil studi kasus pada sistem keuangan lembaga pemerintah kabupaten Buru bertujuan untuk menyediakan sebuah aplikasi yang nantinya bisa digunakan untuk melakukan perekapan data pada lembaga tersebut dalam jangka waktu yang disesuaikan dengan kebutuhan dari penggunaannya. Sehingga penggunaan anggaran keuangan serta target pendapatan keuangan daerah bisa sesuai antara target dan realisasinya.
	Pembuatan aplikasi disesuaikan dengan kebutuhan dari lembaga yang menjadi objek penelitian. Pengujian aplikasi setelah selesai dikerjakan dilakukan oleh user atau individu yang bekerja pada lembaga keuangan dari pemerintah daerah kabupaten Buru. Hasil pengujian aplikasi dapat menampilkan perbandingan data antara target dan realisasi dari pendapatan dan pengeluaran daerah baik dalam kurun waktu satu tahun maupun dalam rentang waktu setiap bulan. 
","Financial statement recap is a summary of the content or an overview at the end of the report or the end of the counting process. The process of recapitulating financial statements is very beneficial because it is a tool to carry out activities in planning, controlling, supervision and decision making.
The design of the application takes a case study on the financial system of Buru district government institutions and aims to provide an application that can later be used to perform data recording at that institution within the time period adapted to the needs of its use. It is designed so that the use of the financial budget and the target revenue of the district finance can be compatible between the target and its realization.
The application was made tailored to the needs of the institution that became the object of research. Application testing is done by user or individual working in financial institutions from the local government of Buru district once the application was completed. The result of the application test can show the comparison of data between the target and the realization from both district income and expenditure either within one year or each month span.
",,,65385,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110523,,MULYAHADI JATIKUSUMA,Simulasi dan Validasi Metode Smoothed Particle Hydrodynamic Untuk Model Hidrodinamika Rip Current,,,"simulasi, validasi, fluida, smoothed particle hydrodynamic, rip current, grafika komputer","Faizal Makhrus S.Kom., M.Sc., Ph.D",2,3,0,2017,1,"Rip current adalah arus normal pantai, mengarah ke laut pada surf zone (daerahdimana ombak pecah) yang semakin luas ketika berada di luar breaking region
(daerah dimana ombak tepat pecah). 80% kecelakaan pantai di dunia disebabkan oleh rip current. Untuk meminmalisir korban, informasi kapan dan dimana rip current muncul dapat didapatkan melalui sistem prediksi rip current.
Sistem prediksi rip current membutuhkan dua bagian penting yaitu model hidrodinamika dan peramalan meteorologi. Pada sistem prediksi rip current, model
hidrodinamika memiliki peran dalam mensimulasikan ombak dan fluida. Model hidrodinamika yang umum digunakan adalah model berbasis Eularian, tetapi akibat
peningkatan daya komputasi, model berbasis Lagrangian mulai digunakan secara luas.
Smoothed Particle Hydrodynamic (SPH) adalah salah satu model numeris berbasis Lagrangian yang disebut memiliki keunggulan pada akurasi dibandingkan dengan model berbasis Eularian. SPH telah digunakan untuk berbagai aplikasi teknis simulasi fluida dan menunjukkan hasil validasi yang baik.
Kehandalan model SPH untuk simulasi rip current menjadi fokus pada penelitian ini. Kehandalan tersebut dibuktikan dengan melakukan proses validasi antara data eksperimen rip current pada laboratorium oleh Haller et al. (2002) dan data hasil simulasi rip current model SPH.
Penelitian ini memberikan kontribusi berupa penemuan parameter simulasi fluida model SPH yang sesuai untuk mensimulasikan rip current. Hasil simulasi menunjukkan
nilai persetujuan sebesar 48%, 46%, dan 28% untuk data kecepatan arus cross-shore, kecepatan arus alongshore, dan kenaikan tinggi permukaan air secara berurutan.","Rip currents are shore-normal, seaward directed jets that originate within the surf zone and broaden outside the breaking region. Rip currents account for more than 80% of lifeguard rescue efforts around the world. To minimize risk, information about when and where rip currents are likely to occur can be obtained by prediction
system.
Prediction system of rip current needs two main component, those are hydrodynamic model and meteorological forecast. In rip current prediction system, hydrodynamic model has role in simulating wave and fluid. Most commonly used hydrodynamic model is Eularian based, however due to improvement in computational power, Lagrangian based model rise.
Smoothed particle hydrodynamic is one of the Lagrangian based model which has advantage in accuracy over Eularian based model. SPH has been used in many engineering applications which involving fluid simulation and shows reasonable agreement.
Reliability of SPH model is main concern in this research. The reliability of SPH model is proven by validating Haller et al. (2002)&acirc;€™s rip current experiment data and SPH&acirc;€™s rip current simulation data.
This research provides discovery of SPH model parameter which suitable for rip current simulation. Simulation results with mentioned parameter had been validated with experimental data and shown 48%, 46%, and 28% agreement values for cross-shore velocity, alongshore velocity, and mean water level respectively.",,,61230,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110779,,MUHAMMAD RIDWAN A B,PARALELISASI PEMANGKATAN MODULAR POLINOMIAL,,,"pemangkatan modular, pemangkatan modular polinomial, komputasi paralel, load balancing","Mhd. Reza M.I Pulungan, M.Sc., Dr.-Ing",2,3,0,2017,1,"Pemangkatan modular merupakan operasi yang penting dalam protokol keamanan jaringan. Pemangkatan modular polinomial adalah pemangkatan modular dengan basis dan modulus berupa polinomial. Salah satu kegunaannya adalah pada algoritma AKS, salah satu algoritma pengecekan bilangan prima tercepat saat ini.
Pada tugas akhir ini dikembangkan algoritma pemangkatan modular paralel yang diusulkan oleh Lara et al. (2012) untuk melakukan pemangkatan modular polinomial secara paralel, dan dibandingkan dengan 2 algoritma paralel lain yang diusulkan. Pada hakikatnya ketiga algoritma tersebut hanya berbeda pada metode load balancing-nya. Diperoleh hasil bahwa untuk bilangan eksponen sepanjang 2048 bit, implementasi algoritma paralel yang diusulkan oleh Lara et al. (2012) memiliki waktu eksekusi yang 28.01% lebih kecil dibandingkan implementasi algoritma sekuensial. Algoritma tersebut juga lebih efisien dibandingkan 2 algoritma lain yang diusulkan.","Modular exponentiation is very important in the network security protocol. Modular polynomial exponentiation is a type of modular exponentiation where the base and modulus are polynomials. One of its usages is on the AKS Algorithm, one of the fastest prime checking algorithm today.
In this research, parallel modular exponentiation algorithm which proposed by Lara et al. (2012) was developed to solve modular polynomial exponentiation in parallel, and compared with two other proposed parallel algorithms. Essentially those algorithms differ only in the load balancing method. The result was, for exponents with a length of 2048 bits, the implementation of parallel algorithm proposed by Lara et al. (2012) has 28.01% smaller execution time compared to the sequential algorithm. The algorithm was also more efficient than the two other proposed algorithms.",2017-04-12 00:00:00,53,61499,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107452,,MUFTI IKHLASUL IKHSAN,PENGGUNAAN MEDIA ACCESS CONTROL PADA PERANGKAT JARINGAN WIFI SEBAGAI IDENTITAS DALAM PENGAMBILAN PRESENSI ,,,"mac address, presensi, aircrack-ng, raspberry pi, attendance, wifi","Bambang Nurcahyo Prastowo, Drs., M.Sc.",2,3,0,2017,1,"Pengambilan presensi merupakan kegiatan yang dapat ditemukan salah satunya pada institusi pendidikan. Pengambilan presensi secara manual, baik menggunakan kertas yang diedarkan ataupun dengan memanggil satu persatu, merupakan cara yang sangat tidak efisien. Dengan cara seperti itu timbul masalah tentang penggunaan waktu pelajaran, keterangan kehadiran palsu, serta pemborosan kertas. Sudah banyak sistem yang diusulkan untuk mengatasi masalah tersebut, seperti menggunakan pemindai sidik jari, RFID, dan face recognition.
Dalam penelitian ini diusulkan sistem yang memanfaatkan MAC Address perangkat WiFi yang terdapat pada smartphone mahasiswa untuk keperluan  presensi. MAC Address dirancang untuk bersifat unik sehingga sangat mungkin dijadikan identitas dalam pengambilan presensi. Dalam pengujiannya mahasiswa hanya diminta untuk menyalakan WiFi pada smartphone dan sistem akan memindai paket yang berisi informasi MAC Address dari perangkat. Sistem diuji pada kelas mata kuliah E-Governement yang diselenggarakan di Fakultas MIPA, Universitas Gadjah Mada. Pengujian dilakukan dua kali, dengan pengujian yang pertama bertujuan untuk melihat karakter pemindaian di kelas, sedangkan untuk pemindaian kedua bertujuan untuk mengetahui hubungan durasi pemindain yang dipilih dengan akurasi yang didapat. 
Pada penelitian ini nilai akurasi diperoleh dari perbandingan pencatatan oleh sistem dengan pencatatan secara manual dengan kertas edaran. Untuk penelitian ini diperoleh akurasi terbaik pemindaian pada durasi 120 detik dengan nilai 90%. Untuk durasi 20 detik diperoleh akurasi sebesar 87,5%, untuk durasi 30 detik diperoleh akurasi sebesar 75%, dan yang terakhir untuk durasi 50 detik diperoleh akurasi 72,5%. Dengan akurasi tersebut penggunaan MAC Address sebagai identitas dalam pengambilan presensi dinilai masih kurang cocok diterapkan. ","Taking student's attendance is common activity especially in educational institutions. The manual process of taking student's attendance using paper or calling student name one-by-one is not that efficient. That way, it can cause another problem such as time-consuming, false attendance marking, also wasting a lot of paper. To decrease the possibility of these problems, there are some solution that already developed by some researcher such as fingerprint scanner, RFID and face recognition. 
In this research, the Attendance tracking system using WiFi MAC Address found in student's smartphone. MAC Address is designed to be unique, so it's possible to use it as personal identification in attendance tracking system. During the experiment, students required to turn on their WiFi on the smartphone, then system will scan the packet containing MAC Address information from the device. The system was tested on E-Government class held at FMIPA, Universitas Gadjah Mada. The tests will be performed twice, with the first test aims to see the scanning behavior, while the second test aimed to determine the relationship between selected scanning duration and scanning accuracy.
The results of this research obtained the greatest accuracy scanning on the duration of 120 seconds with a value of 90%. 87,5% accuracy are obtained scanning using duration of 20 seconds, 75% for duration 30 seconds, and the last is 72,5% accuracy for duration of 50 seconds. With that accuracy, the use of MAC addresses as an identification in attendance tracking system is still considered less suitable to be applied.",,,58080,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
130751,,HIBATUL AZIZI,SISTEM PEMERIKSAAN TANDA VITAL PASIEN DIINTEGRASIKAN DENGAN EMAIL DAN SISTEM INFORMASI REKAM MEDIS ELEKTRONIK,,,"tanda vital, rekam medis elektronik, email","Lukman Heryawan, S.T., M.T",2,3,0,2017,1,"Perkembangan teknologi dewasa ini telah menyentuh hampir semua aspek kehidupan manusia, tidak terkecuali dalam bidang kesehatan. Pemeriksaan tanda vital pasien menjadi salah satu poin utama bagi dokter dalam menuliskan rekam medis. Berkembangnya sistem informasi rekam medis elektronik yang juga lebih praktis dalam penyimpanan data dan kebutuhan ruang, sekaligus memudahkan dalam penulisan data. Penelitian yang ada hanya mengembangkan sistem yang menggunakan suatu alat pengukur tanda vital atau pengiriman data rekam medis melalui berbagai sistem. 

Dalam penelitian ini, dikembangkan sistem pemeriksaan tanda vital yang terintegrasi dengan email dan sistem informasi rekam medis elektronik. Pengembangan sistem dengan memanfaatkan alat medis elektronik untuk membantu memperoleh data pemeriksaan tanda vital dengan lebih akurat. Tanda-tanda vital yang diperiksa adalah tekanan darah, denyut nadi, dan saturasi oksigen. 

Dengan memanfaatkan aplikasi web, database MySQL dan bahasa pemrograman PHP, penelitian ini mengembangkan sistem yang mampu mengintegrasikan alat medis elektronik pemeriksaan tanda vital pasien dengan sistem informasi rekam medis elektronik. Sistem tersebut juga dapat mengirimkan email kepada dokter melalui Simple Mail Transfer Protocol (SMTP) yang berisi data hasil pemeriksaan tanda vital pasien.
","The growth of information and communication technology nowadays has touched almost all aspects of human life, including in the health field. Examination of vital signs of the patient becomes one of the main points for doctors in writing medical record. The development of electronic medical records information system is also more practical in data storage and space needs, as well as facilitate the writing of data. Existing research only develops systems that use a vital sign examination tool or data transmission of medical records through various systems.

In this research, developed a vital sign examination system integrated with email and electronic medical record information system. The development of the system by utilizing electronic medical devices to helps obtain the more accurate vital signs data. The vital signs that examined are blood pressure, pulse rate, and oxygen saturation.

By utilizing web application, MySQL database and PHP programming language, this research developed a system that capable of integrating electronic medical devices for examination of vital signs of the patient with the electronic medical record information system. The system can also send email to the doctor through the Simple Mail Transfer Protocol (SMTP), which contains the data results of the patient's vital signs.
",,,81745,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
117186,,DEWI SHANTY WULANDARI,SHOWY: APLIKASI POINT OF SALE STREET FOOD VENDOR BERBASIS ANDROID MENGGUNAKAN RESTFUL WEB SERVICE UNTUK PERTUKARAN DATA,,,"Web Service, RESTful API, Pertukaran Data, JSON, Android, Firebase, Point of Sale, Street Food Vendor","Faizah, S.Kom., M.Kom.",2,3,0,2017,1,"Street food business merupakan salah satu usaha di bidang kuliner. Sajian makanan yang ringan dan simple serta terjangkau dari segi harga menjadi daya tarik masyarakat modern. Hal ini membuat industri ini berkembang lebih modern dengan istilah streetbar. Permasalahan mulai muncul ketika streetbar memiliki banyak cabang, mulai dari permasalahan teknis, operasional, hingga finance. Salah satu yang cukup penting adalah mengenai permasalahan pengelolaan data transaksi order dan absensi pegawai di tiap streetbar outlet. Kesulitan manajemen pada vendor dengan banyak outlet dalam melakukan pertukaran data, baik pertukaran data laporan dari cabang (outlet) ke pusat (vendor), maupun pertukaran data pembaruan (update data) dari vendor ke outlet diselesaikan dengan pembangunan sistem informasi yang sesuai dengan karakteristik perusahaan. Banyaknya cabang yang dikelola membuat sistem harus bisa berkomunikasi dengan pusat agar data dapat dipertukarkan. Penerapan point of sale system yang terhubung dengan server, penggunaan android sebagai client platform, web sebagai server platform, dan RESTful web service sebagai protokol pertukaran data pada prototype sistem POS dapat menjadi salah satu alternatif solusi untuk permasalahan ini.
Pada penelitian ini, sistem POS mampu melakukan komunikasi antara server dengan client dan melakukan pertukaran data pada masing-masing database. Data yang dipertukarkan adalah data detail outlet, detail personnel dan detail menu yang diambil melalui Showy API. Data absensi, transaksi order, log, dan data pembaruan dari server yang memanfaatkan Firebase API. Kedua API ini menggunakan arsitektur RESTful dengan format JSON. Server menggunakan dua jenis basis data yaitu RDBMS yang ditempatkan pada hosting 000webhost dan NoSQL yang ditempatkan pada Firebase, sedangkan client menggunakan basis data SQLite. Sementara itu, aplikasi mobile android showy yang dikembangkan sebagai client sistem POS ini mampu menangani fungsi absensi pegawai, transaksi order customer, dan menyimpan receipt pada Firebase storage.Aplikasi dapat berkomunikasi dengan server dan dapat digunakan pada banyak street food outlet.","Street food business is one kind of business in culinary industry. Food that has been served are simple and affordable in price. Those make urban people enjoying this kind of food. This reason, make this industry developing more modern known as streetbar. Problem start to persist when the streetbar has a lot of outlet, like technical problem, operational problem, and finance. One of the main problem is order transaction data management problem and employee attendance in each streetbar outlet. This management problem in streetbar vendor that have a lot of outlet, especially report data update from outlet to vendor or vice versa, be completed by building information system suitable with company characteristic. Many outlet that need to be managed make the system should be able to communicate with the vendor, so the data could be exchanged. Point of sale system that connected with the server will be implemented. The used of android as client platform, web as server platform, and RESTful web service as data exchange protocol in prototype POS system could be one of the main solution alternative for this problem.
In this research, POS system able to make a communication between server and client, and doing data exchange in every database. Data that have been exchanged are detail outlet data, detail personnel and detail menu that taken through Showy API. Attendance data, order transaction, log, and data update from server would be using Firebase API. Both API using RESTFul architecture with JSON format. Server using two kind of database, that are RDBMS that have been placed in 000webhost hosting and NoSQL that have been placed in Firebase, while client will be using SQLite database. Meanwhile, android mobile application showy that developed as a client in POS system able to handle employee attendance, customer order transaction, and receipt saving through Firebase storage. Application could communicate with the server and could be used in a lot of street food outlet.",,,68033,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129219,,ADHIKA WISAKSONO,Klasifikasi Berita Berkategori Olahraga dengan Algoritma Multivariate Bernoulli Naive Bayes dan Multinomial Naive Bayes,,,"klasifikasi teks, text mining, Multinomial Naive Bayes, Multivariate Bernoulli Naive Bayes","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2017,1,"Bertambahnya pengguna internet di Indonesia menyebabkan muncul dan berkembangnya situs-situs penyedia informasi yang banyak ditemukan dalam bentuk berita. Jenis berita yang dicari oleh masyarakat bermacam-macam. Oleh karena itu banyak kategori disajikan oleh situs penyedia berita. Antara satu situs berita dengan situs berita lainnya tidak sama dalam menentukan kategorisasi berita. Tetapi terdapat kategori yang cukup populer dan ada hampir di setiap situs berita, yaitu kategori olahraga menjadi dasar penentuan kelas pada penelitian ini. Pada penelitian ini dilakukan klasifikasi teks berita menggunakan algoritma Multinomial Naive Bayes dan Multivariate Bernoulli Naive Bayes. Kategori yang dipakai dalam penelitian ini adalah 9 kelas cabang olahraga utama, 1 kelas cabang olahraga lain dan 1 kelas non-olahraga. Proses dimulai dari pengumpulan data dengan metode scraping, preprocessing, lalu seleksi fitur. Tahap selanjutnya dibentuk model klasifikasi pada tahap training. Prediksi dan evaluasi dilakukan pada tahap testing. Untuk memvalidasi uji performa atas data training yang terkumpul, digunakan metode 10-fold cross validation. Didapatkan bahwa nilai performa maksimum algoritma Multinomial Naive Bayes didapatkan saat digunakan metode seleksi fitur TF-IDF dengan top-n = 2000 yang menghasilkan rata-rata nilai akurasi 97,1%, presisi 97,3%, recall 97,1% dan f-measure 97%. Sementara metode klasifikasi multivariate bernoulli mendapatkan nilai performa maksimal saat menggunakan metode seleksi fitur most common dengan top-n = 1500 yang menghasilkan rata-rata nilai akurasi 94%, presisi 94,8%, recall 94% dan f-measure 93,9%. Metode klasifikasi Multinomial Naive Bayes memiliki nilai performa yang lebih tinggi daripada Multivariate Bernoulli Naive Bayes pada kasus ini. Metode seleksi fitur TFIDF meraih performa maksimal jika jumlah kata fitur lebih banyak pada model multinomial dan lebih sedikit pada model bernoulli. Sementara most common memiliki performa maksimal saat jumlah kata fitur banyak pada kedua model.","As the number of internet users in Indonesia grows, there has been noticeable increase in the emergence and development of information providing sites that are mostly found in the form of news sites. People search for various types of news. Therefore, news sites classify their posts into different categories. News categorization itself varies from one site to another. Nonetheless, sport is a particular category that can be found in almost every news site and therefore served as the base for class determination in this research. This research has done news text classification using Multinomial Naive Bayes algorithm and Multivariate Bernoulli Naive Bayes algorithm. The categories used in this research are 9 primary sport branches, 1 class of other sport and 1 class of non-sport. The process starts with collecting the data using scraping method, preprocessing, and then selecting feature. The next phase is training phase in which classification model is formed. Prediction and evaluation done in testing phase. To validate performance test value from training data collected, 10-fold cross validation method is used. It is found that the maximum performance value of Multinomial Naive Bayes algorithm is obtained when it used TF-IDF feature selection method using top-n value = 2000 which has average performance value of 97,1% accuracy, 97,3% precision, 97,1% recall and 97% f-measure. On the other hand, Multivariate Bernoulli Naive Bayes method reached its maximum performance value when it used most common feature selection method with top-n value = 1500 which has average performance value of 94% accuracy, 94,8% precision, 94% recall and 93,9% f-measure. The Multinomial Naive Bayes method has higher performance value than Multivariate Bernoulli Naive Bayes method in this case. TFIDF method reached its maximum performance when it used more features in multinomial model but fewer features in bernoulli model. While most common method reached its maximum performance when it used more features on both models.",,,80155,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
128964,,AMRUN LUKMAN,PENEREAPAN DEEP LEARNING UNTUK KLASIFIKASI GAMBAR X-RAY PATAH TULANG TELAPAK TANGAN MANUSIA,,,"deep learning, classification, fracture, x-ray","Lukman Heryawan, S.T., M.T",2,3,0,2017,1,"Fraktur adalah patah tulang. Ini terjadi dari retak yang tipis hingga patah keseluruhan. Tulang bisa patah melintang, memanjang, di beberapa tempat, atau sampai berkeping-keping. Sebagian besar fraktur terjadi ketika tulang dikenai oleh gaya yang besar atau tekanan yang tidak dapat disangga.Tulang pada tangan berfungsi sebagai kerangka. Kerangka ini mendukung otot yang membuat pergelangan tangan dan jari bergerak. Bila salah satu dari tulang tangan ini patah (retak), manusia tidak dapat menggunakan tangan, pergelangan tangan dan jari.
Deep learning adalah penerapan jaringan syaraf tiruan (JST) untuk mempelajari tugas yang mengandung lebih dari satu lapisan tersembunyi. Deep learning adalah bagian dari Machine learning yang lebih luas berdasarkan pada representation learning, yang bertentangan dengan algoritma task-specific.Pada penilitian ini dikembangkan sistem dengan deep learning untuk mengklasifikasi fraktur pada telapak tangan manusia. Pada penilitian ini digunakan input berupa gambar x-ray tulang manusia.
Berdasarkan pengujian yang telah dilakukan, klasifikasi fraktur pada tulang manusia ini mendapatkan performa 77.6% dengan akurasi 0.81 , recall 66.8% dan precission 92.5% untuk kelas boxer, performa 86% dengan akurasi 0.95, recall 93.4% dan precission 79.6% untuk kelas mallet dan untuk kelas dislokasi memiliki performa 72% dengan akurasi sebesar 0.90, recall 90% dan precission 60%. ","Fracture is a broken bone. This happens from a thin crack to a whole fracture. Bones can be broken across, elongated, in some places, or to pieces. Most fractures occur when the bone can&acirc;€™t support to a large force or pressure. The bone in the hand serves as the skeleton. This skeleton supports muscles that are making wrists and fingers move. If one of these bones is broken (cracked), it can
hamper human from using hands, wrists and fingers.
Deep learning is the application of artificial neural networks (ANN) to learn the tasks which contains more than one hidden layer. Deep learning is part of wider machine learning based on representation learning, as opposed to taskspecific algorithm. This research developed system with deep learning to classify fractures in the palms of human hands. This research is also used input X-ray images of human bones.
Based on test that has been done, the classification of fractures in this human bone get performance 77.6% with accuracy 0.81, recall 66.8% and precision 92.5% for boxer class, performance 86% with accuracy 0.95, recall 93.4% and 79.6% precision for mallet class and for dislocation class has performance 72% with accuracy of 0.90, 90% recall and 60% precision. ",,,79900,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112587,,HOWARD MARTIN,PREDIKSI NILAI TUKAR DOLAR AMERIKA TERHADAP RUPIAH DENGAN MENGGUNAKAN PENGELOMPOKKAN SINYAL DATA DAN ALGORITMA GENETIKA,,,"prediction, USD/IDR exchange rate, genetic algorithm, linear regression, sliding window, linear combination.   ","Faizal Makhrus, S.Kom, M.Sc, Ph.D.",2,3,0,2017,1,"Pada sistem pertukaran mata uang asing, nilai tukar mata uang selalu berubah secara fluktuatif. Prediksi pergerakan nilai tukar mata uang diperlukan agar dapat mengambil keputusan secara tepat. Dalam penelitian ini, data nilai tukar akan dibagi menjadi beberapa bagian dengan menggunakan sliding window. Kemudian data akan di clustering berdasarkan range koefisien regresi. Algoritma genetika digunakan untuk mencari solusi optimal pada setiap cluster.
	Dalam penelitian ini data input dibagi menjadi data training dan data validasi. Data input merupakan data nilai tukar Dolar Amerika terhadap Rupiah dari tahun 2001 hingga 2015. Data yang digunakan untuk melakukan pengujian merupakan data nilai tukar dengan kurun waktu 1 tahun yaitu dari Januari 2016 hingga Desember 2016. Parameter terbaik yang diperoleh dalam penelitian ini adalah dengan lebar window sebesar 2 dan range gradien sebesar 30. Hasil akurasi yang didapatkan adalah sebesar 99,73067% dan nilai RMSE sebesar 51,33503 rupiah.
","In the foreign currency exchange system, exchange rate is always changing. Prediction of exchange rate is needed to take appropriate decisions. In this research exchange rate data will be divided into several segments using sliding window, then data will be clustered based on its regression coefficient. Genetic Algorithm is used to find the optimal solution for each cluster.
	In this research data is divided into training data and validation data. The data is USD/IDR exchange rate from 2001 to 2015. Test data is USD/IDR exchange rate for the span period of 1 year from January 2016 to December 2016. The best parameter which is resulted by this research for the window size is 2 and the gradient range is 30. The accuracy of prediction that is obtained by this research is 99,73067% and the RMSE value is 51,33503 rupiah.
",,,63419,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116683,,SONIA YUNITA,APLIKASI EMERGENCY BUTTON UNTUK PENGIRIMAN PESAN SINGKAT DARURAT PADA PERANGKAT BERGERAK,,,"SMS, Emergency Button, Mobile Aplication","Anny Kartikasari, M.Sc, Ph.D ",2,3,0,2017,1,"Tidak ada yang dapat menjamin keselamatan diri di jalan. Banyak korban yang tidak selamat pada kecelakaan saat malam hari, ketika korban sulit untuk melihat lokasi di mana korban berada. Beberapa faktor yang membuat korban kecelakaan tidak cepat mendapat pertolongan adalah kurangnya cara untuk meminta bantuan dengan cepat dan mudah.
Dengan menggunakan fungsi SMS (Short Message Service) yang sudah disediakan oleh mobile phone dengan sistem operasi Android, penelitian ini berusaha untuk membuat aplikasi yang akan mengirimkan pesan singkat yang berisi permintaan pertolongan dan informasi lokasi pengguna. Dalam pesan singkat tersebut akan dicantumkan lokasi dalam bentuk informasi Latitude dan Longitude, sehingga penerima pesan dapat mencari keberadaan korban dengan mudah.
Berdasarkan hasil pengujian, telah berhasil dibuat aplikasi dengan nama EmergencySMS sesuai kebutuhan sistem yang dirancang. Aplikasi ini mampu untuk mengirimkan SMS darurat berisikan pesan dan informasi lokasi. Adapun fitur lain yang terdapat pada aplikasi ini adalah Call Police, dan kemampuan untuk mengirim lebih dari satu SMS, dan juga menambatkan pesan kecil untuk dikirim.
","No one can always guarantee their safety on the road. Most of victim can't survive an accident at night. One of the important factor of that is a failure to get help fast enough.  And not easy to get help.
With a text messaging feature that Android has, this research tried to make an application to send a messages which also contain a location information in Longitude and Latitude format. So the receiver of that messages can find the victim fast enough.
Based on the test result, a text message based application has been created from the plan that was made before called EmergencySMS. This application can send a text message containing a danger message and a location information. Another feature that this application has is the ability to call the police like a speed dial, and the chance to put the another number to receive the messages, and also attach a small message.
",2017-08-07 00:00:00,53,67569,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
130763,,SUHARTONO,SISTEM PEMBANDING HARGA DAN INFORMASI PRODUK PADA E-COMMERCE DI INDONESIA DENGAN TEKNIK WEB SCRAPING,,,"Pembanding E-Commerce, Web Scrapping","Drs. Sri Mulyana, M.Kom",2,3,0,2017,1,"Maraknya E-Commerce di Indonesia memicu tumbuhnya situs pembanding harga pada E-Commerce. Umumnya, situs pembanding E-Commerce yang ada menerapkan teknik mining dengan memanfaatkan teknik web data extraction. Kecepatan proses pencarian terhadap sebuah keyword tertentu serta kemudahan dalam pengolahan data dan konten menjadi alasan utama berbagai situs pembanding E-Commerce memanfaatkan teknik web konten mining ini. Namun penerapan teknik seperti ini sebenarnya masih menyisakan beberapa permasalahan. Di antaranya, update data yang dilakukan oleh penjual membutuhkan waktu yang lama untuk dapat dilihat perubahannya pada sistem pembanding.
Penelitian ini bertujuan untuk melakukan pengembangan prototype sistem pembanding situs E-Commerce dengan teknik web scraping yang memiliki kemampuan update data dan harga secara up to date serta mengetahui perbandingan response time nya. Penelitian ini memanfaatkan library Simple HTML DOM Parser untuk mempermudah dalam penyeleksi elemen HTML sumber situs E-Commerce yang digunakan. Pengujian fungsional terhadap sistem ini meliputi pengujian update data, pengujian update harga dan pengujian response time.
Hasilnya sistem ini mampu memberikan update data dan harga secara up to date serta memiliki rata-rata response time terhadap pencarian keyword tertentu yaitu 6.63 s.
","The rise of e-commerce in Indonesia triggers the growth of comparison sites that can compare among e-commerce sites. Generally, existing e-commerce comparison sites apply mining techniques by utilizing web data extraction techniques. The speed of searching for a particular keyword and ease of processing data and content is the main reason e-commerce comparison sites utilize this web content mining technique. However the application of this technique is still leaving some problems. Among them, the update data conducted by the seller takes a long time to be seen changes in the existing comparison system.
	This research is aimed to develop prototype of e-commerce site comparison system with web scraping technique which has updating data and price up to date capability and also know the comparison of  response time. This research used library Simple HTML DOM Parser to simplify the selection of HTML elements of e-commerce site source. Functional testing of this system is updating data test, updating of price test and  response time test.
	The result of this system is able to provide data and price updates in up to date and has an average response time to search for a particular keyword is 6.63 s.
",,,81794,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132556,,RIZKI HADIATURRASYID,Kategorisasi Berita dalam Bot Telegram Feeder Berita,,,"Na&Atilde;&macr;ve Bayes, Support Vector Machine, News Classification, Multinomial Na&Atilde;&macr;ve Bayes, Linear Support Vector Classifier",Bambang Nurcahyo Prastowo,2,3,0,2017,1,"Dalam pengkategorian berita, satu judul berita bisa jadi termasuk dalam lebih dari satu kategori. Sementara sumber berita secara umum tidak selalu memasukkan tiap berita ke semua kategori yang relevan.
Dalam penelitian ini dilakukan pengkategorian judul berita dengan salah satu metode Na&Atilde;&macr;ve Bayes yaitu Multinomial Na&Atilde;&macr;ve Bayes (MultinomialNB), dan Support Vector Machine yaitu Linear Support Vector Classifier (LinearSVC). Lalu berita yang sudah dikategori ditampilkan satu per satu dalam bot Telegram untuk mendapat input dari human judgement mengenai kesesuaian kategori. Berikutnya, masing-masing metode pengkategorian akan dibandingkan tingkat akurasi, presisi, dan recall, dengan input human judgement tadi.
Dari hasil penelitian didapatkan bahwa Multinomial Na&Atilde;&macr;ve Bayes lebih baik digunakan daripada Linear Support Vector Machine dalam pengkategorian berita berdasarkan judul.
","In news categorization, one news title can belong to more than one category. However, not every news source categorized one news in more than one category.
In this experiment, news title samples are categorized with both Na&Atilde;&macr;ve Bayes which is Multinomial Na&Atilde;&macr;ve Bayes, and Support Vector Machine which is Linear Support Vector Classifier. After categorizing, the news titles will be asked to users as human judgement to check whether or not the category is relevant. Next, both methods are compared with the human judgement then accuracy, precision, and recall were calculated.
From this experiment Multinomial Na&Atilde;&macr;ve Bayes was shown to be better than Linear Support Vector Machine to categorize news based on its title.
",,,83533,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
132557,,GILANG MUGIARDI,Perbandingan Metode Seleksi Fitur Pada Klasifikasi Artikel Berita Berbahasa Indonesia Menggunakan Naive Bayes,,,"seleksi fitur, klasifikasi teks, naive bayes, perbandingan","Arif Nurwidyantoro, S.Kom., M.Cs.",2,3,0,2017,1,"Banyaknya fitur yang biasanya muncul pada masalah klasifikasi teks menjadi perhatian utama karena bisa menyebabkan bertambahnya waktu pelatihan. Oleh karena itu, banyak metode seleksi fitur yang dikembangkan untuk memilih fitur-fitur yang ada. Namun demikian, performa dari proses klasifikasi juga akan terpengaruh. Beberapa metode seleksi fitur diajukan baik itu metode seleksi fitur baru atau yang merupakan modifikasi dari metode yang sudah ada dengan tujuan untuk mendapatkan performa terbaik. Beberapa dari metode tersebut diantaranya adalah Distinguishing Feature Selector, Term Significance, Improved Information Gain dan Improved Mutual Information.
Pada penelitian ini dilakukan perbandingan performa dari keempat metode seleksi fitur tersebut. Perbandingan dilakukan dengan menggunakan 3 varian dari Naive Bayes yaitu : Multinomial Naive Bayes, Binarized Multinomial Naive Bayes dan Multivariate Bernoulli Naive Bayes. Data yang digunakan adalah artikel berita berbahasa Indonesia. Pada tahap sebelum pemrosesan, dilakukan pembuangan stop words dan stemming. Pengujian dilakukan untuk mengukur akurasi, Macro-F1, Micro-F1 dan waktu pemrosesan dengan menggunakan metode k-fold cross validation.
Hasil pengujian menunjukkan bahwa metode Improved Mutual Information merupakan metode seleksi fitur dengan pengaruh performa klasifikasi terbaik pada 15 dari 21 kombinasi pengujian. Algoritma Multinomial Naive Bayes merupakan algoritma dengan performa klasifikasi terbaik ketika jumlah fitur yang digunakan lebih dari atau sama dengan 3000. Algoritma Multivariate Bernoulli Naive Bayes merupakan algoritma dengan performa klasifikasi paling stabil terhadap penggunaan jumlah fitur yang berbeda-beda. Metode Improved Information Gain merupakan metode seleksi fitur dengan waktu pemilihan tercepat.","The high number of features that usually appear on text classification problem are one of the main concerns as they can lead to increased training time. Therefore, many feature selection methods being developed to select the available features. However, the performance of the classification process will also be affected. Some feature selection methods are proposed whether it is a completely new feature selection method or that is a modification of an existing one in order to get the best performance. Some of these methods are Distinguishing Feature Selector, Term Significance, Improved Information Gain and Improved Mutual Information.
A performance comparison between the four feature selection methods was done in this research. The comparisons were made by using 3 variants of Naive Bayes : Multinomial Naive Bayes, Binarized Multinomial Naive Bayes and Multivariate Bernoulli Naive Bayes. The data used is news articles in Indonesian language. In the preprocessing stage, the stop words removal and stemming is done. Testing is done to measure accuracy, Macro-F1, Micro-F1 and processing time by using k-fold cross validation method.
The test results show that the Improved Mutual Information method is a feature selection method with the best classification performance effect on 15 out of 21 test combinations. Multinomial Naive Bayes algorithm is the best performing classification algorithm when the number of features used is more than or equal to 3000. Multivariate Bernoulli Naive Bayes algorithm is the most stable classification algorithm against different number of features. The Improved Information Gain method is a feature selection method with the fastest selection time.",,,83548,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112333,,ALFONSUS ANDARU WIDYA SVARA ,IMPLEMENTASI PENGENALAN POLA UNTUK KLASIFIKASI JENIS SUARA PENYANYI DENGAN TINJAUAN AUDIO,,,"music classification, voice, pattern recognition, MFCC, pitch, Hidden Markov Models","Drs. Sri Mulyana, M.Kom.",2,3,0,2017,1,"Suara penyanyi memiliki pola-pola dan karakteristik yang umum yang membuatnya dapat diklasifikasi, meskipun juga memiliki keunikan antarindividu. Pada kasus paduan suara, ada 4 (empat) jenis suara penyanyi yang umum digunakan, yaitu Sopran, Alto, Tenor, dan Bas. Klasifikasi jenis suara umum dilakukan oleh pelatih vokal yang menangani paduan suara tersebut. Namun persepsi individu setiap pelatih bisa berbeda sehingga seorang penyanyi bisa diklasifikasi berbeda pula menurut masing-masing.
Penelitian ini mengembangkan klasifikasi jenis suara dengan menggunakan Hidden Markov Models (HMM) untuk mengenali pola masing-masing jenis suara. HMM dibangun berdasarkan fitur Mel-Frequency Cepstral Coefficients (MFCC) dan pitch yang diekstraksi dari rekaman sampel. Jumlah keadaan juga menjadi variabel membangun HMM. Sampel uji dihitung probabilitasnya terhadap setiap model dan model dengan nilai terbaik menjadi hasil prediksi. 
Pengujian melibatkan anggota Paduan Suara Mahasiswa Universitas Gadjah Mada. Pengujian yang dilakukan terhadap sistem klasifikasi menghasilkan HMM dengan fitur MFCC dan 5 keadaan sebagai model dengan akurasi terbaik yaitu 70% diikuti HMM dengan fitur pitch dan 3 keadaan sebagai dengan akurasi 66,67%. HMM dengan fitur MFCC 5 keadaan kuat dalam mendeteksi jenis suara Tenor sementara HMM dengan fitur pitch 3 keadaan kuat mendeteksi jenis suara Sopran. Kedua jenis konsisten dalam mendeteksi jenis suara Bas, tetapi tidak cukup baik untuk mendeteksi jenis suara Alto.
","A singer's voice has some common characteristics and pattern, though it has unique factors which distinguish a voice with the others. As in the choir which basically has 4 types (Soprano, Alto, Tenor, and Bass), one can classify a singer into a type based on the common characteristics. But the classification, in which is done by voice teachers or choir directors, may differ depending on the classifier's perception.
This work developed a voice type classifier system using Hidden Markov Models (HMM) as the pattern recognition method. MFCC and pitch features extracted from recording samples was used as the feature to build a voice type HMM. The system varies in the number of states used. 
A test involving choir singers of Paduan Suara Mahasiswa Universitas Gadjah Mada (Universitas Gadjah Mada Student Choir) showed 5-state HMM with MFCC features as the best-performed model, followed by 3-state HMM with pitch feature. The MFCC-based HMM worked well on classifying Tenor voices. Meanwhile, pitch-based HMM performed well on classifying Soprano voices. Both have consistent performance on classifying Bass voices and fairly low on classifying Alto voices.
",,,63099,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110798,,SHAULA BALQIS Z,KLASIFIKASI HOST VIRUS AVIAN INFLUENZA SUBTIPE H5N1 DI INDONESIA MENGGUNAKAN METODE JARINGAN SYARAF TIRUAN BACKPROPAGATION,,,"Jaringan Syaraf Tiruan, Backpropagation, H5N1, Host Manusia, Klasifikasi. ","Sri Mulyana, Drs., M. Kom",2,3,0,2017,1,"Virus Avian Influenza (AI) H5N1 telah bersirkulasi di Indonesia sejak kejadian AI pada tahun 2003. Virus ini memiliki kecenderungan mutasi yang cukup tinggi. Penanggulangan virus ini menjadi agenda yang cukup penting bagi pemerintah karena terdapat variasi virus H5N1 yang dapat menginfeksi manusia. Karena virus-virus ini bermutasi dari induk yang sama, maka akan terbentuk kemiripan antar variasi virus hasil mutasi. Ada beberapa karakteristik virus yang dapat ditelusuri, antara lain melalui  RNA (protein). Untuk kasus dimana virus dapat menginfeksi manusia, terdapat substitusi yang khas pada protein M1 dan M2 virus tersebut.
Jaringan syaraf tiruan (JST) backpropagation adalah salah satu metode yang dapat digunakan untuk melakukan klasifikasi data ke dalam kelas-kelas yang berbeda. Metode ini merupakan metode pembelajaran terawasi, dimana data yang akan diklasifikasikan dipelajari terlebih dahulu untuk kemudian diklasifikasikan ke dalam kelas-kelas yang sudah ditentukan. Terdapat parameter penting dalam membangun JST yang optimal, yaitu pemilihan laju pembelajaran dan arsitektur JST. Pada penelitian ini, digunakan metode backpropagation dengan 2 lapisan tersembunyi untuk melakukan klasifikasi host dari variasi virus H5N1 protein M1, M2 di Indonesia. Hasil klasifikasi kemudian dimanfaatkan untuk mengukur performa JST yang sudah dibangun.
Hasil klasifikasi host virus H5N1 protein M1, M2 di Indonesia menggunakan metode backpropagation dapat mencapai akurasi sebesar 89% dengan tingkat kesalahan pelatihan sebesar 0.05192925.
","H5N1 Avian Influenza (AI) virus has circulated in Indonesia since AI event in 2003. This virus has quite high mutation inclination. Tackling this virus becomes an important agenda for the government because there are variations of H5N1 virus which can infect human. Because these viruses mutate from the same prime, thus similarity can be formed among the variation of viruses resulted from the mutation. There are some virus's characteristics which can be traced, one of them is through RNA (protein). For the case where the virus can infect human, there is a specific subtitution in protein M1 and M2 from that virus. 
Backpropagation artificial neural network (ANN) is one of the method which can be used for classifying data into different classes. This method is a supervised learning method, where the data will be learned before being classified into determined classes. There are some important parameters in building an optimum ANN, which are learning rate and ANN's architecture. In this research, backpropagation method with 2 hidden layer is used to classify the host of H5N1 virus protein M1, M2 variation in Indonesia. Then, the result of the classification will be used to measure the performance of the already built ANN.  
The classification result of H5N1 virus protein M1, M2's host in Indonesia using backpropagation method can achieve 89% accuracy with the training error as much as 0.05192925.
",,,61526,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110800,,ALFRIZAL FAKHRI FEBR,PERBANDINGAN KINERJA CPU DAN GPU DALAM PENGHITUNGAN PERKALIAN MATRIKS,,,"pemrosesan paralel, CPU, GPU","Ahmad Ashari, Drs., M.I.Kom., Dr.Techn.",2,3,0,2017,1,"CPU mampu menyelesaikan pekerjaan semakin cepat seiring dengan
berkembangnya teknologi, disamping itu muncul multi-core CPU yang menambah
kapabilitas CPU dalam melakukan lebih dari satu pekerjaan dalam waktu
bersamaan dan GPU untuk membantu percepatan proses CPU penghitungan untuk
kasus tertentu.
Pada penelitian ini diteliti kemampuan CPU multi-core dalam melakukan
pemrosesan paralel. Platform yang diuji adalah CPU non-paralel, CPU paralel
dengan OpenMP, dan GPU dengan OpenCL. Terdapat dua skenario penelitian,
skenario pertama adalah perkalian dua matriks berdimensi 1000 kali 1000, skenario
kedua adalah perkalian dua matriks berdimensi 2000 kali 2000.
Hasil dari penelitian menunjukkan bahwa implementasi algoritma divide
and conquer menggunakan GPU dengan OpenCL dapat menyelesaikan tugas
dengan lebih cepat dibanding CPU dengan threads C++ atau OpenMP. Selain itu,
implementasi algoritma divide and conquer tidak membuat penghitungan lebih
cepat apabila hanya tersedia 1 core yang melakukan proses komputasi.","CPU is able to finish it's job faster as technology evolves, other than that,
the emergence of multi-core CPU increases CPU's capability in processing more
than one job at a time and also the emergence of GPU that helps accelerating CPU's
compute process for certain cases.
This study investigates the performance of multi-core processor in parallel
processing. The investigated platforms are non-parallel CPU, parallel CPU with
OpenMP, and GPU with OpenCL. The investigation consists of two main scenarios,
the first is multiplication of two matrices with 1000 times 1000 dimension, the
second is multiplication of two matrices with 2000 times 2000 dimensionss.
The result shows that implementation of divide and conquer algorithm on
GPU using OpenCL is able to finish the job faster than CPU using C++ threads or
OpenMP. Other than that, when the divide and conquer algorithm does not give
performance benefit when implemented on single core system where only one core
computing the process.",2017-04-13 00:00:00,53,61514,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112855,,FIRDAUZ FANANI,KLASIFIKASI REVIEW SOFTWARE PADA GOOGLE PLAY MENGGUNAKAN PENDEKATAN ANALISIS SENTIMEN,,,"Klasifikasi Review, HAM, SPAM, Preprocessing, Support Vector Machine(SVM), Naive Bayes Classifier(NBC).","Isna Alfi Bustoni S.T., M.Eng.",2,3,0,2017,1,"Review pada google play yang merupakan salah satu fitur yang digunakan untuk memberikan suatu penilaian terhadap suatu aplikasi. Namun terdapat penyalahgunaan oleh pihak-pihak tertentu untuk mengambil keuntungan atau merugikan pihak lain dengan membuat review SPAM. Review SPAM dapat merugikan para pengguna dan mengganggu para developer yang menggunakan review tersebut sebagai bahan dalam melakukan evaluasi terhadap aplikasinya. Untuk itu dibutuhkan sistem yang dapat mengklasifikasikan review-review tersebut sebagai review SPAM atau bukan SPAM(HAM). 
Dalam penelitian ini diterapkan tahap-tahap preprocessing yang akan mengolah data-data mentah dari suatu review menjadi data yang siap diolah dan juga dibuat sebuah model yang merepresentasikan data-data yang sudah dilakukan preprocessing menjadi matriks lalu dilakukan pembuatan vektor kata-kata pada review tersebut. Algoritma yang digunakan dalam klasifikasi ini adalah algoritma Support Vector Machine(SVM) dan Naive Bayes Classifier(NBC). Pengujian akhir sistem dilakukan dengan menguji dan membandingkan performa dari algoritma Support Vector Machine dan Naive Bayes Classifier.
Hasil akhir dari penelitian ini menunjukkan performa sistem klasifikasi dengan algoritma Support Vector Machine menghasilkan akurasi lebih baik dibandingkan dengan menggunakan algoritma Naive Bayes Classifier.","Review on google play which is one of the features used to provide an assessment of an application. However there is a misuse by certain parties to take advantage or harm others by making SPAM reviews. SPAM reviews may harm users and disrupt developers who use that reviews as a material in evaluating their applications. This requires a system that can classify these reviews as SPAM or non-SPAM (HAM) reviews.
In this study applied preprocessing stages that will process raw data from reviews into ready-to-process data and also created a model that represents data that has been done preprocessing into a matrix and then make vector of words on the reviews. The algorithms used in this classification are the Support Vector Machine (SVM) and Naive Bayes Classifier (NBC) algorithms. The final test of the system is done by testing and comparing the performance of the Support Vector Machine and Naive Bayes Classifier algorithms.
The final result of this research shows the performance of classification system with Support Vector Machine algorithm yield accuracy better than using Naive Bayes Classifier algorithm.",,,63626,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114396,,PRIYANGGA JANMANTARA  A.,PERBANDINGAN KINERJA OPENCL DAN CUDA UNTUK KASUS ALGORITMA MERGE SORT,,,"paralelisasi, GPGPU, CUDA, OpenCL, merge sort","Dr.Techn. Ahmad Ashari, M.Kom.",2,3,0,2017,1,"Pemrosesan paralel bertujuan untuk menyelesaikan masalah dengan waktu yang secepat mungkin. Pemrosesan paralel dapat dilakukan dengan multi-core, GPGPU atau gabungan dari keduanya. Tugas akhir ini membahas tentang perbandingan kinerja dua sistem paralel yang menggunakan GPU. Sistem paralel yang dibandingan berupa API, yaitu CUDA dan OpenCL. Kasus yang digunakan untuk penelitian yaitu algoritma merge sort, karena algoritma tersebut sangat stabil dan mudah untuk diimplementasikan terkhusus untuk paralelisme. Konten data yang digunakan antara lain: semi-terurut, acak, dan terbalik-terurut. Parameter yang diuji adalah waktu eksekusi dari kernel, aplikasi, dan waktu transfer data dari memori host kedalam memori device dan sebaliknya. Dari semua skema pengujian yang telah dilakukan pada penelitian ini menunjukkan bahwa OpenCL memberikan waktu eksekusi yang lebih cepat dibandingkan CUDA.","Parallel processing aims to solve problem as fast as possible. Parallel processing can be done with multicore, GPU or the combination of both. This thesis discuss performance comparison between two parallel systems that use GPU.   This two parallel systems are API, such as CUDA and OpenCL. Merge sort algorithm is used for this experiment, because this algorithm very stable and easy to implement especially for parallelism. Data content tha are used such as: semi-sorted, random, and reversed-sorted. The tested parameters are the execution time of the kernel, application, and the time of data transfer from the host memory into device memory and vice versa. Of all the test schemes that have been done in this study shows that OpenCL provides a faster execution time than CUDA. ",,,65152,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114397,,DANNY STEVEN,IMPLEMENTASI BULKLOAD UNTUK DATABASE HBASE PADA LINGKUNGAN HDFS,,,"HBase, BulkLoad, core, memory, killed map task","Mardhani Riasetiawan, M.T., Dr.",2,3,0,2017,1,"Saat ini, dengan perkembangan teknologi seperti Mobile Data yang semakin pesat dan data yang dihasilkan semakin banyak jumlah dan ragamnya, akibatnya Relational Database Management System (RDBMS) tidak mampu mengkontrol data yang terkumpul.  Salah satu solusi alternatif adalah menggunakan Not Only SQL (NoSQL) database seperti HBase.  Salah satu fitur spesial yang dimiliki HBase adalah BulkLoad.  BulkLoad adalah fitur HBase untuk menyimpan data dalam jumlah besar sekaligus dan tidak harus memiliki bentuk tabel yang terstruktur dan kaku seperti Relational Database Management System (RDBMS).
Penelitian ini ditujukan untuk mengeksplorasi penggunaan BulkLoad pada HBase dari sisi durasi waktu yang diperlukan untuk menyimpan data.  Untuk menjalankan program BulkLoad, yang pertama dilakukan adalah mengumpulkan sampel data.  Sampel data yang digunakan adalah dokumen XML.  Setiap dokumen XML akan di preprocessing menjadi input splits sebanyak 150 dan akan disimpan kedalam Hadoop Distributed File System (HDFS) sebagai dataset dengan nama batch.  Batch sebanyak 14 akan terdiri dari input splits dengan jumlah yang berbeda setiap batch.  Selanjutnya, program BulkLoad akan dijalankan untuk setiap batch dan masing-masing batch akan dijalankan sebanyak 3 kali pengulangan dengan menggunakan core yang berbeda setiap kali dijalankan.  Core yang digunakan pada penelitian ini sebanyak 2,3 dan 4.
Program BulkLoad memberikan hasil yang berbeda setiap kali program dijalankan.  Hasil paling jelas terlihat pada batch 14 yang memiliki input splits paling banyak, dimana durasi waktu pada 2-core sebesar 13 menit, jauh lebih lama dibandingkan 3-core dan 4-core dengan durasi waktu 9 menit.  Hasil penelitian juga menunjukkan 3-core dan 4-core tidak berbeda jauh dan 4-core memberikan hasil yang lebih buruk dibandingkan 3-core, seperti pada batch 4 dan batch 11 dimana durasi waktu 3-core lebih cepat 1 menit dibandingkan 4-core.  Hal ini terjadi karena durasi waktu dipengaruhi oleh error berupa killed map task yang menunjukkan berapa kali program BulkLoad gagal membaca input splits.  Killed map task pada 2-core sebesar 18, 3-core sebesar 13 dan 4-core sebesar 15.
","The rapid development of technology, such as Mobile Data technology resulting in increase of volume and variety of data.  This increase will likely continue and limit the usage of Relational Database Management System (RDBMS).  Therefore, it is a necessity to find an alternative database such as Not Only SQL (NoSQL) database like HBase.  One of the special feature of HBase is BulkLoad.  BulkLoad is a method prepared by HBase to move large amount data to HBase database at the same time without the need to create the table in a rigid form like on Relational Database Management System (RDBMS).
This research intend to observe the usage of BulkLoad in HBase using time measurement in minutes to see the speed of BulkLoad. Firstly, data sample is collected in the form XML document.  After that, every XML document enter preprocessing phase to create input splits as much as 150.  All input splits then moved to Hadoop Distributed File System (HDFS) as batch.  Each Batch have different amount of input splits and batch is created up to 14 batch.  Finally, BulkLoad run on each batch for 3 times, each times using different core.  This research used 3 different numbers of core, which is 2-core, 3-core and 4-core.
The result showed different elapsed time for every BulkLoad that is ran on each batch.  The most significant result showed that in batch 14, which has the largest amount of input splits, BulkLoad need 13 minutes to finish its job at 2-core, slower than 3-core and 4-core which need only 9 minutes.  Next, experiment also showed that 3-core and 4-core did not have much difference in elapsed time and 4-core shows result worse than 3-core.  On batch 11, BulkLoad in 3-core showed elapsed time 1 minute faster than 4-core.  The elapsed time difference in BulkLoad execution is influenced by the number of errors in killed map task.  Killed map task recorded the numbers of how much BulkLoad failed to read the input splits.  Killed map task in 2-core is 18 errors, 3-core is 13 errors and 4-core is 15 errors.
",,,65154,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110563,,FIRLY ARMANDA,EKSTRAKSI METADATA BERITA ONLINE BERBAHASA INDONESIA MENGGUNAKAN TEXT DETECTION FRAMEWORK,,,"ekstraksi informasi, ekstraksi metadata, text detection framework","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2017,1,"Media web berita mengandung sumber teks yang menyediakan informasi yang luas dan kaya untuk aplikasi penggalian teks salah satunya adalah ekstraksi informasi. Tetapi konten utama dari berita biasanya dikelilingi berbagai konten yang tidak berelasi dengan topik seperti iklan, spanduk dan tautan navigasi. Kemampuan untuk memilah dan memilih informasi tetapi tetap dapat menjaga keutuhan informasi yang penting akan berguna untuk membangun teks corpora yang bersih dan berpotensi meningkatkan sistem pencarian teks.
Pada penelitian ini dilakukan sebuah pendekatan untuk ekstraksi metadata berita online berbahasa Indonesia dengan judul berita, tanggal rilis dan konten berita sebagai target ekstraksi, dengan menggunakan metode text detection framework. Metode ini menghitung nilai Compound text-tag difference (CTTD) yang dijadikan sebagai acuan untuk mengekstrak metadata berita. Data set yang digunakan berjumlah 102 halaman HTML yang berasal dari 21 kanal berita yang tersebar pada 10 portal berita. Berdasarkan hasil pengujian didaptkan nilai presisi sebesar 94.5%, recall sebesar 99.3% dan f-measure sebesar 96.6%.
","Web news media contains a huge amount of text sources that provide wide coverage and rich text information for many text mining application such as information extraction. However the main text in web news is usually surrounded by various unrelated content, such as commercial ads, banners, link navigation and user comments. The ability to filter noisy information while preserving the important information is useful for constructing clean text corpora and potentially improving text retrieval system.
In this research was conducted on approach for metadata extraction of Indonesian online news, with news title, release date and the main content as targets, using a method based on text detection. This proposed method calculates the value of Compound Text-Tag Differences (CTTD) which is used as a reference for news metadata extracting. Dataset are used as much as 102 HTML documents which came from 21 kanal news which spread from 10 online news sites. Based on the evaluation results obtained from precision is 94.5%, recall is 99.3% and f-measure is 96.6%.
",,,61275,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
114659,,DIMAS YUNIAR SYUHADA,EKSTRAKSI ATURAN JARINGAN SYARAF TIRUAN BACKPROPAGATION MENGGUNAKAN ALGORITMA C4.5,,," jaringan syaraf tiruan, backpropagation, C4.5, ekstraksi aturan","Anifuddin Azis, S.Si., M.Kom",2,3,0,2017,1,"Jaringan syaraf tiruan backpropagation mudah diimplementasikan dan memiliki akurasi yang tinggi, namun pengetahuan yang didapat sulit dimengerti karena berupa black box. Untuk memecahkan masalah ini, akan dicoba dengan menggunakan algoritma C4.5.
Algoritma C4.5 adalah satu metode pembentukan pohon keputusan. Salah satu fitur dari C4.5 adalah mengubah pohon keputusan menjadi aturan. Dengan munggunakan C4.5, aturan dapat diekstrak dari jaringan syaraf backpropagation.
Data dilatih dengan backpropagation, yang menghasilkan classifier yang digunakan untuk membentuk data dengan kelas yang baru. Data baru hasil dari klasifikasi jaringan syaraf dilatih dengan C4.5 yang akan mengekstrak aturan dari jaringan syaraf.  Aturan yang dihasilkan berupa aturan IF-THEN.
Rata-rata jumlah aturan yang diekstrak dari jaringan syaraf lebih sedikit dibandingkan dengan rata-rata jumlah aturan yang dihasilkan algoritma C4.5 dan akurasinya bertambah.","Backpropagation artificial neural networks are easy to implement and have high accuracy, but the learned knowledge is hard to understand because it's a black box. To solve this problem, C4.5 algorithm will be used.
	 C4.5 is an algorithm to generate a decision tree. One of its feature is converting decision tree to rule. Using C4.5, a rule will be extracted from backpropagation neural networks. 
Data is trained using backpropagation, the resulted classifier is used to create new data with new classes. The new data created from neural network classification is trained using C4.5 that will extract the rule from neural networks. The generated rules are IF-THEN rules.
The average number of generated rules from neural network  less than rules generated from C4.5 algorithm and the accuracy is increased.",,,65382,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129507,,ILYASA MICCO H,USER TRANSACTION PROFILING PADA PROSES AUDIT TRAIL BUSWAY DENGAN MAPREDUCE,,,"penerapan big data, big data framework, spark, bus rapid transit, pemrosesan data bisnis, komputasi terdistribusi, transportasi jalan","Dr. Nur Rokhman, S.Si., M.Kom.",2,3,0,2017,1,"Seiring berkembangnya jaman, kebutuhan akan sistem transportasi umum yang cepat dan murah menjadi prioritas bagi masyarakat. Bus Rapid Transit (BRT) merupakan konsep sistem transportasi umum yang sedang berkembang di Daerah Khusus Ibukota Jakarta (DKI Jakarta). Salah satu pengembangan yang sedang dilakukan adalah pengembangan sistem pengolahan data transaksi sehingga dapat digunakan untuk mendukung proses bisnis yang ada. Dalam penelitian ini dikembangkan suatu workflow yang memungkinkan untuk melakukan proses agregasi data transaksi yang berjumlah 164.027.463 baris data dan memvisualisasikannya ke dalam bentuk grafik.
Pada penelitian dan pengujian ini dilakukan proses agregasi menggunakan bahasa pemrograman python yang mengakses basis data lokal file Comma Separated Values (CSV) yang tersimpan pada laptop peneliti dan jaringan laboratorium Sistem Keamanan dan Jaringan (SKJ) FMIPA UGM. Algoritma yang dipakai dalam penelitian adalah user transaction profiling menggunakan mapreduce berdasarkan parameter-parameter tertentu. Parameter dalam penelitian adalah jumlah transaksi dan nilai transaksi berdasarkan gate transaksi terjadi, kartu transaksi yang digunakan, tanggal transaksi terjadi, dan jenis transaksi apakah single trip (cash) atau prabayar. Metode yang dipakai dalam pengujian adalah pengujian fungsional (black box). Penelitian ini menggunakan contoh kasus nyata dalam analisa big data, data transaksi BRT didapatkan dari Direktorat Sistem dan Sumber Daya Informasi (DSSDI) UGM, infrastruktur analisa pada penelitian ini menggunakan 9 node cluster hadoop pada Next Unit on Computing (NUC) dimana dilakukan eksperimen perbandingan processing time setiap node dengan jumlah data yang berbeda. File output penelitian berupa file CSV dan visualisasi informasi dalam bentuk file HTML.
Hasil akhir berupa visualisasi informasi dapat diketahui pola nominal transaksi dan pola jumlah transaksi yang terjadi berdasarkan tiap gate, tiap kartu, tiap hari, dan menentukan apakah transaksi yang terjadi merupakan transaksi single trip (cash) atau transaksi prabayar. Dari informasi tersebut dapat dijadikan analisis pendukung keputusan bisnis penyedia layanan BRT agar dapat memperbaiki dan meningkatkan performa layanan BRT untuk masyarakat. Selain itu mengetahui perbandingan processing time setiap node dengan jumlah data yang berbeda.","As the times progressed, the need for a fast and cheap public transportation system became a priority for the community. Bus Rapid Transit (BRT) is a fast-growing transportation concept in Daerah Khusus Ibukota Jakarta (DKI Jakarta). One of the developments being done is a data transaction management system that can be used to support the existing business process. Through this research, a workflow that allows aggregation process of transaction data which amounted 164,027,43 and visualized it into chart will be developed. 
In this research and testing, would do the aggregation process using Python programming language that directly accesses CSV file local database stored in researcher&acirc;€™s laptop and in system in laboratory of SKJ FMIPA UGM. Algorithm used in this research is user transaction profiling using mapreduce based on certain parameters. The parameters in the research are the number of transactions and the transaction value based on the transaction gate occurs, the transaction card used, the transaction date occurs, and the type of transaction whether single trip (cash) or prepaid. The method used in testing is functional testing (black box). This research uses real case example in big data analysis, BRT transaction data obtained from DSSDI UGM, analysis infrastructure in this research using 9 node cluster hadoop on Next Unit on Computing (NUC) where the experimental comparison of processing time of each node with different amount of data. The output research file is a CSV file and visualizes the information in the form of an HTML file.
The final result of the visualization of information can be known the nominal pattern of transactions and the pattern of the number of transactions that occur based on each gate, each card, every day, and determine whether the transaction is a single transaction (cash) or prepaid transaction. From this information, it can be used as an analysis of business decision support BRT service providers in order to improve and improve the performance of BRT services to the community. Also to find out the comparison processing time of each node with different amount of data.",,,80405,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
158948,,M.RENALDY,PERBANDINGAN KINERJA PORTER STEMMER DAN NAZIEF-ADRIANI STEMMER PADA KLASIFIKASI TEKS BERITA BERBAHASA INDONESIA MENGGUNAKAN METODE MULTINOMIAL NA&Atilde;VE BAYES,,,"klasifikasi teks, text mining, multinomial Na&Atilde;&macr;ve Nayes, stemming bahasa Indonesia, Nazief-Adriani Stemmer, Porter Stemmer untuk bahasa Indonesia/text classification, text mining, multinomial Na&Atilde;&macr;ve Nayes, bahasa Indonesia steeming","I Gede Mujiyatna, S.Kom, M.Kom",2,3,0,2017,1,"Meningkatnya pengguna internet dalam mengakses berita online khususnya berita ekonomi, mendorong Trisniantari D. (2016) untuk melakukan penelitian mengenai klasifikasi teks berita ekonomi. Metode klasifikasi yang digunakan dalam penelitian tersebut adalah Multinomial Na&Atilde;&macr;ve Bayes dan metode untuk stemming adalah Nazief-Adriani Stemmer. Dan dalam penelitian tersebut diperoleh rata-rata processing time untuk melakuan preprocessing berita cukup lama yaitu 7.5182 detik untuk satu teks berita ekonomi. Maka dalam penelitian kali ini akan dilakukan perbandingan processing time tahap preprocessing dan akurasi klasifikasi dengan tiga metode stemming, Nazief-Adriani Stemmer, Porter Stemmer, dan Porter Stemmer dengan tambahan kamus, yang diterapkan pada tahap preprocessing. Processing time pada tahap preprocessing didapatkan dengan menghitung selisih waktu saat preprocessing dimulai dan waktu saat preprocessing selesai. Sedangkan akurasi klasifikasi teks berita ekonomi didapatkan dengan menggunakan metode 10-Fold Cross Validation.
Pada penelitian kali ini didapatkan hasil bahwa Porter Stemmer Tala (2013) tanpa kamus memiliki kecepatan preprocessing paling cepat dengan waktu rata-rata 0.0044 detik untuk setiap teks berita ekonomi pada data training dan waktu rata-rata 0.0044 detik untuk setiap teks berita ekonomi pada data test namun akurasi klasifikasi yang dihasilkan paling rendah yaitu 73.31%. Sedangkan Nazief-Adriani memiliki kecepatan preprocessing paling lambat dengan waktu rata-rata 2.1048 untuk setiap teks berita ekonomi pada data training dan waktu rata-rata 1,8853 detik untuk setiap teks berita ekonomi pada data test namun rata-rata akurasi klasifikasi yang dihasilkan paling tinggi yaitu 74,45%. Dan untuk Porter Stemmer Tala (2013) dengan kamus rata-rata akurasi klasifikasi yang didapatkan adalah 74.12% dengan waktu  rata-rata preprocessing 1.6735 detik untuk setiap teks berita ekonomi pada data training dan waktu rata-rata 1.7 detik untuk setiap teks berita ekonomi pada data test.","The growing of internet users in accessing online news especially economic news, encouraging Trisniantari D. (2016) to conduct research on the classification of economic news text. The classification method used in the research is Multinomial Na&Atilde;&macr;ve Bayes and the method for stemming is Nazief-Adriani Stemmer. And in the research, the average processing time for preprocessing the news is quite long, about 7,5182 seconds for one text of economic news. So in this research will be done comparative processing time for preprocessing stage and classification accuracy with three methods stemming, Nazief-Adriani Stemmer, Porter Stemmer, and Porter Stemmer with additional dictionary, which is applied at the preprocessing stage. Processing time at the preprocessing stage is obtained by calculating the time difference when preprocessing begins and the time when preprocessing is complete. While the accuracy of text classification of economic news is obtained by using the 10-Fold Cross Validation method.
In this study we found that the Porter Stemmer Tala (2013) without a dictionary has the fastest preprocessing speed with an average time of 0.0044 seconds for each text of economic news on training data and an average time of 0.0044 seconds for each text of economic news on test data but the accuracy of the resulting classification is the lowest at 73.31%. While Nazief-Adriani has the slowest preprocessing speed with an average time of 2.1048 for each text of economic news on training data and an average time of 1.8853 seconds for each text of economic news on the data test but the accuracy of the resulting classification is the highest at 74.45%. And for Porter Stemmer Tala (2013) with additional dictionary classification accuracy obtained is 74.12% with an average preprocessing time of 1.6735 seconds for each text of economic news on training data and an average time of 1.7 seconds for each text of economic news on the data test.",,,109931,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110564,,RININTA VITA ROSANTI,ALGORITMA NAIVE BAYES DAN COMPLEMENTARY NAIVE BAYES  PADA HADOOP FRAMEWORK UNTUK ANALISIS SENTIMEN FILM,,,"analisis sentimen, klasifikasi, machine learning, naive bayes, complementary naive bayes, apache hadoop, apache mahout ","Dr. Nur Rokhman, S.Si., M.Kom ",2,3,0,2017,1,"Saat ini, media sosial menjadi alat komunikasi yang sangat populer diantara pengguna internet. Setiap harinya, jutaan pengguna internet membagikan pemikiran dan opininya terhadap topik tertentu, salah satunya adalah opini mengenai film yang sedang populer. Dari opini dan ulasan film tersebut dapat dilakukan analisis sentimen apakah suatu opini termasuk ke dalam sentimen positif, negatif, atau netral.
Pendekatan analisis sentimen dapat dilakukan dengan menggunakan machine learning  dan algoritma klasifikasi. Algoritma Naive Bayes dan Complementary Naive Bayes merupakan metode klasifikasi yang menggunakan perhitungan probabilitas. 
Pada penelitian ini dilakukan implementasi algoritma Naive Bayes dan algoritma Complementary Naive Bayes, untuk melakukan analisis sentimen dan klasifikasi data tweet film berbahasa Inggris. Algoritma ini akan dijalankan di atas Apache Hadoop Framework dan menggunakan Apache Mahout sebagai machine learning library. 
Hasil dari penelitian ini menunjukan bahwa Algoritma Complementary Naive Bayes memiliki performa yang lebih baik dalam melakukan klasifikasi pada data yang tidak seimbang, dengan akurasi 2,57% lebih tinggi, precision 1,92% lebih tinggi, recall 2,57% lebih tinggi, dan F-measure 2,03% lebih tinggi jika dibandingkan dengan algoritma Naive Bayes.","Social media today has become a very popular communication tool among internet users. Millions of users share opinions on particular topic everyday, for example opinion about the latest popular movie. From those opinions and movie review, sentiment analysis can be done to identify whether the opinion expressed in a text is positive, negative, or neutral.
Sentiment analysis approach is often done by machine learning using the classification algorithm. Naive Bayes and Complementary Naive Bayes algorithms are classification algorithms that uses a probability calculation.
In this research, the implementation of Naive Bayes and Complementary Naive Bayes algorithms are used to perform sentiment analysis and data classification on english movie tweet. Those algorithms will be run on top of Apache Hadoop Framework and Apache Mahout as machine learning library.
The result from this research conclude that Complementary Naive Bayes algorithm showed better performance than Naive Bayes algorithm within the classification sentiment on skewed data bias, with 2.57% higher accuracy, 1.92% higher precision, 2.57% higher recall, and 2.03% higher F-measure when compared with Naive Bayes algorithm.",,,61269,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
127972,,RIZKY NURFITHA,SISTEM PAKAR DENGAN MENGGUNAKAN METODE CERTAINTY FACTOR UNTUK MENDIAGNOSIS PENYAKIT GINJAL DAN INFEKSI SALURAN KEMIH,,,"sistem pakar, certainty factor, ginjal, saluran kemih. ","Retantyo Wardoyo, Drs., M.Sc., Ph.D.",2,3,0,2017,1,"Masalah kesehatan dapat ditimbulkan akibat dari gaya hidup yang tidak sehat, seperti timbulnya penyakit yang berhubungan dengan ginjal dan saluran kemih. Seiring dengan semakin banyaknya penderita penyakit ginjal dan infeksi saluran kemih dan terbatasnya jumlah pakar yang ada, dibutuhkan sebuah media yaitu sistem pakar untuk membantu melakukan diagnosis awal penyakit.
Sistem pakar dapat digunakan untuk menghasilkan sebuah kesimpulan yang menggunakan pengetahuan dari seorang pakar. Sistem yang dibangun dalam penelitian ini adalah sistem pakar dengan menggunakan metode certainty factor untuk mendiagnosis penyakit ginjal dan infeksi saluran kemih. Metode certainty factor digunakan untuk memberi nilai kepastian terhadap pengetahuan yang tidak memiliki nilai ukur. Terdapat dua pengguna yang dapat menggunakan sistem ini, yaitu pakar dan paramedis. Pakar dapat melihat dan mengelola pengetahuan yang ada di dalam sistem. Sedangkan paramedis dapat menggunakan fasilitas konsultasi penyakit yang ada di dalam sistem.
Data input berupa gejala yang dialami oleh pasien beserta nilai certainty factor dari setiap gejala. Berdasarkan input yang dimasukkan ke dalam sistem, dihasilkan output berupa hasil diagnosis penyakit ginjal dan infeksi saluran kemih. Berapapun input yang dimasukkan oleh paramedis akan menghasilkan output sebuah kesimpulan. Output yang dihasilkan dapat digunakan untuk membantu mendiagnosis penyakit yang diderita oleh pasien. Output hasil diagnosis penyakit terdiri dari nama penyakit, keterangan penyakit, saran penyakit yang dapat dilakukan oleh pasien selanjutnya, serta tingkat kepastian pasien mengalami penyakit tersebut.
","Various kinds of disease can caused by unhealthy lifestyle,  like kidney disease and urinary tract infection. Along with threatening increasement of kidney disease and urinary tract infection patients also a limited number of expert, an expert system is needed to  help making early diagnosing.
The system that built on this research is an expert system with certainty factor method for diagnosing kidney disease and urinary tract infection. The certainty factor method used to provide assurance against knowledge that has no measuring value. There are two users that can use this expert system, experts and paramedics. An expert can see and modify knowledge base that used for inference process. Paramedics can use  disease consultation facility based on patient's symptomps.
The input data are  the symptomps that suffered by the patient with certainty factor value for each symptomps. Based on the input data, the output will be generated as diagnosis of kidney disease and urinary tract infection. Regardless of the input entered by paramedics will produce the output of a conclusion. Diagnosis of disease that produced after the system is tested is in accordance with the tests perfomed by the expert. Output data shown are disease name, explanation of the disease,  suggestions for the disease, and certainty factor value for the disease.
",,,78805,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129508,,ANDHIKA KURNIA H,PENERAPAN METODE CLUSTERING PADA SPARK DENGAN STUDI KASUS DATA BUS RAPID TRANSIT,,,"Clustering, Bus Rapid Transit, BigData, Spark","Dr. Nur Rokhman S.Kom, M.Kom",2,3,0,2017,1,"Dunia teknologi semakin cepat berkembang di era modern ini. Salah satu permasalahan yang dihadapi pada sistem Bus Rapid Transit (BRT) adalah pengolahan data penumpang yang terus bertambah sesuai dengan pertumbuhan jumlah dan tingkat keramaian penduduk di suatu kota. Data yang semakin bertambah akan sulit dikelola dengan sistem Relational Database Management System (RDBMS) karena pada proses pengolahan data membutuhkan waktu yang lebih lama dan akan semakin berkurang efisiensinya. Sehingga pemrosesan paralel dapat menjadi sebuah solusi dalam mengolah data serta meningkatkan waktu pemrosesan yang lebih cepat. Framework Hadoop digunakan untuk mendukung environment pemrosesan paralel sedangkan untuk proses menganalisis data digunakan platform Spark. Kelebihan yang dimiliki dari Spark yaitu memiliki kemampuan proses yang lebih cepat karena memanfaatkan proses di memory dan sering digunakan untuk analisis BigData. 
Analisis BigData yang dilakukan pada penelitian ini yaitu analisis data dengan menggunakan metode clustering sebagai tahapan dalam data mining. Metode clustering merupakan teknik pengambilan data atau informasi yang mirip di suatu kelompok atau klaster dan data yang tidak sama dengan kelompok lainnya. 
Pengujian Sum of Square Error dan Silhouette coefficient dilakukan pada dua algoritme K-Means dan Scalable K-Means++ dalam melakukan pengelompokan jumlah penumpang terhadap waktu tertentu. Hasil pengujian Sum of Square Error dan Silhouette coefficient menghasilkan k klaster yang optimal yaitu pada jumlah 8 klaster. Kemudian untuk pengujian Silhouette coefficient pada Scalable K-Means++ lebih besar yaitu 0.57380692 dibandingkan dengan hasil Silhouette coefficient dari K-Means yaitu 0.57105644. Sehingga dari hasil tersebut dapat disimpulkan bahwa algoritme Scalable K-Means lebih baik dibandingkan algoritme K-Means.","The world of technology is growing rapidly in this modern era. One of the problems encountered in the Bus Rapid Transit (BRT) system is the growing data processing of passengers in accordance with the growing number and the level of crowds in a city. Increasing data will be difficult to manage with Relational Database Management System (RDBMS) system because data processing takes longer time and will decrease its efficiency. So parallel processing can be a solution in processing the data and increasing the processing time faster. The Hadoop framework is used to support parallel processing environments while for analyzing data Spark platforms are used. The advantages possessed of Spark is that it has a faster processing capability because it utilizes the process in memory and is often used for BigData analysis.
BigData analysis conducted in this research is data analysis by using clustering method as stages in data mining. Clustering method is a technique of data retrieval or similar information in a group or cluster and data that is not the same as other groups.
The Sum of Square Error and Silhouette coefficient evaluation were performed on two K-Means and Scalable K-Means ++ algorithms in grouping the number of passengers over a certain time. Result of testing of Sum of Square Error and Silhouette coefficient produce optimal cluster k which is at number 8 cluster. Then for testing of Silhouette coefficient on Scalable K-Means ++ is bigger that is 0.57380692 compared with result of Silhouette coefficient from K-Means that is 0.57105644. So from the results can be concluded that the algorithm Scalable K-Means better than K-Means algorithm.",,,80410,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112871,,ALI HAFNI,IMPLEMENTASI SISTEM HOME AUTOMATION BERBASIS RASPBERRY PI DAN ANDROID,,,"Home Automation, Raspberry Pi, Android, Flask, Kivy","Ahmad Ashari, Drs., M.I.Kom., Dr.Techn",2,3,0,2017,1,"Raspberry Pi dengan berbagai teknologi pemanfaatannya, dibantu dengan berbagai perangkat lunak dan bahasa pemrograman berbasis open source dapat dimanfaatkan untuk pengembangan sistem home automation.  Android sebagai platform perangkat mobile yang sudah umum untuk pengembangan berbagai aplikasi dapat digunakan sebagai platform antarmuka pengguna.
     Dalam penelitian ini, implementasi home automation berbasis Raspberry Pi menggunakan bahasa Python, antarmuka pengguna dengan platform Android menggunakan bahasa Python dan Kivy untuk desain antarmuka pengguna, menggunakan Arduino Yun untuk mengakses sensor dan aktuator. Modul Python lain yang digunakan dalam penelitian ini adalah Flask untuk framework web.
Pada penelitian ini sistem home automation memberikan kinerja dari segi akurasi  untuk mode otomatis aktuator servo pada pintu adalah 100%, aktuator lampu 70%, aktuator kipas 90%, dan aktuator alarm 100%.
","Raspberry Pi with its variety of technology utilization, helped with open source software and programming language can be utilized for developing home automation system. Android as mobile device platform which is common to the development of various applications can be used as a platform for user interface.
In this research, Implementation of home automation system based on Raspberry Pi using Python, Android based platform user interface using Python and Kivy for designing the user interface, Arduino Yun for accessing sensor and actuator and Android Platform as user interface for controlling the system. Another Python Python module used in this research are Kivy for designing user interface and Flask as Web Framework. 
In this research, the system  provides the performance in terms of accuration for automatic mode servo actuator is 100%, light actuator is 70%, fan actuator 90%, and alarm actuator 100%.
",,,63653,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129511,,STEVEN IVIANTO ,SISTEM DETEKSI DINI KRISIS FINANSIAL DI INDONESIA DENGAN GENETIC ALGORITHM NEURAL NETWORK,,,"algoritma genetika, GANN, jaringan saraf tiruan, finansial.","Faizah, S.Kom., M.Kom.",2,3,0,2017,1,"Indonesia menganut ekonomi terbuka, jadi pertumbuhan ekonomi Indonesia dipengaruhi pertumbuhan ekonomi Internasional. Oleh karena itu, jika terjadi fluktuasi pada perekonomian dunia, maka perekonomian Indonesia juga akan terkena dampaknya seperti pada tahun 1998 dan 2008. Dari permasalahan yang disebutkan, maka perlu suatu deteksi dini terhadap krisis finansial di Indonesia. Dengan adanya deteksi dini ini maka pemerintah dapat mengantisipasi krisis yang akan datang dengan mengeluarkan kebijakan-kebijakan ekonomi tertentu agar dapat terhindar dari krisis atau memperkecil dampak dari krisis yang akan datang.
Sistem yang dibangun untuk deteksi dini krisis finansial di Indonesia menggunakan gabungan metode multilayer perceptron dan algoritma genetika (GANN). Sistem ini menggunakan pendekatan leading indicator yang mengansumsikan bahwa faktor makroekonomi memiliki pola dan kecenderungan tertentu sebelum sebuah krisis terjadi dan perfect signal digunakan untuk menggambarkannya. Sedangkan kejadian krisis sendiri didefinisikan oleh Financial Pressure Index dengan threshold tertentu sehingga dapat dikatakan terjadinya krisis. Fungsi fitness yang digunakan adalah Matthew Correlation Coefficient (MCC) dari pengujian keseluruhan data. Pada penelitian ini, performa sistem dengan metode GANN akan diuji dan dibandingkan dengan metode multilayer perceptron berdasarkan nilai MCC dan akurasi. Parameter yang diuji meliputi ukuran populasi, interval nilai gen dan jumlah neuron tersembunyi. 
Dari hasil penelitian, metode GANN memiliki performa yang lebih baik daripada metode multilayer perceptron sendiri. Parameter terbaik dari hasil pengujian sistem adalah parameter dengan ukuran populasi 10 individu, interval nilai gen [-10,10] dan jumlah neuron tersembunyi 5 neuron. Tingkat akurasi sistem mencapai 96.32% berdasarkan data keseluruhan dan 90% berdasarkan data pengujian.
","Indonesia embraces an open economy, so Indonesia's economic growth is influenced by international economic growth. Therefore, if there a fluctuation in the world economy, then the Indonesian economy will also be affected as in 1998 and 2008. From the problems mentioned, it is necessary an early detection of the financial crisis in Indonesia. With this early detection, the government can anticipate future crises by issuing certain economic policies in order to avoid a crisis or minimize the impact of future crises.
The early warning system of financial crisis in Indonesia built using combination of multilayer perceptron with genetic algorithm (GANN). This system uses a leading indicator approach that assumes that macroeconomic factors have certain patterns and trends before a crisis occurs and perfect signal is used to describe it. While the occurrence of the crisis itself is defined by the Financial Pressure Index with a certain threshold so it can be said of the crisis. The fitness function used is the Matthew Correlation Coefficient (MCC) of the overall data test. In this study, system performance by GANN method will be tested and compared with multilayer perceptron method based on MCC value and accuracy. The parameters tested included population size, gene value interval and number of hidden neurons.
From the research result, GANN method has better performance than multilayer perceptron own method. The best parameters of the results are parameters with population size of 10 individuals, gene value interval [-10,10] and number of neurons hidden 5 neurons. The system accuracy rate reached 96.32% based on overall data set and 90% based on test data set.
",,,80404,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110056,,FAKHRI HAWARI ARIFIN,ANALISIS  DAN PENGEMBANGAN DETEKSI RETINOPATI DIABETIK,,,"retinopati diabetik, deteksi microaneurysm, citra fundus mata","Lukman Heryawan, S.T.,M.T.",2,3,0,2017,1,"Retinopati diabetik merupakan komplikasi yang diakibatkan oleh diabetes melitus. Semakin banyak orang menderita diabetes melitus semakin tinggi kemungkinan orang terkena retinopati diabetik. Retinopati diabetik jika tidak ditangani dengan tepat dapat mengakibatkan kebutaan. Sebagian besar penderita retinopati diabetik pada tahap awal tidak mengalami gangguan penglihatan. Langkah penting untuk mencegah kebutaan adalah deteksi dini . Deteksi dini dapat dilakukan dengan menemukan gejala awal yakni microaneurysm
Pada penelitian ini dibuat sistem untuk mendeteksi retinopati diabetik dengan menggunakan algoritma deteksi microaneurysm dengan morfologi matematika. Algoritma  terbagi menjadi tiga tahap yakni  preprocessing, menentukan kandidat microaneurysm dan postprocessing. Pada tahap preprocessing dilakukan peningkatan kualitas pada citra fundus mata. Kemudian dilanjutkan pada tahap menentukan kandidat microaneurysm, tahap ini dilakukan untuk menemukan objek pada citra fundus mata yang akan dijadikan sebagai kandidat microaneurysm. Terakhir tahap preprocessing, tahap ini dilakukan untuk menghilangkan fitur yang tidak digunakan. Pada penelitian ini sistem akan dibuat dengan menggunakan raspbery pi sebagai medianya. Untuk melihat seberapa baik kemampuan sistem mendeteksi retinopati diabetik dilakukan pengujian. Pengujian sistem dilakukan dengan menggunakan data citra fundus mata dari DIARETDB1 dan e-ophtha. 
Pada pengujian yang dilakukan diperoleh hasil dengan akurasi 86,51%, sensitivitas 90,00% dan spesifitas 55% menggunakan data dari DIARETDB1. Sedangkan pengujian   menggunakan data dari e-ophtha diperoleh hasil dengan akurasi 70,5%, sensitivitas 80% dan spesifitas 60%.
","Diabetic retinopathy is a complication caused by diabetes mellitus. More people suffer from diabetes mellitus, the higher the chances of people affected by diabetic retinopathy. Diabetic retinopathy, if not handled properly can lead to blindness. Most patients in the early stages of diabetic retinopathy do not have visual impairments. A necessary step to prevent blindness is early detection. Early detection can be done by finding the initial symptoms that microaneurysm.
	In this study, a system to detect diabetic retinopathy using microaneurysm detection algorithms with mathematical morphology. The algorithm is divided into three stages of preprocessing, determining the candidate microaneurysm. and postprocessing	. In the preprocessing stage to improve the quality in the image of the fundus. Then proceed to the stage of determining the candidate microaneurysm, this step is done to find objects in the fundus image that will serve as a candidate microaneurysm. Last the preprocessing stage, this stage is performed to remove unused features. To see how well the system's ability to detect diabetic retinopathy, the system will be tested. System testing is done by using the image data of the fundus of DIARETDB1 and e-ophtha.
",,,60707,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112104,,FARIZ PRAWIRA,PENGARUH PENDETEKSIAN SARKASME TERHADAP UKURAN KUALITAS ANALISIS SENTIMEN PADA TWITTER,,,"Analisis sentimen, Pendeteksian sarkasme, Multinomial Na&Atilde;&macr;ve Bayes, Random Forest Classifier.","Dr. Techn. Khabib Mustofa, S.Si., M.Kom.",2,3,0,2017,1,"Di masa pemilihan umum, banyak opini-opni tentang tokoh politik yang disuarakan oleh masyarakat secara langsung atau melalui media sosial seperti Twitter. Dengan analisis sentimen, opini-opini tersebut dapat diklasifikasikan ke dalam opini positif yang berarti dukungan atau ke dalam opini negatif yang berarti ejekan atau cemoohan. Akan tetapi, terkadang ditemukan opini dengan sentimen sarkasme yang membuat suatu opini salah diklasifikasikan.
Penelitian ini mengombinasikan analisis sentimen dengan pendeteksian sarkasme untuk pengklasifikasian opini-opini yang terdapat pada Twitter. Untuk analisis sentimen, terdapat 3 fitur yang diimplementasikan, yaitu unigram, select k-best, dan TF-IDF, sedangkan untuk pendeteksian sarkasme, terdapat 4 set fitur yang diimplementasikan, yaitu sentiment-relate, punctuation-relate, lexical and syntactic, dan pattern-relate. Classifier yang digunakan untuk analisis sentimen adalah Multinomial Na&Atilde;&macr;ve Bayes dan untuk pendeteksian sarkasme adalah Random Forest Classifier.
Pada penelitian ini, diperoleh hasil bahwa pendeteksian sarkasme dengan 1500 data latih dapat meninggkatkan akurasi dari analisis sentimen sebesar 1.20%. Selain itu, pendeteksian sarkasme juga dapat meningkatkan presisi sebesar 2.43%. Akan tetapi, peningkatan akurasi dan preisisi dari analisis sentimen berdampak pada menurunnya recall sebesar 0.27%.","In the period of a general election, there are much opinion about political figures that expressed by people directly or by social media such as Twitter. By sentiment analysis, opinions can be classified into positive opinion which mean support or negative opinion which mean mockery. However, sometimes there is sarcasm in the opinion that make opinion incorrectly classified.
This research combines sentiment analysis and sarcasm detection for opinions classification in Twitter. For sentiment analysis, there are 3 features, which are unigram, select k-best and TF-IDF. For sarcasm detection, there are 4 sets of features, which are sentiment-relate, punctuation-relate, lexical and syntactic, and pattern-relate. Classifier used for the sentimen analysis is Multinomial Na&Atilde;&macr;ve Bayes and for sarcasm detection is Random Forest Classifier.
In this research, the result of sarcasm detection combined with sentiment analysis are increased of accuration by 1.20%. Beside that, sarcasm detection can increase precision by 2.43%. However, the impact on increasing of the accuracy and precision is decreasing of the recall by 0.27%.",2017-06-02 00:00:00,53,62875,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107753,,LUKAS SAHTA GINTING,Design and Implementation of News Portal Interface Based On Telegram Bot and RSS,,,"RSS Technology, News Collection, News Filtering, Information Analysis, Telegram Bot","Bambang Nurcahyo Prastowo, Drs., M.Sc",2,3,0,2017,1,"Pesatnya perkembangan internet telah membentuk sumber daya informasi yang luas; Internet membawa perubahan besar dalam cara hidup manusia sehari-hari; namun, kekhawatiran terbesar terhadap pertumbuhan yang cepat dari informasi digital adalah bagaimana untuk secara efisien mengelola dan menyaring data yang tidak diinginkan. Berita, sebagai salah satu informasi yang memiliki pertumbuhan cepat di internet, dapat menjadi berlimpah jika tidak terorganisir dengan rapi dan ringkas, apalagi sebagian besar kantor berita memiliki aliran tinggi publikasi berita di waktu yang singkat, sehingga pengguna membutuhkan sistem/ aplikasi yang bisa memberikan berita terbaru dari berbagai situs-situs berita. Jadi inisiatif untuk mengembangkan sistem yang dapat membantu pengguna mendapatkan berita terbaru dari banyak situs-situs berita di satu tempat.

Telegram Messenger sebagai salah satu aplikasi chat yang memiliki reputasi baik, memiliki beberapa fitur yang membuat telegram Messenger berbeda dari aplikasi chatting lainnya. Terbuka, bebas biaya, serta aman, membuat Telegram bersahabat untuk pengembang aplikasi, terutama untuk pengembang yang ingin memodifikasi kode Telegram untuk kepentingan tertentu. Selain itu, fitur tambahan adalah Bot oleh Telegram messenger, dengan menggunakan itu, pengembang dapat membangun berbagai aplikasi di atas Telegram dan menyajikan kepada pengguna Telegram.

Dengan melepaskan potensi Telegram Bot dan teknologi RSS, pengumpan berita dengan kemampuan untuk menampilkan berbagai berita dari berbagai situs berita dapat dirancang dan diimplementasikan di atas aplikasi Telegram messenger.","The rapid development of internet have been formed extensive information resources; Internet brought huge changes in the way humans live their daily life; however, the biggest concern with the rapid growth of the digital information is how to efficiently manage and filter unwanted data. News, as one of the rapid growth information on the Internet, can be overwhelming if not organized neatly and briefly, moreover most of the news agency has the high flow of news publication in short-time, so users need a system/ application that can deliver most recent news from various news websites. Thus the initiative to develop this system which can help users get the most recent news from many news websites in one place.

Telegram Messenger as one of the reputable chat application, has several features that made Telegram Messenger different from other chat application. Open source, free, and secure, makes Telegram user-friendly for the application developer, especially for developer who want modify the source code of Telegram messenger for any particular reason. Other than that, an additional feature is Bot by Telegram messenger, by using it, the developer can build a various application over Telegram and presents it to Telegram users.

By unleashing Telegram Bot potential and RSS technology, news feeder which has capabilities to display various news from various news sites can be designed and implemented over Telegram messenger application.",,,58349,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
110059,,FEBRIAL WINARTA P,PERBANDINGAN PERFORMA ALGORITMA TWCNB DAN K-NEAREST NEIGHBOR PADA KLASIFIKASI SENTIMEN TWITTER,,,"algoritma TWCNB, algoritma KNN, klasifikasi teks, analisis sentimen","Anifuddin Azis, S.Si., M.Kom.",2,3,0,2017,1,"&acirc;€œApa yang orang lain pikirkan&acirc;€ telah menjadi sesuatu yang penting untuk menjadi pertimbangan dalam pengambilan keputusan yang biasa diutarakan dalam bentuk opini atau sentimen. Adanya internet dan media sosial Twitter mendukung ketersediaan data dalam jumlah besar. Hal ini menjadi faktor pendukung adanya penelitian di bidang analisis sentimen dan klasifikasi teks. Berbagai penelitian dengan berbagai kasus klasifikasi dan implementasi algoritma telah dilakukan, seperti algoritma Transformed Weight-normalized Na&Atilde;&macr;ve-Bayes (TWCNB) dan K-Nearest Neighbor (KNN) telah dikembangkan dalam domain penelitian sama yaitu klasifikasi teks. Namun di antara kedua algoritma tersebut belum diketahui secara pasti mana yang memiliki performa yang lebih baik dalam pengklasifikasian sentimen.
Penelitian ini mengimplementasikan algoritma TWCNB dan KNN dalam mengklasifikasi teks tweet berbahasa Indonesia pada suatu topik yang ditentukan. Tweet diekstraksi sebagai data mentah dilewatkan pada berbagai preprocessing dan digunakan sebagai fitur pada pemodelan dua algoritma tersebut. Skenario pengujian yang dilakukan meliputi pengaruh peningkatan jumlah data dan keseimbangan persebaran data pada performa pengklasifikasian. Performa diukur dari segi akurasi, f-measure, dan waktu pengujian.
Kedua skenario pengujian memiliki pengaruh pada perfoma klasifikasi kedua algoritma. Adanya peningkatan akurasi, f-measure, dan waktu pengujian seiring dengan bertambahnya jumlah data. Juga pada skenario perbedaan persebaran data di mana klasifikasi memiliki akurasi dan waktu pengujian lebih baik pada dataset yang tidak seimbang dibandingkan dataset yang seimbang, namun memberikan f-measure yang kurang baik. Hasil evaluasi performa pengklasifikasian pada penelitian ini menyimpulkan bahwa algoritma KNN memiliki performa pengklasifikasian lebih baik dibandingkan algoritma TWCNB. Antara algoritma TWCNB dan KNN memiliki gap akurasi dan f-measure rata-rata sebesar 1,15% dan 2,22% namun TWCNB memiliki waktu pengujian 2-3 kali lipat dari algoritma KNN seiring meningkatnya jumlah dataset pengujian.
","&acirc;€œWhat the other people think&acirc;€ is an important thing for consideration in decision making which is usually expressed in the form of opinion or sentiment. The internet and social media such Twitter provide data with great availability. These factors support many researches in field of sentiment analysis and text classification. Several researches with any kind of case and implementation of algorithm, like Transformed Weight-normalized Na&Atilde;&macr;ve-Bayes (TWCNB) and K-Nearest Neighbor (KNN) that had been developed beforehand in same research domain. However it is not certain yet which one of them that has better performance in sentiment classification.  
This research implements TWCNB and KNN algorithm in classifying Indonesian tweets on decided topic. The extracted tweets was processed through some preprocessing procedure and used as modeling feature of algorithms. Testing scenario used in this research divide into two scenarios. First scenario is testing on different amount of dataset and second scenario is testing on different dataset class proportion. Classification performance is measured by three metrics. Those metrics are accuracy, f-measure, and testing time.
	These scenarios definitely give different effect on both algorithm classification performance. Accuracy, f-measure value, and testing time increase along with increase size of dataset. Also in second scenario which results better accuracy and testing time on imbalance dataset than balanced dataset., but results worse f-measure value. This research performance evaluation concludes that KNN algorithm has better classification performance than TWCNB algorithm. There are gaps between TWCNB and KNN accuracy about 1.12% in average and f-measure about 2.22% in average, but TWCNB has testing time which is relatively two to three times longer than testing time of KNN algorithm along with the increase of dataset size.
",,,60699,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111086,,MEISYARAH DWIASTUTI,PENGENALAN UCAPAN PADA ORANG DENGAN DYSARTHRIA MENGGUNAKAN CONVOLUTIONAL NEURAL NETWORK,,,"speech recognition, convolutional neural network, dysarthria","Afiahayati, S.Kom., M.Cs., Ph.D.",2,3,0,2017,1,"Dysarthria merupakan gangguan berbicara motorik yang isebabkan oleh gangguan neurologis. Orang dengan dysarthria pada umumnya sulit untuk menggerakkan otot pada tubuh, termasuk otot pada bagian sekitar mulut sehingga artikulasi pada ucapan orang dengan dysarthria menjadi tidak terlalu jelas. Karena pada umumnya penderita dysarthria kesulitan menggerakkan anggota tubuh, Automatic Speech Recognition (ASR) lebih dipertimbangkan untuk diterapkan pada assistive technology dibandingkan metode input berupa tombol atau alat ketik. Akan tetapi ASR komer-
sial yang berkembang luas saat ini memiliki performa yang tidak terlalu baik ketika digunakan oleh orang dengan dysarthria.

Metode Convolutional Neural Network (CNN) terkenal dengan kemampuannya untuk mengenali pola, termasuk untuk mengenali ucapan. Penerapannya pada ASR untuk orang tanpa gangguan berbicara menghasilkan performa yang baik.

Pada penelitian ini diimplementasikan metode CNN untuk masalah pengenalan ucapan digit terpisah bersifat speaker-dependent pada orang dengan dysarthria. Model dibangun dan dievaluasi menggunakan tiga data subjek dengan dysarthria dan
satu subjek tanpa gangguan yang diperoleh dari UA Speech Database. Evaluasi performa terbaik yang diperoleh ialah rata-rata akurasi 90, 43% dan NRMSE 0, 1366. Model CNN yang dibangun menghasilkan rata-rata akurasi yang lebih tinggi dibandingkan dengan model MLP ketika dievaluasi dengan tiga subjek dysarthria.","Dysarthria is a motoric speech impairment caused by neurological impairment. People with dysarthria often find difficulty in moving their muscles, including the ones around mouth and articulators, thus the speech produced is not too intelligible. Since speakers with dysarthria are often physically incapacitated, Automatic Speech Recognition (ASR) is more preferred to be implemented in an assistive technology than conventional input method such as buttons or typing tools. However, commercial ASRs available today have not reached a good performance when being
used by speakers with dysarthria.

Convolutional Neural Network (CNN) is well-known for its capability at recognizing pattern, including speech. Its implementation in ASR is able to achieve good performance.

In this research, CNN is implemented to build a speaker-dependent isolated-word digit speech recognizer for speakers with dysarthria. The recognizer model is built and evaluated with data of 3 speakers with dysarthria and 1 control speaker. Data speech is provided by UASpeech Database. The best performance obtains average
accuracy of 90, 43% and NRMSE of 0, 1366. The CNN model results higher average accuracy than MLP model when being evaluated with 3 subjects with dysarthria.",,,61764,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
116467,,MUHAMMAD IQBAL M,ANALISIS WEBSITE PEMERINTAH KOTA SEBAGAI INDIKATOR TINGKAT KORUPSI (STUDI KASUS SURVEI PERSEPSI KORUPSI 2015),,,"E-Government, web, transparency, corruption, survey, correlation","Drs. Bambang Nurcahyo Prastowo, M.Sc.,Lukman Heryawan, S.T., M.T.,Mhd. Reza M.I Pulungan, M.Sc., Dr.-Ing.",2,3,0,2017,1,"Korupsi merupakan masalah terkini yang tengah melanda segala sisi kehidupan.
Tidak jauh beda dengan pemerintah, korupsi pun juga merayapi aspek dan pejabatnya.
Di era digitalisasi ini website berguna untuk penyampaian informasi. Transparansi dan
transparensi pun merupakan menjadi solusi pendeteksian korupsi. Dengan
menggunakan website yang terpadu dan transparan, kiranya dapat mengurangi
tindakan korupsi dari akarnya
Pada penelitian ini akan berfokus pada Survei Persepsi Korupsi 2015 oleh
Transparency International Indonesia, sebuah organisasi yang melek korupsi dan
secara rutin menggalakkan survei tentang korupsi demi Indonesia yang lebih baik.
Penelitian ini akan mengorelasikan segala elemen, baik dengan observasi manual dan
tool daring yang sudah ada, yang bisa dijadikan parameter untuk dijadikan indikasi
pendeteksian korupsi dari website 11 kota yang tercantum di dalamnya.
Setelah diteliti dari survei tersebut. hasil survei berkorelasi terhadap 12 dari 15
parameter yang diobservasi secara manual. Enam parameter berkorelasi positif yakni
Domain, LPSE, Alih Bahasa, Produk Hukum (JDIH) dan Media Sosial. Sedang
berkorelasi negatif terhadap lima parameter yang seharusnya penting untuk
diperhatikan. Di lain sisi, hasil survei berkorelasi terhadap 12 dari 29 tool daring yang
diteliti dalam penelitian ini, yang masing-masingnya memiliki satu dan/atau lebih
parameter.","Corruption is a current issue that is engulfing all sides of life. Not really
different from the government, corruption is also crawling on every aspect and officials.
In the era of digitizing, the website is useful for the delivery of information.
Transparency is also a solution to prevent corruption. By using an integrated and
transparent website, it may reduce corruption from its roots.
This research will focus on the 2015 Corruption Perceptions Survey by
Transparency International Indonesia (TII), an organization that is corrupt-focused and
regularly promotes better surveys of corruption in Indonesia. This research will
correlate all elements, both with manual observation and existing online tools, which
later can be used as parameters to be an indication of corruption prevention from the
11 cities websites that listed in the survey.
After being examined from the TII survey. The survey results correlated to 12
of the 15 parameters that were observed manually. Six parameters are positively
correlated ie Domain, LPSE, Interpreting, Legal Products (JDIH) and Social Media.
Being negatively correlated with five parameters that should be important to note. On
the other hand, the survey results correlated to 12 of the 29 online tools studied in this
study, each of which has one and/or more parameters.",,,67412,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
111092,,ZAKI INDRA SUKMA,Prediksi Struktur Sekunder Protein Menggunakan Convolutional Neural Network,,,"prediksi struktur sekunder protein, convolutional neural network, deep learning, akurasi Q8, akurasi Q3, CullPDB, Q8 accuracy, Q3 accuracy","Afiahayati, S.Kom, M.Cs, Ph.D",2,3,0,2017,1,"Struktur sekunder protein adalah salah satu struktur penting dalam menentukan bentuk tiga dimensi protein yang menentukan fungsi protein. Menentukan struktur sekunder protein dari struktur primernya menggunakan pendekatan konvensional adalah proses yang sukar dan mahal. Pendekatan komputasi dapat digunakan untuk memprediksi struktur sekunder protein, salah satu metodenya yaitu convolutional neural network.
Convolutional neural network adalah salah satu metode dalam deep learning, telah digunakan untuk klasifikasi citra dan pengenalan suara dengan hasil yang sangat baik. Convolutional neural network merupakan jaringan syaraf tiruan yang di dalamnya terdapat operasi konvolusi dan pooling, dan memiliki jaringan yang dalam untuk meningkatkan akurasi pembelajaran.
Dalam penelitian ini dibangun model prediksi struktur sekunder protein dengan arsitektur convolutional neural network. Arsitektur yang dibangun memiliki 3 convolutional layer dan diberikan modifikasi dengan teknik shift and stitch. Dengan pencarian hyperparameter optimal yang mencakup panjang kernel, dropout, regularisasi L2, dan perbandingan input profile, didapatkan hasil berupa akurasi Q8 mencapai 70.36% dan akurasi Q3 mencapai 82.06% pada dataset CullPDB.
","The secondary structure of protein is one of the important structure to determine the three-dimensional form of protein in which determine protein's function. Determining secondary structure of protein using conventional approach is difficult, yet expensive. Computational approach can be leveraged to predict the secondary structure of protein, and convolutional neural network is one such method. 
Convolutional neural network is one method in deep learning, has been used to image classification, speech recognition with decent results. Convolutional neural network is neural network in which has convolution and pooling, and also has deep network to improve learning accuracies.
In this research, protein secondary structure prediction model is built using convolutional neural network architecture. Architecture which has been build has 3 convolutional layer and modification is given by shift and stitch technique. By optimal hyperparameter search that includes kernel length, dropout, L2 regularization, and input profile comparison, result is achieved. Q8 accuracy of 70.36% and Q3 accuracy of 82.06% is gained on CullPDB dataset.",,,61778,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107253,,RATNA JULIA PURBA,IMPLEMENTATION OF AUDIO FINGERPRINTING IN BATAKNESE MUSIC RECOGNITION,,,"audio fingerprint, music recognition, Fast Fourier Transform algorithm","Mardhani Riasetiawan, S.E., Akt., M.T",2,3,0,2017,1,"Interaksi antara komputer dan musik memiliki keterbatasan karena kurangnya pemahaman antara musik dan komputer. Jika komputer tidak memiliki pengetahuan dasar tentang pemahaman struktur, kunci dan frasa musik, maka interaksi antara musik dan komputer akan sulit untuk dijangkau. Oleh karena itu, pemahaman komputer untuk dapat meningkatkan tingkat interaksi manusia dan komputer dalam hal musik sangat diperlukan. Saat ini, ada banyak aspek tentang musik yang telah dipelajari lebih jauh, terlebih tentang bagaimana musik dikenali dan disintesis.
	Pengenalan musik adalah bagian dari pengambilan musik di mana topik yang diberikan terutama akan membahas tentang identifikasi dan pemahaman tentang struktur dan pola yang dapat diperoleh dari file musik yang akan memberikan informasi tentang musik yang diberikan kepada pengguna. Karya ini sebagian besar difokuskan pada pengambilan koleksi sidik jari yang disimpan dalam database dengan menggunakan asli dengan file .mp3 asli tanpa kebisingan sebagai kunci pencarian. Tingkat kesulitan yang dapat ditemukan dalam pengenalan musik adalah pemahaman komputer dalam struktur musik, topik yang juga merupakan dasar dalam bidang musik komputer dan persepsi musik. Hal yang paling penting dalam penelitian ini adalah keharmonisan antara dua bidang dan juga pengeluaran dari hasilnya.
	Kesamaan antar audio adalah hal yang sangat diperlukan dalam konsep pengenalan musik. Sidik jari dari sebuah audio adalah ringkasan digital unik yang dapat diperoleh dari sinyal audio yang dapat digunakan untuk mengidentifikasi sampel audio atau menemukan kemiripan antara file audio yang satu dan lainnya. Seperti sidik jari seseorang, sidik jari audio adalah jejak dan struktur audio, terutama seperti yang digunakan untuk mengidentifikasi audio dari puncak tertinggi yang ditemukan dari spektrum. Penerapan sidik jari audio dapat diterapkan di berbagai hal, seperti perlindungan hak cipta dari seorang penulis lagu (penyanyi / musisi) di Digital Right Management (DMR), monitoring audio dan meta data pemulihan pada koleksi audio. Penelitian ini akan fokus pada pnerapan sidik jari audio yang dapat digunakan untuk mengidentifikasi musik Batak.
","Interaction between computer and music has a limitation because of the lack understanding of music by computer. If a computer does not have the basic knowledge of understanding the structure, chord, phrases of music, then the interaction between music and computer will be hard to reach. Therefore, comprehension of computer to be able to raise the level of human computer interaction in musical task is highly needed. Nowadays, there are many aspects about music those have been further learned, exceedingly regarding about: How can music be recognized and synthesized?
Music recognition is a part of music retrieval where the topic given will especially discuss about the identification and understanding of structure and pattern those can be obtained from a music file that will give information about the given music to users. This work is mostly focused in the retrieval of fingerprints collection those are stored in database with using original with no noise .mp3 file as the search key. The degree of difficulty that can be found in music recognition is the comprehension of computer in music structure, a topic that is also the basic in computer music and music perception fields. The most important thing in this research is the harmony between the two fields and also the output of the result.
The similarity between audios is a necessary thing in the concept of music recognition. Audio fingerprint is a condensed digital summary that can be obtained from audio signal that can be used to identify audio sample or discover the resemblance between an audio file and another. Like a person's fingerprint, audio fingerprint is an impression of the audio's structure, especially as used for identifying audios from the unique peaks found from the spectrums. The application of audio fingerprint involves the protection of copy-right from a song author (singer/musician) in Digital Right Management (DMR), audio monitoring and meta data restoration in audio library. This research will focus on the implementation of audio fingerprinting that can be used to identify Bataknese music.
",,,57834,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
129527,,FATHURROCHMAN HABIBIE,MULTIPLE SEQUENCE ALIGNMENT SECARA PARALEL MENGGUNAKAN MPI (MESSAGE PASSING INTERFACE),,,"Parallelization, Multiple Sequence Alignment, ClustalW, Message Passing Interface, Paralelisasi","Afiahayati, S.Kom., M.Cs., Ph.D.",2,3,0,2017,1,"Seiring dengan berkembangnya teknologi dibidang bioinformatika, biaya untuk melakukan sequencing DNA dari tahun ke tahun semakin murah. Hal ini menyebabkan pertumbahan basis data genetik naik bahkan melampaui hukum Moore's law. Pertumbuhan basis data genetik yang sangat cepat merupakan salah satu hambatan utama dalam melakukan sequence alignment. ClustalW merupakan salah satu metode MSA yang sering digunakan oleh praktisi biomolekuler. Metode ini dibagi menjadi tiga tahap yaitu perhitungan distance matrix, pembuatan guided tree dan progressive alignment.
Pemrograman sekuensial membutuhkan waktu yang lama untuk memproses data yang besar. Selain itu, pemrograman sekuensial memiliki keterbatasan memory, sehingga dapat menyebabkan stack pada program. Salah satu cara untuk mempercepat kinerja pemrosesan adalah menggunakan pemrograman paralel. MPI merupakan salah satu teknologi komputasi paralel yang populer. Proses paralel dijalankan pada kluster MPI yang terdiri dari empat buah komputer single-board. Percobaan menggunakan data sekuens dari BAliBase versi 3. Dari hasil penelitan menunjukkan bahwa pada tahap perhitungan distance matrix dapat mencapai waktu 12,7 kali, sedangkan pada tahap progressive alignment dapat mencapai waktu 5,71 kali lebih cepat dibanding proses sekuensialnya.","Due to the rapid growth of bioinformatics technology, the cost of sequencing DNA from year to year is cheaper. This led to the growth of genetic databases to rise even beyond Moore's law. The rapid growth of genetic databases is one of the problems with sequence alignment. ClustalW is a popular method used by biomolecular practitioners. It is divided into three steps. There are distance matrix calculations, guided tree reconstruction, and progressive alignment.
Sequential programming takes a long time to process large dataset. Furthermore, sequential programming has limited memory, so it can cause a stack on the program. One way to speed up the performance is to use parallel programming. MPI is one of the most popular parallel computing technologies. The parallel process is run on the MPI cluster consisting of four single-board computers. The experiments used sequence data from BAliBase version 3. The results obtained that the distance matrix calculation can reach 12.7 times faster than sequential process. While at the progressive alignment can reach time 5.71 times faster than sequential process.",,,80461,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
113916,,ALIYYAH DAMAR ,Analysis on File Indexing with Elastic Search and Map Reduce Processes,,," indexing, Elasticsearch, extracting, Apache Tika, HDFS, Hadoop, Big data, mapper, reducer","Mardhani Riasetiawan SE Ak, MT,",2,3,0,2017,1,"Dokumen indexing banyak digunakan untuk proses pencarian dokumen ataupun publikasi sains. Indexing documents used for searching documents or scientific publications. Kinerja aplikasi yang bekerja dengan database besar bergantung pada seberapa cepat database dapat memproses permintaan. Semakin cepat database bisa mengolah hasil yang diindeks menjadi penting. Seiring dengan konfigurasi yang memberikan hasil paling optimal.
Penelitian ini mencoba menghubungkan dua infrastruktur yang berbeda; Elasticsearch adalah mesin pencari open source, terdistribusi dan hampir real-time, berdasarkan Lucene. Di bidang cloud storage dan cloud calculation, Elasticsearch dapat diandalkan, cepat, dan dapat mendukung indeks tipe data JSON dengan menggunakan HTTP; Hadoop Distributed File System (HDFS) adalah sistem penyimpanan utama yang digunakan oleh aplikasi Hadoop. Metode yang digunakan pada peneletian ini adalah; Akuisisi data, ekstraksi data menggunakan Apache Tika, pengaturan struktur indeks, mengekspor file yang diindeks ke dalam csv, kemudian memproses file yang telah diindeks dan kemudian diekspor ke MapReduce example program yang berjalan di HDFS.
Hasil percobaan menunjukkan bahwa, tidak mungkin menghubungkan Elasticsearch secara langsung dengan HDFS; Dibandingkan dengan file data yang tidak diindeks dengan data yang sudah diindeks untuk proses yang berjalan pada MapReduce example program, ditunjukkan bahwa yang diindeks, memiliki waktu proses yang signifikan daripada yang tidak diindeks; Terakhir, semakin banyak konfigurasi mapper dan reducer yang tidak menjamin kinerja proses yang lebih baik.","Indexing documents used for searching documents or scientific publications. The performance of applications working with large databases depends on how quickly the database can process a request. The faster the database can process the indexed result become important. Along with the configuration that gave the most optimum result.
This research trying to connect two different infrastructures; Elasticsearch is an open source, distributed and nearly real-time search engine, based on Lucene. In the fields of cloud storage and cloud calculation, Elasticsearch is reliable, fast, and steady, and it can support the index of JSON data type by using HTTP; and The Hadoop Distributed File System (HDFS) is the primary storage system used by Hadoop applications. Methods used is; the data acquisition, data extraction using Apache Tika, setting the index structure, exporting the indexed file into csv, then processing the exported indexed file on the MapReduce Example programs that runs on the HDFS.
The results of the experiment show that, it is not possible to connect Elasticsearch directly with HDFS; compared to the non-indexed data file with the indexed data for the processes runs on the MapReduce Example program, it shown that the indexed one, have a significant process time than the non-indexed one; lastly, the greater number of mapper and reducer configuration does not guarantee the better performance of the process.",,,64944,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107524,,M. RIZKY LUTHFIANTO,Prediksi Struktur Sekunder Protein menggunakan Model Deep Learning dengan Konvolusi dan Bidirectional Gated Recurrent Unit,,,"Protein secondary structure prediction, Deep Learning, Convolutional Neural Network, Bidirectional Gated Recurrent Unit, Sequence Labelling","Afiahayati, Ph.D",2,3,0,2016,1,"Prediksi struktur sekunder protein adalah permasalahan penting di dalam  bioinformatika karena struktur tiga dimensi protein menentukan fungsi dari sebuah protein. Sedangkan, protein itu sendiri adalah unit fungsional utama dalam organisme hidup dan terlibat dalam banyak proses biologis dalam sel. 
Dalam penilitian ini diimplementasikan sebuah model Deep Learning untuk kasus prediksi struktur sekunder protein yang menggabungkan antara Convolutional Neural Network (CNN) dan Recurrent Neural Network (RNN). Pada model ini terdapat tiga lapisan konvolusi dengan panjang filter berbeda yang disusun secara paralel untuk mengekstrak fitur lokal. Karena adanya long-range dependencies dalam sekuen asam amino yang dibuktikan oleh Zhou &amp; Troyanskaya (2014), maka digunakan Gated Recurrent Unit yang disusun secara bidireksional untuk menangkap fitur global. 
Model yang dihasilkan penelitian ini mampu meraih akurasi Q8 sebesar 66.9% pada CB513 dan 69.7% pada test set CullPDB yang umum dijadikan sebagai benchmark.","Protein secondary structure prediction is an important problem in bioinformatics because the three-dimensional structure of a protein determines the function of the protein itself. Meanwhile, protein is the main functional units in living organisms and are involved in many biological processes in cells.
In this research, we implemented a Deep Learning model for protein secondary structure prediction that combines Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In this model, there are three layers with different filter lengths which are arranged in parallel to extract local features. Because long-range dependencies are proven to be exists in amino acid sequences as discovered by Zhou &amp; Troyanskaya (2014), we used Bidirectional Gated Recurrent Units to capture the global context features.
The resulting model is able to achieve both Q8 accuracy of 66.9% on CB513 and 69.7% on CB513, which both datasets are commonly used as benchmark.",,,58154,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
93701,,HATTA NUR ROCHIM,RANCANG BANGUN TELEGRAM BOT PADA TELEGRAM MESSENGER DENGAN METODE LONG POLLING UNTUK KOPERASI KOPMA UGM,,,"Telegram Messenger, Telegram Bot, API, Long Polling, Koperasi Kopma UGM","Lukman Heryawan, S.T., M.T.",2,3,0,2016,1,"Perkembangan smartphone yang semakin pesat dan tingginya pengguna smartphone sebagai alat bantu informasi, membuat banyak sekali munculnya aplikasi pesan instan dengan berbagai pilihan untuk digunakan. Telegram, sebagai salah satu aplikasi pesan instan yang cukup baru menawarkan berbagai kelebihan dalam fiturnya dibanding aplikasi pesan instan lain, sehingga dapat berkembang dengan pesat dan banyak diminati hanya dalam 2 tahun ini. Salah satu fitur yang cukup menarik adalah Telegram Bot, sebuah akun khusus tanpa nomor yang dapat meng-handle sebuah Command  dari pengguna dan memberika jawaban sesuai fungsionalitas Command. Akun tersebut berfungsi sebagai antarmuka dari sistem yang berjalan yang berkomunikasi melalui Telegram Bot API.
Penelitian ini berusaha membangun sebuah Telegram Bot untuk Koperasi Kopma UGM menggunakan metode komunikasi Long Polling. Metode ini dipilih karena dapat diimplementasikan sesuai dengan kondisi infrastruktur Koperasi Kopma UGM yang mempunyai sebuah PC server. Bot yang dibuat dapat melayani kebutuhan informasi dari sisi anggota mengenai seluk beluk Koperasi Kopma UGM melalui Command yang dibuat, pengoperasian Bot melalui sebuah Command yaitu perintah tertulis yang hanya dikenali Bot. Bot yang dikembangkan juga mempunyai admin dan Command-Command  umum yang dapat digunakan diluar konteks Koperasi Kopma UGM. 
Dari hasil pengujian, Telegram Bot untuk Koperasi Kopma UGM telah  berjalan sesuai dengan perancangan. Telegram Bot yang dibuat mampu memberikan layanan informasi dari sisi keanggotaan di Koperasi KOPMA UGM bagi pengguna. Kecepatan Bot untuk memproses Command dipengaruhi oleh kecepatan internet PC server. Rentang waktu yang dapat Telegram server gunakan untu menahan sambungan koneksi hanya sekitar 60 detik.
","The development of the smartphones that rapidly increasing, and high smartphone Users as a tool to make a lot of information, make emergence of instant messaging applications with a variety of options for use. Telegram, as one of the instant messaging application that is fairly new, offering in its various features compared to other instant messaging applications, so it can grow rapidly and much in demand in only 2 years. One of the interesting features is the Bot telegram, a special account without number that can handle Command from the User and gives answers appropriate to Command functionality. The account serves as the interface of the running system that communicate via Telegram Bot API.
This study seeks to build a Bot telegram for Kopma UGM Cooperative using the Long Polling method communication. This method was chosen because it can be implemented in accordance to the conditions Kopma UGM Cooperative infrastructure that have a PC server. Bot created to serve the information needs of the members of the intricacies of the Kopma UGM Cooperative through the Commands are made, the operation of the Bot through a Command that is only  recognizable by Bot. Bot that developed also have admin and general Command-Command that can be used outside the context of  Kopma UGM Cooperative.
From the test results, Telegram Bot for Kopma UGM Cooperative has been run in accordance with the design. Telegram Bot are made to provide information services in terms of membership in the Cooperative KOPMA UGM for the User. Bot speed to process the Command influenced by internet speed PC server. Time span that can Telegram server use the to hold the  connections only about 60 seconds.
",,,43847,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98821,,M REZA PAHLEVI,PENGEMBANGAN DATA WAREHOUSE DAN IMPLEMENTASI ALGORITMA APRIORI UNTUK TRANSAKSI KOPERASI (STUDI KASUS KOPERASI PEGAWAI TELKOM PURWOKERTO) ,,,"Analisis, Pentaho, Data Warehouse, Data Mining, Apriori, Association Rule, Minimarket","Arif Nurwidyantoro, M.Cs.",2,3,0,2016,1,"Salah satu unit usaha dari Koperasi Pegawai Telkom (Kopegtel) Purwokerto yaitu minimarket/toko membutuhkan informasi yang akurat untuk dilakukan pengambilan keputusan. Saat ini data yang dimiliki oleh Kopegtel masih berupa data transaksional sehingga menyulitkan pihak manajemen Kopegtel untuk mencari informasi yang dibutuhkan. Salah satu alternatif untuk  menangani permasalahan tersebut adalah dengan dibangunnya data warehouse.
Pada penelitian ini dilakukan perancangan dan pembangunan data warehouse dengan domain swalayan menggunakan tools petaho. Dari hasil pembuatan data warehouse tersebut akan dilakukan analisis pola penjualan barang menggunakan algoritma apriori. Pola penjualan barang ini akan memuat pasangan barang yang sering dibeli bersamaan dengan menghitung support dan confidence tiap pasang barang. Untuk mempermudah pihak manajemen Kopegtel melakukan analisis dan membuat keputusan, data warehouse dan analisis yang dihasilkan kemudian divisualisasikan.
Dari hasil penelitian ini, dapat dilihat beberapa parameter informasi, diantaranya detail item penjualan pembelian, produk terlaris, kategori terlaris dan pelanggan yang paling sering membeli. Selain tersaji beberapa informasi, analisis yang dihasilkan pada penelitian ini berupa pasangan penjualan barang yang paling sering dibeli bersamaan. Semakin besar nilai support dan nilai confidence pasangan barang, maka semakin besar pula peluang pasangan barang tersebut dibeli secara bersamaan. 
","One of the business unit of Telkom Employees Cooperative (Kopegtel) Purwokerto is minimarket/store that needs accurate information for decision making. Currently, the data of Kopegtel still form of the transactional data so it&Atilde;ƒ&iuml;&iquest;&frac12;&Atilde;‚&Acirc;&cent;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;&Atilde;ƒ&Acirc;&macr;&Atilde;‚&Acirc;&iquest;&Atilde;‚&Acirc;&frac12;s difficult to find the accurate information required by management of Kopegtel.  One of alternatives to handle the problem is build the data warehouse.
This research is conducted design and develop the data warehouse the mart domain with pentaho tools. From the result of making the data warehouse will be conducted to analysis pattern sale of goods with apriori algorithm. These pattern sale of goods will contain the pairs of goods often bought at the same time by counting support and confidence every pairs of goods. To simplify the management of Kopegtel do the analysis and make decisions afterwards, the data warehouse and its analysis resulted then will be visualized.
From the result of this research, it can be seen some parameters information which are detail item sales and purchases, best-selling products, best-selling category, and customer who are most often buy. Beside some information presented, in this research the analysis resulted is pair sale of goods that most often bought simultaneously. The bigger value of support and confidence pair of goods, then the bigger probability that pairs of goods most often bought simultaneously
",,,49185,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100102,,SABRINA WORO A,ANALYSIS OF DIABETES MELLITUS TYPE 2 BASED ON ENSEMBLE LEARNING WEIGHTED VOTING,,,"machine learning, classification, electronic health records, ensemble learning weighted voting","Arif Nurwidyantoro, M.Cs.",2,3,0,2016,1,"Fokus pada studi ini adalah untuk membuat sebuah model klasifikasi untuk memprediksi pasien yang memiliki penyakit diabetes mellitus tipe 2 dari data Electronic Health Records (EHR) setiap pasien. Metode klasifikasi ini akan menggunkan ensemble learning weighted voting yang mengkombinasikan algoritma Naive Bayes, K Nearest Neighbor dan Random Forest. Tujuan dari studi ini adalah untuk membangun model klasifikasi yang dapat memprediksi pasien dari data yang sudah ada, bukan untuk memprediksi pasien yang akan datang yang belum memiliki diagnosa kesehatan. Untuk melakukannya, terdapat beberapa proses yang dilakukan; mulai dari pengumpulan data, praproses, pengurangan jumlah dimensi data (terdiri dari pemilihan fitur dan ekstraksi fitur), pelatihan data dan pengujian data.

Data yang digunakan berasal dari Practice Fusion yang memiliki 9,943 data pasien dengan 17 tabel yang berbeda. Setiap table mewakili diagnosa kesehatan dari beberapa (tidak semua) pasien yang terhubung antara satu sama lain. Data yang diperolah terdiri informasi umum mengenai pasien (umur, asal, tahun lahir, dsb.) dengan data pribadi yang disamarkan, informasi fisik (BMI, tinggi badan, berat badan, tekanan darah, dsb.), track record diagnosa yang pernah terdeteksi, status perokok, obat yang pernah dikonsumsi, observasi laboratorium, data resep obat, imunisasi dan alergi. Fitur dipilih bedasarkan pengetahuan umum mengenai penyakit terkait dari World Health Organization (1999), penelitian Gilies dkk (2012) dan kondisi kelengkapan data setelah di praproses.

Penelitian ini menyimpulkan bahwa metode ensemble learning weighted voting tidak memiliki hasil yang lebih baik dari segi performa precision (75%), recall (71%) dan accuracy (91%). Kesimpulan ini dapat dilihat dari hasil precision dan accuracy terbaik dihasilkan oleh Random Forest (92%), serta recall terbaik diperoleh Naive Bayes (91%).","The focus of the study is to build a classification model that predicts diabetes mellitus type 2 from patient's Electronic Health Records (EHR) data using ensemble learning weighted voting that combines Naive Bayes, K Nearest Neighbor and Random Forest. This study aims to build a classification model to predict patients from existing data, not to predict future patient that have not yet received medical diagnosis. To do so, several phases were conducted; data collection, feature processing, dimensionality reduction (feature selection and extraction), data training and data testing.

The dataset were Electronic Health Records obtained from Practice Fusion in Kaggle that consists of 9,943 patient data with 17 different tables that were connected to each other. The data contains information about patient's general information (age, state, year of birth, etc.) with anonymous personal information, physical information (BMI, height, weight, blood pressure, etc.), past diagnosis, smoking status, medication received, lab observation, prescription records, immunization and allergy. Features was chosen based on domain knowledge from World Health Organization (1999), past research from Gilies et al., (2012) about diagnoses of diabetes mellitus type 2 and data completeness condition in each feature after preprocessing.

Experiment shows that ensemble learning weighted voting does not produce a better result in terms of performance in precision (75%), recall (71%) and accuracy (91%). This can be concluded from the experiment result that finds the highest precision and accuracy was obtained by Random Forest (92%), while the highest recall is obtained by Naive Bayes (91%).",,,50429,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104199,,ANNISA PRIMADINI K,RANCANG BANGUN APLIKASI BLOKIR PANGGILAN TELEPON DAN SMS SPAM PADA SISTEM OPERASI ANDROID,,,"Android, Call Blocking, SMS Blocking, Blacklist  ","Drs. Bambang Prastowo, M.Sc ",2,3,0,2016,1,"Penggunaan chat messenger yang terus meningkat tidak membuat penggunaan Telepon dan SMS menjadi berkurang. Telepon dan SMS masih menjadi media komunikasi untuk saling bertukar informasi. Dari segi fungsionalitas, Telepon dan SMS tidak hanya sekedar untuk berkomunikasi tetapi juga digunakan untuk menyampaikan informasi yang bersifat penting. Penyalahgunaan Telepon dan SMS yang terjadi bersifat mengganggu dan tidak dikenal saat menerima panggilan dan SMS. Salah satu sistem operasi yang populer pada smartphone saat ini adalah Android.
Pada penelitian ini akan dibuat sebuah aplikasi pada sistem operasi Android yang memungkinkan pengguna untuk memfilter Telepon dan SMS yang masuk. Aplikasi ini akan melakukan blacklist pada nomor telepon yang dianggap mengganggu melalui perangkat mobile. Aplikasi yang dibangun dapat digunakan untuk memblokir Telepon dan SMS dengan fitur memberikan informasi waktu dan catatan nomor telepon yang telah diblokir oleh aplikasi baik telepon masuk maupun SMS, beserta isi pesan dari SMS yang diterima. 
Berdasarkan hasil pengujian, seluruh fungsionalitas yang disebutkan dapat berjalan dengan baik sesuai rancangan yang dibuat. Selain itu aplikasi juga telah berhasil diujicoba pada beberapa perangkat Android yang berbeda. 
","The use of chat messenger application have been rapidly increasing, stills Phonecall and SMS are the most being used as a communication tools to exchange informatios. In terms of functionality, Phonecall and SMS not only able to communicate but also used to convey essential information. Getting lots of unwanted calls and texts is annoying and can be worrying if you don't know where the calls are coming from. One of the most popular operating system in mobile devices today is Android.
This research will build an application on Android operating system that allows users to filter the incoming Phonecalls and SMS. The application will be able to blacklist the phone number that is considered interrupt user through mobile devices. Built-in application may be used to block Phonecalls and SMS with features that will provide information such time and phone numbers record that has been blocked by the application of both incoming Phonecalls and SMS, along with the contents of received message.
Based on test results, the entire functionality that have been mentioned  can already be fulfilled by the application. In addition, the application also has been successfully tested on several different Android devices.
",,,54826,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107279,,TANTRI PANDU PERTIWI,PERANGKUMAN RANGKAIAN PERISTIWA OTOMATIS ARTIKEL BERITA BAHASA INDONESIA MENGGUNAKAN ALGORITMA PAGERANK,,,"rangkuman rangkaian peristiwa, perangkuman otomatis, ekstraksi informasi, timeline summarization, automatic summarization, information extraction, pagerank","Drs. Sri Mulyana, M. Kom.",2,3,0,2016,1,"Jumlah artikel berita untuk suatu peristiwa tertentu di internet yang sangat banyak menyebabkan pembaca berita kesulitan untuk memahami alur dari peristiwa yang rumit dan berkepanjangan. Peran rangkuman rangkaian peristiwa tentu sangat membantu pembaca untuk mengetahui garis besar dari sebuah peristiwa. Situs berita biasa menyajikan rangkuman peristiwa, namun proses perangkuman masih dilakukan oleh manusia secara manual. Hal ini tentu kurang efisien baik dari segi waktu, tenaga, dan biaya, sehingga diperlukan sebuah sistem yang dapat merangkum peristiwa secara otomatis.
Penelitian ini fokus dalam pembangunan sistem yang dapat menghasilkan rangkuman rangkaian peristiwa secara otomatis. Sistem ini bekerja dengan melakukan penilaian terhadap kalimat-kalimat berita, kemudian mengekstrak kalimat dengan skor tertinggi untuk  dijadikan rangkuman. Proses penilaian dilakukan secara lokal dan global menggunakan algoritma PageRank. Skor lokal dan skor global kemudian dikombinasikan untuk mendapat skor kombinasi kalimat. Kalimat dengan skor kombinasi tertinggi kemudian diekstrak untuk dijadikan rangkuman.
Evaluasi ROUGE terhadap 4 topik berita menunjukkan bahwa rangkuman lokal (rangkuman yang hanya bergantung pada skor lokal kalimat) menghasilkan kualitas yang lebih baik daripada rangkuman global dan rangkuman kombinasi. Hal ini ditunjukkan dengan nilai F-score rata-rata rangkuman lokal sebesar 0,297005, sedangkan rangkuman global sebesar 0,281987, dan rangkuman kombinasi sebesar 0,2878084. Semakin tinggi nilai F-score berarti kualitas rangkuman semakin baik.
","Online news agencies tend to publish news article as soon as a new event happened. This is causing the number of news articles of a story become very large and readers can not follow the development of the story. Timeline summaries are an effective way for readers to keep track of long-lasting news stories. Some online news agencies indeed provide timeline summaries, but most of them are human generated summaries, which are not efficient. An automatic timeline summarization is needed to save time and cost on generating timeline summaries.
This research focuses on generating automatic timeline summaries using PageRank algorithms to score news sentences. This system works by scoring sentences globally and locally, then combine those two scores to get combined score of the sentences. The sentence with highest score will represent  the summary of a certain date. 
Evaluation is done using ROUGE on four news topic samples. The result shows that local summaries (summaries that generated from local score only) give better quality than global summaries or combination summaries. The F-score from local summaries is 0,297005, while global summaries is 0,281987, and combination summaries is 0,2878084. The higher the F-score means the better the quality of the summaries.
",,,57881,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99344,,HAYDAR ALI ISMAIL,CPU AND MEMORY PERFORMANCE ANALYSIS ON DYNAMIC AND DEDICATED RESOURCE ALLOCATION USING XENSERVER IN DATA CENTER ENVIRONMENT,,,"virtual data center, virtual machine, XenServer, CPU allocation, memory allocation, resource allocation","Mardhani Riasetiawan, M.T",2,3,0,2016,1,"Server sudah menjadi kebutuhan penting pada perhitungan skala besar dan perusahaan. Dengan menggunakan teknologi cloud computing, komputer dan server tidak dikelola secara fisik lagi. Cloud computing memungkinkan sekelompok jaringan terkoneksi berbagi sumber daya komputer untuk memberikan layanan yang diakses melalui Internet oleh pengguna. Untuk menangani beban lebih besar, sumber daya harus dioptimalkan.
Di penelitian ini, percobaan dilakukan pada virtual machine dengan alokasi CPU dan memori yang beragam di XenServer. Virtual machine akan menjalankan aplikasi benchmark dan stress test dengan tes dan durasi yang sudah dikonfigurasikan untuk memahami performa virtual machine saat diberikan beban terus menerus. Aplikasi terdiri dari aplikasi yang intensif menggunakan CPU, memori, disk dan juga benchmark. Parameter performa di penelitian ini adalah penggunaan CPU, memori, alokasi memori, dan hasil benchmark.
Berdasarkan hasil penelitian, virtual machine dengan alokasi CPU Priority Weight dan Cap memberikan performa lebih baik pada transfer file dan query MySQL dengan perbedaan antara 25% dan 76% untuk transfer file dan antara 9% dan 74% untuk query MySQL dibandingkan dengan alokasi CPU Default. Virtual machine dengan alokasi CPU Default memberikan performa lebih baik pada tes stres CPU Stress-ng dengan perbedaan antara 46% dan 48%, tes stres VM dengan perbedaan performa antara 14% dan 51%, tes UnixBench dengan perbedaan antara 5% dan 50%, dan server web antara 5% dan 29% jika dibandingkan dengan alokasi CPU Priority Weight dan Cap. Virtual machine dengan memori alokasi statis memberikan performa mentah lebih baik pada tes stres VM Stress-ng dengan perbedaan antara 7% dan 53% dapat menjalankan lebih banyak proses bersamaan antara 9% dan 74% dibandingkan dengan menggunakan memori alokasi dinamis.","Servers nowadays are vital resources for most large-scale computations and enterprises. By using cloud computing technology, computers and servers were not hosted physically anymore. Cloud computing allows a large group of an interconnected network to share computer resources hence could provide a service that can be accessed using the Internet by a broad group of users. To handle bigger loads, the existing resources should be optimized.
In this research, a series of experiments conducted to virtual machines with different CPU and memory allocation techniques that implemented in XenServer. The created virtual machines will be running benchmarks and stress test applications with a preconfigured test and duration to understand the performance of virtual machines when given a continuous load. The applications consist of CPU, memory, disk intensive app and benchmark app. The performance parameter in this research is CPU usage, memory usage, memory allocation, and result from benchmark app.
Based on experiments, virtual machines with Priority Weight and Cap CPU allocation able to give better performance in file transfer and MySQL queries with performance margin around 25% to 76% for file transfer and 9% to 74% for MySQL queries compared to Default CPU allocation. Virtual machines with Default CPU allocation able to give better performance based on Stress-ng CPU stress test with performance margin around 46% to 48%, Stress-ng VM stress test with performance margin around 14% to 51%, UnixBench test with performance margin around 5% to 50%, and web server around 5% to 29% compared to Priority Weight and Cap CPU allocation. Virtual machines with Static Memory allocation able to give better raw performance based on Stress-ng VM stress test with performance margin around 7% to 53% and able to run 9% to 74% more processes simultaneously compared to Dynamic Memory allocation.",,,49669,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104980,,MUHAMMAD HAFIDZ ANSHARI,ANALISIS POPULARITAS TWEET BERBAHASA INDONESIA PADA TWITTER MENGGUNAKAN METODE PERBANDINGAN EKSPONENSIAL,,,"Twitter, Retweet, Metode Perbandingan Eksponensial","Arif Nurwidyantoro, M.Cs",2,3,0,2016,1,"Twitter merupakan salah satu media sosial yang saat ini perannya lekat sebagai penyebar informasi. Kemudahan akses dan pemanfaatan fitur Retweet pada Twitter, membuat penyebaran informasi menjadi sangat cepat dan memiliki cakupan yang luas. Inilah yang kemudian dimanfaatkan pengguna untuk menarik perhatian pengguna lain perihal informasi yang dimilikinya. 

Penyebaran informasi menggunakan tweet berulang pada Twitter, sering kali masing-masing tweet memiliki jumlah retweet yang berbeda meskipun memiliki konteks dan konten yang sama. Hal ini disebabkan adanya faktor-faktor yang dapat mempengaruhi jumlah retweet pada sebuah tweet. Faktor-faktor tersebut antara lain faktor urutan tweet, faktor informatif dan faktor jumlah subyek. Faktor itulah yang akan diteliti mana yang lebih dominan, menggunakan perhitungan dengan Metode Perbandingan Eksponensial (MPE) yang merupakan salah satu metode untuk menentukan prioritas alternatif keputusan dengan kriteria jamak. 

Setelah melalui proses analisis dan perhitungan, diperoleh kesimpulan bahwa tweet yang memiliki isi dan konten media yang lebih informatif (faktor informatif), lebih menarik perhatian pengguna Twitter lainnya untuk melakukan retweet. Hal ini karena tweet informatif dilengkapi dengan media baik berupa gambar, video, maupun link. Selain itu penambahan keterangan pelengkap seperti keterangan waktu maupun tempat, juga cukup menarik perhatian pengguna lain untuk melakukan retweet sehingga cakupan sebaran tweet tersebut menjadi sangat luas. 
","Twitter is one of social media that is currently attached to role as a disseminator of information. Many parties then use Twitter as a bridge to deliver information they have. Ease of access and use features retweet on Twitter, making the dissemination of information to be very fast and has a broad scope. This is then used by users to attract the attention of other users about the information they have. 

Dissemination of information using repeated tweet on Twitter, often times each tweet has a number of different retweet despite having the same context and content. This is due to factors that can affect the number of retweets a tweet. These factors include tweet sequence factor, informative factor and factor the number of subjects. Factors that would later be studied which is more dominant, then calculate with the Exponential Comparative Method (MPE). MPE is one of method to determine the priority of decision alternatives with multiple criteria. 

After a thorough analysis and calculation, the conclusion that tweet in content and media content more informative (Informative Factor), attract more other Twitter users to retweet. This is because the tweet informatif equipped with media content in the form of images, videos, and links. Besides the addition of supplementary information such as description of time and place, also attract other users to retweet, so the scope of the tweet distribution becomes very wide. ",,,55451,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104222,,RAIS MOHAMAD NAJIB,PENERAPAN ALGORITMA GENETIKA UNTUK PROYEKSI KOMPOSISI HERO DAN  KEMENANGAN PADA PERTANDINGAN TIM DOTA 2 PROFESIONAL,,,"DOTA 2, Algoritma Genetika, Komposisi hero, hero role, tournamen","Faizah, S.Kom, M.Kom.",2,3,0,2016,1,"Dota 2 merupakan permainan free-to-play yang berjenis Multiplayer Online Battle Arena. Pada DOTA 2, pemain terdiri dari 10 orang yang masing masing memilih 1 hero yang dibagi menjadi dua tim yang bertujuan untuk menghancurkan markas tim Lawan terlebih dahulu. DOTA 2 berkembang menjadi Esports atau Electronic Sport yang sudah memiliki sistem turnamen dan liga. Captain Mode adalah mode standar dalam pertandingan professional. Dimana kedua tim yang bertanding memilih hero untuk dilarang dan dimainkan. Komposisi hero yang dihasilkan digunakan dalam memprediksi kemenangan tim yang bertanding. 
	Algoritma genetika merupakan salah satu soft computing yang sering digunakan dalam melakukan proyeksi. Pada penelitian ini , digunakan algoritma genetika untuk memproyeksi komposisi susunan hero dalam Captain Mode secara optimal. Kemudian dengan menggunakan faktor Track record pertandingan, Track record Head-to-Head , Hero Meta tim, Hero Meta Pemain, Hero Patch dan Hero Pair maka akan diproyeksikan kemenangan dari hasil susunan hero yang dihasilkan.  
	Pengujian Sistem pada penelitian ini akan dilakukan dengan menguji hasil pertandingan tim professional pada turnamen Shanghai Major. Dari hasil pengujian terhadap 10 pertandingan babak final Shangha Major, Tingkat persentase identik komposisi ban hero 12,3.%, tingkat persentase identiik komposisi pick hero 21,2 %, Namun Tingkat akurasi proyeksi mencapai 80%.
","DOTA 2 is a free-to-play with genre of Multiplayer Online Battle Arena. On DOTA 2, players consisting of 10 people divided into two teams with object to destroy opponent team&acirc;€™s stronghold. Each player choose one hero,a in game character to play. DOTA 2 became Esport or Electronic Sport which have a tournament and league system. In tournament. There are a special mode called Captain Mode which became standard in professional game. In this mode every captain from every teams will choose heroes to Ban and Pick heroes to play. Heroes draft resulted from Captain Mode will be use to pproject the winner from the match.
	Genetic algorithm is one method of soft computing are often used in making projections. In this research, the genetic algorithm is used to project the heroes draft in Captain Mode. With factor consist of team track record, head-to-hed track record, team Meta Hero, player Meta Hero, Hero Patch, and Hero Pair will be calculated from heroes draft from genetic algorithm. The result will to project Winner from game. 
	The victory projection from heroes draft will be compared to Official result of professional game in tournament Shanghai Major Finals. From 10 match identically percentage of heroes draft projection for ban is 12,3% and for pick 21,2%. But for Acuracy for victory projection is up to 80%. 
",,,54741,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99874,,MUHAMMAD FAUZAN DZ,PERBANDINGAN KINERJA SNORT DAN SURICATA SEBAGAI SISTEM PENDETEKSI INTRUSI,,,"Sistem pendeteksi intrusi, Intrusion Detection System, IDS, Snort, Suricata","Khabib Mustofa, S.Si., M.Kom., Dr. Techn.",2,3,0,2016,1,"Keamanan menjadi salah satu faktor penting yang mulai diperhatikan saat akan membangun jaringan komputer. Intrusion detection system atau sistem pendeteksi intrusi merupakan salah satu usaha yang dikembangkan untuk tujuan melindungi sistem. Dua tools yang banyak digunakan dalam usaha membangun sistem pendeteksi intrusi adalah Snort dan Suricata.
Pada penelitian ini dilakukan perbandingan terhadap kinerja kedua tools pembangun sistem pendeteksi intrusi tersebut dengan menggunakan serangan scanning berupa Quick scan dan Intense scan dari Zenmap serta serangan DoS dari Xerxes. Parameter yang dibandingkan diantaranya meliputi banyaknya kejadian yang berhasil terdeteksi, lamanya waktu pemindaian yang dibutuhkan, serta banyaknya paket data yang berhasil dikirim.
Dari hasil penelitian didapatkan bahwa Snort IDS lebih unggul daripada Suricata IDS saat diberikan serangan scanning oleh Zenmap, baik Quick scan maupun Intense scan. Tetapi, keduanya menunjukkan tingkat kinerja yang sama saat diberikan serangan DoS oleh Xerxes.","Security becomes an important factor that began to be noticed when a computer network was built. Intrusion detection system is one of the ways which was developed for the purpose of protecting the computer system. Snort and Suricata are the most used tools to built an intrusion detection system.
A performance comparison between the tools was done in this research using scanning attacks such as Quick scan and intense scan from Zenmap and DoS attack from Xerxes. The parameters in this comparison comprised the number of events that successfully detected, the scanning time, and the amount of data packets that was successfully sent.
The result shows that Snort IDS is better than Suricata IDS when the scanning attack was given by Zenmap, both Quick scan and Intense scan. Though, both of them shows the same performance level when the DoS attack is given by Xerxes.",,,50393,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96293,,RIAN ADAM RAJAGEDE,Deep Learning untuk Pengenalan Pelafalan Huruf Hijaiyah Berharakat,,,"Deep learning, Convolutional Neural Network, klasifikasi, huruf hijaiyah","Afiahayati, S.Kom, M.Cs, Ph.D",2,3,0,2016,1,"Pengembangan software untuk pembelajaran Al-Quran banyak dikembangkan guna mempermudah pengguna untuk mempelajari Al-Quran. Namun, masih terdapat beberapa kendala, salah satunya adalah sulitnya membangun sistem yang mampu mengenali pelafalan sebuah teks berbahasa Arab oleh penggunanya.
Deep learning sebuah model jaringan syaraf tiruan yang akhir-akhir ini mulai ramai dikembangkan, telah menunjukkan hasil yang baik dalam meningkatkan akurasi pengenalan suara atau kasus-kasus lainnya. Deep learning berprinsip menggunakan model jaringan yang lebih dalam untuk meningkatkan akurasi pembelajaran. Untuk itu perlu ada beberapa penyesuaian seperti arsitektur, algoritma, dan optimasi-optimasi lain yang bisa mendukung deep learning.
Dalam penelitian ini diimplementasikan model pembelajaran deep learning utnuk penyelesaian kasus pengenalan pelafalan huruf Arab (huruf Hijaiyah berharakat). Arsitektur Convolutional Neural Network (CNN) digunakan untuk menyelesaiakan masalah tersebut. Juga digunakan beberapa metode optimasi regularisasi untuk meningkatkan akurasi dan mengurangi overfitting, seperti dropout dan L2 regularization. Penelitian ini menguji data rekaman suara dan mengklasifikasikan ke 10 kelas huruf hijaiyah berharakat. Hasil penelitian ini diperoleh akurasi 78.75% ketika dilakukan tanpa regularisasi dan mencapai 80.75% ketika menggunakan regularisasi.
","Development of software for Al-Qur'an learning has been developed in order to facilitate the users to learn the Al-Qur'an. However, there are problems in the process, one of the obstacles on the development of software-based Al-Quran is the difficulty of building a system that is able to recognize the pronunciation of an Arabic text by the user.
Deep learning, an artificial neural network model that lately began bustling developed, has shown good results to improve the accuracy of voice recognition or other softcomputing cases. The principle in deep learning models is to make a deeper network to improve the accuracy of learning. Because of that, there needs to be some adjustments such as architecture, algorithm, and other optimizations which can support deep learning.
In this research, deep learning model implemented for resolving cases of Arabic letter utterance recognition. Convolutional Neural Network (CNN) was used as architecture for solving the problem. Also some regularization optimizations were used to improve accuracy and reduce overfitting, such as dropout and L2 regularization. This research examined the sound recording and classified it into 10 classes Arabic letters. The results of this research, gained the accuracy 78.75% when performed without regularization and reached 80.75% when using regularization. ",,,46603,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
106533,,AMIRAH FARAH FAWZIYYAH,PREDIKSI KEMUNCULAN KEMBALI KANKER PARU-PARU BERDASARKAN DATA EKSPRESI GEN MICROARRAY MENGGUNAKAN HIDDEN NAIVE BAYES,,,"ekpresi gen, kemunculan kembali kanker, klasifikasi, diskritisasi, seleksi fitur, hidden na&Atilde;ƒ&Acirc;&macr;ve bayes","Aina Musdholifah, Ph.D.",2,3,0,2016,1,"Relapse, atau kemunculan kembali kanker setelah perawatan, adalah fenomena yang harus diwaspadai oleh penderita kanker. Jika bisa diperkirakan apakah pasien akan mengalami relapse atau tidak, dokter dapat merencanakan perawatan untuk pasien ke depannya. Salah satu cara untuk memprediksi relapse adalah dengan melakukan analisis ekpresi gen microarray.
Dalam penelitian ini dilakukan 3 tahap utama yaitu diskritisasi, seleksi fitur, dan klasifikasi. Metode diskritisasi yang digunakan adalah Weighted Proportioned k-Interval Discretization. Sedangkan seleksi fitur digunakan untuk mengurangi fitur data asli yang masif menjadi lebih sedikit namun tetap dapat digunakan untuk klasifikasi. Terdapat 3 metode seleksi fitur yang digunakan, yaitu: Forward Selection, Correlation-based Feature Selection, dan Information Gain-based Feature Selection. Untuk klasifikasi, digunakan metode Hidden Naive Bayes dan Naive Bayes sebagai pembandingnya. Performa dari model dievaluasi berdasarkan accuracy, sensitivity, dan specificity menggunakan Stratified Cross Validation dengan k=5.
Hasil terbaik Hidden Naive Bayes didapatkan dengan menggunakan menggunakan 70 fitur hasil dari teknik seleksi fitur Information Gain-based Feature Selection dengan accuracy 0.952, sensitivity 0.947, dan specificity 0.966 sedangkan hasil terbaik Naive Bayes didapatkan dengan menggunakan 70 fitur hasil dari teknik seleksi fitur Correlation-based Feature Selection dengan accuracy 0.955, sensitivity 0.958, dan specificity 0.947.","A cancer patient must be aware of a phenomenon called relapse. Relapse is condition where the cancer appeared again even after the treatment. If a relapse can be predicted before its appearance, it could help doctor to plan the next treatment for the patient beforehand. One way to predict the relapse is by doing microarray gene expression analysis.
In this research, there were 3 main procedures: discretization, feature selection, and classification. The discretization aim was to change the numeric data into categorical. The discretization method used in this research was Weighted Proportioned k-Interval Discretization. The feature selection was used to lessen the massive features amount while keeping its quality for classification. There were 3 methods of feature selection used in this research. They were Forward Selection, Correlation-based Feature Selection, and Information Gain-based Feature Selection. For classification, Hidden Naive Bayes was used as the main algorithm and Naive Bayes was used to as a comparison. The performance of the model was evaluated by accuracy, sensitivity and specificity using Stratified Cross Validation.
The best Hidden Naive Bayes performance was achieved using 70 features obtained from Information Gain-based Feature Selection with 0.952 accuracy, 0.947 sensitivity, and 0.966 specificity while the best Naive Bayes was achieved using 70 features obtained from Correlation-based Feature Selection with 0.955 accuracy , 0.958 sensitivity, and 0.947 specificity.",,,57108,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
101670,,DIMAS TRI NOVLI ANANTO,Analisis Sentimen Entitas pada Twitter Berbahasa Indonesia Menggunakan Support Vector Machine,,,"pengenalan entitas, analisis sentimen, hidden markov model, support vector machine, 10-fold cross validation","Arif Nurwidyantoro, S.Kom., M.Cs",2,3,0,2016,1,"Twitter adalah salah satu media sosial yang menghasilkan banyak data berupa status, berita, artikel dan lain sebagainya. Data Twitter memiliki entitas tertentu, diantaranya nama orang, lokasi dan organisasi. Selain memiliki entitas, status di Twitter terkadang juga mengandung sentimen tertentu, diantaranya positif, negatif dan netral. Analisis Sentimen yang tekandung dalam data twitter dapat dijadikan acuan oleh masyarakat dalam memberikan penilaian terhadap seseorang, lokasi dan suatu organisasi serta menilai akurasi dari hasil proses klasifikasi yang dilakukan sistem.  
Penelitian ini menggunakan metode Hidden Markov Model (HMM) untuk melakukan pengenalan entitas dan metode Support Vector Machine (SVM) untuk melakukan analisis sentimen hasil dari pengenalan entitas.
Pengujian sistem dengan menggunakan 10-fold cross validation untuk menentukan entitas dengan HMM menghasilkan rata-rata akurasi 90,09% dan 10-fold cross validation untuk analisis sentimen dengan SVM menghasilkan rata-rata akurasi tertinggi 71,64% pada kernel Gaussian RBF Gamma 5 dengan 100 atribut. Sedangkan pengujian dengan data uji menggunakan HMM menghasilkan akurasi tertinggi sebesar 96,75%. Untuk analisis sentimen menggunakan SVM menunjukkan bahwa penggunaan kernel dan jumlah atribut yang berbeda tidak memberikan pengaruh yang besar terhadap hasil. Jumlah atribut sebanyak 500 menghasilkan nilai akurasi tertinggi dengan nilai 79,26% pada algoritma binary occurence kernel Gaussian Gamma 0.5 dengan 500 atribut.
","Twitter is a on of social media that contain a certain data including news, article, and status. That kind of data contain information which can be send a positive, negative, or neutral sentiment. Sentiment analysis in twitter data can be used as a guide by society to give a rating to people, location, or organization. This rating can also evaluating the accuracy of the result of classification process performed by the system.
This research used Hidden Markov Model (HMM) method to named-entity recognition and Support Vector Machine (SVM) method to analyze the sentiment on its entity.
     The examination system process are using 10-fold cross validation to determine the information with HMM, in which produce 90,0% accuration and 10-fold cross validation to analysis information with SVM produce 71,46% accuration in kernel Gaussian RBF Gamma 5 with 100 attributes. However, by using HMM, the highest accuration arround 96,74%. For information analysis with SVM shows that kernel using and with different atribute does not give impact to the result. With 100 attributes, produce 79,26% accuracy in binary occurence algorithm Gaussian RBF Gamma 0.5 kernel shows with 500 attributes.
",,,52076,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99370,,MUHAMMAD HIDAYATULLAH,PERBANDINGAN PERFORMA TINYOS DAN CONTIKI SEBAGAI EMBEDDED OPERATING SYSTEM UNTUK PLATFORM LAUNCHPAD MSP430,,,"Wireless Sensor Network, Sistem Operasi Embedded, TinyOS, Contiki, Launchpad, MSP430, Energy Consumption, Memory Usage, Lines of Code, Datalogger.","Triyogatama W. W., S.Kom., M.Kom",2,3,0,2016,1,"Perkembangan yang pesat terjadi dalam dunia teknologi khususnya Wireless Sensor Networks, Sistem Operasi dan Platform yang digunakan untuk membangun dan mengembangkan WSN sangat beragam, salah satunya adalah Launchpad MSP430. Sebagai Sistem Operasi Embedded untuk platform pada WSN, TinyOS dan Contiki mempunyai performa yang dikategorikan dalam proses Development dan Execution. Maka dari itu akan dilakukan penelitian perbandingan performa TinyOS dan Contiki sebagai Sistem Operasi Embedded pada platform Launchpad. Hasil dari penelitian ini diharapkan dapat digunakan oleh pengembang sebagai referensi atau acuan pada saat menentukan Sistem Operasi Embedded yang akan digunakan ketika akan membangun dan mengembangkan sebuah Wireless Sensor Networks.   	
	Pada penelitian ini dilakukan pengujian terhadap Sistem Operasi Embedded TinyOS dan Contiki untuk mengetahui bagaimana perbandingan performa dari kedua kedua sistem operasi tersebut untuk platform Launchpad MSP430. Parameter yang digunakan dalam penelitian ini adalah Energy Consumption, Memory Usage, dan Lines of Code. Program aplikasi yang akan dijalankan oleh Launchpad adalah Blank, Blink, dan Sense. Penelitian ini menggunakan Datalogger sebagai alat untuk proses pengumpulan data yang digunakan untuk mengetahui seberapa besar energi yang dikonsumsi oleh Launchpad saat menjalankan program aplikasi.
	Dari data hasil pengujian, TinyOS menunjukkan hasil konsumsi energi dan pemakaian memori yang lebih rendah, sedangkan Contiki membutuhkan jumlah baris kode yang lebih sedikit pada pengembangan program aplikasi. Sehingga hasil dari penelitian ini menunjukkan bahwa TinyOS mempunyai performa yang lebih baik pada proses Execution, sedangkan Contiki mempunyai performa yang lebih baik pada proses Development sebagai Sistem Operasi Embedded untuk platform Launchpad MSP430.  	   
","The rapid development of technology, particularly in the world of Wireless Sensor Networks, platform used to build and develop WSN is very diverse, one of them is the MSP430 Launchpad. As an Embedded Operating System for the WSN platform, TinyOS and Contiki has a performance that categorized as Development and Execution. Therefore this research will be conducted performance comparisons TinyOS and Contiki as Embedded Operating System on Launchpad platform. The results of this research can be used by developers as a reference or benchmark when choosing Embedded Operating System that will be used when building and developing a Wireless Sensor Networks.
In this research, Embedded Operating System TinyOS and Contiki will be tested to find out how the comparison of the performance of the two operating systems for Launchpad MSP430 platform. The parameters used in this research is Energy Consumption, Memory Usage, and Lines of Code. Application programs run by Launchpad is Blank, Blink, and Sense. This research uses the datalogger as a instrument for process data collection that is used to determine how much energy is consumed by Launchpad when running the application program.
From the test data, TinyOS shows the results of lower energy consumption and lower memory consumption, while Contiki requires a few lines of code than TinyOS on the development of application programs. So the results of this research indicate that the TinyOS has a better performance on the Execution, while Contiki has a better performance on the Development as an Embedded Operating System for the Launchpad MSP430 platform.`	`	 	 
",,,49707,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107309,,DENI PARULIAN LOI,SISTEM PAKAR UNTUK MENDIAGNOSIS PENYAKIT PARU-PARU YANG DISEBABKAN OLEH ROKOK MENGGUNAKAN METODE FORWARD CHAINING DENGAN CERTAINTY FACTOR,,,"sistem pakar, paru-paru, forward chaining, certainty factor","Retantyo Wardoyo, Drs., M.Sc., Ph.D.",2,3,0,2016,1,"Kebiasaan merokok dalam jumlah yang berlebihan dapat menyebabkan berbagai macam penyakit, khususnya paru-paru. Merokok dapat mengakibatkan perubahan fungsi, struktur jaringan dan saluran pernapasan pada paru-paru. Selain itu, apabila seseorang dengan faktor resiko penyakit paru-paru memiliki kebiasaan merokok akan semakin  meningkatkan peluang penyakit paru-paru yang lebih parah. Penderita penyakit paru-paru biasanya tidak menyadari gejala penyakit tersebut. Seiring dengan tingginya tingkat kematian akibat penyakit paru-paru yang disebabkan oleh asap rokok dan terbatasnya jumlah pakar yang ada, dibutuhkan sistem pakar yang dapat digunakan untuk membantu diagnosa penyakit.
Sistem pakar adalah sistem yang meniru kepakaran seorang pakar dalam suatu bidang. Sistem yang dibangun dalam penelitian ini adalah sistem pakar untuk mendiagnosa penyakit paru-paru yang disebabkan oleh rokok. Sistem ini dibuat dengan metode forward chaining dengan menggunakan metode certainty factor untuk masalah ketidakpastian. Sistem dibuat dengan dua jenis user, yaitu pakar dan paramedis. Pakar dapat melakukan update pada pengetahuan yang digunakan untuk proses inferensi, sementara paramedis hanya dapat melakukan konsultasi penyakit terhadap pasien.
Proses pengujian dilakukan dengan melakukan pencocokan dari seluruh input terhadap output sistem. Input adalah gejala-gejala yang dialami oleh pasien serta tingkat kepercayaan masing-masing gejala tersebut. Output yang diperoleh berupa jenis penyakit, keterangan penyakit, saran penanganan penyakit, gejala penyakit yang terpilih dan tingkat kepercayaan hasil diagnosa.
","Smoking habits in excessive amounts can cause various diseases, particularly lung diseases. Smoking  can lead to lung function disorders, affection of lung tissue, and disorders of the respiratory system. Therefore, people with risk factor for lung disease can increases the risk of the diseases if exposed to smoke. Along with the high rate of deaths from lung diseases caused by smoking and a limited number of experts, an expert system can be used to assist in making a diagnosis of lung diseases.
Expert system is a system that emulate the expertise of human experts in a field. The system is made to diagnose lung disease caused by smoking. The inference method used in this research is forward chaining. The system is built by implementing certainty factor for the uncertainty problem. The system is made with two types of user: experts and paramedics. Experts can update the knowledge used for inference process, while paramedics can only do the disease consultation to the patients.
The testing process is done by matching the entire input to the output of the system. Inputs are the symptoms experienced by the patient and the confidence level of each of these symptoms. The output will be shown in the formulir of diseases, causes, treatments, symptoms experienced and confidence level of diagnosis.
",,,57884,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
97583,,ERRIC ALFAJRI,APLIKASI PENGELOMPOKAN HASIL PENCARIAN DOKUMEN BERITA MENGGUNAKAN ALGORITMA BISECTING K-MEANS,,,"clustering, bisecting k-means, carrot2, google custom search","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2016,1,"Penelusuran dokumen berita merupakan aktivitas umum yang dilakukan oleh pengguna internet.  Dalam mendapatkan informasi, pengguna dapat menggunakan search engine yang dapat mengembalikan hasil pencarian berdasarkan sebuah kata kunci. Pencarian informasi dapat menjadi sebuah permasalahan karena terdapat beberapa subtopik dari kata kunci dicampur bersama-sama dalam satu daftar panjang. Fitur clustering dapat diimplementasikan terhadap hasil pencarian untuk mempersempit pilihan pencarian melalui teknik penyajian per-cluster.
Pada penelitian ini, telah dibuat aplikasi search engine dokumen berita. Aplikasi dilengkapi dengan fitur pengelompokan hasil pencarian. Terdapat beberapa pilihan sumber situs berita yang dijadikan sumber pencarian dokumen.  Google Custom Search dimanfaatkan untuk mendapatkan dokumen sesuai kriteria pencarian, setelah itu dimplementasikan clustering terhadap hasil pencarian menggunakan algoritma bisecting k-means dengan library yang disediakan oleh carrot2. 
Hasil dari penelitian ini adalah sebuah aplikasi search engine dokumen berita secara online yang dapat menyajikan hasil pencarian berdasarkan kelompok-kelompok. Setiap kelompok diwakili oleh label kelompok. Evaluasi dilakukan terhadap hasil clustering, didapatkan precision 0,57 dan cluster label quality 0,78.
","Search for news document is common activity of internet users. To reach the information, user can use internet search engines, which can return search results depends on the keyword. Finding the information needed on the search results can be a problem, because several of subtopic are mix together in a long list of search results. Clustering feature can be implemented in search engine to present the search results in several clusters.
In this research, a search engine application of news document is build. A search result clustering feature is added to the search engines. There are news sites which are used as sources for searching news documents. Google Custom Search is used for searching documents, then clustering is implemented on the search results using bisecting k-means algorithm with a library supplied by carrot2. 
This research produces a search engine application of news documents which can present search results based on clusters. Each cluster is represented by a label cluster. The evaluation was done on the clustering results, found that the precision is 0.57 and the cluster label quality is 0.78.
",,,47912,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
106033,,MIA RAHIMITI,ANALISIS PERFORMA PROTOKOL ROUTING AODV DAN DSR PADA JARINGAN AD-HOC UNTUK VIDEO STREAMING,,,"MANET, AODV, DSR, video streaming, throughput, delay, packet loss","Mardhani Riasetiawan, S.E., M.T.",2,3,0,2016,1,"Mobile Ad-hoc Network (MANET) adalah sebuah jaringan wireless tanpa infrastruktur yang bersifat dinamis dan setiap node dapat bergerak bebas ke segala arah. Node dapat berperan sebagai router, yaitu penghubung antara node satu dengan yang lainnya. Protokol Ad-hoc On-demand Distance Vector (AODV) dan Dynamic Source Routing (DSR) merupakan protokol reaktif yang menggunakan mekanisme route discovery sehingga memiliki delay yang kecil.
Dalam penelitian ini, dilakukan perbandingan performa protokol AODV dan DSR. Penelitian dilakukan dengan menggunakan 4 buah laptop yang dihubungkan secara ad-hoc berdasarkan skenario multihop dan route discovery. Pengujian dilakukan dengan video streaming pada kondisi jaringan yang real menggunakan format dan ukuran video yang berbeda serta durasi streaming yang berbeda. Parameter pengukuran yang digunakan adalah throughput, delay dan packet loss.
Hasil penelitian menunjukkan bahwa protokol AODV lebih baik dibandingkan protokol DSR. Delay dan packet loss protokol DSR lebih besar dibandingkan protokol AODV. Penambahan jumlah node memengaruhi perolehan throughput dan delay masing-masing protokol yang semakin besar.","Mobile Ad-hoc Network (MANET) is a infrastructureless wireless network that is dynamic and nodes can move freely in any direction. Node can act as a router, that is link one node to another. Protocol Ad-hoc On-demand Distance Vector (AODV) dan Dynamic Source Routing (DSR) are reactive protocol that use route discovery mechanism so they have small delay.
In this research, comparing performance of AODV dan DSR protocols. Research was conducted by using 4 laptops that is connected in ad-hoc based on multihop and route discovery scenarios. Testing was conducted by video streaming on real-network  using different format and size of video, and different duration of streaming. The parameter of measurement are throughput, delay and packet loss.
The results show that is AODV protocol is better than DSR protocol. Delay and packet loss DSR protocol is higher than AODV protocol. Increasing nodes affect acquisition of throughput and delay each protocols increasing.",,,56673,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
109873,,SAIFUL BAKHRI,PERBANDINGAN AKURASI SUPPORT VECTOR MACHINE DAN DECISION TREE UNTUK PREDIKSI HASIL PERTANDINGAN DOTA 2 BERDASARKAN PEMILIHAN HERO,,,"Akurasi prediksi, SVM, Decision Trees, DOTA 2, pemilihan hero","Medi, Drs., M.Kom.",2,3,0,2016,1,"Permainan daring saat ini sangat populer di kalangan remaja bahkan orang dewasa. Salah satu genre permainan yang paling diminati adalah MOBA (Multiplayer Online Battle Arena). DOTA 2 merupakan salah satu permainan MOBA yang memiliki peminat yang cukup tinggi, yaitu mencapai 12 juta pemain. DOTA 2 memiliki karakteristik permainan yang unik, yaitu tiap tim memilih masing-masing lima hero atau karakter yang bisa dimainkan dari 111 hero yang tersedia. Pemilihan hero tersebut dapat menjadi strategi awal dan dapat mempengaruhi hasil akhir pertandingan.
Metode pembelajaran mesin digunakan untuk membuat model yang dapat memprediksi label dari suatu dataset tertentu. Dua metode yang bisa digunakan adalah Support Vector Machine (SVM) dan Decision Tree (DT). SVM dan DT merupakan dua metode supervised machine learning yang mempelajari dataset yang sudah ada dan membuat model prediksi untuk dataset tersebut. Dataset yang dibentuk adalah dataset hero pilihan dari tiap pemain dalam DOTA 2, dan dari dataset tersebut dilakukan feature extraction untuk membuat fitur baru yaitu roles, attack type, dan attribute. Dataset didapat dari pengumpulan data yang dilakukan menggunakan Steam Web API.
Hasil penelitian ini menunjukkan bahwa metode SVM lebih unggul dalam memprediksi hasil pertandingan dengan nilai akurasi 65,78% dengan menggunakan fitur hero ID. Metode SVM mengungguli metode DT di setiap dataset yang dibentuk, yaitu dataset fitur tunggal, dataset gabungan dua fitur, dataset gabungan tiga fitur, dan dataset gabungan seluruh fitur. Terbukti bahwa metode SVM mampu menghasilkan akurasi yang lebih tinggi dibandingkan metode DT untuk memprediksi hasil pertandingan DOTA 2 berdasarkan pemilihan hero.","Online games are very popular among teenagers and even adults in the recent years. One of the games&acirc;€™ genre is MOBA (Multiplayer Online Battle Arena). DOTA 2 is one of the MOBA&acirc;€™s games which have pretty high enthusiasts, reaching about 12 million players. DOTA 2 has a unique gameplay characteristic that every team has to pick five out of 111 heroes available. Heroes pick could be the first strategy and could influence the outcome of a match.
Machine learning methods are used to build a prediction model from a certain dataset. Support Vector Machine (SVM) and Decision Tree (DT) are among them. SVM and DT are two supervised machine learning methods which learn from the available datasets and build a prediction model based on the datasets. Heroes pick on DOTA 2 matches are formed into a dataset, and from the dataset, new features; roles, attack type and attribute are formed as datasets using feature extractions. The dataset is formed from data samples collected using Steam Web API.
Results show that SVM method are better at predicting match outcomes with accuracy score of 65,78% using the hero ID feature. SVM method is better than DT in every dataset that was formed, which are single feature datasets, two features datasets, three features datasets, and all features dataset. This proves that SVM method can produce better accuracy than DT in predicting DOTA 2 match outcome based on heroes pick.",,,60523,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
102453,,TASHIA INDAH NASTITI,WEB APPLICATION VULNERABILITY ASSESSMENT USED OWASP APPLICATION SECURITY VERIFICATION STANDARD (ASVS) FOR UGM WEBSITES,,,"OWASP, Web Application, Vulnerability, Assessment, Security, Google Hacking Database","Mardhani Riasetiawan, S.E., Akt., M.T.",2,3,0,2016,1,"Pesatnya pertumbuhan penggunaan aplikasi web yang disebabkan serangan cyber terhadap itu juga meningkat. Ini akan memberikan dampak buruk bagi perusahaan atau organisasi besar, termasuk organisasi pendidikan.Dalam situasi seperti ini, penting untuk membuat aplikasi web lebih aman dari sebelumnya. Oleh karena itu, penilaian kerentanan sangat diperlukan untuk menentukan kerentanan yang terjadi pada aplikasi web. Penelitian ini menyediakan penilaian kerentanan untuk Website UGM dan analisis kerentanan umum yang terjadi di Website UGM. Pengujian utama dalam penelitian ini adalah search engine discovery menggunakan google hacking database dan vulnerability scanning menggunakan Zed Attack Proxy. Hasil pengujian akan menjadi pedoman untuk menentukan 10 kerentanan tertinggi dan merancang solusi untuk mengurangi kemungkinan kerentanan di Website UGM. Untuk mendukung solusi tersebut, OWASP Application Security Verification Standard atau The Open Web Application Security Project ASVS akan digunakan untuk menjadi standar keamanan aplikasi web dalam penelitian ini.","The rapid growth of web application usage caused the cyberattacks against it have also increased. It will give bad impact for large company or organization, including educational organization. In situation like this, it&acirc;€™s important to make web application more secure than before. Therefore, vulnerability assessment is very needed to determine the vulnerabilities that occurs in web application. This research provides the vulnerabilities assessment for UGM Websites and analyzes the common vulnerabilities that occurs in UGM Websites. The main testing approaches in this research are search engine discovery using google hacking database and vulnerability scanning using Zed Attack Proxy. The testing result will be a guideline to determine top 10 common vulnerabilities and designing the solutions to decrease the possibility of vulnerabilities in UGM Websites. To support the solutions, OWASP Application Security Verification Standard or The Open Web Application Security Project ASVS will be used to be a web application security standard in this research.",2016-08-26 00:00:00,53,52917,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
97086,,HENDI RUSFANDI,IMPLEMENTASI PENCARIAN FULL TEXT PADA DATA BERITA ONLINE MENGGUNAKAN SISTEM INDEXING APACHE SOLR,,,"indexing, pencarian full text, pencarian berita, MySQL indexing, Solr, realtime indexing, Media Monitoring","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2016,1,"Media Monitoring merupakan aktivitas untuk melakukan monitoring terhadap berita yang diterbitkan oleh media online. Kegiatan monitoring biasa dilakukan oleh perusahaan penyedia barang/jasa untuk mengetahui bagaimana reaksi pasar terhadap barang/jasa yang ditawarkannya. Untuk membantu kegiatan monitoring, dibangun sebuah aplikasi untuk membantu mengoleksi, mencari, review, dan report berita. Data berita online yang besar membuat beberapa proses pada aplikasi Media Monitoring terganggu. Salah satu masalah yang terjadi yaitu proses pencarian berita pada aplikasi yang lambat. Masalah tersebut terjadi karena tingginya transaksi read and write pada basis data aplikasi. 
Untuk mengurangi beban kerja basis data, maka pencarian berita perlu dilakukan di luar basis data. Teknik yang dapat digunakan yaitu melakukan indexing data berita di luar dari basis data dan mencari berita menggunakan teknik pencarian full text. Pada penelitian ini, Apache Solr digunakan sebagai search server. Analisis kinerja pencarian full text antara MySQL dan Solr dibandingkan dengan melakukan pengujian menggunakan berbagai kata kunci full text. Analisis mengenai metode delta indexing dan realtime indexing pada Solr juga dilakukan dengan simulasi penambahan berita dan update index secara berkala serta melakukan pencarian full text untuk mengetahui metode indexing yang memenuhi kebutuhan aplikasi Media Monitoring.
Hasil pengujian pencarian berita diperoleh hasil bahwa pencarian full text pada Solr lebih cepat 2,5 kali lipat dibandingkan dengan MySQL. Metode realtime indexing dirasakan lebih cocok untuk diterapkan pada aplikasi Media Monitoring yang memiliki pertumbuhan data yang cepat, membutuhkan ketersediaan data yang up-to-date, dan pemakaian resource hardware yang optimal.","Media Monitoring is an activity to monitor news published
by online media. Monitoring activity is usually done by companies to know
publics reaction over offered goods or service. To assist the monitoring activity,
an application is built to help collecting, searching, reviewing, and reporting the
news. Big online news data makes some processes of the application disrupted.
One of the problems that occur is news searching process on the application that is
slowing down. This problem occurs because of high read and write transaction on
the application's database.
To reduce the database workload, the news searching process should
be done outside of the database. Techniques that can be used are to do
news indexing outside the database and to search the news using full text
search technique. In this research, Apache Solr is used as a search server.
Performance analysis of full text search between MySQL and Solr by testing with
various full text keyword. An analysis of delta indexing and real-time
indexing method is also conducted by simulating the addition of news data to
database, periodically update the index and doing full text searching to meet
Media Monitoring application's needs.
The news search result produced a result that the full text searching on
Solr is 2,5 times faster than MySQL. The real-time indexing method is more
suitable to implement on the Media Monitoring application that has rapid data
growth, requires up-to-date data availability, and optimal use of hardware
resource.",,,47348,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
105792,,SATRIO GHAFFAR F,Permainan Berbasis Lokasi dengan Deteksi Fitur ORB,,,"Android, Permainan Berbasis Lokasi, Google Maps API Android v2, Deteksi Fitur, ORB","Arif Nurwidyantoro, M.Cs",2,3,0,2016,1,"Permainan berbasis lokasi merupakan salah satu kategori permainan
dimana jalannya permainan melalui perpindahan lokasi pemainnya. Jenis
permainan ini biasanya memanfaatkan teknologi GPS, QR code, maupun NFC
untuk menentukan lokasi penggunanya sehingga lebih banyak dikembangkan
pada perangkat yang mendukung teknologi tersebut seperti smartphone dan
tablet. Permainan ini mengajak pemainnya untuk bermain dengan tidak hanya
diam di satu tempat.
Pada penelitian ini, akan dikembangkan jenis permainan berbasis lokasi
pada platform android yang memanfaatkan teknologi GPS untuk menentukan
lokasi pemainnya dan komputer visi seperti deteksi fitur. Pada aplikasi ini, deteksi
fitur digunakan untuk membandingkan dua buah gambar apakah sama atau tidak.
Metode yang digunakan untuk melakukan deteksi fitur yaitu ORB.
Berdasarkan hasil pengujian, semua fitur aplikasi dapat dijalankan sesuai
dengan rancangan yang telah dibuat. Dari pengujian pencocokan gambar, aplikasi
dapat mendeteksi gambar dengan perubahan rotasi hingga 90 derajat serta skala
hingga 1:2.","Location based game is a type of game in which the progress of the game
is based on player's location. This type of game usually uses technology such as
GPS, QR code, or NFC to determine the player's location, make this game usually
developed for devices that support those technology such as smartphone and
tablet. This game persuade the player to play not only in one location.
This research will develop a type of location based game for android
platform using GPS and computer vision like feature detection to determine the
player's position. Feature detection will be used to compare two images to
determine if the two images are same or not using ORB method.
Based on the test result, all features of the application can be run properly
according to system design. From the image matching test, application can detect
an image with rotation until 90 degree, and scale with ratio until 1:2.",,,56374,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
95297,,YUSUF SYAIFUDIN,IDENTIFIKASI KALIMAT KUTIPAN DARI TEKS BERITA ONLINE BERBAHASA INDONESIA DENGAN METODE BERBASIS ATURAN,,,"ekstraksi kutipan, ekstraksi opini, rule-based, ekstraksi informasi","Arif Nurwidyantoro, S.Kom., M.Cs",2,3,0,2016,1,"Berita merupakan suatu sumber informasi yang sering digunakan sebagai data dalam ekstraksi informasi. Pernyataan atau opini narasumber merupakan salah satu informasi dalam berita yang cukup penting untuk mengetahui hal-hal yang dikatakan oleh seseorang atau organisasi terkait peristiwa tertentu. Pernyataan narasumber ini menjadi penting karena merupakan salah satu informasi yang dapat mempengaruhi pikiran pembaca berita. Selain itu, pernyataan tersebut juga dapat menunjukkan sentimen pembicara terkait suatu kejadian yang diberitakan. Pernyataan tersebut sering ditulis menggunakan metode kutipan, baik kaidah penulisan kutipan langsung maupun tidak langsung.

Penelitian ini menggunakan pendekatan metode berbasis aturan (rule-based method) di mana kutipan-kutipan dalam berita diekstrak berdasarkan kaidah penulisan kutipan yang umum terdapat pada berita. Setiap pola penulisan kutipan yang ditemukan dalam kumpulan data berita dibuat menjadi satu kelas aturan tersendiri yang mewakili pola tersebut. Fitur yang digunakan pada pembuatan kelas aturan ialah keberadaan reporting verb dan entitas dalam suatu kalimat. 

Hasil penelitian ini menunjukkan bahwa seluruh kelas aturan yang dibuat telah mampu mengekstraksi kalimat pernyataan dengan cukup baik. Sistem mampu mendeteksi kalimat pernyataan dengan akurasi sebesar 88,618%, presisi sebesar 99,013% dan recall sebesar 79,936%. 
","News is a source of information that is often used as data in information extraction. Interviewees statement or opinion is one of the sources of information in the news, which is quite important to know things that are said by a person or organization related to a specific event. Interviewees statement is important because it is one of the information that can affect the mind of a newsreader. Moreover, the statement may also indicate the speaker's sentiment related to an event that was reported. The statement is often written using quotation, by writing rules of either direct quotes or indirect quotes.

This study uses a rule-based method which quotes in the written news are extracted based on common rules contained in the news. Each pattern of writing quotations found in a collection of news data is made into a class of rule that represents the pattern. Features used in the class of rules is the existence of reporting verb and entities in a sentence.

The result of this study indicate that the entire class rules have been made are able to extract sentence statement pretty well. The system is able to detect sentence statement with an accuracy of 88.618%, precision of 99.013% and recall 79.936%.",,,45510,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104257,,IMAM BUDI SANTOSO,IMPLEMENTASI METODE VECTOR SPACE MODEL DALAM SISTEM DIAGNOSA PENYAKIT INFEKSI BERDASARKAN ILMU PATOFISIOLOGI,,,"Penyakit Infeksi, Vector Space Model, Text Mining, Patofisiologi"," Afiahayati, S.Kom., M.Cs., Ph.D",2,3,0,2016,1,"Salah satu penyakit yang membahayakan dan berkembang dengan cepat adalah penyakit infeksi yang menyerang tubuh manusia. Penyakit infeksi ditangani dengan berbagai jenis tindakan dan pendekatan khusus dalam ilmu kesehatan, tergantung tanda dan gejala yang dialami oleh pasien. Salah satu cara untuk mendeteksi penyakit infeksi secara cepat dan tepat adalah dengan membangun teknologi aplikasi yang dapat mempermudah para praktisi di bidang kesehatan dalam mempelajari dan menentukan kemungkinan penyakit yang dialami oleh pasien sebagai tolak ukur dalam proses penanganannya sekaligus menyediakan informasi tentang penyakit berdasarkan ilmu patofisiologi. Pada penelitian ini, dibangun suatu sistem diagnosa penyakit infeksi berdasarkan tanda dan gejala dalam ilmu patofisiologi. 
Metode komputasi yang digunakan dalam penelitian ini adalah Vector Space Model (VSM). Metode tersebut merupakan salah satu bagian dalam text mining yang akurat. Metode VSM dapat melihat tingkat kesamaan atau kedekatan suatu kata dengan melakukan pembobotan. Dokumen gejala yang terdapat pada basis data dipandang sebagai sebuah vektor yang memiliki jarak dan arah. Tingkat kecocokan antara data tanda dan gejala yang dimasukkan oleh pengguna dan dokumen yang terdapat dalam basis data dihitung dengan rumus cosine similarity. Dalam implementasinya, penelitian ini menggunakan beberapa algoritma pendukung, yaitu algoritma stemming Nazief Adriani dan algoritma pembobotan TF-IDF (Term Frequency-Inverse Document Frequency).
Pengujian sistem dilakukan dengan menggunakan data yang dikumpulkan dari praktisi kesehatan. Terdapat 100 data penyakit yang digunakan dalam pengujian ini yang berdasarkan pada tanda dan gejala penyakit. Hasil penelitian ini menunjukkan bahwa pengujian berdasarkan term memiliki akurasi 90% dengan nilai similaritas penyakit yang cocok dengan diagnosa praktisi kesehatan mencapai 0.823281111, sedangkan pengujian berdasarkan frasa memiliki akurasi 88% dengan nilai similaritas penyakit yang cocok dengan diagnosa praktisi kesehatan mencapai 0.776355556.
","One of the dangerous diseases and expanding rapidly is an infectious disease that attacks the human body. Infectious diseases treated by various types of actions and specific approach in the health sciences, depend on the signs and symptoms experienced by the patient. One way to detect infectious diseases quickly and accurately is to build technology applications that can facilitate practitioners in the field of health in studying and determining the likelihood of disease experienced by patients as a reference in the handling process as well as provide information about disease based on pathophysiology science. In this study, it has built a system of infectious disease diagnosis which is based on the signs and symptoms in the science of pathophysiology. 
The computational method which is used in this research is Vector Space Model (VSM). The method is one part inaccurate text mining. VSM methods can see the degree of similarity or proximity of a word by weighting. Documents symptoms which are contained in the database are seen as a vector that has the distance and direction. A match rate between the signs and symptoms of the data entered by the user and the documents contained in the database are calculated by the formula of cosine similarity. In the implementation, this study uses several supports of algorithms, namely stemming Nazief-Adriani algorithm and TF-IDF (Term Frequency-Inverse Document Frequency) weighting algorithm. 
System testing was done by using data collected from health practitioners. There were 100 disease data used in these tests, which are based on disease signs and symptoms. The study result showed that the test based on term reached 90% accuracy which had a similarity value of 0.823281111 that matched with the diagnosis of the health practitioners, while the other test result was based on phrase had an accuracy of 88% which had a similar value that matched the diagnosis of health practitioners reached 0.776355556.",,,54833,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98370,,RAFINKANISA WITARAYOGA ,PREDIKSI TINGKAT KEMISKINAN INDONESIA MENGGUNAKAN ALGORITMA GENETIKA ,,,"prediksi, kemiskinan, inflasi, pdb, algoritma genetika","Faizah, S.Kom., M.Kom",2,3,0,2016,1,"Penduduk miskin dikategorikan oleh BPS sebagai penduduk yang memiliki rata-rata pengeluaran per kapita per bulan di bawah garis kemiskinan. Pemerintah pun giat melakukan upaya pengentasan kemiskinan. Salah satu caranya dengan membuat berbagai program sosial. Besaran bantuan yang diberikan pun juga berubah-ubah berdasarkan gejolak ekonomi seperti kenaikan inflasi maupun menurunnya pertumbuhan ekonomi yang terjadi di Indonesia. Oleh karena itu, pemerintah harus dapat memprediksi tingkat kemiskinan berdasarkan inflasi dan PDB. Hasil prediksi tersebut dapat digunakan sebagai acuan pemerintah dalam merumuskan kebijakan-kebijakan penanggulangan kemiskinan maupun kebijakan-kebijakan fiskal yang dapat mempengaruhi kestabilan ekonomi Indonesia.
Penelitian ini membahas tentang prediksi tingkat kemiskinan menggunakan metode algoritma genetika. Berdasarkan hasil beberapa pengujian, ditemukan nilai error yang mengindikasikan terdapat faktor lain yang mempengaruhi tingkat kemiskinan selain inflasi dan PDB. Nilai error tersebut berupa sebuah fungsi regresi linear sederhana. Dengan penambahan nilai error tersebut, pengujian hasil prediksi tahun 2015 mengalami peningkatan akurasi dari 60,58% menjadi 90,24% Dengan menggunakan parameter terbaik hasil dari pengujian sistem ini yakni 10 kromosom, 200 generasi, pc sebesar 0,7, dan pm sebesar 0,05, akurasi pengujian tingkat kemiskinan pada tahun 2015 sebesar 90,93%.","BPS - Statistics Indonesia defines poor people as a person whose expenditure per capita per month below the poverty line. The government has tried many ways to end poverty such as making social programs. The amount of the support that government gives always changes based on the economic changes such as high inflation rates and low growth of domestic products that occur in Indonesia. Therefore, the government should be able to predict the rate of poverty based on inflation rates and growth domestic products. The prediction can be used by the government to make policies in order to stabilize the national economic by controlling the poverty rate and fiscal policies. 
This thesis discusses about the prediction of poverty rates using genetic algorithm. Based on several tests, it is found an error value that indicates another factor which affect poverty rate besides the inflation rate and growth domestic product. The error value is modelled by simple linear regression. With additional factor, the test results for 2015 gained accuracy improvement from 60,58% to 90,24%. With the result of testing that indicate optimal parameter in this system such as 10 individuals, 200 generations, crossover probability (cp) 0,7, and mutation probability (mp) 0,05, the accuration of forecasting result for 2015 is 90,93%.
",,,48656,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104770,,DHAFIN MUKTAFIN,Network Performance Analysis of Internet of Things Infrastructure for Remote Smart Room\newline using Kaa Platform,,,"Internet of Things, Smart Environment, IoT infrastructure, Quality of Service, Kaa Platform","Mardhani Riasetiawan, M.T.",2,3,0,2016,1,"Arsitektur jaringan adalah bagian penting dalam membangun Cerdas Lingkungan. Lingkungan jaringan IoT ditandai dengan keberagaman unsur jaringan. Jaringan yang heterogen memberikan lebih dari satu aplikasi, layanan, atau node. Ini berarti tidak hanya keberadaan beberapa jenis lalu lintas dalam jaringan, tetapi juga kemampuan jaringan tunggal untuk mendukung semua aplikasi ini tanpa mengorbankan kualitas kinerja jaringan pada aplikasi yang lain. Lingkungan jaringan IoT perlu dianalisa untuk mendukung sensitivitas ketepatan waktu dengan kondisi nyata dari aplikasi.

Pada penelitian ini penulis membuat desain topologi jaringan untuk Smart Environment menggunakan Kaa Platform sebagai middleware. Pada penelitian ini penulis juga membangun jaringan sensor menggunakan Raspberry Pi sebagai klien dan komputer berbasis Linux sebagai penerima data dari klien. Selanjutnya, peneliti juga melakukan analisis kinerja jaringan berdasarkan empat skenario pengujian dengan menggunakan tiga parameter seperti throughput, delay time, dan packet loss.

Berdasarkan penelitian ini, dari Skenario 1 diperoleh bahwa nilai rata-rata throughput, delay time, dan packet loss yaitu 83,74 Kbps, 33,2 ms, dan 0%. Kemudian dari Skenario 2 diperoleh bahwa nilai rata-rata throughput, delay time, dan packet loss yaitu 94,29 Kbps, 29,3 ms, dan 0%. Sedangkan dari Skenario 3 diperoleh bahwa nilai rata-rata throughput, delay time, dan packet loss yaitu 85,62 Kbps, 34,9 ms, dan 0%. Lalu yang terakhir, dari Skenario 4 diperoleh bahwa nilai rata-rata throughput, delay time, dan packet loss yaitu 80,35 Kbps, 36,89 ms, dan 0%.","The network architecture is the important part in building Smart Environment. The IoT networking environment is characterized by the heterogenity of networks. Heterogeneous networks provides more than one distinct applications, service, or nodes. This implies not only the existence of multiple traffic types within the network, but also the ability of a single network to support all of these applications without compromising QoS for any of them. IoT networking environment needs to be analyzed in order to support the real-time sensitivity of the applications.

This research was implemented network topology design for Smart Environment using Kaa Platform as middleware. In this research the author built sensor networks using Raspberry Pi as client and a Linux-based computer as a data sink. Furthermore, the researcher also conducts network performance analysis based on the four testing scenarios by using three parameters such as Throughput, Delay time, and packet loss.

Based on this research, from the Scenario 1 obtained that the average value of throughput, delay time, and packet loss are 83.74 Kbps, 33.2 ms, and 0%. Then from the Scenario 2 obtained that the average value of throughput, delay time, and packet loss are 94.29 Kbps, 29.3 ms, and 0%. While from the Scenario 3 obtained that the average value of throughput, delay time, and packet loss are 85.62 Kbps, 34.9 ms, and 0%. And the last, from the Scenario 4 obtained that the average value of throughput, delay time, and packet loss are 80.35 Kbps, 36.89 ms, and 0%.",,,55299,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100931,,MUH KOZIN,APLIKASI PERSONAL ASSISTANT BERBASIS AIML,,,"Personal Assistant, AIML, Android","Sri Mulyana, Drs., M.Kom ",2,3,0,2016,1,"Personal Assistant adalah perangkat lunak yang bekerja secara semi-otonom. Fungsi dari sistem ini adalah menggantikan peran pengguna, membuat gambaran tentang apa yang diinginkan pengguna dan memberikan layanan kepada pengguna ketika dibutuhkan. Salah satu permasalahan dalam mebangun sebuah sistem Personal Assistant adalah pemilihan aksi yang akan dilakukan oleh sistem terhadap masukan dari pengguna. 
Dalam pengembangan Personal Assistant tersebut, digunakan AIML sebagai basis pengetahuannya. Namun, kemampuan AIML sebagai basis pengetahuan terbatas pada memberi keluaran berupa teks. Oleh karena itu, pada penelitian ini, ditambahkan tag baru pada AIML agar AIML bisa berinteraksi dengan sistem dengan penambahan fungsi-fungsi lainnya. 
Dalam penelitian ini, sistem Personal Assistant dikembangkan di dalam sistem operasi Android. Sistem tersebut dengan adanya penambahan tag baru, bisa melakukan beberapa fungsi baru seperti, mencari informasi di Wikipedia, mencari tempat yang direkomendasikan dan mengecek cuaca.","Personal Assistant is a software that works in semi-autonomous. Their function is to replace user, get what user wants,  and provide services when needed.  One of main problem in building Personal Assistant System is the selection of actions to be taken by system based on user inputs.
In the development of the Personal Assistant, AIML is used as knowledge base. However, the ability AIML as a knowledge base is limited to giving the text output. Therefore, in this study, AIML is give new tag, so it can interact with the system by the addition of other functions.
In this study, the system of Personal Assistent were developed in the Android operating system. The system with the addition of a new tag, can do some new functions, such as searching for information on Wikipedia, find a place that was recommended and check the weather.
",,,51235,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104259,,TITO ABDULLAH,PERANCANGAN SISTEM INFORMASI PERSEWAAN PERLENGKAPAN PESTA DAN SIMULASI PENERAPAN METODE STEPPING STONE (STUDI KASUS PERSEWAAN TOTOK SRI MURNI SOLO),,,"Sistem Informasi, Persewaan Perlengkapan Pesta, Transportasi, North West Corner, Least Cost, Vogel&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s Approximation, Stepping Stone","Yohanes Suyanto, Drs,.M.I.Kom.",2,3,0,2016,1,"Penelitian ini dilatar belakangi oleh sistem transaksi persewaan perlengkapan pesta Totok Sri Murni yang masih dilakukan secara manual, yaitu dengan menulis data-data persewaan pada buku catatan dan letak gudang Persewaan Totok Sri Murni yang berada di beberapa tempat di Solo. Hal ini dapat menyebabkan proses pengolahan data-data persewaan membutuhkan waktu yang lebih lama dan alokasi barang dari gudang ke lokasi acara belum teratur.
Oleh karena itu, dirancang sebuah aplikasi Sistem Informasi untuk mengolah data persewaan dengan lebih cepat dan mengatur pencatatan jadwal pengiriman barang yang kemudian diterapkan metode transportasi North West Corner, Least Cost, Vogel&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s Approximation, dan Stepping Stone supaya distribusi pengiriman barang kepada pelanggan optimal. Aplikasi ini dibuat menggunakan database dan pemrograman web xampp, sedangkan data persewaan diambil dari observasi buku catatan persewaan.
Hasil penelitian ini berupa website xampp Sistem Informasi persewaan, jalur alokasi barang, dan perkiraan biaya transportasi. Dari data yang diambil pada tanggal 20 April 2016, dapat diperoleh alokasi yaitu gudang P mengirim 25 kursi ke Bapak Anto, 800 Kursi ke Ibu Bella, dan 425 kursi ke Bapak Chairil, lalu gudang M mengirim 225 kursi ke Bapak Chairil, kemudian gudang S mengirim 550 kursi ke Bapak Anto. Perkiraan perhitungan biaya pengiriman adalah Rp 250600,00. Penerapan metode transportasi pada Persewaan Totok Sri Murni saat ini masih belum dapat diterapkan dikarenakan banyak faktor lain seperti barang yang disewa beraneka ragam, data jarak gudang ke lokasi acara yang kurang sesuai dengan lapangan, kecelakaan yang mungkin terjadi, dan lain sebagainya.
","This research was motivated by party equipments rental transaction system of Totok Sri Murni which is still written manually by writing the data in the rental notebook, and in case that Totok Sri Murni rental warehouses are located in some places in Solo. This could cause rental data require a longer time to process and the allocation of goods from the warehouse to the event location isn't optimized.
Therefore, an Information System application was designed to process rental data more quickly and set delivery schedule recording of party equipments which then transportation methods such as North West Corner, Least Cost, Vogel&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s Approximation, and Stepping Stone were applied so that equipments delivery distributions to customers are optimized. This application was created using the database and xampp web programming, while the data was taken through observing notebook rental.
The results of this research was a xampp website of rental information systems, track of the equipment allocations, and an estimation of transportation costs. From the data taken on April 20, 2016, this showed an allocation where warehouse P sent 25 chairs to Mr. Anto, 800 chairs to Ms. Bella, and 425 chairs to Mr. Chairil, then warehouse M sent 225 chairs to Mr. Chairil, after that warehouse S sent 550 chairs to Mr. Anto. Estimated shipping cost calculation was 250,600 IDR. Transportation method at Totok Sri Murni rental is yet to be implemented due to many other factors such diverse items rented, the distance data from warehouse to the event was not relevant to the field, any accident that may occur, and so forth.
",,,54829,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99399,,FIKRI MAULANA,"E-RPJMD: LOCAL GOVERNMENT PLANNING INFORMATION SYSTEM DATA CENTRALIZATION, MONITORING, AND EVALUATION",,,"information system, government planning monitoring &amp; evaluation, RPJMD","Agus Sihabudin, S.Si., M.Kom",2,3,0,2016,1,"Penelitian ini membahas sejumlah permasalahan dalam proses monitoring dan evaluasi perencanaan pemerintah daerah pada level Kabupaten / Kota di Indonesia. Permasalahan-permasalahan itu, seperti data yang tidak terpusat, berujung pada kesulitan-kesulitan pada proses monitoring dan evaluasi, ditambah dengan kurangnya transparansi pada masyarakat, meskipun telah ada panduan dari Kementrian Dalam Negeri, dalam bentuk Dokumen RPJMD atau Rancangan Pembangunan Jangka Menengah Daerah
Untuk menghadapi masalah-masalah ini, penelitian ini mencoba untuk mendesain dan mengembangkan suatu sistem informasi berbasis web yang diharapkan dapat membantu meringankan permasalahan yang dihadapi. Sistem informasi ini dinamai e-RPJMD, sebagaimana fungsinya sebagai sistem informasi yang membantu pemerintah daerah untuk urusan pemusatan data, monitoring, dan evaluasi RPJMD. Pada proses pengembangan, penelitian ini menggunakan data dari Kota Palangka Raya, Kalimantan Tengah, Indonesia.
Hasil dari penelitian ini adalah sebuah sistem informasi yang bisa menyelesaikan permasalahan berkaitan monitoring dan evaluasi RPJMD yang dihadapi oleh pemerintah daerah. Sistem informasi pemerintahan e-RPJMD dapat memusatkan data perencanaan pemerintah daerah sesuai dengan struktur RPJMD. Funsgi monitoring dan evaluasi telah bekerja sesuai dengan analisis kebutuhan. Fungsi monitoring dan evaluasi yang dikembangkan berdasarkan pada diskusi dengan para ahli di bidang yang terkait, serta studi dari sejumlah publikasi, juga peraturan Kementrian Dalam Negeri. Fungsi monitoring dan evaluasi yang dikembangkan juga telah diverifikasi oleh ahli di bidang perencanaan, yang membuktikan validitas dari e-RPJMD.","This research looks into several issues of monitoring and evaluation Indonesia's local government planning. These issues, such as the non-centralized data, result in difficulties of monitoring and evaluating, in addition to the lack of transparency to society, despite the existence of guidelines from the Ministry of Interior, in the form of RPJMD (Rancangan Pembangunan Jangka Menengah Daerah) Document (Medium-Term Local Government Development Plan).
To tackle these issues, this research attempts to design and develop a web-based information system which is expected to aid in relieving the issue. It is named e-RPJMD, as it functions as an information system which assists government regarding data centralization, monitoring and evaluation of RPJMD. During the development process, this research utilized data from Palangka Raya City, Central Kalimantan, Indonesia.
The outcome of this research is an information system which can solve the RPJMD monitoring &amp; evaluation issues faced by local government. The e-RPJMD Government Information System is able to centralize the government planning data according to the RPJMD structure. The monitoring and evaluating function works according to the requirement analysis. The monitoring measurement and the evaluation function were built based on discussion with experts in the related field as well as studies from several publications, and the regulation of the Ministry of Interior. The monitoring and evaluation functions were also verified by an expert practitioner in their field, which proves its validity.",,,49725,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100168,,NABILA ANGGITA PUTRI L,Visualisasi Informasi Tingkat Perkembangan Industri Berdasarkan Data Media Monitoring,,,"Media Monitoring, Visualisasi Informasi, Industri Teknologi","I Gede Mujiyatna, S.Kom., M.Kom.              ",2,3,0,2016,1,"Perkembangan media online menyebabkan begitu banyak informasi yang tersebar. Informasi - informasi tersebut digunakan oleh berbagai pihak antara lain pihak perusahaan untuk melakukan pemantauan terhadap brand yang berkaitan dengan mereka, perusahaan - perusahaan tersebut melakukan media monitoring dengan memantau seberapa sering dan bagaimana sentiment suatu brand dalam suatu media online. Salah satu sumber mereka adalah media berita online.

Gamatechno menjadi salah satu perusahaan yang bergerak di bidang teknologi memanfaatkan media berita online untuk mengetahui perkembangan brand - brand teknologi yang berkaitan dengan mereka. Namun informasi pada media berita online disajikan dalam artikel yang terpisah - pisah sehingga akan memakan waktu dan tenaga untuk membacanya satu per satu. Untuk itu dibangun sistem visualisasi yang dapat menampilkan informasi - informasi yang dibutuhkan. Sistem visualisasi dibangun dengan nested model yang terdiri dari empat layer yaitu domain problem characterization, data/operation abstraction, interaction technique design dan algorithm design. Hasil visualisai berupa chart popularitas dan sentiment brand yang ditampilkan pada halaman web.
","Nowadays, online media has grown so fast and makes many information widely spread. These informations had been used by various parties, and one of them is the companies. They used it to monitor the brands which relate to their cause by using media monitoring or news monitoring. They monitor the issues on some particular brands as how often those brands had been mentioned and how good their sentiments are.

Gamatechno is one of the company which been using online media to monitor technology brands according to their needs. But the informations in online news medias are presented in article, so to find the informations they want they need to read those article one by one. The solution to these problem is to show those informations with a visualization system. Therefore, a visualization information system is built using nested model which consist of four layers. The layers are domain problem characterization, data/operation abstraction, interaction technique design and algorithm design. The results are popularity and sentiment charts which presented in interactive webpage. 
",,,50538,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
97609,,FARIKH FADLUL HUDA,Kompatibilitas Lintas Versi Android pada Aplikasi: Studi Kasus Aplikasi Bergerak Kontak UGM,,,"Android, backward-compatibility, Kontak UGM, API, Paperless office ","Bambang Nurcahyo Prastowo, Drs., M. Sc",2,3,0,2016,1,"Kontak UGM merupakan sebuah sistem komunikasi yang dikembangkan oleh Pusat Teknologi Informasi dan Komunikasi (PPTIK) UGM. Kontak ini digunakan sebagai sistem komunikasi yang dapat digunakan oleh civitas akademik UGM. Sistem Kontak ini akan lebih memberikan manfaat ketika dapat digunakan secara mobile, kapanpun, dan dimanapun. Sehingga pada penelitian ini diimplementasikan sebuah aplikasi berbasis Android yang dapat menjalankan fungsi dan fitur dari sistem Kontak UGM. Namun, sistem operasi Android memiliki berbagai versi yang hingga pada saat ini terus dikembangkan. Setelah aplikasi diimplementasi, aplikasi akan diuji bagaimana lebar rentang versi Android ini dapat memberikan dukungan terhadap aplikasi yang dikembangkan.
 Implementasi aplikasi Kontak memanfaatkan REST Application Programming Interface (API) Kontak yang telah dikembangkan pada penelitian sebelumnya. API tersebut memudahkan aplikasi untuk bertukar data dari server Kontak ke aplikasi. Setelah aplikasi diimplementasi, pengujian untuk kompatibilitas aplikasi dilakukan dngan mengubah konfigurasi kebutuhan minimal Android API. Jika aplikasi tidak kompatibel dengan versi Android tertentu maka dilakukan penyesuaian. Namun jika penyesuaian tidak dapat membuat aplikasi kompatibel maka diambil versi tersebut sebagai versi Android minimum untuk aplikasi dapat berjalan.
Aplikasi Android Kontak dapat diimplementasikan dengan memanfaatkan REST API Kontak. Aplikasi Kontak dapat berjalan pada sistem operasi Android dengan luas rentang versi Android API dari 23 hingga 7. Implementasi aplikasi dengan lebar tersebut dimungkinkan dilakukan dengan dukungan dari library support Android yang menyediakan fitur backward compatibility untuk beberapa komponen Android API terbaru.","Kontak UGM is a one of the communication system that developed by Pusat Teknologi Informasi dan Komunikasi (PPTIK) UGM. Kontak&acirc;€™s main purpose is for communication which can be used by academicians in UGM. However, Kontak will be more beneficial if it can be used in a mobile, at anywhere, and anytime. Thus, in this research we will study how to develop Kontak as a Android application which can fulfill the function and main feature of Kontak UGM system. But, Android is an operating system which have a multiple version that currently being developed. After the implementation, we will learn about how wide the range of the Android versions which can give the application support to be developed.
This Kontak application will be developed with the support of REST API Kontak from the previous research. This API ease the application to transfer data from the server to the application. After the implementation phase, the application compatibility test is done by change the minimum requirement of Android API. If application is not compatible with specific Android version then we have to make an adjustment. If the adjustment cannot make the application compatible with the Android version then we took that version as a minimum Android version that application required.
Kontak Android application can be implemented with the support of REST API Kontak. Kontak application can be deployed in Android that have a range of Android API from 23 to 7. This wide-range implementation is possible by the usage of support library which provide the backward compatibility feature for some component in latest version of Android API.",,,47922,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99401,,THORIQ AZ ZUHRI Y,IMPLEMENTASI TANDA TANGAN DIGITAL BERBASIS ALGORITMA ELGAMAL PADA PERANGKAT BERGERAK,,,"Tanda Tangan Digital, ElGamal, Android, Verifikasi Pesan","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2016,1,"Dewasa ini, perkembangan ilmu dan teknologi telah mempengaruhi segala aspek kehidupan. Tak terkecuali aspek komunikasi, seperti dalam pengiriman pesan. Semakin berkembangnya teknologi, pengiriman suatu pesan juga menjadi kurang aman. Digital Signature (tanda tangan digital) adalah suatu tanda tangan elektronik yang dapat digunakan untuk membuktikan keaslian identitas pengirim pesan dan juga untuk memastikan isi asli dari pesan sudah dikirim tanpa perubahan. Tanda tangan digital memberikan hal-hal seperti otentikasi pesan, integritas pesan, dan non-repudiation (tak bisa disangkal).
ElGamal signature scheme adalah salah satu skema tanda tangan digital yang diperkenalkan oleh Taher ElGamal pada tahun 1985. Skema tanda tangan digital ini berbasis pada sulitnya menghitung logaritma diskrit. Penelitian ini mengimplementasikan algoritma tanda tangan digital ElGamal ke dalam aplikasi perangkat bergerak. Aplikasi dibangun untuk dapat membuat tanda tangan digital dari pesan teks dan mampu memverifikasinya. Pengembangan dilakukan pada platform android karena merupakan platform yang paling banyak digunakan saat ini.
Hasil pengujian yang dilakukan dalam penelitian ini menunjukkan bahwa sistem yang dibangun berdasarkan algoritma tanda tangan digital ElGamal mampu menandatangani pesan secara digital serta memverifikasinya. Meskipun proses ini membuat pesan yang dikirim oleh pengguna menjadi lebih panjang karena disisipi kunci serta tanda tangan digital, tapi hal ini adalah sebuah keharusan untuk proses verifikasi pesan oleh penerima. Sistem sangat cocok untuk perangkat bergerak karena kecepatannya dalam menandatangani dan verifikasi pesan.
","Today, technology and science development has influenced every aspect in our life. Communication is not an exception, such in messaging. The more technology develops, the more unsafe it is for us to deliver a message. Digital Signature is an electronic signatures that could be used both to authenticate sender&acirc;€™s identity and to make sure that messages sent without changes. Digital Signature gives us message authentication, message integration, and non-repudiation.
ElGamal signature scheme is one of digital signature scheme that introduced by Taher ElGamal in 1985. This Digital Signature scheme is based on how hard it is to count discrete logarithm. This research implements Digital Signature algorithm ElGamal on a mobile device application. This application is built for making digital signature of text message and verified them. Development is done for android platform because it&acirc;€™s the most used platform nowadays.
The result in this research shows us that a system build based on ElGamal digital signature algorithm could sign a message digitally and verify it. Even though this process made messages sent by user longer than it should be, but this is a must for verification process on the receiving side. This system is very recommended for mobile device because it is very fast at sign and verify message.",,,49716,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100170,,PRAMUDYA RIAN D.,Implementasi Teknik Web Scraping Pada Proses Topic Modelling Portal Berita,,,"Web Scraping, Topic Modeling, LDA, Text Processing","Dr.techn. Ahmad Ashari, M.Kom",2,3,0,2016,1,"Seiring berkembang pesatnya teknologi, menyebabkan penyebaran informasi yang begitu cepat. Perkembangan teknologi ini mendorong manusia untuk bisa mendapatkan informasi lebih cepat khususnya berita. Berita menampilkan berbagai macam topik. Dengan pesatnya arus berita, diperlukan metode yang lebih cepat dan efisien untuk mendapatkan topik. 
Skripsi ini dibuat untuk mengetahui topik-topik pada portal berita. Metode yang digunakan yaitu dengan mengimplementasikan web scraping pada proses topic modeling. Teknik scraping mengambil artikel dari portal berita dan kemudian diolah menggunakan topic modeling. Topic modeling yang digunakan adalah Latent Dirichlet Allocation (LDA). LDA adalah model probabilitas generative dari koleksi data diskrit seperti kumpulan-kumpulan teks. Pada proses scraping, perancangan sistem dilakukan dengan identifikasi kelas tag HTML. Tag HTML yang digunakan yaitu tag yang mengapit judul, isi dan tanggal berita untuk kemudian dibuatkan template scraping. Data yang diperoleh kemudian diolah dengan pemodelan LDA, sehingga dapat diketahui topik-topik yang sering muncul dari portal berita nasional.
Sistem ini dibuat dengan menggunakan bahasa pemrograman Python 2.7.11 dengan modul-modul pendukungnya. Sistem ini bisa memproses web scraping dari portal berita Kompas dan kemudian disimpan ke file CSV. Berita Kompas diambil dalam rentang 7 hari. Dokumen CSV yang diperoleh kemudian diolah menggunakan modul python LDA untuk diperoleh topic modelling. Hasil keluaran berupa topik pada portal berita yang paling sering muncul.
","Information technology is currently growing fast. Human need to develop technology to obtain information, especially news. Online news is spread faster than conventional news. News portal is one of the media for human to reach any information. Each news portal contain any topics that was helped out to know news topics. For news that spread faster with the various topics, required a method to gain the specific topic. 
The method which is implemented in this study is the Web Scraping on topic modeling process. This sort of technique grabs the article from the news portal then processed using topic modeling. Latent Dirichlet Allocation (LDA) is a method that use for topic modeling process. LDA a generative probabilistic model for collections of discrete data such as text corpora. 	The system design is conducted by identifying the tag HTML class. The HTML Tags that used are the tags that are included within the news title, news content and the news date. The grabbed tags then filled into the scraping template to get its data to be collected. The Data that is obtained then will be processed by the LDA modeling, hence the user will be acknowledged about the topics that frequently appeared in the national news portal.
This system is built using the Phyton Programming version 2.711 by utilizing the supporting libraries. This system can process the web scraping from Kompas news portal and store it into CSV files. Kompas news article is taken in 7 days span. The obtained CSV document then will be processed using the LDA Phyton Libraries to get the modeling topic. The result will be in the most frequent topic form on the news portal.
",,,51021,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99404,,JOANNA DYAS EKARISTI PEPE,PREDIKSI NILAI TUKAR RUPIAH TERHADAP DOLAR AS BERDASARKAN FLUKTUASI NILAI TUKAR YUAN TERHADAP DOLAR AS DENGAN ALGORITMA GENETIKA,,,"prediksi, nilai USD/IDR, nilai USD/CNY, algoritma genetika","Faizah, S.Kom., M.Kom.",2,3,0,2016,1,"Pergerakan nilai tukar di Indonesia dikontrol melalui kebijakan moneter yang dikeluarkan pemerintah melalui Bank Indonesia, dengan mempertimbangkan kondisi makroekonomi global dan domestik. Pada bulan Agustus 2015, perekonomian dunia dikejutkan dengan kebijakan Bank Sentral Tiongkok untuk mendevaluasi yuan yang memberikan efek domino berupa penguatan dolar AS dan perlemahan mata uang negara-negara emerging yang menjadi mitra dagang. Melihat adanya pengaruh yang besar ini, maka penting bagi pemerintah Indonesia, sebagai salah satu negara emerging untuk mewaspadai resiko dari kebijakan perekonomian Tiongkok seperti devaluasi yuan dalam pengambil kebijakan moneter Indonesia. 
Kemajuan teknologi informasi memungkin kegiatan prediksi dilakukan melalui berbagai teknik. Salah satu teknik yang banyak dikembangkan adalah dengan penggunaan algoritma genetika. Dalam beberapa penelitian terdahulu, algoritma genetika terbukti mampu memberikan hasil yang superior dibandingkan dengan metode lainnya. Pada penelitian ini sistem dibangun untuk melakukan prediksi harian tunggal nilai tukar rupiah terhadap dolar AS berdasarkan fluktuasi yuan dengan menggunakan algoritma genetika. Parameter algoritma genetika terbaik yang berhasil diperoleh dari penelitian ini adalah nilai ukuran populasi 10 kromosom, nilai jumlah generasi 500 generasi, nilai Pc sebesar 0,6 dan nilai Pm sebesar 0,05. Dalam pengujian sistem prediksi ini, didapatkan nilai rata-rata error prediksi sebesar 331,2562. Nilai ini digunakan dalam pembentukan fungsi baru guna meningkatkan nilai akurasi hasil prediksi yang dilakukan. Dengan fungsi baru tersebut diperoleh peningkatan hasil rata-rata akurasi dalam sistem yang dibangun dari 96,73% menjadi 98,19%.","In Indonesia, exchange rate movements are controlled through monetary policies that established by the government through Bank Indonesia, by considering the global and domestic macroeconomic stability. On August 2015, the world economy was shocked by People's Bank of China's policy to devaluate yuan. This gave a domino effect by increasing dollar US exchange rate but then decreasing the emerging country exchange rate which is its trading partner. Considering the great effect of yuan devaluation, it is important for Indonesia government as one of emerging country, to keep aware of China's economic policy risks such as yuan devaluation to decide the Indonesia monetary policies. 
Advancement of information technology gives a big change for doing predictions in many ways. One method that mostly develop is the genetic algorithm. The past study in genetic algorithm was proved to give a superior result than other methods. This study is developing a daily-single prediction system of rupiah exchange rate against dollar US based on yuan fluctuation in using genetic algorithm. The best parameter of genetic algorithm which is known by this study is ten chromosomes per population, 500 generations, 0.6 number of Pc and 0.05 number of Pm. Within the system testing, there is obtained the error value average of prediction as much as 331.2562. In purpose to increase the accuracy of prediction value, that number is used to form a new prediction formula. The new prediction formula is increasing the average accuracy value from 96.73% to 98.19%.",,,49718,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100172,,FATTAH AZZUHRY,ANALISIS KINERJA DAN CAKUPAN WLAN DI KAMPUS SEKIP UNIT III FAKULTAS MIPA UGM,,,"wireless, wi-fi, access point, bandwidth, throughput, latency, jitter, packet loss, QoS, hotspot","Drs. Dr.techn. Ahmad Ashari, M.Kom.",2,3,0,2016,1,"Karakteristik propagasi dari jaringan wireless perlu dipelajari karena semakin banyaknya jumlah penerapan dari teknologi wireless ini dengan kinerja QoS yang memadai. Selain itu, pemeliharaan jaringan untuk QoS yang optimal lebih sulit dilakukan di jaringan nirkabel dibandingkan dengan jaringan dengan kabel. Hal ini dikarenakan sifat propagasi radio yang kompleks dan sulit ditebak. Oleh karena itu, dibutuhkan adanya penempatan access point yang efisien agar didapatkan kinerja yang baik dengan biaya pemasangan yang lebih hemat.
Penelitian ini bertujuan untuk mengukur kinerja Wi-Fi UGM Hotspot yang telah dipasang di gedung Sekip Unit III FMIPA UGM. Selain itu, penelitian ini juga bertujuan untuk menyelidiki dan mencari hubungan antara kekuatan sinyal yang diterima (RSSI) dengan QoS pada jaringan IEEE 802.11 seperti throughput, latency, jitter, dan packet loss melalui pengukuran propagasi radio. Pengukuran dengan
propagasi radio secara langsung di lapangan dilakukan karena lebih akurat daripada simulasi komputer atau pemodelan matematis, di mana karakteristik propagasi radio itu kompleks dan tidak dapat ditebak. Hasil dari penelitian ini adalah hubungan antar parameter kinerja jaringan dan rekomendasi penempatan access point yang baik agar didapat kinerja yang lebih baik.","The behavior of the wireless networks needs to be studied extensively due to the wide variety of applications that can make use of this technology. Those applications, ranging from IoT devices, Wi-Fi, and indoor localization need sufficient QoS performance in order to work properly. Furthermore, network maintenance for an optimal QoS is more difficult to be done on wireless network compared to wired network. This is caused by the complex and unpredictable nature of radio signal propagation. Therefore, an appropriate AP placement is required to obtain a greater
performance of WLANs with lower cost.
This research aims to measure the network performance of UGM Hotspot Wi-Fi deployment in Sekip Unit III building of FMIPA UGM. Moreover, this research also aims to investigate and find a relationship between received signal strength (RSS) and IEEE 802.11n network&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s QoS in terms of throughput, latency, jitter, and packet loss through extensive radio propagation measurements. Direct propagation measurement is done because it is difficult to derive real and accurate signal strength and QoS
parameters using analytical modeling and computer simulation. The result of this research is a relationship between those network performance parameters and a
recommendation for placing access point in order to achieve better performance.",,,50621,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100173,,OLIVIA KARTIKA S,PENGEMBANGAN BUSINESS INTELLIGENCE DASHBOARD DENGAN MENGGUNAKAN PENTAHO BI SERVER,,,"business intelligence, pentaho, saiku, sql developer, reporting","Agus Sihabuddin, S.Si., M.Kom ",2,3,0,2016,1,"Pada suatu database, data yang terkandung di dalamnya bisa jadi tidak memiliki relasi yang optimal, dengan kata lain masih mentah. Bentuk data yang relasinya tidak optimal ini akan berpengaruh dalam pemrosesan pelaporan, sehingga perlu diterapkan konsep business intelligence (BI). BI merupakan alat dan teknik untuk mentransformasi data mentah menjadi informasi yang berguna dan bermakna untuk tujuan analisis bisnis.
BI tidak lepas dari proses ETL (Extraction, Transaction, Loading). Proses extraction menggunakan sql developer, yaitu memasukkan data sample sebagai data source. Data yang tidak lolos proses ekstraksi akan dieliminasi. Kemudian pada proses tranformation, data source akan dianalisis menurut karakteristiknya terutama relasi antar tabel lalu dibagi ke dalam beberapa grup. Proses loading adalah memasukan data ke tool BI, yaitu Pentaho BI server. Plugin Saiku diperlukan dalam pembuatan cube pada BI tools. Data tersebut kemudian akan disajikan dalam bentuk chart dengan tujuan pengguna lebih mudah memahami informasi yang disajikan.
Berdasarkan hasil pengujian, telah didapatkan bentuk struktur database baru yang siap digunakan sebagai source pada BI, kemudian juga dihasilkan pola pemrosesan dari bentuk database asli ke dalam bentuk database baru sehingga data bisa ditampilkan dalam reporting berbasis BI. Selain itu hasil penelitian yang didapatkan adalah pengaruh ukuran dan struktur database terhadap waktu proses pengolahan business intelligence tool. Analisis penelitian mendapatkan hasil bahwa bentuk data source awal dan bentuk relasinya sangat berpengaruh pada informasi akhir report BI.","The data inside database can be not optimal relational to each other, in other word is still raw. This form of data that not optimal relational will take effect on reporting process, so we need to apply business intelligence concept here. BI is a tool and technique to transform raw data into more useful and more valuable information with analysis business objective.
BI can not be separate with ETL (Extraction, Transaction, Loading). Extraction process use sql developer, that is used to insert data sample as data source. The data which is not passing the extraction process will be eliminated. Then on the transformation process, data source will be analyzed based on its characteristic especially the relation of the table, then it will be divided into several group. Loading process is insert data to BI tools, that is Pentaho BI Server. Plugin Saiku is needed in cube making process. The data will be presented in a chart form so that user can easily understand the presented information.
Based on test result, this research has obtained form new database structure that is ready to be used as source in the BI, then also generated pattern database processing from the original form into the new database so that the data can be displayed in a BI-based reporting. In addition the results obtained is the effect of the size and structure of the database for processing time business intelligence tools as well as users' understanding of how big a successful BI report is displayed on the dashboard. The result of research analysis is the initial forms of data source and the shapes of their relationships are very effecting the final report of business intelligence information.",,,50618,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107087,,ROCHMAT SETIYAWAN,EVALUASI DAN PEMERINGKATAN PHP FRAMEWORK BERDASARKAN OBJECT ORIENTED METRICS,,,"framework, php, Analytic Hierarchy Process (AHP), object-oriented, metrics, quality design,","Anny Kartika Sari, S.Si, M.Sc, Ph.D",2,3,0,2016,1,"Kemunculan berbagai php framework menjadi alternatif dalam mengembangkan suatu aplikasi web. Hal ini mendorong Kementerian Perindustrian untuk melakukan evaluasi terhadap php framework internal Wisanggeni.  Tujuannya adalah mengukur kualitas disain dari Wisanggeni. Dalam penelitian ini dilakukan evaluasi terhadap php framework menggunakan parameter object oriented metrics dan kualitas disain. Hasil penilaian terhadap parameter object oriented metrics dievaluasi dengan metode Analytic Hierarchy Process (AHP) untuk menentukan kualitas disain model framework terbaik yang digunakan sebagai referensi untuk pengembangan Wisanggeni ke depan. Penelitian ini juga menggunakan kombinasi tool PhpDepend dan PhpMetrics untuk menghitung nilai object oriented metrics pada masing-masing php framework.

Hasil akhir dari penelitian ini menunjukkan bahwa framework Laravel memiliki kualitas disain model framework dengan nilai tertinggi pada aspek understandability dan maintainability. Sedangkan framework CodeIgniter dengan nilai tertinggi apabila aspek reusability dan efficiency diutamakan.","The use of the framework can create a programming standard, it is one option to reduce programming time problems. This prompted The Indonesia Ministry of Industry (Kementerian Perindustrian) to conduct an evaluation of internal php framework which is called Wisanggeni. The objective is to measure the quality of design of Wisanggeni. In this research an evaluation of php framework using object oriented metric and quality of design. Parameters is conducted the obtained value of the object oriented metric parameter is the obtained evaluated with obtained analytic hierarchy process (AHP) method to determine the quality design of model framework which is to be used best as a reference to the development of Wisanggeni forward . The research also uses a combination of Phpdepend and Phpmetrics tools to calculate the object oriented metric values in each php framework .

The final results of this study indicate that Laravel framework quality the highest value on the aspects of understandability and maintainability. CodeIgniter framework has the highest score when the reusability and efficiency aspects take precedence.",,,57682,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100176,,BRYAN GEMILANG P,PEMBUATAN VIDEO GAME 3D DRAGON FURY MENGGUNAKAN UNITY 3D DENGAN KONTROL SUARA ,,,"Video game, speech recognition system, perintah suara, Unity 3D, Word detection","Drs. Medi, M.Kom.",2,3,0,2016,1,"Terdapat berbagai macam ragam video game yang saat ini beredar di kalangan masyarakat. Video game sangat populer di masyarakat karena fungsi hiburan nya yang sangat kuat. Dari berbagai jenis game yang beredar, masih belum banyak game yang menggunakan input suara sebagai kontrolnya.
	Pada penelitian ini, peneliti membangun sebuah aplikasi game tiga dimensi yang menggunakan input suara sebagai kontrolnya. Untuk membangun game tersebut, diperlukan suatu game tools atau game engine yang dikombinasikan dengan suatu speech recognition system yang dapat menerima input suara dan menerjemahkannya menjadi suatu perintah yang dapat dipahami oleh video game. Unity 3D merupakan game engine yang digunakan untuk membangun game tiga dimensi, sedangkan untuk penerapan speech recognition system di dalam Unity menggunakan sistem Word Detection. 
Hasil pengujian dari aplikasi game yang telah diberi nama Dragon Fury ini menunjukkan akurasi pengenalan kontrol suara secara keseluruhan sebesar 74% dengan perintah suara yang berbahasa Indonesia. Aplikasi juga dapat mengenali perintah suara pendek dengan akurasi 60%.
","There are many kind of video games that are currently exist among people. Video games are very popular among people because it can entertain them well. Of the variety of video games that exist nowadays, only few of them that have voice controller as their input.
	In this research, researcher building a three dimensional game application that uses voice controller as its input. For the purpose of build the game, we need game tools or game engine that combined with a speech recognition system that can accept voice input well and translate it into a command that can be processed in the video game. Unity 3D is a game engine that used for building three dimensional game, and Word detection for the implementation of speech recognition system in Unity. 
The Result of this application test that named Dragon Fury show us that the accuracy of voice recognition to be amount of 74% using Indonesian language. This application also can recognize short voice command with 60% accuracy.
",,,50496,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
105810,,BENY LIANTRIANA,PENGGUNAAN PONSEL ANDROID SEBAGAI PERANGKAT ABSENSI ALTERNATIF MELALUI FITUR MOBILE NFC,,,"Absensi, Android, Apache POI, NFC, SQLite","Lukman Heryawan, S.T., M.T",2,3,0,2016,1,"Absensi adalah kegiatan pengambilan data guna mengetahui jumlah kehadiran seseorang pada suatu acara. Proses absensi sudah menjadi salah satu tolok ukur untuk menilai kinerja seseorang dalam suatu instansi. Semakin akurat data yang diambil pada proses absensi semakin tinggi juga tingkat kepercayaan untuk menggunakan data itu sebagai salah satu tolok ukur penilaian. Oleh karena hal itu diperlukan proses pengumpulan data yang cepat dan akurat guna mengoptimalkan proses absensi. Salah satu media yang dapat digunakan untuk melakukan pertukaran data adalah media pengiriman melalui NFC pada Android.
	Pada penelitian ini digunakan fitur mobile NFC pada Android untuk melakukan proses transfer data pengguna pada proses absensi. NFC menyederhanakan komunikasi data tanpa adanya penyamaan koneksi pada antar perangkat sehingga proses otentikasi dapat dilakukan dalam waktu yang sangat singkat. NFC digunakan sebagai media pengirim teks nomor mahasiswa sebagai input dalam proses absensi yang kemudian disimpan ke dalam suatu database. Penggunaan NFC dalam sistem ini adalah dengan mengetukkan perangkat Android pengirim pada perangkat Android penerima sehingga pengiriman data teks nomor mahasiswa berhasil tersimpan pada perangkat penerima.
	Hasil pada penelitian ini adalah sebuah sistem absensi digital dengan memanfaatkan NFC sebagai media pengiriman data pada ponsel Android dalam proses absensi, Apache POI sebagai pengolah data excel, dan dimanfaatkannya SQLite sebagai media pengolah data yang dapat digunakan sebagai sistem absensi alternatif untuk menggantikan sistem absensi manual.","	Attendance is a data collection activities in order to determine the amount of a person's presence at an event. The attendance has become one of the benchmarks for assessing the performance of a person in an institution. The more accurate data taken on the attendance higher the level of confidence to use the data as a benchmark assessment. Therefore it is necessary that the process of collecting data quickly and accurately in order to optimize the process of attendance.
	This research use the mobile NFC on Android to make transfer process of user&acirc;€™s data on the attendance. NFC simplifies data communication without make equalization connection between devices so the authentication process can be done in a very short time. NFC is used as media to transfer student number as text input in the process of attendance which is then stored into a database. The using of NFC in this system is by tapping the sender Android device to the recipient Android device, so the student number succesfully can be received and stored in receiving device.
	The results in this research is an digital attendance system by utilizing NFC as a media for data transmission on mobile Android in the process of attendance, Apache POI as data processor excel and SQLite as data processing media that can be used as alternative attendance system to replace the attendance system manually.",,,56395,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
112212,,ADAM PRANATA,PURWARUPA ANTARMUKA UNTUK RUMAH CERDAS DENGAN PENDEKATAN WEB OF THINGS (WoT),,,"Smart home, Web of Things, WoT, Internet of Things, IoT, Interface","Khabib Mustofa, S.Si., M.Kom., Dr. Techn.,",2,3,0,2016,1,"Piranti cerdas pada umunya telah memiliki antarmuka pendukung berupa native app dan telah mendukung arsitektur Internet of Things (IoT). Native apps kemudian seolah - olah telah menjadi standar antarmuka untuk piranti cerdas saat ini, namun demikian native apps memiliki kelemahan berupa ketergantungan pada spesifikasi perangkat antarmuka. Dengan mengganti antarmuka dari native apps menjadi web browser yang dapat dijalankan pada semua perangkat antarmuka piranti cerdas apapun baik mobile (Android Smartphone, tablet, iPad dll.) maupun desktop (PC) dapat memberikan alternatif yang lebih terjangkau bagi seluruh pengguna perangkat cerdas. Selain permasalahan antarmuka, smart home juga seringkali dihadapkan pada permasalahan mengenai keberadaan jaringan pada smart home. Jaringan internet dapat sewaktu waktu mengalami kesalahan dalam melayani komunikasi data pada smart home sehingga dibutuhkan alternatif jaringan lokal yang mampu menggantikan peran internet sebagai media alternatif untuk komunikasi data. 
Purwarupa antarmuka yang dibuat melingkupi antarmuka pengguna dengan sistem (human to machine interface) dan antarmuka antara perangkat elektronik dengan sistem (Machine to machine interface). Pembuatan purwarupa antarmuka dilakukan dengan memanfaatkan REST API sebagai media komunikasi data dan desain responsive web sebagai Graphical User Interface (GUI) sehingga purwarupa antarmuka dapat beradaptasi dengan perbedaan resolusi dan jenis browser pada masing-masing perangkat. 
Hasil uji running time pada sistem melalui antarmuka untuk dapat melakukan eksekusi terhadap perangkat elektronik melalui internet didapatkan hasil sebesar 4.175 ~ 16.841 detik sedangkan melalui jaringan Lokal didapatkan rata-rata sebesar 1.251 ~ 1.828 detik. Dan Hasil uji penampilan GUI melalui smartphone, tablet dan desktop didapatkan penampilan halaman web yang sesuai dengan resolusi perangkat antarmuka.
","Smart devices in general have had support form native app interface and supports the architecture of the Internet of Things (IOT). Native apps then seems to have become the standard interface for smart devices at this time. Native apps have a weakness in the form of dependency on the device interface specifications. By replacing native apps into web browser based, interface can run on all devices such as mobile devices (Android smartphone, tablet, iPad etc.) And desktop (PC) and provide more affordable alternative for all users of smart devices. Smart home also faced with the network availability issue. Internet network may at any time have any errors in serving the communication of data, so an alternative local network that can replace the role of the Internet is needed.
The prototype interface includes user interface with the system (human to machine interface) and the interface between electronic devices to the system (Machine to machine interface). Prototyping interface is done by utilizing REST API as data communication media and responsive web design as GUI so interface can adapt to differences in resolution and type of browser on each device.
Test results on a system running time through the interface to be able to make the execution of electronic devices over the Internet obtained 4,175 ~ 16 841 seconds while through Local network obtained 1,251 ~ 1,828 seconds. The results of appearance test for GUI via smartphones, tablets and desktop obtained responsive web pages that match the resolution of interface device. 
",,,63233,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
97624,,FAZA AZIZ,PERHITUNGAN POPULARITAS FIGUR PUBLIK PADA PORTAL BERITA ONLINE DENGAN MENGGUNAKAN TEXT MINING,,,"Figur Publik, Portal Berita, Text Mining, Web Application, Regular Expression.","Anny Kartika Sari, S.Si., M.Sc., Ph.D",2,3,0,2016,1,"Figur publik merupakan sosok atau tokoh yang dikenal secara luas oleh masyarakat umum, seperti artis dan orang-orang dari instansi pemerintah. Figur publik sering muncul di media, salah satunya media berita online. Portal berita online merupakan salah satu media yang banyak disorot oleh masyarakat dikarenakan informasi tersedia secara cepat. Seiring dengan banyaknya pemberitaan mengenai figur publik di portal berita online, hal ini dapat membuka peluang untuk menghitung tingkat popularitas seorang figur publik di portal berita online, sehingga bisa bermanfaat untuk lembaga survei dan informasi bagi masyarakat. 
	Sistem yang dibangun merupakan sistem perhitungan popularitas yang akan menghitung tingkat popularitas figur publik pada 3 portal berita online, yaitu viva.co.id, merdeka.com dan detik.com. Penggalian data pada portal berita tersebut dilakukan dengan metode text mining menggunakan regular expression. Hasil perhitungan akan diklasifikasikan dengan tingkat popularitas dan  ditampilkan secara visual dalam bentuk visualisasi data yang sesuai. Sistem ini dibangun dengan menggunakan bahasa pemrograman PHP.
Dari hasil rata-rata uji performa, bahwa pada proses penjelajahan viva.co.id dengan waktu 236.2 detik dapat menemukan 1678 tautan. Proses penjelajahan merdeka.com dengan waktu 222.4 detik dapat menemukan 982 tautan. Proses penjelajahan detik.com dengan waktu 272.7 detik dapat menemukan 1984 tautan. Selain itu sebanyak 90% pengguna (terdiri atas 2 sangat setuju dan 25 setuju) berpandangan bahwa aplikasi mudah digunakan dan 66.6% pengguna (terdiri atas 20 setuju) berpandangan bahwa visualisasi grafik menarik.
","Public figure is someone who is widely recognized by the public such as artists and people from the institutional government. They often appear in the media like online news. The online news portal is one of the media that is highly highlighted by the public because the information is served quickly. Along with many reporting about public figures in the online news portal, this can be an opportunity to calculate the popularity level of a public figure in the online news portal, so it will be useful for the survey institutions and considered as the information for the public.

	The system built is a system calculation the popularity that will calculate their level of popularity in three online news portals, namely viva.co.id, merdeka.com and detik.com. The data mining on the news portal carried out by the text mining method using regular expression. The results of the analysis are classified by the level of popularity and displayed visually in the form of appropriate data visualization. This system is built using the PHP programming language.
The results of the average of performance test, it showed that the viva.co.id browsing process with a time of 236.2 seconds found 1678 links. On the other hands, the merdeka.com browsing process with a time of 222.4 seconds found 982 links. Meanwhile, the detik.com browsing process with a time of 222.4 seconds found 982 links. In addition, there were 90% users (consisting of two strongly agreed and 25 agreed) said that the application is easy to use and the other 66.6% users (consisting of 20 agreed) said that the visualization graph is interesting.
",,,47982,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99930,,WAHYU KEMALAJATI,IMAGE PERSPECTIVE CORRECTION MENGGUNAKAN MATRIK HOMOGEN PADA OBJEK QUADRILATERAL,,,"image perspective correction, edge detection, corner detection, top view, homogeneous matrix","Sigit Priyanta, S.Si., M.Kom",2,3,0,2016,1,"Semakin meningkatnya pengguna smartphone dan kamera DSLR yang banyak digunakan untuk mengabadikan suatu momen atau sesuatu yang penting seperti dokumen teks salah satunya. Dokumentasi teks menggunakan kamera sudah sangat umum digunakan dan tentunya pengambilan sudut gambar dari masing-masing pengguna juga berbeda. Untuk mengatasi hal tersebut diperlukan sebuah program yang dapat digunakan untuk mengatasi permasalahan tentang sudut pengambilan gambar tersebut. 
Sebagai langkah awal dalam pembuatan program tersebut akan dilakukan pengujian terhadap metode-metode yang dapat mendukung permasalahan tersebut. Fokus penelitian ini adalah untuk melakukan transformasi gambar sehingga gambar dengan berbagai macam sudut pengambilan gambar dapat ditransformasikan menjadi gambar dengan sudut pandang top view dengan melalui metode-metode seperti edge detection yang dipadukan dengan hough transform kemudian corner detection dengan melakukan perhitungan intersection dan image transformation menggunakan homogeneous matrix.","Increasingly users of smartphones and DSLR cameras are used to capture a moment or something important like text documents one of them. Documentation text using the camera is very common and certainly taking a picture angle of each user is also different. To overcome this we need a program that can be used to overcome the problems of the angle.
As a first step in the making of the program will be testing of the methods that can support these issues. The focus of this research is to transform the image so image with a wide range of angles can be transformed into image viewing angle top view through methods such as edge detection, combined with hough transform then corner detection by calculating the intersection and image transformation using homogeneous matrix.",,,50268,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96863,,MUCHLIS ARIFUDIN,VISUALISASI INFORMASI PERKEMBANGAN PONDOK PESANTREN AL-HIKMAH KARANGMOJO GUNUNGKIDUL,,,"visualisasi informasi, media pengawasan dan evaluasi, pondok pesantren / visualization of information, media of monitoring and evaluation, Islamic boarding school&acirc;€ƒ","Moh Edi Wibowo, S.Kom., Mkom., Ph.D",2,3,0,2016,1,"Pondok Pesantren Al-Hikmah Karangmojo Gunungkidul merupakan salah satu Pondok Pesantren yang ada di Indonesia. Pengawasan dan evaluasi merupakan salah satu bagian penting bagi Pondok Pesantren Al-Hikmah Karangmojo Gunungkidul untuk mencapai kemajuan. Dalam proses pengawasan dan evaluasi perkembangannya Pondok Pesantren Al-Hikmah membutuhkan media bantu untuk mengelola dan menyajikan data dengan baik.  
Penilitian ini dilakukan dengan pembuatan visualisasi informasi untuk mendukung proses pengawasan dan evaluasi perkembangan Pondok Pesantren Al-Hikmah. Visualisasi informasi dalam penelitian ini dibuat dalam bentuk sistem yang berbasis web yang dibangun dengan framework CI, library highchart untuk menampilkan diagram dan Google Maps API untuk menampilkan peta. Data yang digunakan dalam visualisasi informasi Pondok Pesantren Al-Hikmah ini adalah data santri baru, data asatidz dan data pembangunan. Data santri meliputi data santri masuk, jenis kelamin, usia santri saat masuk, pendidikan yang ditempuh, pekerjaan orang tua dan status santri. Data asatidz meliputi data jenis kelamin, pendidikan yang sudah ditempuh dan status mengajar. Data pembangunan meliputi data catatan pembangunan setiap tahun, jenis bangunan dan lembaga pemilik bangunan.
Hasil dari penelitian ini adalah diperolehnya sistem visualisasi informasi untuk melihat perkembangan Pondok Pesantren Al-Hikmah Karangmojo Gunungkidul. Hasil dari uji kemanfaatan sistem menunjukan bahwa 33,3% dari pimpinan dan pengurus pesantren menyatakan setuju dan 66,7% menyatakan sangat setuju dengan sistem ini sebagai media yang membatu proses pengawasan dan evaluasi perkembangan Pondok Pesantren Al-Hikmah Karangmojo Gunungkidul. ","Al-Hikmah Islamic Boarding School Karangmojo Gunungkidul is one of the boarding schools are there in Indonesia. Monitoring and evaluation is one important part for Al-Hikmah Boarding School Karangmojo progress. In the process of monitoring and evaluation of its development, Al-Hikmah boarding school needs assistive media to manage and present the data properly. 
This research is conducted by making the visualization of information to support the process of monitoring and evaluation the development of Al-Hikmah Islamic Boarding School. The visualization of information in this study was made in the form of a web-based system that is built with the CI framework, library highchart to display the diagram and Google Maps Api to display the map. The datas used in the visualization of information of Al-Hikmah are the new student data, the teacher data and the development data. The student data includes the number of incoming students, gender, the students age at admission, students pursued education, the occupation of parents and the status of students. The teacher data includes gender, education that have been taken and the status of teaching.  The development data includes the construction records data each year, the type of building and the owners of institution building.
The results of this research is the obtaining of the information visualization systems to observe the Al-Hikmah Islamic Boarding School Karangmojo Gunungkidul development. The result of the beneficial system test showed that 33.3% from the leader and boarding school administrators stated agree and 66.7% were either strongly agree with this system as a media that helps in the monitoring and evaluation process of the Al-Hikmah Islamic Boarding School Karangmojo Gunungkidul development. ",,,47112,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99426,,BAGUS RIANTO, IMPLEMENTASI DAN PERBANDINGAN METODE PRAPEMPROSESAN PADA ANALISIS SENTIMEN GUBERNUR DKI JAKARTA MENGGUNAKAN METODE SUPPORT VECTOR MACHINE DAN NAIVE BAYES,,,"analisis sentimen, teks mining, opinion mining","Nur Rokhman, S.Si., M.Kom",2,3,0,2016,1,"Analisis sentimen merupakan ilmu lintas disiplin yang melingkupi Natural Language Processing(NLP), kecerdasan buatan, dan text mining. Penggunaan anali- sis sentimen biasa digunakan dalam melihat sentimen publik terhadap produk, tokoh terkenal, ataupun kejadian.

Pendekatan analisis sentimen sering dilakukan dengan machine learning de- ngan menggunakan klasifikasi yang muktahir seperti Support Vector Machine (SVM). Selain menggunakan SVM pendekatan lainnya juga banyak dilakukan, salah satunya Naive Bayes. Pendekatan - pendekatan tersebut sangat sering ditemukan namun sa- ngat sedikit yang dilakukan dengan menggunakan bahasa Indonesia.

Penelitian ini menggunakan sumber data dari detik.com dan kompas.com un- tuk menjadi pembelajaran model klasifikasi sentimen analisis. Dengan variasi-variasi proses prapemprosesan didapatkan model yang terbaik yaitu dengan menggunakan SVM dengan proses prapemprosesan cleansing, casefolding dan eliminasi KBBI. Dengan performa precision sebesar 65.61%, recall sebesar 65.36% dan F-measure 65.06%.","Sentiment analysis is an interdisciplinary science that surrounds Natural Lan- guage Processing (NLP), artificial intelligence, and text mining. The use of senti- ment analysis commonly used in observing sentiment on products, famous figures, or events.

Sentiment analysis approach is often done by machine learning using the mod- ern classification such as Support Vector Machine (SVM). In addition there are many other approaches, such as Naive Bayes. Those approaches are very common but just few have done using Indonesian language.

This research uses data from the online news portal detik.com and kompas.com to be training data to train against sentiment analysis model. Proprocess was cus- tomed by the variation of preprocessing function to find the best preprocessing step. The result is SVM with preprocessing function consist of cleansing, casefolding and elimination of KBBI. Measured performance for precision is 65.61%, recall is 65.36% and F-measure is 65.06%.",,,49765,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
94820,,BARATA TEDDY I. T.,PEMANFAATAN MAC ADDRESS PADA JARINGAN WIFI UNTUK MENGHITUNG STATISTIK PENGUNJUNG PADA RUANG PUBLIK,,,"statistik pengunjung, rasio perbandingan, MAC address, Raspberry, Aircrack-ng","Bambang Nurcahyo Prastowo, Drs., M.Sc",2,3,0,2016,1,"Beberapa tahun sebelumnya perhitungan statistik pengujung terkadang dilakukan dengan alat hitung manual. Seiring dengan pesat nya perkembangan teknologi, menjadi suatu kebutuhan dari sistem komputer untuk mengekstrak informasi mengenai statistik pengunjung dalam suatu area ruang publik. Pendekatan yang dapat menyelesaikan masalah ini biasanya terbagi menjadi dua kategori, yaitu: dengan menggunakan video atau gambar dan bukan gambar. Metode berbasis bukan gambar biasanya memanfaatkan pengenal RFID, MAC address pada jaringan Wi-Fi, ponsel, node sensor, dll. 

Dalam penelitian ini ditawarkan sistem yang memanfaatkan MAC address pada jaringan Wi-Fi untuk menghitung statistik pengujung.  Sistem berjalan pada perangkat Raspberry Pi, dengan menggunakan perangkat lunak Airodump-ng, dan Airmon-ng dari Aircrack-ng suite untuk menangkap MAC address. Sistem diuji di salah satu kafe yang ada di Yogyakarta, yaitu Blanco Coffee and Books. Parameter pengujian dalam penelitian ini adalah untuk melihat rasio perbandingan antara jumlah pengujung dengan jumlah MAC address yang didapatkan sistem dan statistik pengujung itu sendiri.

Hasil dari penelitian ini menunjukkan rata-rata rasio perbandingan 0.7630 dengan standar deviasi sebesar 0.1743. Dengan memanfaatkan MAC address bisa didapatkan statistik pengunjung, seperti berikut: titik teramai kunjungan lebih banyak pada saat malam hari yaitu pada tanggal 10 - 12 Desember. 5 lama kunjungan tertinggi dari pengunjung yang datang ke toko berdasarkan MAC address yang dibawa menunjukkan lebih dari 4 jam. Jumlah pengunjung pada tanggal 9-10 menunjukkan jumlah sekitar 220 orang, pada tanggal 11 menjadi 103 orang dan meningkat lagi pada tanggal 12 menjadi 162 orang.
","In the recent years, calculating the visitors' statistic has been done with manual counting.  Along with the rapid of technology development, it is a requirement of computer to extract the information about visitors' statistic in public space. The approach that can solve this problem usually divided into two categories:  using video or image and non-image. Non-image-based method usually uses RFIDs, MAC addresses in Wi-Fi network, phone, node sensor etc.

This research is offering a system that utilizes MAC addresses in Wi-Fi network to calculate the visitors' statistic. The system works on Raspberry, using Airodump-ng and airmon-ng softwares from Aircrack-ng suite to get the MAC addresses.  The system was tested in one of the cafes in Yogyakarta, Blanco Coffee and Books. The parameters in this research are the ratio between number of visitors with the number of MAC addresses that is obtained by the system and the visitors' statistic itself.

Results from the research show the ratio average is 0.7630 with standard deviation is 0.1743. By utilizing the MAC addresses, the following visitors' statistic can be obtained: the crowdest time usually happened during night time at December 10-12. The top five longest visits between December 9-12 are above four hours. The total of visitors between December 9-10 were about 220 people. In December 11, the number of visitors were about 103 people and in December 12 it was increased to 162 people.
",,,45167,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99429,,MUH BAGUS TESA K,Clustering Teks dengan Algoritma LINGO Menggunakan Latent Semantic Indexing pada Cluster Content Discovery,,,"Clustering, LINGO, Latent Semantic Indexing","Mardhani Riasetiawan, M.T.",2,3,0,2016,1,"Hasil pencarian dokumen teks pada umumnya disajikan dalam bentuk daftar panjang yang diurutkan berdasarkan relevansinya terhadap query yang diberikan. Hasil pencarian tersebut tentu saja mengabaikan kedekatan antar dokumen itu sendiri. Kemiripan antar dokumen dapat disajikan melalui algoritma pengelompokan (clustering) yang mengelompokkan dokumen berdasarkan topik-topik tertentu.
Penelitian ini mengeksplorasi Algoritma LINGO untuk mengelompokkan dokumen teks dengan menggunakan latent semantic indexing. Algoritma ini terdiri atas 5 tahap. Tahap pertama adalah preprocessing yang melibatkan identifikasi bahasa, tokenisasi, serta stemming. Tahap kedua merupakan feature extraction untuk menentukan kandidat label untuk cluster yang akan dibentuk. Tahap selanjutnya adalah cluster label induction dimana kandidat label diseleksi menggunakan reduksi dimensi. Setelah kandidat label diseleksi, dilanjutkan dengan tahap cluster content discovery untuk menentukan keanggotaan kelompok masing-masing dokumen. Pada tahap cluster content discovery dalam penelitian ini digunakan latent semantic indexing. Penggunaan LSI pada tahap cluster content discovery memungkinkan pengelompokan dokumen tidak hanya didasarkan pada lexical matching dari label kelompok yang ada. Tahap kelima merupakan finalisasi kelompok yang terbentuk dalam bentuk perhitungan skor kelompok yang terbentuk.
Pengujian dengan mengelompokkan 5 hingga 135 dokumen yang diambil secara acak dari 300 dokumen tersedia menunjukkan bahwa penggunaan latent semantic indexing memberikan keberhasilan terbatas. Jumlah cluster yang hanya memiliki satu anggota saja semakin sedikit. Implementasi Algoritma LINGO tanpa LSI memberikan rata-rata single cluster sebesar 14,01. Sedangkan penggunaan LSI pada tahap cluster content discovery menghasilkan rata-rata jumlah single cluster sebesar 2,59. Meski demikian, terdapat lebih banyak kelompok yang saling tumpang tindih mengakibatkan nilai evaluasi dunn index, partition coefficient, serta partition entropy menjadi lebih buruk. Pengujian tanpa latent semantic indexing memberikan dunn index dengan rata-rata 0,37 sedangkan penggunaan LSI 0,17. Rata-rata nilai partition coefficient tanpa latent semantic indexing adalah 0,71 dan 0,49 ketika menggunakan latent semantic indexing. Evaluasi partition entropy memberikan rata-rata 0,10 untuk implementasi tanpa LSI dan 0,18 untuk implementasi dengan latent semantic indexing.","Text document search results mostly presented as a long list of documents in descending order of relevance. The way search result being presented disregard the similarity of the documents itself. Clustering algorithm is capable to present text documents relations in topical manner.
This research explores LINGO algorithm to cluster text document with latent semantic indexing. This algorithm consist of 5 major steps. Preprocessing stage covers language detection, tokenization, and stemming. The second stage is the feature extraction to determine the candidates for cluster labels. The next stage is the cluster label induction where candidates are selected through dimension reduction over the term-document matrix. Once the candidate labels are selected, cluster content discovery phase determines the membership of each documents. At this stage, latent semantic indexing is being used. Hence the discovery of documents membership is not only based on lexical matching to given labels. Finalization stage calculates the final score for given clusters.
The implementation tested with 5 to 135 documents taken randomly from 300 available documents. The test shows that latent semantic indexing offers limited success on addressing several issues in LINGO. The number of cluster that only consist of single document greatly reduced. LINGO implementation without LSI gives an average number of single cluster of 14.01. Latent semantic indexing reduces it into 2.59 on average. However, the number of overlapping clusters worsen cluster evaluation index. Tests without latent semantic indexing gives dunn index score 0.37 on average, but falls into 0.17 with latent semantic indexing employed. On average, partition coefficient without latent semantic indexing score around 0.71. LINGO that uses latent semantic indexing only able to score 0.49 on average for partition coefficient. Partition entropy also scores worse with 0.18 with LSI while it can score 0.10 on average without LSI.",,,49766,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99431,,NURDIAN NUGRAHA,Automatic Summarization Bahasa Indonesia Menggunakan TextRank,,,"TextRank, Peringkasan, Semantic Networks, Corpus Statistic, WordNet, ROUGE","Mardhani Riasetiawan, M.T",2,3,0,2016,1,"Seiring dengan perkembangan teknologi di masa ini, penyebaran informasi menjadi hal yang sangat mudah dilakukan. Kemudahan dalam penyebaran informasi ini diiringi dengan bertambah banyaknya informasi yang tersedia, seperti dokumen. Namun, bertambah banyaknya informasi yang tersedia tidak selalu memberikan kemudahan bagi pembaca. Hal ini dikarenakan setiap orang tidak mempunyai cukup waktu untuk membaca keseluruhan informasi yang tersedia. Apalagi setelah dibaca, ternyata informasi yang tersedia pada suatu dokumen tidak sesuai dengan informasi yang diinginkan pembaca. Oleh karena itu, dibutuhkan sebuah sistem peringkas dokumen otomatis untuk memberikan pembaca gambaran umum dari sebuah dokumen sebelum membacanya.
Pada penelitian ini, akan diimplementasikan sistem peringkas dokumen otomatis menggunakan algoritma TextRank dan Semantic Networks and Corpus Statistic. Penggunaan TextRank ini memungkinkan ekstraksi kalimat-kalimat utama dari sebuah dokumen yang selanjutnya akan digunakan sebagai kalimat pada ringkasan keluaran. Peringkasan dokumen pada TextRank ini terdiri dari beberapa proses, yaitu tokenisasi kalimat, pembentukan graf, perhitungan nilai edge dengan menggunakan algoritma Semantic Networks and Corpus Statistic, perhitungan nilai vertex, pengurutan nilai vertex, dan pembentukan ringkasan.
Pengujian dilakukan dengan menghitung nilai recall, precision, dan F-Score dari ringkasan keluaran sistem dengan ringkasan idealnya menggunakan metode ROUGE-N untuk mengukur kualitas dari ringkasan keluaran sistem. Dari hasil pengujian yang dilakukan, diketahui bahwa kualitas ringkasan dipengaruhi oleh gaya penulisan dokumen, pemilihan kata-kata dan simbol pada dokumen, serta panjang ringkasan keluaran sistem. Panjang ringkasan optimal yang memberikan nilai F-Score terbesar dari hasil pengujian adalah 10% dari panjang dokumen dengan nilai F-Score 0,1635 dan 150 kata dengan nilai F-Score 0,1623.","With the advancement of technology in this era, spreading information becomes an easy thing to do. This ease of information dissemination is accompanied by an increase in available information, such as document. However, increasing the amount of available information does not always make it easy for the reader. This is because not everyone have enough time to read through all available information. Especially after reading through all available information, it turns out that the information is not in accordance with the desired information. Therefore, an automatic summarization system is needed to give the reader general image of a document before reading it.
In this research, automatic summarization system is implemented using TextRank, and Semantic Networks and Corpus Statistic algorithm. The use of this TextRank allows the system to extract the important sentences from a document which is used as the sentences for the output summary. Summarization within TextRank consists of several processes, namely sentence tokenization, graph creation, edge value calculation using Semantic Networks and Corpus Statistic algorithm, vertex value calculation, sorting the vertex value, and summary creation.
Testing is done by calculating the recall, precision, and F-Score of the system summary with its ideal summary using ROUGE-N method to measure the quality of system summary. Based on the test results, it is known that the quality of the summary is affected by the writing style, the selection of words and symbols in the document, as well as the length of the output summary. The optimal length of the summary that gives the largest F-Score value from the test results is 10% of the document length with an average F-Score value of 0.1635 and 150 words with an average F-Score value of 0.1623.",,,49743,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96361,,PUTRI DWITYA WENIWANDARI,PENGEMBANGAN APLIKASI BERBASIS LOKASI UNTUK PENCARIAN RUTE OPTIMAL TRANS JOGJA PADA SISTEM OPERASI ANDROID,,,"Android, Layanan Berbasis Lokasi, Google Maps API Android v2, Algoritma Dijkstra, Trans Jogja","Sigit Priyanta, S.Si., M.Kom",2,3,0,2016,1,"Trans Jogja merupakan sarana transportasi umum yang disediakan oleh Dinas Perhubungan DIY. Trans Jogja memiliki delapan rute yang melingkupi daerah Yogyakarta. Aplikasi ini dapat mengetahui rute perjalanan berdasarkan lokasi pengguna dan lokasi tujuan. Selain itu pengguna juga dapat melakukan pemilihan lokasi berangkat secara manual. Rute yang dihitung mempertimbangkan bobot transit sehingga rute yang dihasilkan memiliki transit yang sedikit dengan jarak yang terdekat.
	Pada penelitian ini akan dibangun sebuah aplikasi pada sistem operasi Android yang memungkinkan pengguna mengetahui lokasi dia berada dan melakukan pencarian rute optimal berdasarkan lokasi tersebut (location based service). Pencarian rute perjalanan didasarkan oleh bobot jarak antar halte, bobot jarak antara lokasi berangkat atau tujuan dengan halte terdekat dari lokasi tersebut, serta pertimbangan transit antar jalur jika memang diperlukan. Pencarian rute perjalanan didukung dengan algoritma Dijkstra yang melakukan pencarian dengan mencari total bobot paling sedikit dari beberapa rute yang mungkin berdasarkan halte terdekat dari lokasi-lokasi yang dipilih. Aplikasi didukung oleh Google Maps API Android v2 dalam penyajian petanya. Selain itu aplikasi dapat menampilkan informasi halte, jalur, serta nomor telepon taksi dan ojek.
	Berdasarkan hasil pengujian, fungsionalitas yang disebutkan dapat berjalan dengan baik sesuai dengan perancangan yang telah dibuat. Dari pengujian data 100 lokasi berangkat dan lokasi tujuan aplikasi mampu memberikan rute terbaik dengan halte keberangkatan dan halte tujuan terdekat serta dengan jumlah transit sedikit berdasarkan peta jalur Trans Jogja.","Trans Jogja is one of public transportation provided by Department of Transportation DIY. Trans Jogja has eight routes that cover the area of Yogyakarta. This application can provide the best route based on user's location and destination. Besides, user can choose their departing location manually. Route is calculated by considering transits between stops, and shortest distance between stops.
	This reasearch will develop an application that allows users to locate his location and search an optimal route based on user's input location. Route finding is based on weighting the distances between stops, user's input location and nearest stops form it, and considering transits between routes if necessary. Route finding is provided by Dijkstra Algorithm that calculates minimum weight from possible routes. Application is powered by Google Maps API Android v2. In addition, application provides information about stops, routes, and taxi/motorcycle taxi's telephone number.  
	Based on the test results, the functionality of application can be run properly according to system design. From the data test at 100 different locations, application is able to provide the best nearest shelter from location and provide route with minimum transit based on Trans Jogja maps.",,,46666,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96365,,FAUZI YUDHI S,Prediksi Nilai Tukar Mata Uang dalam Sistem Forex Trading dengan menggunakan Algoritma Genetika,,,"prediksi, FOREX, algoritma genetika, prediksi time-series","Afiahayati, S.Kom., M.Sc., Ph.D",2,3,0,2016,1,"Pada sistem pertukaran mata uang asing (FOREX), selalu terjadi perubahan harga secara fluktuatif. Banyak orang berusaha melakukan prediksi pergerakan nilai tukar mata uang pada FOREX untuk dapat mengambil keputusan secara tepat. Metode prediksi menggunakan algoritma genetika dengan data time-series merupakan salah satu metode yang dapat digunakan dalam permasalahan prediksi.
	Algoritma genetika merupakan sebuah algoritma yang terinspirasi dari alam yaitu evolusi. Pada penelitian ini langkah algoritma genetika yang digunakan dimulai dengan membangkitkan suatu populasi, melakukan seleksi dengan metode roulette wheel, dilanjutkan dengan proses whole aritmethic crossover dan mutasi secara acak. Hasil populasi tersebut akan diseleksi dengan metode steady-state update. Penghitungan nilai evaluasi fitness dilakukan dengan metode regresi linear berganda dengan jumlah variabel yang dapat ditentukan. Tingkat akurasi prediksi diukur dengan menggunakan MAPD (Mean Absolute Percentage Deviation) dan nilai MSE (Mean Squared Error).
	Proses pengujian ini menggunakan data time-series masing-masing jenis data mata uang yang ingin diprediksi dengan kurun waktu 1 tahun yaitu dari Januari 2015 hingga Desember 2015. Pengujian dilakukan pada 4 jenis pasangan mata uang utama dalam sistem FOREX yaitu USDJPY, USDCHF, GBPUSD dan EURUSD. Nilai MAPD dan MSE hasil prediksi yang didapatkan untuk bulan Januari hingga Februari 2016 untuk pasangan mata uang USDJPY sebesar 0,6930% dan 1,1186533, USDCHF sebesar 0,7425% dan 0,0000734709051153, GBPUSD sebesar 0,7190% dan 0,0001600516436978 serta EURUSD sebesar 0,4601% dan 0,0001180997.","In the foreign currency exchange (FOREX) system, there are always fluctuating price changes. Many people attempt to predict currency movements on the FOREX to be able to take appropriate decisions. Prediction method using a genetic algorithm with time-series data is one method that can be used in prediction problems.
The genetic algorithm is an algorithm which inspired by the natural evolution. This study used genetic algorithm which begin by generate a population, perform roulette wheel selection method, followed by the whole process aritmethic crossover and random mutation. The results of the population will be selected by the method of steady-state updates. The calculation of the fitness evaluation was conducted using multiple linear regression with the number of variables that can be determined. Level prediction accuracy was measured by using the value of MAPD (Mean Absolute Percentage Deviation) and MSE (Mean Squared Error).
This testing process use time-series data for each type of currency data which we want to predict for the span period of 1 year from January 2015 to December 2015. The tests were conducted on four types of major currency pairs in FOREX system that is USDJPY, USDCHF, GBPUSD and EURUSD. MAPD and MSE value which obtained on prediction from January to February 2016 for the USDJPY currency pair at 0,6930% and 1,1186533, USDCHF at 0,7425% and 0,0000734709051153, GBPUSD at 0,7190% and 0,0001600516436978  and for EURUSD at 0,4601% and 0.0001180997.",,,46610,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
97902,,YUD KARISMOLLAH CHOIR,DETEKSI OUTLIER PADA DATA CAMPURAN MENGGUNAKAN ALGORITMA AVF (ATTRIBUTE VALUE FREQUENCY) DAN Z-SCORE (Studi Kasus: Data Transaksi Penjualan di Apotek UGM),,,"outlier, deteksi outlier, data campuran, data transaksi, apotek, AVF, Z-Score, data mining","Nur Rokhman, S.Si., M.Kom",2,3,0,2016,1,"Outlier merupakan objek yang berbeda dibandingkan objek-objek lainnya dalam suatu data. Sehingga outlier pada suatu data perlu mendapatkan perhatian khusus karena dapat mengganggu distribusi data dan pada akhirnya mempersulit dalam penemuan informasi penting dari kumpulan data. Saat ini, telah banyak metode yang dikembangkan untuk mengatasi permasalahan outlier. Akan tetapi metode tersebut hanya fokus pada data yang seragam, yaitu data kategorik atau data numerik saja. Padahal pada data yang nyata kebanyakan tidak hanya mempunyai atribut yang seragam, tetapi juga memiliki atribut campuran, yaitu numerik dan kategorik. Contoh data nyata dengan atribut campuran yaitu data transaksi penjualan di Apotek UGM.
Pada penelitian ini akan dilakukan deteksi outlier pada data campuran menggunakan algoritma AVF dan algoritma Z-Score. Data yang digunakan dalam penelitian adalah data transaksi penjualan di Apotek UGM yang memiliki atribut kategorik dan numerik. Algoritma AVF digunakan untuk deteksi outlier pada atribut kategorik. Sedangkan algoritma Z-Score digunakan untuk deteksi outlier pada atribut numerik. Kemudian dilakukan perankingan dari hasil deteksi outlier menggunakan algoritma AVF dan algoritma Z-Score sehingga didapat ranking outlier pada data campuran.
Hasil dari penelitian ini, algoritma AVF dan algoritma Z-Score dapat melakukan deteksi outlier pada data transaksi penjualan di Apotek UGM yang berjenis campuran. Hasil dari deteksi outlier memberikan gambaran pola yang aneh dalam transaksi penjualan di Apotek UGM dari tanggal 1 &acirc;€“ 16 April 2016.","Outlier is a different object compared to other objects in a data, so the outlier in a data needs to have a special attention because it can cause bugs in the data distribution and eventually makes it difficult to find some important information from the data collection. There have been many methods developed to overcome the outlier problem today. However, they only focus on one kind of data, that is, either categorical or numerical data, where as most real data do not only have one kind of attribute, but a mixed one, that is, both numerical and categorical. One of the example of a real data with mixed attribute is the sales transaction data of Apotek UGM.
This research will detect the outlier on mixed data using AVF and Z-Score algorithms. The data used in this research is the sales transaction data at Apotek UGM (which has both categorical and numerical attribute). AVF algorithm is used to detect the outlier on the categorical attribute, and Z-Score the numerical one. A ranking from the outlier detection using AVF and Z-Score algorithms will be done so a ranking on the mixed data can be conceived. 
The result of this reseach is that AVF and Z-Score algorithms are able to detect outlier on a sales transaction data of Apotek UGM, which is that of a mixed attribute. The result of the outlier detection shows a strange pattern in Apotek UGM&acirc;€™s sales transaction data from 1-16 April 2016.",,,48227,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96624,,MEGA PUSPITASARI,RANCANG BANGUN APLIKASI PRESENSI PERUSAHAANBERBASIS GLOBAL POSITIONING SYSTEMPADA SISTEM OPERASI ANDROID,,,"Android, Sistem Presensi, Web Service, Database, Location Based Service","Edi Winarko, Drs., M.Sc., Ph.D.",2,3,0,2016,1,"Perkembangan teknologi melahirkan perangkat mobile dengan sistem operasi yang bersifat open source sebagai contoh Android. Perangkat Android memberikan kemudahan mobilitas dan fleksibilitas bagi penggunanya. Salah satu kemudahan yang dirasakan adalah perkembangan pesat berbagai macam aplikasi berbasis Android. Besarnya jumlah pengguna dan beragam jenis perangkat yang tersebar di pasaran menyebabkan penggunaan aplikasi yang tidak hanya bersifat individual, namun dapat dimanfaatkan pula oleh organisasi atau perusahaan.

Pada penelitian ini, dilakukan rancang bangun sistem ini meliputi aplikasi Android dan aplikasi web. Aplikasi Android diakses oleh karyawan melalui perangkat mobile masing-masing yang berfungsi untuk melakukan proses presensi. Data hasil proses presensi dengan format JSON diolah melalui web service sehingga dapat tersimpan pada database server dalam format MySQL dan sebaliknya. Sedangkan aplikasi web berfungsi sebagai area yang dapat diakses oleh admin atau manajer untuk menampilkan data-data yang terkait dengan hasil proses presensi dan dapat mengakses database pada database server secara real time.

Implementasi sistem presensi ini diadaptasi dari fleksibilitas lingkungan kerja perusahaan kontraktor. Meskipun demikian, sistem ini tetap dapat memberikan data realtime dan akurat, yang selanjutnya dapat lebih mudah digunakan pada proses manajemen data di perusahaan.","The evolution of technology has created mobile devices with open-source based operating system with Android OS as an example. Android devices provide easier mobility and flexibility to their user. One of the main feature is rapid development of Android based applications with many purposes. That large number of users and wide variety of Android devices on the market lead to the use of applications are not only individual but also by organization and company.

In this research, development of system consist of Android application and web application. Android application is used by the employees on their own mobile devices as a tool for doing attendance. The data that has generated from attendance process to JSON data format is processed through web service so the data can stored on the database in MySQL format vice versa. At the same time, web application functioned as area that can be accessed by manager or admin to show data that are related to presences data and can access the database in real time.

Implementation of this system is adapted from the flexibility of contractors company work environment. Despite of that, it can still deliver realtime and accurate data, so it can be easier to be used in the next process data management in the company.",,,46918,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
106608,,TEA TRIANA TALABIA,ALGORITMA GENETIKA UNTUK SISTEM PREDIKSI TINGGI MUKA AIR PADA PINTU AIR BENDUNG KATULAMPA BOGOR,,,"prediction, genetic algorithm, water level, katulampa","Faizah, S.Kom., M.Kom.",2,3,0,2016,1,"aat musim penghujan. Banjir Jakarta erat kaitannya dengan Bogor dan Sungai Ciliwung karena Jakarta sering mendapat banjir kiriman dari Bogor. Salah satu upaya pemerintah dalam menanggulangi banjir adalah dengan membangun pos pengamatan berupa pintu air di beberapa titik guna mengamati ketinggian muka air. Pintu air Bendung Katulampa Bogor menjadi pos pengamatan pertama aliran Sungai Ciliwung. Prediksi tinggi muka air adalah salah satu upaya untuk mengantisipasi banjir dan kekeringan. Prediksi ini menggunakan data time series tinggi muka air.
Pada penelitian ini dilakukan prediksi tinggi muka air pada pintu air Bendung Katulampa menggunakan algoritma genetika. Algoritma genetika merupakan salah satu model untuk prediksi. Algoritma ini terinspirasi dari proses evolusi untuk mencari solusi terbaik. Algoritma genetika pada penelitian ini menggunakan representasi bilangan real. Tahap awal dari algoritma ini adalah pembangkitan populasi awal, lalu seleksi dengan roullete wheel selection, perkawinan silang dan mutasi serta seleksi survivor dengan metode elitism. Fungsi fitness yang digunakan pada penelitian ini adalah nilai MAPE dari perhitungan autoregressive.
Penelitian dilakukan dengan menggunakan data tinggi muka air per jam dari 1 Januari 2016 hingga 10 Februari 2016. Sistem melakukan prediksi tinggi muka air satu jam ke depan dengan menggunakan kromosom solusi terbaik dari hasil pelatihan. Pada proses pelatihan menggunakan parameter algoritma genetika yang telah didapat, diperoleh nilai rata-rata fitness sebesar 0,208735 yang memiliki nilai MAPE sebesar 4,790763% dan untuk pengujian diperoleh rata-rata MAPE sebesar 6,339328%. Model autoregressive yang dioptimasi dengan algoritma genetika mampu menghasilkan nilai error yang lebih rendah dibandingkan dengan model autoregressive biasa.","Flood is one of the disasters that frequently hit the Jakarta capital city, especially during the rainy season. Floods in Jakarta is closely related to Bogor and Ciliwung river because Jakarta often get flood from Bogor. One of the government's efforts in tackling flooding is to build sluice gates at some point in order to observe the water level. Katulampa weir is the first point to observe Ciliwung river flow. Prediction water level is one of the efforts to anticipate flood and drought. This prediction system used time series data of water level.
This research conducted on the water level predictions of Katulampa weir used genetic algorithms. The genetic algorithm is one of the models to predict. This algorithm inspired by natural evolution process to find the best solution. Genetic algorithms in this study used representation of real numbers. The initial stage of the algorithm was the generation of an initial population, then the selection using roullete wheel selection, crossover and mutation up to survivor selection with elitism method. Fitness function that used in this study is the MAPE of autoregressive calculations.
The study was conducted using hourly data of water level from January 1, 2016 to February 10, 2016. The system predicted water levels one hour ahead using the best chromosome solution of the results of training process. In the process of training using the best genetic algorithm parameter values, system obtained average of fitness values amounted to 0,208735 which has MAPE amaounted to 4,790763% and for the test results obtained average of MAPE amounted 6,339328%. Autoregressive with genetic algorithm has less error than manual autoregressive.",,,57149,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104821,,AURELIA DYAH SWADESI PUTRI,Mapping of Livestock Based On Land Potential Using Simple Additive Weighting and GIS (Case Study: Special Region of Yogyakarta,,,"local-web based system, Decision Support System, land potential, Simple Additive Weighting, Google Map API","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2016,1,"	Stok makanan termasuk produsi dari hewan ternak diperlukan untuk kelangsungan hidup manusia. Akan tetapi sekarang ini sedang terjadi keterbatasan produksi dari hewan ternak. Oleh karena itu diperlukanlah pemetaan untuk mengetahui kondisi dari lokasi mana yang cocok untuk mengembang biakkan suatu jenis hewan ternak. Oleh karena itu fokus dari studi ini adalah untuk membuat sistem pendukung keputusan (SPK) untuk pemetaan hewan ternak dengan tujuan untuk memberikan fasilitas guna meningkatkan pengembangan hewan ternak.
	Metode sistem pendukung keputusan (SPK) yang digunakan dalam riset ini adalah simple additive weighting (SAW). Dengan tujuan untuk memfasilitasi pengguna untuk mempermudah dalam memahami hasilnya, hasil dari perhitungan akan ditampilkan ke dalam bentuk visual peta GIS dengan menggunakan aplikasi pemetaan populer, yaitu Google Map API. Metode ini di implementasikan dalam riset ini dengan tujuan untuk mendapatkan hasil yang maksimal untuk mengetahui dimana lokasi yang baik untuk suatu jenis hewan ternak.
	Dari hasil proses test dengan menggunakan data hasil observasi, berdasarkan rankingnya untuk kabupaten Kulon Progo diraih oleh Purwoharjo dengan memiliki nilai total terbanyak yaitu 372,52. Kemudian untuk kabupaten Bantul, Piyungan memiliki nilai terbesar dengan total 233,77. Kemudian untuk kabupaten sleman, Pakem memiliki hasil terbanyak dengan total 285,83, dan untuk kabupaten Gunung Kidul dengan Playen dengan total 177,14. Selain dilakukan test untuk setiap kabupaten, dilakukan juga test dengan mengambil 1 sample lokasi dari setiap kabupaten. Dari hasil testing ini, Pakem memiliki nilai terbanyak dengan total 391,43","Food supply, included livestock product is necesary for human living.. However there is a shortage of livestock. Therefore it is necessary to do a mapping to know the condition of which location suitable to raise a specific kind of livestock. Therefore the focus of this study is to create a decision support system (DSS) for livestock mapping with purpose to provide a facility to increase the deployment of livestock.
	Decision Support System (DSS) method used in this research is simple additive weighting (SAW).  In order facilitate user to make it easier in understanding of the results, the results displayed in a visual form of GIS map by using the popular mapping application, the Google Map API. This method implemented in this research in order reach the optimal value where is the right location for a specific livestok.
	From testing prosess by using data from observation, according to the rank of  Kulon Progo regency, Purwoharjo have the greatest results with total 372,52. Then for Bantul regency, Piyungan have the greatest results with total 233,77. Then for Sleman regency goes to Pakem with total 285, 83, and Gunung Kidul with Playen with total 177,14. Besides testing performed for each regency, which also done by taking one sample from each regency. From the results of this testing, Pakem have the greatest results with total 391,43.
",,,55298,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99702,,INEZ FIONA SUTANTO,Komputasi Transient Performability Measures pada Markov Reward Model Asiklik Menggunakan Metode Diskritisasi,,,"Markov reward model, Markov reward model asiklik, algoritma diskritisasi, transient performability measure","M. Reza M. I. Pulungan, S.Si., M.Sc., Dr.-Ing.",2,3,0,2016,1,"Markov reward model (MRM) adalah bentuk rantai Markov waktu kontinu yang dibebani dengan state reward untuk setiap state. Aplikasi MRM pada sistem komputer mengenai analisis serentak performance dan dependability sistem komputer disebut sebagai performability. Salah satu metode yang dapat menghitung performability measure adalah metode diskritisasi, yang didesain untuk keseluruhan MRM sehingga mampu menangani model dengan cycle. Metode diskritisasi mendiskritkan waktu dan akumulasi reward, menghitung probabilitas berada pada state saat ini dengan secara rekursif mengumpulkan probabilitas singgah pada state-state waktu sebelumnya. Namun dalam MRM asiklik, tidak ada transisi dari state dengan orde topologi tinggi ke state orde lebih rendah sehingga probabilitasnya bernilai nol. Algoritma diskritisasi khusus MRM asiklik bekerja dengan mengurangi ruang penyimpanan probability density function waktu sebelumnya yang pasti tidak mempengaruhi transient performability measure, sehingga mengurangi kebutuhan komputasi dan mempercepat kinerja sistem. Penelitian ini juga memuat pengujian sederhana terhadap algoritma yang diusulkan untuk memperlihatkan efektifitas algoritma.","Markov reward model is such a Markov chain loaded with state reward which is assigned on each state. The application of Markov Reward Models to computer system concerns the simultaneous analysis of performance and dependability of computer systems referred as performability. One of the methods that compute performability measure is the discretization method, which was designed for universal Markov Reward Models thus intended to handle the cyclic models. The method discretize both the time interval and accumulated reward, then computes the probability of being on a state by collecting the occupation probability of prior states in recursive manner. However in acyclic models, there are no transitions that shift from topologically higher states to the lower ones, thus the probability worth zero. The discretization algorithm specifically for acyclic Markov reward models reduces the memory space containing ineffectual state occupation probability, therefore reduces computation needs and speeds up the system performance. This paper also contains a simple experiment to show the efficacy of the proposed algorithm.  ",,,50005,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
102263,,NORMA GADIS   ISTIARTI,Sistem Pakar Berbasis Kasus untuk Diagnosa Penyakit Berdasarkan Gaya Hidup Pasien,,,"sistem pakar, case-based reasoning, nearest neighbor","Drs. Retantyo Wardoyo, M.Sc., Ph.D",2,3,0,2016,1,"Penyakit pola hidup merupakan penyakit yang disebabkan oleh gaya hidup yang tidak sehat, gaya hidup tidak sehat ini dipengaruhi oleh faktor resiko yang menjadi kontributor utama gejala penyakit. Faktor penyebab sekaligus menjadi kontributor utama terjadinya peningkatan PTM tersebut adalah pola hidup yang tidak sehat seperti kebiasaan merokok, minum alkohol, pola makan dan obesitas, kurang aktivitas fisik, stress, dan pencemaran lingkungan.Penyakit PTM yang diakibatkan oleh gaya hidup perlu di tekan perkembangannya. Sedangkan kurangnya ketersediaan dokter ahli menjadi masalah dilihat dari jumlah penduduk Indonesia yang mencapai 240 juta jiwa.
Case-Based Reasoning merupakan suatu metode yang mampu memecahkan masalah baru dengan menggunakan kasus-kasus yang sudah ada sebelumnya sebagai solusi. Sistem yang dibangun dalam penelitian ini adalah sistem CBR untuk melakukan diagnosa penyakit akibat gaya hidup tidak sehat. Proses diagnosa dilakukan dengan cara memasukkan kasus baru yang berisi gejala dan faktor resiko, kasus tersebut akan dibandingkan dengan kasus lama untuk dihitung nilai similaritasnya menggunakan metode nearest neighbor similarity. Hasil yang diperoleh berupa jenis penyakit dan tingkat kepercayaan kebenaran hasil diagnosa.","Lifestyle disease is caused by unhealthy lifestyle. The unhealthy lifestyle is affected by risk factors that become a main contributors of symptoms. The main contributors that increasing a non communicable disease (NCD) are unhealthy lifestyle such as smoking, alcohol consumption, obesity, less activity, stress and pollution. The non communicable disease (NCD) that caused by unhealthy lifestyle should be pressed. Otherwise, a lack of availability of doctors becomes a problem, because population of Indonesian is about 240 million.
Case-Based Reasoning (CBR) is a method that solve a new problem using previously existing cases as a solution. System developed in this study is a CBR system for diagnosis the disease caused by unhealthy lifestyle. Diagnosis process is done by input a new case which contain the symptom and risk factor. Then the case will be compared with old case, and it will be calculated a similarity value using nearest neighbor similarity method. A result is type of disease and validity of diagnosis.",,,52729,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96121,,MICHAEL B. ATHEYA,LOCATION-BASED SERVICE IMPLEMENTATION TO MATCH CUSTOMER NEEDS BASED ON PREVIOUS BEHAVIOR,,,"data mining, classification, CHAID Decision Tree Algorithm, Location-Based Service, Transaction Historical Dataset.","Sigit Priyanta, S.Si., M.Kom",2,3,0,2016,1,"Fokus pada penelitian ini adalah mengklasifikasikan karakteristik pembelian kategori menggunakan algoritma CHAID dan menerapkan hasilnya menggunakan promosi berbasis lokasi (LBA). Untuk melakukannya, dilakukan beberapa fase yang berurutan, yaitu pengolahan awal, pelabelan, klasifikasi, analisis, dan implementasi.
Kumpulan data berisi data lebih dari 24.000 pelanggan dan lebih dari 32.000 transaksi sekitar 1 Agustus 2015 - 30 Oktober 2015 di Grand Indonesia, Jakarta. Dalam menyiapkan data, model menggunakan 6 fitur seperti kelompok umur, jenis kelamin, kelompok kartu kredit, jumlah kelompok transaksi, membeli frekuensi dalam 2-3 bulan sebelumnya dan perilaku pembelian dalam 1 bulan lalu. Kemudian, algoritma CHAID diaplikasikan pada dataset. Sebagai hasil, whitelist akan terbentuk dan siap diimplementasikan melalui LBA core.
Percobaan menunjukkan bahwa ketepatan klasifikasi tergantung pada banyak faktor seperti jumlah fitur, jumlah dataset, periode transaksil, dan varietas transaksi. Kesimpulan ini dapat dilihat meskipun proses validasi pada kategori bahan makanan memiliki akurasi terbesar (94,64%), diikuti oleh Makanan dan Minuman (75,759%) dan Fashion (73,454%). Model yang dibuat dalam penelitian ini bekerja dalam dua intitusi yaitu, telekomunikasi dan sistem perbankan.","The focus of the study is to classify the characteristics of category's buying behavior using CHAID algorithm and implement the result using location-based service. To do so, several consecutive phases were conducted; they are pre-processing, labeling, classification, Analysis, and implementation.
The dataset originally contained record more than 24,000 customers and more than 32,000 transactions around 1st August, 2015 - 30th October 2015 in Grand Indonesia, Jakarta. In preparing the data, the model used 6 features such as age group, gender, credit card group, amount of transaction group, buying frequency in the previous 2-3 months and buying behavior in the last 1 month. Then, CHAID algorithm was applied to the datasets. As the result whitelists are made and ready to be implemented through the LBA core.  
Experiment shows that the accuracy of the classification depends on many factors such as number of features, total dataset, transactional log period, and varieties of transaction. This conclusion can be seen though validation process where groceries category has the biggest accuracy (94.64%), followed by Food and Beverage category (75.759%) and fashion (73.454%). The model created in this research worked in both telecommunication and bank system.
",,,46661,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99962,,FAISAL MUSAFAR,STEGANOGRAFI BERBASIS LEAST SIGNIFICANT BIT PADA MEDIA VOICE OVER INTERNET PROTOCOL,,,"Steganografi, Least Significant Bit, VoIP, Real-time Steganography, Client-to-client Threading.","Nur Rokhman S.Si., M.Kom.",2,3,0,2016,1,"Keamanan informasi telah berkembang secara signifikan selama beberapa tahun terakhir. Sebagian besar organisasi menyadari bahwa salah satu aset paling berharga yang dimiliki adalah data mereka. Data pada sebuah organisasi dapat dibagi menjadi dua, yaitu data umum dan data yang bersifat rahasia. Steganografi merupakan cara yang mendasar untuk membuat data rahasia dapat disembunyikan tanpa diketahui oleh pihak yang tidak diinginkan. Steganografi bekerja dengan cara menyembunyikan data rahasia pada carrier yang tidak dicurigai. Salah satu media komunikasi yang dapat digunakan sebagai media penyisipan pada proses steganografi adalah Voice over Internet Protocol. 
Sinyal suara analog yang didapat dari hasil rekaman suara percakapan VoIP kemudian diubah menjadi sinyal digital sehingga menghasilkan paket suara yang sudah berbentuk biner. Paket suara tersebut diubah terlebih dahulu menjadi 8 bit menggunakan teknik kompresi a-law untuk kemudian disisipi pesan rahasia dengan menggunakan metode LSB dan dikirimkan kepada penerima.
Dari pengujian, diperoleh bahwa data rahasia dapat dikirimkan dengan media VoIP menggunakan teknik steganografi yang dilakukan setelah paket suara dari VoIP diubah menjadi 8 bit untuk kemudian disisipkan data rahasia dan dikirimkan melalui proses VoIP. Hasil pengujian juga menunjukan bahwa setiap karakter kunci mempengaruhi 0,06% waktu total proses pengiriman pesan rahasia.","Information security has evolved significantly over the last few years. Most organizations realised that one of the most valuable assets are their data. Data in an organization can be divided into two, the general data and the confidential data. Steganography is a fundamental way to make confidential data hidden without being noticed by an unwanted party. Steganography works by hiding secret data on a carrier that is not suspected. One medium of communication that can be used as a medium insertion in steganography is Voice over Internet Protocol.
Analog voice signal obtained from the VoIP conversation is then converted into a digital signal so as to produce voice packets that have been shaped binary. The sound package is first converted into 8 bits using a compression technique-law and then inserted a secret message by using LSB and sent to the recipient.
From the test result, confidential data can be transmitted with VoIP media using steganographic technique performed after the voice packets of VoIP converted into 8 bits and then pasted data is confidential and sent via VoIP process. The test results also show that every key character affects 0.06% of total time of secret message delivery process.
",,,50365,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104317,,WINANTYO IMAN RAMADHA,PREDIKSI HARGA SAHAM MENGGUNAKAN JARINGAN SYARAF TIRUAN RESILIENT BACKPROPAGATION,,,"prediksi, saham, jaringan syaraf tiruan, resilient backpropagation, timeseries","Dr. Suprapto, M.I.Kom.",2,3,0,2016,1,"Salah satu alternatif dalam melakukan investasi adalah membeli saham, namun harga saham setiap waktunya selalu mengalami perubahan. Banyak orang yang berusaha melakukan prediksi harga saham untuk mengambil keputusan dalam melakukan investasi dengan tepat. Salah satu metode dalam melakukan prediksi adalah dengan menggunakan jaringan syaraf tiruan dan data historis harga saham (time series). Pada penelitian ini diimplementasikan jaringan syaraf tiruan resilient backpropagation untuk melakukan pelatihan data time series masing-masing perusahaan hingga diperoleh bobot jaringan dengan error terkecil. Bobot tersebut akan digunakan untuk melakukan prediksi untuk masing-masing perusahaan. Proses pengujian dalam sistem ini menggunakan data time series  dari harga saham 3 perusahaan besar di indonesia yaitu BCA, Gudang Garam, dan Indofood dengan kurun waktu 1 Mei 2015 hingga 1 Mei 2016. Penghitungan tingkat akurasi pelatihan dan prediksi diukur dengan menggunakan MSE (Mean Squared Error). Nilai MSE hasil uji kasus pelatihan resilient backpropagation untuk data BBCA sebesar 0.002708159, GGRM sebesar 0.001074818 dan INDOFOOD sebesar 0.002440852. Hasil prediksi dari tanggal 1 Mei 2016 hingga 15 Mei 2016 dengan data BBCA diperoleh 0.012026, untuk data GGRM sebesar 0.003485 dan untuk data INDOFOOD sebesar 0.001289.  Hasil pelatihan resilient backpropagation memperoleh nilai error yang kecil dibandingkan dengan backpropagation pada iterasi yang sama. Resilient backpropagation dapat mencapai error 0.00286354 sedangkan backpropagation hanya mencapai 0.01733526.","One of the alternatives to invest is to buy stocks, but stock price always changes. Many people have been trying to predict the stock price to decide which stocks are the most profitable to invest. One of the methods in predicting the price is to use the artificial neural network and historical data of the stock price (time series). In this research implemented resilient backpropagation neural network to do the time series data training of each company to obtain the weight of the network with the smallest error. The weights with the smallest error will be used to make predictions. The testing process in this system using time series data from May 1, 2015 until May 1, 2016 of 3 big companies in Indonesia that are BCA, Gudang Garam, and Indofood. The calculation of the level of training and prediction accuracy is measured using MSE (Mean Squared Error). MSE value of the training results of the resilient backpropagation neural network training is 0.002708159, 0.001074818 and 0.002440852  for data BBCA, GGRM and INDOFOOD respectively. The prediction result from May 1 until May 15, 2016 is 0.012026, 0.003485 and 0.001289  for data BBCA, GGRM and INDOFOOD respectively. Training results of the resilient backpropagation also faster in obtaining an the value of error is small compared to backpropagation. On the 100th iteration elastisch error propagation can achieve only 0.01733526 while resilient backpropagation reached 0.00286354.",2016-10-11 00:00:00,53,54822,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99456,,TRI MAYA SARI ,SEGMENTASI CITRA SATELIT BERBASIS CLUSTERING DENGAN KOMBINASI ALGORITMA K-MEANS DAN FUZZY C-MEANS,,,"segmentasi citra, citra satelit, clustering, k-means, fuzzy c-means, image segmentation, satellite image","Drs. Sri Mulyana, M.Kom.",2,3,0,2016,1,"Saat ini banyak negara telah meluncurkan satelit-satelit untuk berbagai penerapan seperti pertanian, pemetaan, manajemen bencana, penilaian dan pengawasan lingkungan, militer, dan metrologi. Citra-citra yang diperoleh dari satelit memuat informasi yang sangat banyak untuk diproses dan dianalisis. Pada pengolahan citra satelit, segmentasi adalah salah satu langkah vital untuk mengumpulkan informasi dari citra satelit. 
Algoritma dan/atau metode yang digunakan untuk segmentasi citra pun bermacam-macam, bahkan telah banyak pula usulan berbagai algoritma untuk segmentasi citra ini. Tiap-tiap algoritma yang diusulkan memiliki kelebihan dan kekurangannya masing-masing.
Dalam penelitian ini dikombinasikan algoritma k-means dan fuzzy c-means untuk melakukan segmentasi citra satelit berbasis clustering. Kombinasi algoritma ini mampu menghasilkan citra hasil segmentasi dengan kualitas yang lebih baik dan memerlukan waktu yang lebih cepat dibanding algoritma fuzzy c-means dan k-means, apabila diterapkan pada ruang warna YIQ dan jumlah cluster 5. Hal ini dapat dilihat dari jumlah citra hasil segmentasi yang dinilai lebih baik oleh beberapa  responden, yaitu 21.67% citra dengan menggunakan algoritma k-means, 38.33% citra dengan menggunakan algoritma fuzzy c-means, 40% citra dengan menggunakan algoritma kombinasi. Rata-rata waktu eksekusi pada ruang warna YIQ dan jumlah cluster 5 ini yaitu 10.477 detik dengan menggunakan algoritma k-means, 21.761 detik dengan mengggunakan algoritma fuzzy c-means, dan 10.925 detik dengan menggunakan algoritma kombinasi.","Nowadays many countries are launching satellites for various applications such as agriculture, mapping, disaster management, environmental assessment and monitoring, military nd metrology. The images collected from satellite contain huge amount of information for analysis and processing. One of the vital steps in satellite image processing is image segmentation, which is used to gather informations from satellite image. 
Algorithms and/or methods used for image segmentation are varied, there have been many researches conducted to find the most efficient algorithms for image processing. Each proposed algorithm has its own advantages and disadvatages.
This research combines k-means and fuzzy c-means algorithms for satellite image segmentation based on clustering. This combination gives better image segmentation quality and is faster than k-means or fuzzy c-means algorithms on YIQ color space and 5 clusters. It can be seen from the number of segmented images which is considered better by the respondents, 21.67% segmented images using k-means algorithm, 38.33% segmented images using fuzzy c-means algorithm, and 40% segmented images using combination algorithm. The average of execution time of satellite image on YIQ color space and 5 clusters is 10.477 second using k-means algorithm, 21.761 second using fuzzy c-means algorithm, and 10.925 using combination algorithm.",,,49775,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
102786,,SHEPTIANI PUTRI P,IMPLEMENTASI DAN ANALISIS ALGORITME CLUSTERING DBSCAN PADA SISTEM REKOMENDASI ITEM-BASED COLLABORATIVE FILTERING,,,"Sistem rekomendasi, collaborative filtering, clustering, DBSCAN.","Moh. Edi Wibowo, M.Kom., Ph.D.,",2,3,0,2016,1,"Pertumbuhan volume dan keberagaman informasi yang sangat pesat pada situs
internet dewasa ini membuat pengguna semakin sulit menemukan informasi yang
relevan. Untuk itu dibutuhkan sistem yang tepat untuk mengatasi permasalahan ini
salah satunya dengan sistem rekomendasi. Sistem rekomendasi yang ada diterapkan
menggunakan algoritme salah satunya item-based collaborative filtering yang mampu
memberikan rekomendasi data berdasarkan prediksi dari nilai kemiripan antar item.
Salah satu upaya dalam meningkatkan kualitas rekomendasi yaitu dengan
diterapkan metode clustering untuk pengelompokkan data. Pada penelitian ini
algoritme clustering DBSCAN diterapkan pada dataset e-commerce www.expedia.com.
Setelah proses clustering, dataset kemudian diproses oleh sistem rekomendasi
menggunakan item-based collaborative filtering. Hasil rekomendasi ini kemudian
dihitung jumlah rekomendasi dan Mean absolute error yang merupakan parameter
evaluasi. Hasil evaluasi ini yang dibandingkan dengan hasil rekomendasi sistem tanpa
clustering.
Implementasi algoritme clustering DBSCAN menurunkan presentase jumlah
rekomendasi dengan rata-rata penurunan sebesar 10.763%. Penurunan presentase
terjadi pada seluruh peningkatan nilai epsilon maupun jumlah titik minimum
berdasarkan komposisi data yang digunakan pada eksperimen. Implementasi algoritme
clustering DBSCAN meningkatkan akurasi prediksi dengan rata-rata penurunan nilai
Mean absolute error sebesar 0.0907. Implementasi algoritme clustering DBSCAN
mengurangi efisiensi dalam proses rekomendasi dengan rata-rata peningkatan waktu
yang dibutuhkan sebesar 0.28769 detik dalam satuan detik.","The Growth of volume and diversity of information which is very rapidly in
internet site today makes users harder to find a relevant information. Therefore, it is
need an appropriate system to solve this problem, that system is recommendation
system. These recommendations system are implemented by using one of algorithms
which is item-based collaborative filtering that able to recommend data based on
prediction of similar value between items.
One way to improve the quality of the recommendation is by implementing
clustering methods for grouping data. In this study DBSCAN clustering algorithm is
applied to e-commerce datasets from www.expedia.com. After the clustering process,
datasets are processed by a recommendation system using item-based collaborative
filtering. The result of this recommendation are then calculated the number of
recommendations and Mean absolute error which is an evaluation parameter. The
results of this evaluation will be compared with the results on the system without
clustering.
DBSCAN clustering algorithm implementation decrease the percentage of the
number of recommendations with the value of average decrease 10.763%. Decreasing
percentage does occur in an overall increase in the value of epsilon and the minimum
number of clustering points based on the composition of the data that used in the
experiment. DBSCAN clustering algorithm implementation improve prediction
accuracy with an average decrease in value of Mean absolute error of 0.0907. DBSCAN
clustering algorithm implementation decrease efficiency in the process of
recommendation with an average enhancement of the time amounted to 0.28769 in
seconds.",,,53244,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104841,,MUH FATKHAN ARIFUDIN,Analisis Sentimen Pada Tweet Tentang Pemain Bola Liga Primer Inggris Untuk Prediksi Fans' Player Of The Year Dengan Metode Support Vector Machine,,,"analisis sentimen, svm, support vector machine, linear regression, twitter","Edi Winarko, Drs., M.Sc., Ph.D",2,3,0,2016,1,"Liga Primer Inggris merupakan kompetisi sepak bola yang memiliki popularitas tinggi di dunia. Berbagai hal mengenai Liga Primer Inggris selalu ramai diperbincangkan, mulai dari tim, pemain, hingga pelatih. Penggunaan media sosial sekarang ini sebagai sarana untuk memperbincangkan berita dan opini pribadi para penggemar. Twitter merupakan media sosial yang populer digunakan baik tim, pemain, maupun penonton.  Informasi yang ada pada Twitter sering kali memuat sentimen penonton terhadap tim dan pemain. Informasi tersebut dapat dimanfaatkan untuk prediksi voting Fans' Player of The Year, salah satu penghargaan pemain terbaik tiap tahun versi penonton. Namun media sosial memiliki kredibilitas yang rendah dalam menyediakan informasi.
Pada penelitian ini, dibangun model klasifikasi sentimen yang mengklasifikasi tweet ke dalam 3 kelas yaitu positif, negatif, dan netral. Pendekatan analisis sentimen menggunakan metode Support Vector Machine (SVM). Hasil model klasifikasi kemudian digunakan sebagai variabel independen untuk pembuatan prediksi Fans' Player of The Year. Pendekatan model prediksi menggunakan metode regresi linier. Hasil model klasifikasi sentimen yang dibuat diharapkan mampu memprediksi hasil voting.
Model klasifikasi sentimen dibuat dengan 2 metode pembobotan yaitu tf dan tf-idf dengan variasi 3 kernel pada SVM yaitu linear, polinomial, dan RBF. Hasil performa metode SVM secara keseluruhan rata-rata nilai precision, recall, dan f1 score adalah 0.822, 0.825, dan 0.809. Model prediksi hasil voting menggunakan metode regresi linier memiliki nilai R2 sebesar 30,7% dan rata-rata mean absolute error sebesar 31,9%.","Premier League is a competition which had high popularity in the world. Various things about the Premier League is always discussed, like the teams, players, and coaches. The use of social media nowadays to discuss news and personal opinions of the fans. Twitter is a popular social media used both teams, players, and fans. The information contained on Twitter often contains the sentiments of teams and players. The information can be used for prediction of voting Fans' Player of the Year, the best player award of audiences version. But social media has low credibility in providing information.
In this research, a model created to classify tweet sentiment into three classes: positive, negative, and neutral. These sentiment analysis using Support Vector Machine (SVM) method. The results of the classification model then used as independent variables for prediction Fans' Player of the Year. The approach method of predictive models using linear regression. Results of sentiment classification model that has been created is expected to predict the outcome of voting.
Sentiment classification models created with two methods of weighting that is tf and tf-idf with 3 variations on SVM kernel which is linear, polynomial, and RBF. SVM performance results for the average values of precision, recall, and f1 score is 0822, 0825, and 0809. Prediction model of voting using linear regression method has a R2 value of 30,7% and average mean absolute error of 31,9%.",,,55535,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104075,,AGNES TODING,Sistem Pakar Menggunakan Metode Forward Chaining dengan Certainty Factor untuk Diagnosa Indikasi Intoksikasi Akibat Interaksi Obat,,,"sistem pakar, forward chaining, interaksi obat, intoksikasi, certainty factor / expert system, forward chaining, drug interactions, intoxication, certainty factor","Drs. Retantyo Wardoyo, M.Sc, Ph.D",2,3,0,2016,1,"Interaksi obat merupakan kondisi dimana terjadi perubahan efek yang dihasilkan oleh obat yang dikonsumsi bersamaan atau dalam jangka waktu berdekatan. Interaksi obat dapat menimbulkan efek positif atau negatif. Efek negatifnya dapat ditandai dengan munculnya gejala-gejala tertentu yang dapat berujung pada indikasi intoksikasi. Sementara itu, hanyalah ahli medis serta ahli farmakologi yang dapat mengetahui gejala-gejala tersebut. Sistem pakar dalam hal ini dapat digunakan untuk membantu dalam melakukan diagnosa.
Sistem pakar merupakan sistem yang meniru kepakaran seorang ahli dalam suatu bidang. Sistem ini dibuat dengan metode Forward Chaining dengan menerapkan Certainty Factor untuk masalah ketidakpastian. Sistem dibuat dengan 2 jenis user, yaitu pakar dan paramedis. Pakar dapat melakukan update pada data-data yang akan digunakan untuk proses inferensi, sementara paramedis hanya dapat menjalankan menu cek keracunan bagi pasien. 
Proses pengujian pada sistem dilakukan dengan melakukan pencocokan dari seluruh input terhadap output sistem serta perhitungan Certainty Factor yang dilakukan sistem dengan hasil perhitungan manual.
","Drug interaction is a condition where there is a change of the effects produced by drugs simultaneously. Drug interactions may lead to positive or negative effects. Negative effects can be characterized by the appearance of certain symptoms that can lead to an indication of intoxication. Meanwhile, only a medical expert and pharmacologist know about the symptoms. Expert systems in this case can be used to assist in making a diagnosis.
Expert system is a system that emulate the expertise of an expert in a field. The system is built with Forward Chaining method by applying Certainty Factor for the uncertainty problem. The system is made with two types of user, those are experts and paramedics. Expert can update the data to be used for the inference process, while paramedics can only operate a check toxicity menu for patients.
The testing process of the system is done by matching the input to the output of the entire system and Certainty Factor calculations performed by the system with the results of manual calculations.
",,,54603,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
95118,,RIZKI DERMAWAN,KLASIFIKASI TWEET DAN PENGENALAN ENTITAS BERNAMA PADA TWEET BENCANA DENGAN SUPPORT VECTOR MACHINE,,,"support vector machine, pengenalan entitas bernama, sosial media, bencana, named entity recognition, social media, disaster","Edi Winarko, Drs., M.Sc.,Ph.D.",2,3,0,2016,1,"Indonesia termasuk negera yang sering terkena bencana. Ketika terjadi sebuah bencana pada suatu daerah, banyak pengguna media sosial, khususnya pengguna Twitter, memberikan informasi terkait bencana. Informasi yang diberikan dapat berupa lokasi terjadinya suatu bencana, kondisi dari suatu daerah, ataupun kebutuhan dari masyarakat pada daerah bencana. Informasi tersebut dapat dimanfaatkan untuk pemetaan kejadian bencana atau pemetaan kebutuhan masyarakat ketika terjadi suatu bencana. Akan tetapi,
media sosial memiliki kredibilitas yang rendah sebagai penyedia informasi. Selain itu, tidak terstrukturnya data pada media sosial membuat pendataan akan lokasi, kondisi, dan kebutuhan masyarakat menjadi sulit.

Pada penelitian ini, dibangun sistem yang dapat mengklasifikasi apakah suatu tweet terkait bencana atau tidak. Jika tweet tersebut terkait dengan bencana, dilakukan pengenalan entitas terhadap tweet tersebut sehingga dapat diketahui lokasi yang dibicarakan, kondisi yang terjadi, dan kebutuhan masyarakat pada daerah bencana. Sistem pengklasfikasian tweet dan sistem pengenalan entitas bernama dibangun dengan supervised learning. Algoritma supervised learning yang digunakan adalah support vector machine.

Pada pengklasifkasian tweet, dicoba dua metode pembobotan yakni tf dan tf-idf . Pada penelitian ini, tf-idf unggul dari tf dengan rata-rata akurasi 78%. Sedangkan pada sistem pengenalan entitas bernama, dibandingan dua metode segmentasi entitas yakni BIO dan BILOU. Pada penelitian ini metode segmentasi BIO lebih unggul dibanding BILOU dengan F 1 Score 86%.","Indonesia is a country that is often affected by disaster. When there is a disaster in an area, many users of social media, especially Twitter users, provide information related to the disaster. The information provided can be the location of a disaster, the conditions of an area, or the needs of the people in the disaster area. Such information can be used for mapping the event of disaster or mapping needs of the community in the event of a disaster. However, social media has low credibility as an information provider. Moreover, unstructured of data on social media will make data collection of location, condition, and needs of the community becomes difficult.

In this research, system that can classify whether a tweet related to the disaster or not has been built. If the tweet is related to the disaster, the named entity is recognized so the location of disaster, a condition of the area, and the need of community is known. Tweet classification system and named entity recognition system built by supervised learning. Supervised learning algorithm that is used is a support vector machine.

In the classification of the tweet, two methods of weighting tf and tf - idf has been tried. In this study, tf-idf is better than tf with an average accuracy of 78%. In the named entity recognition system, two methods of segmentation entities BIO and BILOU is compared. In this study, BIO segmentation method is better than BILOU with F1 Score 86%.",,,45347,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
101006,,KARTIKA DWI BASWARA,Decision Support System for Selecting Personalized Tourism Destination Recommendation by Using AHP and Quantitative Scoring Method,,,"Decision Support System, AHP, Quantitative Scoring Method","Retantyo Wardoyo, Drs., M.Sc., Ph.D",2,3,0,2016,1,"Perkembangan informasi yang tersedia di internet dapat membuat turis merasa kebingungan dalam menentukan daerah wisata mereka. Kondisi ini disebabkan oleh berlimpahnya informasi pariwisata yang tidak disesuaikan dengan preferensi tiap individu. Sektor pariwisata dapat meningkatkan tingkat kepuasan turis dan meningkatkan level efektivitas menejemen pariwisata dengan mengintegrasikan dengan teknologi informasi dan komunikasi. Alasan tersebutlah yang melatarbelakangi pengembangan sistem yang dapat memberikan rekomendasi destinasi wisata untuk turis berdasarkan preferensinya.
Sistem pendukung keputusan didasarkan pada kecocokan preferensi turis dengan profil dari tiap daerah wisata. Ada beberapa parameter yang harus dibandingkan dengan metode AHP. Setelahnya sub parameter akan dinilai dengan menggunakan metode quantitative scoring dengan mencocokan atribut antara preferensi turis dengan atribut dearah wisata","The ever growing information available on the internet can be overwhelming for some tourists especially in defining their tourism destination. This condition is caused by the abundance of travel information which are not personalized. Tourism sector can increase tourist satisfaction as well as the effectiveness of resource management by integrating more with information and communication technology. Thus the initiative to develop this system which supports tourists to give destination recommendations based on their preferences.
The decision support system is based on the suitability of the tourist's preferences and the profile of each destinations. There are several parameters which tourists need to compare with pairwise method and evaluate from the least to the most important using AHP approach. Later on, the sub parameters from these parameters are being rated using quantitative scoring method as the approach based on their matched attributes between tourist's preferences and the destination profiling attributes.",,,51335,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96401,,CHRISTINA C L,"Comparison of K-Nearest Neighbor, Naive Bayes and Support Vector Machine for Character Recognition in Mobile Application",,,"mobile application, character recognition, OpenCV library, machine learning, svm, nbc, knn ","MHD. Reza M.I. Pulungan, S.Si., M.Sc., Dr.Ing.",2,3,0,2016,1,"         Dikarenakan adanya variasi teknik dan aplikasi, pengenalan karakter optis masih menjadi topik penelitian yang populer dalam bidang pengolahan citra digital. Dengan bertambahnya jumlah piranti bergerak yang menjadi sistem operasi masa depan, aplikasi ini dibuat berbasis piranti bergerak tersebut. Dengan adanya aplikasi ini, pengguna dapat dengan mudah mengenali karakter hanya dengan mengambil gambar melalui kamera telepon seluler. Open Computer Vision Library digunakan dalam membangun aplikasi ini karena library ini merupakan yang terpopuler.
 	Selain itu, algoritma machine learning juga digunakan dalam aplikasi ini pada proses pengenalan karaker. Beberapa algoritma machine learning yang digunakan dalam penelitian ini antara lain, k-nearest neighbor, na&Atilde;&macr;ve Bayes classifier dan support vector machine. Performa dari ketiga algoritma ini yang terdiri dari akurasi pengenalan karakter, memori yang digunakan dan waktu yang digunakan akan dibandingkan. 
	Dalam proses pengujian, algoritma k-nearest neighbor menunjukkan performa terbaik. Algoritma ini memiliki akurasi pengenalan karakter paling tinggi sebesar 94% dan menggunakan memori dan waktu paling sedikit dalam proses pelatihan dan proses prediksi. Hasil pengujian menunjukkan memori rata-rata yang digunakan sebesar 650300.7 KB pada proses pelatihan dan 650409.1 KB pada proses prediksi dan waktu rata &acirc;€“ rata yang digunakan sebesar 128.5 mikrodetik pada proses pelatihan dan 520.6 mikrodetik pada proses prediksi. 
","          Due to its variations of techniques and applications, optical character recognition is still the most popular research in digital image processing. As the increasing number of mobile devices and it becomes the future operating system, this application is built on mobile based. With this application, user can easily recognized the characters by capturing image using mobile phone camera. Open Computer Vision library is used to build the application as this library is the most popular one.
          Besides, machine learning algorithms are used for the recognition process in the application. There are k-nearest neighbor, naive Bayes classifier and support vector machine algorithms which are used in this research. The performances of these three algorithms are calculated including recognition accuracy, memory usage and time consumed.
          From testing process, k-nearest neighbor show the best performance. It has the highest character recognition accuracy at 94% and the least average memory usage and time consumed in the training and prediction process. The result shows that it consumed average memory of 650300.7 KB in training and 650409.1 KB in the prediction and consumed average time of 128.5 microseconds in training and 520.6 microseconds in prediction. 
",,,46714,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104594,,NUR RISTA KURNIAWATI,PENENTUAN DESTINASI WISATA FAVORIT BERBASIS ATURAN DAN ANALISIS SENTIMEN PADA TWEET BERBAHASA INDONESIA,,,"information extraction, social media, tourism, sentiment analysis, sentiment lexicon, rule-based, dictionary-based, gazetteer","Edi Winarko, Drs., M.Sc.,Ph.D.	",2,3,0,2016,1,"        Peningkatan industri pariwisata Indonesia saat ini tidak lepas dari peran teknologi. Selama berwisata turis juga akan membagikan pengalamannya secara real-time melalui upload foto maupun video serta aktif berkomentar di media sosial. Salah satu media sosial yang digemari di Indonesia yaitu Twitter. Dari tweet dapat diketahui destinasi wisata yang sedang hangat dibicarakan oleh masyarakat maupun wisatawan yang berkunjung di suatu kota. Selain itu tweet tersebut dapat mengandung informasi seputar kondisi pariwisata, pengalaman pengunjung, opini pengunjung di suatu tempat wisata, dan rekomendasi tempat wisata lainnya. Untuk itu diperlukan suatu sistem untuk memberikan informasi seputar tren pariwisata di suatu kota. Tentu saja informasi ini sangat bermanfaat untuk semua kalangan mulai dari wisatawan, masyarakat, pemerintah, hingga industri pariwisata di kota tersebut. 

        Dalam penelitian ini dibuat sistem yang dapat menentukan destinasi wisata favorit di suatu kota, memberikan informasi tren masyarakat berkaitan dengan pariwisata melalui hashtag favorit, memberikan informasi berupa opini masyarakat maupun wisatawan terhadap pariwisata melalui analisis sentimen, serta memberikan informasi mengenai waktu favorit pengguna Twitter di kota tersebut. Dalam penentuan destinasi wisata favorit digunakan kamus lokasi (gazetteer) untuk mengetahui banyaknya tweet di suatu destinasi wisata dan dibantu dengan analisis sentimen pada tweet tersebut. Salah satu pendekatan analisis sentimen adalah menggunakan leksikon sentimen. Pada penelitian ini dikembangkan leksikon sentimen untuk pariwisata berbasis aturan dengan pendekatan kamus.

         Dalam pengujian performa sistem dalam mengenali tweet pariwisata dilakukan tiga kali percobaan dengan tiga search query berbeda yaitu #Yogyakarta, #Bali, dan #Jakarta. Hasilnya didapat rata-rata recall 71,3%, precission 99,2%, f-measure 82,6%, dan akurasi 93,8%. Sementara itu dalam pengujian performa analisis sentimen dilakukan tiga kali percobaan dengan tiga search query berbeda yaitu 'Yogyakarta', 'Bali', dan 'Jakarta' dengan data uji berbeda dengan pengujian sistem dalam mengenali tweet pariwisata atau bukan. Hasilnya didapat rata-rata recall 73,1%, precission 100%, f-measure 84,1%,  dan akurasi 93,2%.
","       Nowadays, improvement of Indonesia's tourism industry can not be separated from technology. During vacation, tourists will share their experiences in real-time via upload photos and videos as well as active comments on social media. One of the most popular social media in Indonesia is Twitter. From tweets will be known top tourist destination that is popular and being discussed by people around city. Beside that, tweet also contain valuable information about the condition of tourist site, tourists opinion at the tourist sites, and alot of recommendation based on their experiences during the trip. Therefore we need a system to provide information about tourism trends in a city. Of course that information is very useful to all people start from tourists, the community, the government, also the tourism industry in the city.

       In this research,  system that can determine favorite tourist destination in the city, providing information about trends tourism through the top hashtag, provide information such as public opinion in tourism through sentiment analysis, as well as providing information about Twitter user's  favorite time has been built. In the determination of top tourist destinations used location dictionary (gazetteer) to find out how many tweets in a tourist sites and then used sentiment analysis on that tweet to know about the opinion of that tourist sites. One of the many approaches to do sentiment analysis is using sentiment lexicon. In this research, sentiment lexicon for tourism based on rule-based with dictionary approach has been developed. 

         Based on system testing that has been done, the systems' performance in recognizing tourism tweet conducted three experiments with three different search query are #Yogyakarta, #Bali, #Jakarta. The results obtained with average recall 71,3%, precission 99,2%, f-measure 82,6%, and accuracy 93,8%. Then the systems' performance in sentiment analysis has been done too with different data testing conducted three experiments with Yogyakarta, Bali, and Jakarta. The results obtained with average recall 73,1%, precission 100%, f-measure 84,1%,  and accuracy 93,2%.
",,,55240,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104083,,DWI CAHYA PRAMANDA,NOISE REDUCTION IN ARDUINO-BASED REAL TIME AIR POLLUTANT MONITORING SYSTEM BY USING SMOOTHING ALGORITHMS,,,"Noise Reduction, ISPU, Arduino, Simple Moving Average, Kalman Filter Algorithm, Savitzky-Golay Algorithm","Dr. Ing. MHD. Reza Pulungan, M. Sc.",2,3,0,2016,1,"Microcontroller seperti halnya Arduino UNO adalah bagian dari sistem tertanam yang telah digunakan secara luas untuk menyelesaikan banyak permasalahan termasuk isu polusi udara. Dengan menggunakan kemampuan Arduino, informasi tentang polusi udara yang tersebar disekitar dapat disajikan dalam bentuk aplikasi Web pada waktu tertentu. Informasi tersebut berdasarkan ISPU atau Indeks Standar Pencemaran Udara sesuai dengan keputusan Menteri Lingkungan No. KEP-45/MENLH/10/1997 tentang indeks polusi udara. Empat dari lima polusi yang dipantau oleh sistem ini yaitu karbon monoksida (CO), partikulat/debu (PM10), ozon (O3) dan nitrogen dioksida (NO2). Sistem ini menggunakan sensor MQ-7, MQ-131 dan DSM501A sebagai pendeteksi polusi dengan menggunakan Arduino.
Tetapi, di penelitian ini, sensor yang dipergunakan memberikan hasil yang tidak terduga ketika dibandingkan dengan pengukur standar. Nilai-nilai keluaran tersebut naik turun secara tidak teratur meskipun kenaikannya terlihat mengikuti nilai standar. Hal tersebut terjadi dikarenakan terdapat noise yang muncul akibat komponen-komponen dan rangkaian yang terdapat pada sistem. Jadi, algoritma smoothing seperti Simple Moving Average (SMA), Kalman Filter Algorithm (KFA) dan Savitzky-Golay Algorithm (SGA) dapat meminimalisir noise tersebut. Untuk itu, penelitian ini mencoba membandingkan ketiga algoritma tersebut dengan mengimplementasikannya ke dalam program pada sensor MQ-7.
Berdasarkan hasil pengujian, Kalman Filter Algorithm memberikan hasil terbaik disbanding algoritma lainnya. Dengan menggunakan metode statistik, Kalman Filter Algorithm memberikan nilai ketidakpastian 0.807 and nilai ketidakpastian regresi 0.847 dibandingkan dengan nilai pada pengukur CO yang standar. Kemudian, KFA menggunakan storage sebesar 16.536 bytes dan mengalokasikan dynamic memory sebesar 855 bytes.","A microcontroller such as Arduino UNO as a part of embedded system has been used widely to solve many problems including air pollutant issues. By using Arduino capabilities, information about air pollutant that spread around can be presented in the form of Web Application in a specific time. The information is based on ISPU (Indeks Standar Pencemaran Udara) or Air Pollutant Standard Index in accordance with the Decree of the Minister of the Environment No. KEP 45 / MENLH / 1997 on air pollution index. Four out of five pollutants are monitored by the system such as carbon monoxide (CO), particulate matter (PM10), ozone (O3) and nitrogen dioxide (NO2). The system use MQ-7, MQ-131 and DSM501A sensors to support Arduino as the pollutant detectors.
However, in this research, the sensors give unexpected output values when compared to the standard meter. Those values go up and down irregularly although the increase may follow the standard values. It happens because there are noises occur due to the components and circuit that is arranged in the system. So, smoothing algorithm such as Simple Moving Average (SMA), Kalman Filter Algorithm (KFA) and Savitzky-Golay Algorithm (SGA) can minimize the noises. Thus, this research compares those three algorithms by implement them into the MQ-7 program.
Based on testing result, Kalman Filter Algorithm gives the best performance than the other algorithms. By using statistical method, Kalman Filter Algorithm gives 0.807 for the uncertainty and 0.847 for the regression uncertainty compared to the value from standard CO meter. Moreover, it uses 16.536 bytes of storage usage and 855 bytes of dynamic memory allocation.",2016-10-03 00:00:00,53,54622,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
94612,,TEGUH PUJI WIDIANTO,TEKNIK  REALTIME DATA PROCESSING UNTUK MONITORING TWEET MENGGUNAKAN ANALISIS SENTIMEN DENGAN ALGORITMA NAIVE BAYES   STUDI KASUS  TWEET YANG TERKAIT INSTITUSI UGM,,,"analisis sentimen, realtime processing, monitoring kehumasan","Agus Sihabuddin, S.Si., M.Kom",2,3,0,2016,1,"Twitter.com mempunyai lebih dari 316 juta pengguna aktif setiap bulannya dan mempunyai lebih dari 500 Juta  tweet dipublikasikan setiap hari. Khususnya Indonesia, negara dengan penduduk kurang lebih 255 juta pada tahun 2015 merupakan salah satu negara teraktif dalam menggunakan Twitter sebagai media sosial. Karena pengguna Twitter di Indonesia lebih dari 50 juta pengguna, menjadikan Twitter sangat berpengaruh terhadap kehidupan masyarakat indonesia. Hal ini dibuktikan dengan beberapa kasus yang menjadi isu nasional karena banyak perhatian di media Twitter. Oleh karena itu, diperlukan perhatian khusus untuk meminimalisir dampak dari Twitter salah satunya dengan memonitor tweet yang relevan secara realtime.  
Untuk mengatasi kendala tersebut, penelitian ini bertujuan untuk membuat tool monitoring tweet menggunakan teknik data processing secara realtime. Dengan menggunakan analisis sentimen data hasil pengolahan diklasifikasikan dan ditampilkan dalam media monitoring secara realtime. Pemrosesan data secara realtime dimulai dengan melakukan filter tweet dengan kata kunci tertentu sehingga mendapatkan tweet yang  mendekati atau berkaitan dengan UGM. Hasil filter tweet kemudian diproses menggunakan librari Storm dan diklasifikasikan  menjadi tweet dengan sentimen positif, negatif dan netral. Analisis sentimen dilakukan  dengan menggunkan algoritma Naive Bayes dengan terlebih dahulu membuat data latih yang disimpan dalam Redis Database. 
Librari seperti Twitter4j, Apache Storm dan Redis dapat digunakan untuk memproses data secara realtime. Teknik tersebut dapat diterapkan untuk memonitoring sentimen tweet yang berkaitan dengan Institusi UGM secara realtime. Protokol websocket dan NodeJs membantu untuk menampilkan data hasil prosesing secara realtime dengan latency yang rendah.


"," 
Twitter.com has aproximately 316 million active users montly and more than 500 million tweets are published every day. More specific in Indonesia, Country that have aproximately 255 Million citizen is country that has most active user in the world to use Twitter. Indonesia  is expected more than 50 Million have Twitter account, so Twitter is most influence social media in indonesian people. Many cases become national effect because published in Twitter. Therefore, It is required spesific attention to minimalize impact from using Twitter. One of way to solve the problem  is monitor the indonesian tweet that relevan with institution in realtime. 
To overcome these obstacles, the study aims to create monitoring tools that use technique of realtime data processing. The result of data processing tweets were classified with Sentiment analisys before  tweets are visualized in web browser realtimely. The data processing is started when application filter tweets with specific keyword to get tweets that related with UGM institution. Storm library is used to process filtered tweet in realtime.  The result of processing tweet is classified into  tweets containing the positive  sentiment, negative  sentiment and neutral sentiment with Naive Bayes Algorithm. Before it used Naive Bayes algorithm, application need training data that save in Redis Database. 
Twitter4J, Apache Storm and Redis Database can be used to process data tweet in realtime. make data processing tool that can monitor tweet in realtime. Websocket protokol and NodeJs can visualize result of processing data in realtime with lower latency. 
",,,44832,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
106391,,ATIKA PRIMA N,IMAGE ENCRYPTION USING RUBIK'S CUBE PRINCIPLE IN MOBILE PHONE ENVIRONMENT,,,"Image Encryption, Rubik's Cube Principle, Mobile Environment","Anny Kartika Sari, S.Si, M.Si, Ph.D",2,3,0,2016,1,"Saat ini, keamanan informasi menjadi lebih penting dalam penyimpanan data dan transmisi. Gambar yang banyak digunakan dalam proses yang berbeda. Oleh karena itu, keamanan sebuah data gambar dari penggunaan yang tidak sah adalah penting. Enkripsi gambar memainkan peran penting dalam bidang menyembunyikan informasi, karena itu tidak ada hacker atau nguping, termasuk administrator server dan lain-lain memiliki akses ke gambar asli.
Algoritma enkripsi gambar yang digunakan dalam penelitian ini adalah prinsip kubus Rubik. Ide dasar dari prinsip kubus Rubik dalam kriptografi gambar menyiratkan permutasi dari piksel dalam baris individu atau kolom dalam sebuah gambar menggunakan pergeseran melingkar di setiap arah mungkin. Penelitian ini juga menguji perbedaan antara gambar asli dan didekripsi gambar untuk memastikan tidak ada kehilangan data.
Kinerja proses enkripsi dan dekripsi baik menggunakan MSE, PSNR, analisis histogram dan uji waktu. Untuk proses enkripsi, MSE rendah, PSNR tinggi dan histogram adalah seragam yang menunjukkan keamanan yang tinggi terhadap serangan. Untuk proses dekripsi, MSE adalah nol, PSNR adalah tak terhingga dan histogram adalah sama dengan gambar histogram asli yang berarti gambar didekripsi mirip dengan gambar aslinya.","Nowadays, information security is becoming more important in data storage and transmission. Images are widely used in different processes. Therefore, the security of an image data from unauthorized uses is important. Image encryption plays an important role in the field of information hiding, therefore no hacker or eavesdropper, including server administrators and others have access to original image.
	The image encryption algorithm used in this research is Rubik's cube principle. The basic idea of Rubik's cube principle within image cryptography implies permutation of pixels within individual rows or columns within an image using a circular shift in any possible direction. This research also tests the differentiation between the original image and the decrypted image to ensure there is no data loss. 
	The performance of encryption and decryption process are good using MSE, PSNR, histogram analysis and timing test. For the encryption process, the MSE is low, the PSNR is high and the histogram is uniform which indicates high security against attack. For the decryption process, the MSE is zero, the PSNR is infinity and the histogram is same as the original image histogram which means the decrypted images are similar to the original images.",,,57025,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98717,,TEGUH APRIYANTO,PERBANDINGAN TEKNIK KOMPRESI TEKSTUR PADA PERANGKAT ANDROID ,,,"tekstur, vector quantization, rendering, kompresi, android","Mhd. Reza Pulungan, S.Si., M.Sc., Dr. - Ing.",2,3,0,2016,1,"Tekstur merupakan bagian penting pada aplikasi 3D maupun 2D. Sekarang telah banyak beredar pula aplikasi 2D/3D pada mobile phone tidak terkecuali android. Melakukan rendering pada android merupakan hal yang menantang, hal ini dikarenakan terbatasnya sumber daya yang tersedia. Kompresi tekstur merupakan salah satu cara guna mengatasi masalah tersebut.
Penelitian ini bertujuan untuk membantu mengatasi masalah diatas dengan cara membangun sebuah skema guna melakukan rendering dari tekstur terkompresi. Tekstur dikompresi dengan metode vector quantization. Skema tersebut kemudian diimplementasikan pada perangkat android dan dilakukan analisis terhadap implementasi tersebut. 
Penelitian ini menghasilkan sebuah skema rendering dari tekstur terkompresi beserta implementasinya pada perangkat android. Pada penelitian ini juga didapatkan rata-rata compression rate 10,2.
","Texture is an essential part of 3D or even 2D application. Nowadays, 2D/3D applications are already widespread among mobile phone including android platform. Rendering on android phone is challenging, since there are plenty amount of resource limitations. Texture compression is one way to surmount this hardware limitations.      
This research is designed to mitigate those problems by building a scheme that could render texture from compressed data. Vector quantization technique is applied to compress the data. This scheme is then implemented on android platform and performed some analysis and comparison over conventional technique.
The implementation of rendering compressed texture on android can be obtained. The average of compression rate obtained from experiments held in this research is 10,2.
",,,49015,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96928,,DERTA ISYAJORA RAKHMAN,KLASIFIKASI TWEET SPAM DAN VALID MENGGUNAKAN SELEKSI FITUR CHI SQUARE DAN ALGORITMA NAIVE BAYES CLASSIFIER PADA TWEET BERBAHASA INDONESIA,,,"Twitter, JalananYogya, Tweet, Spam, Naive Bayes Classifier, Chi Square","Aina Musdholifah, M.Kom., Ph.D.",2,3,0,2016,1,"Pengguna Twitter bebas untuk mengirimkan tweet dengan isi konten yang beragam, termasuk tweet spam. Salah satu aplikasi yang memanfaatkan Twitter API adalah JalananYogya. JalananYogya adalah platform crowdsourcing yang mampu mengumpulkan laporan jalan rusak di Yogyakarta dari masyarakat melalui Twitter. Untuk menjaga integritas data JalananYogya, diperlukan sebuah sistem yang dapat melakukan filtering terhadap tweet valid dan tweet spam. Sebagai langkah awal dalam pembuatan sistem spam filtering, akan dilakukan pengujian terhadap algoritma klasifikasi yang potensial. Fokus penelitian ini adalah untuk melakukan evaluasi performa algoritma Naive Bayes Classifier untuk mengklasifikasikan tweet yang dipadukan dengan seleksi fitur Chi Square untuk meningkatkan performa. Metode  yang  digunakan  dalam  penelitian  ini  adalah Multinomial Naive Bayes Classifier dan Bernoulli Naive Bayes Classifier. Berdasarkan pengujian yang dilakukan, akurasi terbaik dihasilkan oleh sistem yang menggunakan Naive Bayes Classifier model Multinomial yang dipadukan dengan seleksi fitur Chi Square, yakni 95 %.
","Twitter users have no limitation to send any tweet that contain diversified information, including spam. JalananYogya is one of the application that use that feature. JalananYogya is crowdsourcing platform  that  leverages  community  participation  to  report  damaged roads  in  Yogyakarta using Twitter. To maintain JalananYogya data integrity,  system that can perform filtering on valid report and spam report is required. As a first step in making spam filtering system,  potential classification algorithms will be tested. The focus of this research was to evaluate  the performance of Naive Bayes Classifier algorithm to classify tweet combined with Chi Square feature selection to improve performance. The  method used  by  this  reasearch  is Multinomial Naive Bayes Classifier and Bernoulli Naive Bayes Classifier. Based on the tests performed, 95 % is the best accuracy produced by systems that have been built on this research. That system using Multinomial Naive Bayes Classifier methods combined with Chi Square feature selection. 
",,,47327,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104100,,AVITA TRI UTAMI,KLASIFIKASI MUSIK BERDASARKAN GENRE DAN MOOD MENGGUNAKAN SUPPORT VECTOR MACHINE,,,"klasifikasi musik, data mining, support vector machine ","Aina Musdholifah, S. Kom., M.Kom., Ph.D",2,3,0,2016,1,"Musik dengan genre dan mood tertentu memberikan beberapa manfaat. Musik classic dapat merangsang kreativitas dan motivatif dalam otak .Musik yang tenang dapat meningkatkan kemampuan aritmatik dan memori pada umur 10-12 tahun. Selain itu, mengingat koleksi dari musik digital yang semakin bertambah menyebabkan keberadaan program yang dapat melakukan klasifikasi musik berdasarkan genre dan mood menjadi sesuatu yang penting. Dalam penelitian ini dilakukan klasifikasi musik berdasarkan genre dan mood menggunakan Support Vector Machine (SVM). 
Genre yang dipilih dalam penelitian ini adalah pop, rock, jazz dan classic. Sedangkan mood yang digunakan berdasarkan Robert Thayer&acirc;€™s traditional model of mood. Program menerima input berupa data musik yang sebelumnya dilakukan preprocessing. Selanjutnya dilakukan ekstraksi fitur yaitu kombinasi dari MFCC dan rhythm pattern. Vektor fitur yang diperoleh berdimensi tinggi sehingga dilakukan proses PCA. 
Berdasarkan vektor fitur yang diperoleh, dilakukan klasifikasi menggunakan SVM dengan pendekatan ECOC (Error Correcting Output Code). Akurasi terbaik yang diperoleh sebesar 80% untuk genre dan 50,8% untuk mood menggunakan kernel gaussian RBF dengan sigma 2. ","Music with specific genre and mood give several benefits. Music classic can stimulate creatifity and motivation in the brain. Calm music can improve the ability of arithmetic and memory to the children aged 10-12 years.  Moreover, collection of digital music that growing fast cause the existance  of program that can perform classification music based on genre and mood becomes important. In this research, music is classified based on genre and mood using Support Vector Machine. 
Kind of genre that selected in this research are pop, rock, jazz and classic. While kind of the mood based on Robert Thayer's traditional models of mood. Program receives input music that has been do preprocessing. Then, program perform feature extraction, combination of MFCC and rhythm pattern. Dimension of feature vectors is high so it do PCA process. 
Based on feature vector that has been obtained, it classified using SVM with ECOC (Error Correcting Code Output) approach. The best accuracy are 80% for genre and 50,8% for mood using a gaussian kernel with sigma RBF 2.
",,,54629,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98727,,KHOIRUL UMAM  M,PERBANDINGAN ALGORITMA METODE DETEKSI OUTLIER PADA DATA KATEGORIK,,,"deteksi outlier, data kategorik, AVF, NAVF, AEVF, data mining, pre-processing data, detection rate, waktu tempuh","Nur Rokhman, S.Si., M.Kom",2,3,0,2016,1,"Era perkembangan teknologi informasi saat ini, memungkinkan untuk menyimpan data dalam jumlah besar. Data mining merupakan sebuah disiplin ilmu yang mempelajari metode untuk mengekstrak pengetahuan atau menemukan suatu pola tertentu dari suatu kumpulan data yang besar. Pada data mining, data dengan karakteristik yang berbeda dari data pada umumnya serta mempunyai kemunculan yang relatif sedikit disebut sebagai sebuah outlier. Pada saat ini telah banyak dikembangkan metode deteksi outlier, namun umumnya untuk data numerik. Pada penelitian ini akan dibahas perbandingan metode deteksi outlier pada data kategorik. Adapun algoritma yang akan dibandingkan pada penelitian ini adalah AVF, NAVF, dan AEVF. Pemilihan ketiga algoritma tersebut karena masing-masing algoritma tersebut unggul pada saat dilakukan pengujian pada penelitian yang sudah dilakukan sebelumnya.
Pada penelitian ini akan dilakukan analisis perbandingan antara algoritma AVF, NAVF, dan AEVF, meliputi detection rate dan waktu tempuh algoritma. Pada perbandingan detection rate, dataset akan dibagi menjadi data normal dan outlier kemudian data tersebut digabungkan dan dilakukan deteksi outlier. Sedangkan pada perbandingan waktu tempuh, dilakukan perhitungan waktu berdasarkan perubahan dimensi dataset, jumlah atribut dan perbedaan nilai atribut.
Hasil dari penelitian ini, algoritma AVF memberikan akurasi dan waktu yang lebih baik dibanding NAVF dan AEVF. Namun AVF mempunyai kelemahan yaitu harus memasukan k outlier, alternatif solusinya adalah menggunakan NAVF yang mempunyai akurasi dan waktu tempuh yang berbeda sedikit dibawah AVF namun tidak memerlukan masukan k outlier. AVF dan NAVF cocok digunakan pada dataset dengan jumlah atribut dan perbedaan nilai atribut yang sedikit serta persebaran data yang merata. Sedangkan AEVF cocok untuk digunakan pada kondisi sebaliknya. Hal yang mempengaruhi waktu eksekusi algoritma AVF, NAVF dan AEVF adalah perubahan jumlah dimensi data, jumlah atribut data, dan jumlah nilai pada atribut data.","In the development of information technology today, it is possible to store large amounts of data. Data mining is a discipline that studies the method to extract knowledge or find a particular pattern of a large data set. In the data mining, the data with different characteristics and have little appearance known as outliers. At this time has been developed outlier detection methods, but generally for numerical data This research will be discussed outlier detection method comparison on categorical data as for the algorithm to be compared in this study are AVF, NAVF, and AEVF. The third Selection of these algorithm is because each algorithm has a good result in research that has been done before.
This research will perform a comparative analysis between AVF, NAVF and AEVF algorithms, includes the detection rate and the running time algorithm. In the comparison of detection rate, the dataset will be divided into normal data and outliers then the data is combined and performed outlier detection. Then on the comparison running time, the calculation will be done based on change of dimensional dataset, number of attributes, and distict attribute value per attribute.
The results of this research is AVF algorithm provides outlier detection accuracy and a better time than NAVF and AEVF. But AVF has the disadvantage that must include k outlier, an alternative solution is to use NAVF which has the accuracy and the running time is a little different under AVF but don&acirc;€™t require input of k outlier. AVF and NAVF algorithms are suitable for use in a dataset which the number of attributes and distinct attribute values per attribute are little differences and also uneven distribution of data. While AEVF suitable for use in conditions contrary AVF and NAVF. The thing that affects the algorithm running time AVF, NAVF and AEVF are, changes of the number of data dimensions, changes of number attributes, and change of distict attribute value per attribute.",,,48991,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107687,,SIGIT HANAFI,"SISTEM REKOMENDASI KULINER FAVORIT DENGAN INTEGRASI DATA MEDIA SOSIAL (FOURSQUARE, INSTAGRAM DAN TWITTER)",,,"analisis sentimen, lexicon based, alchemy api, simple additive weighting, rekomendasi tempat kuliner, sentiment analysis, lexicon based, alchemy api, simple additive weighting, culinary recommendation","Edi Winarko, Drs., M.Sc., Ph.D.",2,3,0,2016,1,"Perkembangan media sosial dan pengguna media sosial semakin meningkat. Media sosial dapat dijadikan sumber informasi. Salah satu informasi yang didapatkan dari media sosial adalah opini pengguna mengenai suatu hal. Opini memiliki nilai sentimen positif, negatif atau netral. Salah satu opini yang dapat dimanfaatkan adalah opini mengenai lokasi tempat kuliner. Opini lokasi tempat kuliner dapat dijadikan kriteria pertimbangan pengambilan keputusan pemilihan lokasi tempat kuliner.
Metode untuk mendapatkan sentimen data teks disebut Analisis Sentimen. Pada penelitian ini dilakukan dengan menggunakan metode lexicon based dan layanan cloud service Alchemy API. Sentimen data teks dari Twitter dan Instagram diintegrasikan dengan informasi dari media sosial Foursquare sebagai kriteria pembobotan dengan metode SAW. Pembobotan SAW menghasilkan rangking rekomendasi tempat kuliner dengan bobot tertinggi ada pada peringkat pertama.
Pada proses analisis sentimen, metode lexicon based menghasikan nilai akurasi rata - rata 75,5% untuk data teks berbahasa Indonesia, sedangkan Alchemy API menghasilkan nilai akurasi rata - rata 74% untuk data teks berbahasa Inggris, sedangkan pada proses pembobotan dengan SAW, nilai bobot kriteria terbaik adalah check-in pengguna: 0,3, jumlah check-in :0,3, tweet positif : 0,1, tweet negatif : 0,1, caption positif : 0,1 dan caption negatif : 0,1.
Kata kunci: analisis sentimen, lexicon based, alchemy api, simple additive weighting, rekomendasi tempat kuliner","The development of social media and its users are so rapid recently. Social media can be seen as a source of information. Its users opinion towards something is a valuable information for some people. Opinion had a sentiment score of positive, negative and neutral. One of social media users opinion which can be capitalized here is information about culinary location. Opinion on social media can be used as criteria to choose a culinary site by other users.
Sentiment analysis is a method which is used to obtain sentiment score from text that contain opinion. In this study sentiment analysis was conducted by using lexicon based and Alchemy API cloud service. Sentiment data in form of text from Twitter and Instagram would be integrated with the information from Foursquare as value of criterion by using SAW method. The value of this SAW method produces culinary recommendation ranks in which first rank scores the biggest value among others.
The result of this analysis shows sentiment analysis process with lexicon based has accuracy rate of 75% for Indonesian data text, while Alchemy API scores 74% accuracy percentage for English data text. The valuing process with SAW shows the best value criteria is user check-in user: 0,3, the number of check-in: 0,3, positive tweet: 0,1, negative tweet: 0,1, positive caption: 0,1 and negative caption: 0,1.

Keyword: sentiment analysis, lexicon based, alchemy api, simple additive weighting, culinary recommendation
",,,58266,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98217,,STEFAN DERIAN H,EXPERT SYSTEM WITH CERTAINTY FACTOR FOR DIAGNOSING PULMONARY DISEASE AND GENETIC ALGORITHM APPLICATION IN DRUG ADVISORY SYSTEM,,,"Expert system, Certainty factor, Genetic algorithm, Pulmonary disease","Mrs. Aina Musdholifah,S.Kom.,M.Kom.,Ph.D",2,3,0,2016,1,"An expert system has been widely used for solving many problems including diagnosing disease. This research discusses the development of the expert system for detecting the lungs disease such as Tuberculosis. The knowledge base is represented as rule-based on an expert system that contains rules for inferring the solution. The expert system uses forward chaining method for solving inferring the disease based on the rules. In addition, Uncertainty management method namely certainty factor is used by the system for detecting the diseases because there are so many uncertain factors in diagnosing diseases. 
This research also discusses the application of the genetic algorithm. A genetic algorithm has been used in many varieties of fields and solves the problems of the optimization. In this research, optimizing the patent drugs purchase based on the users' budget and the needed generic medicines for the diseases are the aims of the genetic algorithm. Moreover, the encoding of the algorithm is integer encoding and the selection method is roulette wheel selection. Therefore, the method of crossover and mutation are selected to fit in the integer encoding. Finally, the method for updating the generation is steady update method. Iteration of the genetic algorithm is set by the users and the algorithm stops after the maximum iteration or the patent drugs price which is lower than users' budget has been found. 
The expert system is tested based on the 50 data of the patients and 20 data from the doctors' input. The experiment result of 50 data from the patients shows that 98 percent of accuracy in diagnosing the diseases. Meanwhile, 20 data which is from doctors' input shows that the result is 100 percent. In other hand, to test the genetic algorithm part when optimizing the patent drugs purchase, the 50 patients data are used and the result shows that 98 percent of the data has the minimum price suggested below the budget of the patients.
","An expert system has been widely used for solving many problems including diagnosing disease. This research discusses the development of the expert system for detecting the lungs disease such as Tuberculosis. The knowledge base is represented as rule-based on an expert system that contains rules for inferring the solution. The expert system uses forward chaining method for solving inferring the disease based on the rules. In addition, Uncertainty management method namely certainty factor is used by the system for detecting the diseases because there are so many uncertain factors in diagnosing diseases. 
This research also discusses the application of the genetic algorithm. A genetic algorithm has been used in many varieties of fields and solves the problems of the optimization. In this research, optimizing the patent drugs purchase based on the users' budget and the needed generic medicines for the diseases are the aims of the genetic algorithm. Moreover, the encoding of the algorithm is integer encoding and the selection method is roulette wheel selection. Therefore, the method of crossover and mutation are selected to fit in the integer encoding. Finally, the method for updating the generation is steady update method. Iteration of the genetic algorithm is set by the users and the algorithm stops after the maximum iteration or the patent drugs price which is lower than users' budget has been found. 
The expert system is tested based on the 50 data of the patients and 20 data from the doctors' input. The experiment result of 50 data from the patients shows that 98 percent of accuracy in diagnosing the diseases. Meanwhile, 20 data which is from doctors' input shows that the result is 100 percent. In other hand, to test the genetic algorithm part when optimizing the patent drugs purchase, the 50 patients data are used and the result shows that 98 percent of the data has the minimum price suggested below the budget of the patients.

",,,48554,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104107,,DIYAH UTAMI KUSUMANING PUTRI,Implementasi Inferensi Fuzzy Mamdani untuk Keperluan Sistem Rekomendasi Berita Berbasis Konten,,,"sistem rekomendasi, berbasis konten, berita, inferensi fuzzy, mamdani","I Gede Mujiyatna, S.Si, M.Kom",2,3,0,2016,1,"Banyaknya berita online yang tersedia membuat pembaca sulit untuk mendapatkan informasi yang cepat dan relevan sesuai dengan kebutuhannya. Pada penelitian ini akan dilakukan kajian bagaimana mengimplementasikan sistem rekomendasi berita berbasis konten untuk user anonim. User anonim merupakan user yang tidak memiliki profil user dan riwayat akses sebelumnya. Sistem rekomendasi berita berbasis konten menentukan rekomendasi berdasarkan kemiripan artikel berita dan menerapkan sistem inferensi fuzzy untuk menentukan berita mana yang akan direkomendasikan kepada user. 
Penelitian ini menunjukkan bahwa sistem inferensi fuzzy mamdani dapat digunakan untuk memberikan rekomendasi personal kepada user berdasarkan berita yang sedang dibaca user saat ini. Hasil rekomendasi ditampilkan dalam bentuk ranking berdasarkan berita yang tingkat relevansinya lebih tinggi ke yang lebih rendah. Sistem telah berhasil mendapatkan hasil rekomendasi berita dengan nilai rata-rata precision 0,970989, nilai rata-rata recall 0,871204 dan nilai rata-rata
f-measure 0,907818.
","The various number of provided online news make readers find difficulty to get the fastest and most relevant information based on their need. In this research, a study about how to implement content-based news recommendation system for
anonymous users. Anonymous user is user who does not have a user profile and access history before. Content-based news recommendation system determines recommendation based on similarity of news articles and applies fuzzy inference
system to determine which news will be recommended to the user.
This research showed that mamdani fuzzy inference systems could be used for giving personaliz recommendation to the user based on the news that was currently read. Recommendation results were shown in the form of rank based on the news that has highest relevancy rate to the lowest. System has succesfully got news recommendation result with average precision value 0,970989, average recall
value 0,871204 and average f-measure value 0,907818.",2016-10-03 00:00:00,53,54649,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99501,,ZAMAHSYARI,ANALISIS SENTIMEN PADA BERITA EKONOMI BERBAHASA INDONESIA MENGGUNAKAN MAJORITY VOTE CLASSIFIER,,,"majority vote, analisis sentimen, berita online","Arif Nurwidyantoro, S.Kom., M.Cs.",2,3,0,2016,1,"Internet sebagai media publikasi menghasilkan artikel dan berita yang terbagi menjadi beberapa kategori, diantaranya adalah politik, ekonomi, olahraga, dan kesehatan. Setiap media publikasi memiliki kecenderungan untuk mempublikasikan berita dengan sentimen positif atau negatif. Sentimen yang terkandung dalam berita dapat mempengaruhi pandangan masyarakat terhadap suatu hal atau kebijakan pemerintah. Topik ekonomi adalah bahasan yang menarik untuk dilakukan penelitian karena memiliki dampak langsung kepada masyarakat Indonesia. Oleh sebab itu penelitian ini dikhususkan untuk melakukan analisis sentimen berita ekonomi yang didapat dari berbagai media online berbahasa Indonesia.

Decision tree adalah algoritma sederhana yang memiliki alur kerja menyerupai sebuah pohon keputusan yang dapat diterapkan untuk melakukan klasifikasi berita, random forest adalah algoritma yang pada dasarnya adalah kumpulan dari banyak decision tree, dan support vector machine adalah algoritma klasifikasi yang menerapkan konsep pembuatan bidang pemisah antar kelas. Penelitian ini akan menggunakan ketiga algoritma tersebut ditambah dengan algoritma penggabungan antara ketiganya menggunakan metode majority vote classifier.

Pengujian penelitian menggunakan 10-fold cross validation yang didapat dari berbagai media berita online menghasilkan nilai precision terbaik sebesar 75% oleh algoritma majority vote classifier, nilai recall terbaik sebesar 100% oleh algoritma support vector machine, dan nilai accuracy terbaik sebesar 72% yang diraih oleh algoritma support vector machine dan algoritma majority vote classifier.","Internet as a publication media has a lot of article and news, which divided into categories, such as politic, economy, sports, and health. Each publication media has tendency to publish the news with positive or negative sentiment. News sentiment has ability to affect people&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s opinion about a topic or government policy. Economy is one interesting topic which has direct impact to Indonesian citizen. Hence, this research focused on economic news from Indonesian online media.

Decision tree is a simple algorithm using tree-like structure of logic, while random forest basicly is a set of decision tree, and support vector machine classify classes by constructing a divider plane between them. This research will use each of them and combine them into one by majority vote classifier method.

Tested using 10-fold cross validation from various online news media resulting in 75% best precision by majority vote classifier, 100% best recall by support vector machine, and 72% best accuracy by both support vector machine and majority vote classifier.",,,49867,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
102317,,HARUN ALL ROSID,PENENTUAN PERINGKAT KERENTANAN PERMUKIMAN  TERHADAP KEBAKARAN  MENGGUNAKAN METODE ANALYTICAL HIERARCHY PROCESS  (STUDI KASUS DI KOTA YOGYAKARTA),,,"Kebakaran, Permukiman, Analytical Hierarchy Process/  : Fire, Urban Settlement and Analytical Hierarchy Process","Dr. Suprapto, M.Ikom",2,3,0,2016,1,"Sebagai salah satu kawasan administratif di Provinsi Daerah Istimewa Yogyakarta dengan berbagai macam aktivitas yang terjadi didalamnya, Kota Yogyakarta memiliki kepadatan penduduk yang tinggi. Hal tersebut tentunya berimplikasi pada potensi kemunculan berbagai macam masalah sosial yang sulit dihindari seperti kejadian kebakaran permukiman. Konsentrasi permukiman penduduk yang tinggi dan kondisi lingkungan di suatu kawasan secara garis besar merupakan hal yang seringkali menjadi faktor yang berpengaruh signifikan dalam asesmen penentuan peringkat kawasan permukiman yang rentan mengalami kebakaran.
	Penelitian membangun sebuah sistem berupa aplikasi komputer berbasis Web untuk menentukan peringkat kerentanan permukiman terhadap bahaya kebakaran dengan menerapkan metode Analytical Hierarchy Process sebagai sistem pendukung  keputusan. Dimana kriteria-kriteria yang dilibatkan bersumber dari data-data yang relevan terhadap aspek permukiman dan kependudukan yaitu: Kepadatan Penduduk (Per Ha), Akses terhadap Sumber Air, Jenis Atap yang Rentan, Jenis Dinding yang Rentan, Jumlah Pelanggan Listrik, Jumlah Hidran Publik dan Jumlah Rumah Kumuh yang terdapat di masing-masing kecamatan di Kota Yogyakarta.
	Hasil keluaran yang diperoleh dari sistem ini berupa nilai yang digunakan sebagai dasar oleh pengambil keputusan untuk mengetahui peringkat permukiman rentan kebakaran yang terdapat di seluruh Kecamatan di Kota Yogyakarta.","The City of Yogyakarta is known as one of the administrative area in the Special Region of Yogyakarta province, the city where various activities and business of its population are happened. The City of Yogyakarta has such a high population density, which implies the potential of several unavoidable social problems to occur such as the fire accident in the urban settlement. The high concentration of urban settlement and the environmental condition, generally become the common significant factor in determining the rank of urban settlements vulnerability against fire accident.
This Study is conducted to develop a web-based application system to determine the rank of urban settlements vulnerability against fire using the Analytical Hierarchy Process as its decision support system. The following criteria are used in this research: The population density, the accessibility to the water resources, the type of roofing materials, the type of walling materials, The number of electricity&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s subscribers, The Number of the public&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s fire hydrant and The number of settlements that are situated in the slum area located in each sub-district of the city of Yogyakarta.
The output of the system will be represented as the score that shows the rank of vulnerability of each Sub-district, which are gained by inserting their selected attributes to be processed through the Analytical Hierarchy Process, to consider and calculate the rank of the urban settlement regarding to their vulnerability against fire accident.
",,,52811,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100526,,SRI WAHYUNI,Sistem Informasi Pariwisata Gunungkidul dengan Fitur Sistem Pendukung Keputusan,,,"sistem pendukung keputusan, AHP, wisata Gunungkidul, PHP, MySQL","Nur Rokhman, S.Si., M.Kom.",2,3,0,2016,1,"	Gunungkidul merupakan salah satu kabupaten yang terletak di tenggara Yogyakarta. Faktor letak geografis dan kondisi alam menjadikan Gunungkidul memiliki potensi wisata alam yang luar biasa. Potensi yang besar ini tidak akan optimal ketika tidak ada sistem informasi yang mendukungnya. Dengan adanya sistem informasi keberadaan tempat-tempat wisata ini dapat diketahui oleh siapapun, kapanpun, dan dimanapun. Pemanfaatan ilmu di bidang sistem pendukung keputusan sebagai fitur konsultasi wisata membuat fungsi sistem semakin optimal. Sistem pendukung keputusan ini digunakan untuk menentukan objek wisata yang dapat dikunjungi wisatawan berdasarkan kriteria yang diinginkan.
	Sistem ini dibangun dengan menggunakan metode SDLC dalam perancangannya dan metode Analytical Hierarchy Process (AHP) sebagai metode pembobotan. Adapun kriteria yang digunakan adalah keindahan, keamanan, keramaian objek wisata, kemudahan transportasi, dan kelengkapan sarana penunjang. Adapun alternatifnya adalah daftar objek wisata yang berada di kabupaten Gunungkidul. Sistem ini dibangun berbasis web dengan bahasa pemrogramman PHP dan MySQL sebagai databasenya.
	Sistem terdiri atas halaman admin dan halaman user. Pada halaman admin, admin dapat mengelola data objek wisata, kriteria, dan alternatif. Pada halaman user, disamping dapat melihat informasi seputar objek wisata, user juga dapat meminta rekomendasi dari sistem berupa objek wisata yang dapat dikunjungi berdasarkan kriteria yang diinginkan.
","	Gunungkidul is a subdistrict in the eastern of Yogyakarta. Geographically, its land has high potency of natural tourism. It will not be optimized unless it has a support of information system.  By using the system, the existence of the tourist destinations is found out by everyone, everytime, and everywhere. Therefore, the applied science in the field of decision support system generates the system optimally. This decision support system used to determine the attractions that can be visited by tourists based on the desired criteria.
This system is designed by applying the SDLC method and Analytical Hierarchy Process (AHP) as the method of qualification. The criteria consist of beauty, security, crowd, transportation, and complexity of supporting means. The alternative data is every tourist destination in Gunungkidul. Moreover, this is a web-based system with PHP as the programming language and MySQL as the database.
The system consists of admin and user pages. In the admin page, the admin organizes the tourist destinations, criteria, and the alternative data. In the user page, besides finding the information of the tourist destination, the user can also ask for recommendation from the system in the form of suggested places the user needed. 
",,,50806,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107182,,ERWIN EKO WAHYUDI,PEMBUATAN COUNTEREXAMPLE PADA ALGORITMA PENGUJIAN KEAMANAN PROTOKOL PING-PONG,,,"keamanan protokol, protokol ping-pong, kriptografi, finite state automata (FSA)","Dr.-Ing. Mhd. Reza M. I. Pulungan, M.Sc.",2,3,0,2016,1,"Penggunaan protokol pengiriman pesan yang &quot;kurang benar&quot; akan rentan terhadap serangan aktif terhadap jaringan. Suatu protokol pengiriman pesan dikatakan aman jika seorang penyabot Z tidak bisa memperoleh pesan asli M yang dikirim. Pada tugas akhir ini, dilakukan implementasi algoritma pengujian protokol pengiriman pesan yang diajukan oleh Dolev dkk. (1982), yaitu mengecek apakah terdapat string gamma sehingga overline(gamma) = lambda. Pembuatan algoritma pencarian counterexample untuk protokol yang telah teruji tidak aman juga dilakukan. Algoritma yang diajukan sedikit modifikasi dari Algoritma BFS (Breadth-First Search). Jika k adalah panjang string gamma yang memenuhi overline(gamma) = lambda dan l adalah banyak perulangan dari protokol, algoritma yang diajukan memiliki kompleksitas waktu O((kn^(k+1)-(k+1)n^k+1)/((n-1)^2)) dan kompleksitas memori O(kn^(k-1)) dengan n = 10 + 6l.","An improperly designed protocol for sending a message could be vulnerable to an active saboteur. A protocol for sending a message is called secure if for any saboteur Z, one cannot obtain plaintext M. In this research, the implementation of an algorithm for checking protocol security which is proposed by Dolev et. al. (1982), stated that whether string gamma exists so that overline(gamma) = lambda, has been done. The design of an algorithm for searching counterexample in insecure protocol is proposed. The algorithm is slightly modified from BFS (Breadth-First Search) Algorithm. Let k be the length of string gamma satisfies overline(gamma) = lambda and l be the number of looping in a protocol, the proposed algorithm runs in O((kn^(k+1)-(k+1)n^k+1)/((n-1)^2)) time-complexity and O(kn^(k-1)) memory-complexity, where n = 10 + 6l.",,,57777,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99503,,ASTHA LINGGAR F. K.,RANCANG BANGUN APLIKASI PENCARIAN RESEP MASAKAN KHAS INDONESIA PADA SISTEM OPERASI ANDROID,,,"Android, LBS, Google Map, Resep, Masakan","Agus Sihabuddin, S.Si, M.Kom",2,3,0,2016,1,"Smartphone sebagai sebuah terobosan dalam teknologi informasi sudah
sangat berkembang pesat penggunaannya dimasyarakat. Banyak kegiatan seharihari dari masyarakat yang melibatkan penggunaan smartphone, seperti ketika
memasak. Smartphone yang terhubung dengan jaringan internet digunakan untuk
mencari resep masakan yang akan dibuat oleh penggunanya. Resep merupakan
petunjuk dan tata cara memasak suatu masakan sesuai dengan urutan proses atau
instruksi untuk membuat suatu masakan. Oleh karena itu, dibangun sebuah
aplikasi resep masakan khas Indonesia unuk membantu masyarakat mendapatkan
informasi resep masakan dari daerah-daerah yang ada di Indonesia.
Pada penelitian ini dibangun sebuah aplikasi resep masakan khas
Indonesia pada sistem operasi Android yang memungkinkan pengguna untuk
mengetahui informasi detail dari resep masakan yang berasal dari daerah-daerah
yang ada di Indonesia yaitu nama masakan, lamanya waktu memasak, bahan dan
bumbu dari masakan dan tahapan dalam memasak.Aplikasi ini memiliki fitur
yaitu fitur untuk menampilkan detail dari resep, fitur untuk menampilkan lokasi
berbelanja dengan menggunakan teknologi LBS yang dimanfaatkan untuk
mengetahui lokasi pengguna dengan posisi lokasi tempat berbelanja yang akan
dituju dan aplikasi juga dapat menampilkan rute dan navigasi menggunakan
layanan Google Map API, fitur tulis resep yang dapat memfasilitasi pengguna
ketika hendak membagikan resep yang dimiliki untuk dapat ditampilkan di dalam
aplikasi dan fitur share yang difungsikan ketika pengguna hendak membagikan
resep dalam format teks dengan pilihan metode melalui facebook, line dan
bluetooth.
Berdasarkan hasil pengujian, seluruh fungsionalitas yang disebutkan di
atas yaitu aplikasi dapat menampilkan detail dari resep, aplikasi dapat
menampilkan rute dan navigasi ke lokasi berbelanja, memfasilitasi pengguna
ketika hendak membagikan resep yang dimiliki untuk dapat ditampilkan di dalam
aplikasi dan fitur share telah dapat dipenuhi oleh aplikasi yang dibuat","Smartphone as an innovation in information technology has been used
widely by society. A lot of their daily activity involves using smartphone, for
example using smart phone while cooking. Smartphone that connected with
Internet will be use to search recipes that will be cook by user. Recipe is set of
instruction that describe how to prepare or make something, especially a cullinary
dish. Therefore, this recipe mobile application is built so that it will supports user
to search many recipes of Indonesian dish.
This research build a recipes application on Android operating system that
allows user to know about the detail of the recipes such as name, time to cook,
ingredient and step to make the recipes. This application has some feature such as
write recipe from user, show the shopping location that use for knowing the route
to shopping location using LBS technology based service and share feature that
allowed user to share recipe to another user using text format via Facebook, Line
and bluetooth.
Based on test results, the entire functionality system such as to allow user
to know about the detail of the recipes, write recipe from user, show the shopping
location that using LBS technology and share feature already fulfilled by the
made application.",,,49865,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107185,,DIONISIUS MARCELLIUS,Perbandingan Performa SSL VPN Gateway Pada Jaringan Cloud,,,"VPN, SSL VPN, Cloud, Throughput",Ahmad Ashari,2,3,0,2016,1,"Layanan yang menggunakan teknologi cloud computing, terutama yang berbasiskan web seperti layanan dengan model software as a service (SaaS) atau cloud storage saat ini semakin marak digunakan. Terdapat beberapa metode alternatif dalam mengakses layanan cloud, di antaranya yaitu dengan menggunakan virtual private network atau VPN. Salah satu jenis VPN, yakni SSL VPN atau dikenal juga sebagai web-based VPN dapat dijalankan tanpa memerlukan instalasi software tambahan sehingga dapat diimplementasikan pada layanan cloud berbasiskan web sebagai metode akses alternatif. Penggunaan VPN sendiri dapat mengurangi throughput yang dihasilkan oleh jaringan sehingga perlu dilakukan pengukuran terhadap kinerja jaringan saat menggunakan VPN.
Pada penelitian ini, dilakukan pengukuran dan perbandingan terhadap nilai durasi, jumlah paket data dan bytes, serta throughput yang dihasilkan ketika mengakses layanan cloud tanpa menggunakan VPN dengan yang menggunakan 2 jenis implementasi SSL VPN yang tersedia: SSL portal VPN dan SSL tunnel VPN. Pengukuran dilakukan dengan menggunakan aplikasi Wireshark untuk melihat dan menghitung nilai throughput dari arus data yang dihasilkan pada saat mengunduh file dari layanan cloud storage. Dari hasil pengukuran, pengukuran dengan menggunakan SSL portal VPN pada jaringan lokal secara umum menunjukkan hasil yang lebih rendah dibandingkan dengan pengukuran tanpa menggunakan VPN. Sebaliknya, pengukuran dengan menggunakan SSL portal VPN melalui WAN menunjukkan nilai yang lebih tinggi dibandingkan dengan pengukuran tanpa VPN, namun tetap dengan nilai throughput yang lebih rendah. Di sisi lain, pengukuran dengan menggunakan SSL tunnel VPN secara umum menunjukkan nilai yang lebih tinggi untuk keseluruhan parameter pengukuran dibandingkan dengan pengukuran tanpa menggunakan VPN baik pada pengukuran di jaringan lokal maupun pada pengukuran melalui WAN.","The usage of services using the cloud computing technology, especially the web-based ones such as service with software as a service (SaaS) cloud model or cloud storage are increasing nowadays. There are several alternative methods for accessing the cloud service, with virtual private network or VPN as one of such alternatives. SSL VPN is a form of VPN that can be run without installing any additional software, and thus, can be implemented on web-based cloud services as an alternative access method. However, since the usage of VPN can decrease the throughput of a network, an additional network performance measurement is required when using a VPN.
This research will measure and compare the throughput value generated between accessing the cloud network without using VPN and accessing it through the 2 available implementation of SSL VPN: SSL portal VPN and SSL tunnel VPN. Measurement is done by using the Wireshark application to monitor and calculate the throughput value from data stream generated during the download process of a file from cloud storage service. The measurement result shows that the throughput value generated by SSL portal VPN on local network is generally lower than the value generated without using VPN. Conversely, the value generated by the SSL portal VPN over the WAN is higher than the value generated without using VPN, but still generate a lower throughput value. On the other side, the value generated by SSL tunnel VPN for all measurement parameters are generally higher than the value generated without using VPN, both on the local network and over the WAN.",,,57780,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99506,,ARDHI WIRATAMA BY,Akselerasi Algoritma AKS Menggunakan MPI dan OpenMP pada Permasalahan Pengujian Bilangan Prima,,,"algoritma AKS,  pengujian bilangan prima, implementasi hibrid MPI dan OPENMP, AKS algorithm, primality proving, hybrid MPI and OPENMP implementation","Dr.-Ing. Reza M.I. Pulungan, S.Si., M.Sc.",2,3,0,2016,1,"Bilangan prima yang sangat besar banyak digunakan oleh algoritma kriptografi untuk melakukan enkripsi dan dekripsi. Salah satu di antaranya adalah algoritma RSA yang saat ini banyak digunakan untuk mengamankan transaksi perbankan. Namun sampai saat ini belum ada algoritma deterministik yang efisien untuk menentukan primalitas sebuah bilangan. Algoritma deterministik yang terbaik di dunia saat ini adalah algoritma AKS yang memiliki asimtotik polinomial namun belum dapat digunakan secara praktik. 

Pada penelitian ini diimplementasikan algoritma AKS secara paralel dalam 1 dan 2 tingkat paralelisasi dengan MPI dan OPENMP yang lebih load balance. Terdapat 3 strategi paralelisasi dan 3 teknik load balancing yang diusulkan. Untuk menangani masukan bilangan yang besar digunakan pustaka GMP yang dapat menangani berapa pun panjang masukan yang diberikan. Untuk menangani struktur data polinomial dan fungsi pemangkatan polinomial modular yang cepat digunakan pustaka NTL. Program ini dijalankan pada sebuah HPC.

Pada penelitian ini diperoleh hasil bahwa teknik paralelisasi 1 tingkat  MPI dengan 1 core 1 proses lebih cepat dari pada teknik paralelisasi MPI dengan 1 node 1 proses dan teknik paralelisasi 2 tingkat dengan MPI dan OPENMP. Selain itu teknik load balancing 2 lebih baik dari 2 teknik lainnya yang diusulkan. Dengan demikian teknik paralelisasi 1 core 1 proses dengan menggunakan teknik load balancing 2 menjadi yang tercepat. Diperoleh speed up sebesar 39 kali lipat ketika dijalankan pada 23 node.","Big prime used in various cryptographic algorithms for doing encryption and decryption. For example RSA algorithm used big prime for encryption and decryption. RSA now  widely  used to secure banking transaction. Nowadays  there is no efficient deterministic algorithm for primality proving. The best deterministic algorithm  for primality proving is AKS algorithm asymptotically running in polynomial time although impractical used.

In this research AKS algorithm have been implemented on 1 and 2 level parallelization using MPI and OPENMP. Three different parallelization strategies and load balancing strategies have been implemented. GMP library used to handle big arbitrarily large integer and NTL library used to handle polynomial data structure and modular exponentiation operation. These implementations run on HPC.

In this research show experimentally that 1 level parallelization strategy with 1 core 1 process faster than 1 level parallelization with 1 node 1 process and 2 level parallelization with MPI and OPENMP. Furthermore load balancing strategy 2 better than the others two proposed strategy. Therefore parallelization strategy 1 core 1 process and load balancing strategy 2 become the fastest. Obtained 39 times speed up runs on 23 node.
",,,49816,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98740,,IKVI A R,Client Side Encryption in Dropbox Cloud Storage Using AES 128 Algorithm,,,"AES encryption and decryption, cloud storage, data security, client side encryption","Dr.-Ing. Mhd. Reza M.I Pulungan, S,Si., M.Sc",2,3,0,2016,1,"Di dalam sistem cloud computing, data disimpan pada remote server dan diakses melalui internet. Meningkatnya ukuran data yang personal dan data yang penting memberikan perhatian pada keamanan dalam penyimpanan data. Penyimpanan data pada cloud mempunyai banyak keuntungan diantaranyta mengurangi ruang di komputer lokal, dapat diakses darimana saja karena data disimpan dalam internet, dan menjadi tempat yang aman untuk menyimpan cadangan file.

Ketika user meletakan file di internet, tidak akan menjamin file akan aman dari tangan para hacker. Untuk itu riset ini berfokus pada proses enkripsi yang dilakukan oleh user sebelum file tersebut di upload ke Dropbox. Algoritma AES 128 dipilih karena level keamanan nya.

Untuk menyediakan aplikasi yang efisien dan aman, file yang diunggah ke Dropbox terlebih dahulu dienkripsi menggunakan algoritma AES. Namun, file yang berada di komputer lokal dapat diakses kapanpun dengan bentuk plaintext. Hasil menunjukan bahwa aplikasi dapat efisien dan aman. Aplikasi menyimpan file yang telah terenkripsi dengan proses running time yang lebih cepat daripada mengunggah file melalui website Dropbox.","In cloud computing systems, data are stored on remote servers accessed through the internet. The increasing volume of personal and vital data, brings up more focus on storing the data securely. Storing data in the cloud has many advantages such as reducing storage in local computer, can be accessed from anywhere because the files stored on the internet, and becoming a safe file backup.

Once users put a file on the internet, it does not guarantee the file will be safe from hackers. Therefore, this research focuses on client side encryption before the file is uploaded to the Dropbox. AES 128 algorithm is chosen because of the security level.

To provide an efficient and secure application, files to be uploaded to Dropbox is firstly encrypted using AES. However, the file can be accessed through Dropbox desktop client in an unencrypted form. The result shows that this application can be effective and secure. It stores encrypted files into Dropbox with running time process faster than running time of uploading through Dropbox website directly.
",,,49040,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107473,,FITRI HASANAH AMHAR, COMPUTATIONAL TOOL UNTUK EVALUASI  HASIL METAGENOMIC ASSEMBLY,,," evaluasi hasil metagenomic assembly, N50, N-len(x), Nm50, chimeric/evaluating metagenomic assembly result, N50, N-len(x), Nm50, chimeric","Afiahayati, S.Kom., M.Cs, Ph.D",2,3,0,2016,1,"Perkembangan Bioinformatika memungkinkan para ahli melakukan identifikasi genome-genome pada suatu lingkungan yang disebut identifikasi metagenome. Data metagenome yang diperoleh dari suatu lingkungan akan diperoleh potongan DNA-nya melalui alat sequencing. Selanjutnya potongan DNA ini digabungkan oleh assembler agar dapat diketahui spesies asalnya. Terdapat berbagai assembler dengan metode penggabungan yang berbeda satu sama lain. Perkembangan komputer dengan kecepatan berpikir dan kemampuan otomatisasi pemecahan masalah sangat dibutuhkan untuk menyelesaikan persoalan biologi, termasuk dalam hal mengevaluasi hasil metagenomic assembly untuk menemukan hasil penggabungan terbaik. 

Praktisi Biologi membutuhkan data hasil metagenomic assembly yang paling akurat dan mendekati reference genome. Karenanya dibutuhkan computational tool untuk melakukan evaluasi hasil metagenomic assembly. Pada penelitian ini, digunakan parameter N50, N-len(x), dan Nm50 untuk melakukan evaluasi hasil metagenomic assembly serta penghitungan cover rate dan chimeric rate yang menunjukkan keakuratan hasil assembly. Evaluasi data metagenome dengan dua simpul genome yang mengacu pada reference genome berbeda, atau disebut sebagai bagian chimeric, dilakukan dengan menghilangkan bagian tersebut terlebih dahulu sebelum menerapkan penghitungan parameter evaluasi terhadap datanya.

Parameter evaluasi ini diujikan pada simulated dataset dari hasil assembler Xgenovo, MetaVelvet-SL, dan IDBA-UD serta real dataset dari hasil assembler RayMeta, dan SOAPdenovo2. Hasil penelitian menunjukkan bahwa computational tool untuk evaluasi hasil metagenomic assembly telah berhasil dibangun. Parameter N50 dapat digunakan untuk mengevaluasi simulated dataset yang tidak mengandung bagian chimeric. Parameter Nm50 digunakan untuk mengevaluasi simulated dataset yang mengandung bagian chimeric. Parameter N-len(x) digunakan untuk mengevaluasi real dataset yang tidak memiliki reference genome dan ukurannya sangat besar.","The development of bioinformatics allows the expert to identify genomes in any environment, it's called metagenome identification. Metagenome data which is obtained from an environment will pass the sequencing tool to gain the pieces of its DNA. The pieces of DNA are assembled by the assembler in order to know its origin species. There are various assemblers with its own method of merging that differ each other. The development of computer with the speed of thinking and automation problem solving ability is needed to resolve biological problem, including evaluation of metagenomic assembly result to find the best assembly merging result.

Biologists need the most accurate metagenomic assembly result data and the closest to the reference genome. Therefore, it takes computational tool for evaluating metagenomic assembly result. In this study, N50, N-len(x), and Nm50 are used as parameter to evaluate the metagenomic assembly result. Cover rate and chimeric rate calculation are used to show the accuracy of the assembly. Evaluation of metagenome data with two genome nodes that refer to different reference genome, called chimeric node, is done by eliminating its chimeric part first before applying the parameters calculation to evaluate the data.

The evaluation parameters were tested in simulated dataset from Xgenovo, MetaVelvet-SL, and IDBA-UD assemblers and also real dataset from the RayMeta and SOAPdenovo2 assemblers. The results showed that the computational tool for evaluating metagenomic assembly result has been successfully developed. N50 parameter can be used to evaluate simulated dataset that does not contain any chimeric node. Nm50 parameter is used to evaluate simulated dataset containing the chimeric node. N-len(x) parameter is used to evaluate the real dataset which  do not have any reference genome and has very large size.
",,,58060,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99541,,INOSENSIUS LOMAN,IMPLEMENTATION OF LOSSLESS COMPRESSION USING HUFFMAN CODING ALGORITHM IN WIRELESS SENSOR NETWORKS (WSN) BASED EMBEDDED SYSTEM,,,"Lossless Compression, Huffman Coding, Wireless Sensor Networks","MHD. Reza M.I. Pulungan, S.Si., M.Sc., Dr.Ing.",2,3,0,2016,1,"Utilization of speed transmission efficiently has been a main concern in wireless sensor networks (WSN) communication protocol. As sensors system are controlled remotely, bits reduction plays critical role in wireless transmission enhancement. Despite compression techniques always provides the most reliable way in bits reduction, deciding on which techniques and algorithm should be chosen wisely as well. This research have therefore implemented lossless compression techniques using Huffman algorithm in wireless sensor networks based on it&acirc;€™s emphasis on sensor networks for environmental monitoring system. The performance of Huffman coding algorithm is calculated including compression ratio and size reduction.  

	Besides,  a simple and low-cost monitoring system that combines Phidgets Single Board Computer 3 (SBC3), temperature/ relative humidity sensors and Wireless Local Area Networks (WLAN) USB adapter have been built to create sensor networks system.  This networks runs in real time system and can be monitored through  network client over Secure Shell (SSH) platform.  With data compression result, smaller data size is being transmitted to the network client over Secure Copy Protocol (SCP).  The performance of wireless transmission is calculated as well including data transmission speed to ensure the effectiveness of wireless transmission itself. 

	From testing process, Huffman coding algorithm show its excellent result in performing compression process for block data stream. The result shows that its compression ratio is up to 62 % with transmission speed reaching up to 70 Kbps. 
","Utilization of speed transmission efficiently has been a main concern in wireless sensor networks (WSN) communication protocol. As sensors system are controlled remotely, bits reduction plays critical role in wireless transmission enhancement. Despite compression techniques always provides the most reliable way in bits reduction, deciding on which techniques and algorithm should be chosen wisely as well. This research have therefore implemented lossless compression techniques using Huffman algorithm in wireless sensor networks based on it&acirc;€™s emphasis on sensor networks for environmental monitoring system. The performance of Huffman coding algorithm is calculated including compression ratio and size reduction.  

	Besides,  a simple and low-cost monitoring system that combines Phidgets Single Board Computer 3 (SBC3), temperature/ relative humidity sensors and Wireless Local Area Networks (WLAN) USB adapter have been built to create sensor networks system.  This networks runs in real time system and can be monitored through  network client over Secure Shell (SSH) platform.  With data compression result, smaller data size is being transmitted to the network client over Secure Copy Protocol (SCP).  The performance of wireless transmission is calculated as well including data transmission speed to ensure the effectiveness of wireless transmission itself. 

	From testing process, Huffman coding algorithm show its excellent result in performing compression process for block data stream. The result shows that its compression ratio is up to 62 % with transmission speed reaching up to 70 Kbps. 
",,,49828,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
99801,,ROCHANA PRIH HASTUTI,Perbandingan Pengaruh Algoritma Clustering Pada Sistem Pencarian Dokumen,,,"information retrieval, vector space model, k-means, bisecting k-means","MOH. Edi Wibowo, S.Kom., M.Kom.,Ph.D",2,3,0,2016,1,"Sistem pencarian dokumen dalam konteks information retrieval secara cepat menjadi dominan dalam hal akses informasi, menyaingi pencarian database secara tradisional. Sistem pencarian dokumen memberikan hasil berupa dokumen yang memiliki rangking relevansi terhadap kueri yang diberikan pengguna. Beberapa model representasi dalam sistem ini telah dikembangkan misalnya dengan Vector Space Model. Pada umumnya, kendala yang muncul pada sistem pencarian dokumen adalah waktu pencarian dokumen yang lama.
Clustering merupakan salah satu pendekatan yang bisa digunakan untuk menangani masalah waktu pencarian. Proses clustering pada penelitian ini digunakan untuk mengelompokan dokumen menjadi gugusan yang lebih kecil yang kemudian digunakan sebagai basis data dalam pencarian. Algoritma k-means, bisecting k-means dan hierarchical agglomerative centroid clustering diterapkan pada sistem pencarian Hadits Shahih Bukhari. Kualitas masing-masing algoritma dalam menjalankan clustering dievaluasi dari nilai purity dan nilai silhouette. Sementara kinerja algoritma clustering pada sistem pencarian dokumen dievaluasi dari waktu pencarian, presisi, dan recall dari hasil pencarian.
Berdasarkan pengujian, algoritma k-means memiliki kualitas terbaik dengan nilai purity rata-rata 0,49 dan nilai silhouette 0,03. Selain itu, implementasi algoritma tersebut dengan k = 93 terbukti dapat mempercepat waktu pencarian yakni rata-rata 12 kali dibandingkan sistem pencarian biasa. Kualitas hasil pencarian yang dihasilkan oleh k-means pada dua macam uji coba juga dapat mengungguli algoritma lain dengan nilai presisi 0,315 dan 0,482, begitu juga recall dengan nilai yakni 0,131.","A document searching system in the context of information retrieval fast become dominant in information access, overtaking traditional database searching. Information retrieval results list of ranked documents to user in relevance of the query given. A number of representation models used in information retrieval system have been developed, one of them is Vector Space Model. Generally, the problem that arise in information retrieval system is runtime of the system to retrieve the documents that is slow.
Clustering is an approach that can resolve runtime problem. Clustering in this research is used to cluster the document collections to some smaller groups that further used as the main collection in an information retrieval system. K-means, bisecting k-means, and hierarchical agglomerative clustering are implemented in the searching system of Hadith Shahih Bukhari. The quality of the clusters made by each algorithm evaluated based on the purity score and silhouette value. While the performance of the implementation of these algorithms in the information retrieval system is evaluated through runtime, precision, and recall of the search results.
Based on the experiments, k-means get the best average purity score at 0,49 and best silhouette value at 0,03. Besides, the implementation of these algorithms with k = 93 proven can accelerate searching time 12 times faster than the ordinary information retrieval system. The quality of searching results using k-means achieved the best value in precision of both experiments compared to other algorithms which are 0,315 and 0,482 respectively, as well as the recall value is 0,131.",2016-07-15 00:00:00,53,50233,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100057,,BAHRUNNUR,MENEMUKAN REPRESENTASI MINIMUM DARI RECURRENT NEURAL NETWORK MENGGUNAKAN ANALISIS WAVELET,,,"Recurrent Neural Network, Jaringan Saraf Tiruan, Kolmogorov Complexity, Single Pole Balancing Problem","Dr.-Ing. Mhd. Reza M. I. Pulungan, S.Si., M.Sc.",2,3,0,2016,1,"Perkembangan terkini dari Jaringan Syaraf Tiruan (JST) membuatnya menjadi semakin perkasa dibanding pendahulunya. Hal tersebut datang dengan efek samping yakni model JST menjadi semakin kompleks dan mempunyai struktur yang mahabesar. Penelitian dari Schmidhuber (1997) menunjukkan adanya kemungkinan untuk menemukan JST yang simpel dengan kemampuan inferensi yang prima. Koutnik et al. (2010) melanjutkan penelitian tersebut dengan cara pencarian praktis untuk menemukan Recurrent Neural Network (RNN) yang simpel. Koutnik menggunakan Transformasi Fourier sebagai mekanisme untuk menyandikan bobot RNN, sehingga pencarian dapat dilakukan di dimensi yang lebih rendah. Transformasi Wavelet merupakan generalisasi dari Transformasi Fourier dengan tingkat kompresi yang lebih baik. Dengan demikian, penggunaan Transformasi Wavelet sebagai mekanisme penyandian merupakan hal yang patut untuk dicoba.
Penelitian ini berfokus pada metode encoding untuk RNN menggunakan Transformasi Wavelet. Encoding menghasilkan sebuah program yang ingin dicari menggunakan Universal Network Search. Pencarian tersebut didadasarkan dari landasan teori Levin Search (Levin, 1973) yang cenderung memilih program dengan panjang deskripsi yang pendek dan memiliki runtime yang cepat. Dalam kasus JST, jaringan yang memiliki bobot sedikit cenderung memiliki runtime yang cepat dan panjang program yang pendek. Encoding Wavelet ini dibandingkan dengan encoding Fourier dalam hal tingkat kompresi dan waktu untuk mencapai titik konvergen. Simulasi Single Pole Balancing digunakan sebagai cara untuk mengevaluasi performa dari setiap metode encoding.
Hasil pengujian menunjukkan metode Wavelet lebih cepat mencapai titik konvergen dan memiliki tingkat kompresi yang tinggi pada pengujian non-Markov. Dalam pengujian Markov, kedua metode menunjukkan kemampuan yang sama dalam hal kompresi dan kecepatan untuk konvergen. Pengamatan lebih mendalam pada bobot yang dihasilkan dari metode Wavelet menunjukkan bahwa tidak terdapat koneksi recurrent di bobot tersebut (R = 0), menyebabkan RNN untuk bekerja selayaknya JST biasa. Meskipun metode Wavelet menunjukkan kinerja yang luar biasa di atas kertas, ia memiliki kelemahan bahwa terdapat kecenderungan untuk gagal menyeimbangkan tongkat setelah batas evaluasi di pengujian non-Markov.","The recent development of Neural Network shows that it becoming more powerful than the predecessor. Despite having an incredible inference ability, it comes with drawbacks that the model becomes a gigantic structure with complex computation mechanics. There is a research from Schmidhuber (1997) showing that it is possible to find a simple Neural Network with superior generalization/inference ability. Koutnik et al. (2010) continued the research with a practical search method to find a simple Recurrent Neural Network (RNN). Koutnik used Fourier Transform as a mechanism to encode RNN weights, so it could be searched on lower dimension space. Wavelet Transform is a generalization of Fourier Transform with a better compression rate. Thus, it is a good bet to try Wavelet Transform as an encoding mechanism.
This research focuses on the encoding method for RNN using Wavelet Transform. The encoding generates a code to serve as a program to be searched on Universal Network Search. The search is based on a theoretical foundation from Levin Search (Levin, 1973) that favoring program with short description length and fast runtime. In term of Neural Network, a network that has few weights is likely to have fast runtime and short description length. This Wavelet encoding was compared with Fourier encoding in terms of compression rate and time to converge. Single Pole Balancing simulation is used as a way to evaluate the performance of each encoding method.
The result's examination shows that Wavelet Transform has better speed to converge and high compression rate on the non-Markov test suite. In the Markov test suite, both of them have the same rate of compression and speed. The closer look at the weights that generated by Wavelet encoding showing that it doesn't have a recurrent connection (R = 0), so its RNN acts like a regular Neural Network. Even though Wavelet method shows remarkable performance on paper, it has a shortcoming that it could be failed after the evaluation boundary on the non-Markov test suite.",,,50313,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107225,,DENIS TRISNIANTARI,Klasifikasi Berita Ekonomi Menggunakan Metode Multinomial Naive Bayes,,,"klasifikasi teks, text mining, multinomial naive bayes","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2016,1,"Meningkatnya jumlah pengguna internet di kalangan masyarakat menjadi salah satu pemicu munculnya media pemberitaan online yang menghadirkan berbagai macam informasi. Atas dasar itu banyak dari media informasi dimasa sekarang yang melakukan pengklasifikasian dengan kategorisasi terlebih dahulu sebelum disebarkan pada masyarakat luas. Pengklasifikasian tersebut berguna untuk memudahkan masyarakat untuk mencari informasi yang mereka inginkan. Salah satu berita yang tak lekang oleh waktu adalah berita ekonomi. Badan Pusat Statistik dalam websitenya www.bps.go.id yang diakses pada 4 Juli 2016, menyebutkan salah satu indikator penting untuk mengetahui kondisi ekonomi di suatu negara dalam suatu periode tertentu adalah data Produk Domestik Bruto (PDB), baik atas dasar harga berlaku maupun atas dasar harga konstan. Salah satu cara untuk menghitung angka PDB dengan menggunakan pendekatan produksi, yang dikelompokkan ke dalam 9 lapangan usaha (sektor).
	Pada penelitian ini dilakukan klasifikasi berita ekonomi menggunakan metode Multinomial Naive Bayes. Kategori yang dipakai dalam penelitian ini berdasarkan 9 sektor ekonomi yang mengacu pada pendekatan produksi dalam menentukan angka PDB. Data set yang digunakan sebanyak 1233 berita ekonomi online untuk data training dan 57 berita ekonomi online sebagai sebagai data testing. Data set berasal dari 12 situs berita online di Indonesia dalam kurun bulan Oktober 2015 &Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12; 1 Januari 2016. Pembuatan vocabulary sebagai fitur yang mewakili berdasarkan frekuensi kata yang sering muncul di dalam data set. Fitur yang digunakan adalah 1000 kata yang sering muncul di dalam data set.
Berdasarkan fitur yang diperoleh, dilakukan klasifikasi berita ekonomi meggunakan metode Multinomial Naive Bayes. Dari hasil pengujian didapatkan hasil rata-rata dengan nilai akurasi sebesar 74,77%, f-measure sebesar 74,40%, precision sebesar 76,33% dan recall sebesar 74,77%.
","The growing number of internet users among the public to be one of the triggers of the emergence of online news that presents a wide range of information. On that fact, many of media want information right now that do classification with a categorization of first before spread in the wider community. The classification is useful to make it easier for people to find the information they want. One of the everlasting news is economy news. Indonesian Central Bureau of Statistics in its website www.bps.go.id accessed on 4th July 2016, mentioned one of important indicators to know economic conditions in a country in a given period is the data of gross domestic product (GDP), both on the basis of the prices in force as well as on the basis of constant prices. One way to calculate the GDP figures by using the production approach, grouped into 9 field economy sector. 
This research was conducted on the classification of economy news using Multinomial Naive Bayes method. The categories used in this research was based on economic sector 9 refers to the production approach in determining GDP figures. The data sets are used as much as 1233 economy online news for training data and 57 economy online news as data testing. The data sets came from 12 online news sites in Indonesia happened from October 2015 until January 2016. The making of the vocabulary as a feature that represents a frequency based on the words that often appear in the data set. The features that used are 1000 words frequently appear. 
Based on the features that are obtained, done with economic news classification Multinomial Naive Bayes method. From the test results obtained from the findings of the average with accuracy value is 74,77%, f-measure is 74,40%, precision is 76,33% and recall is 74,77%.
",2017-01-05 00:00:00,53,57821,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92897,,AJI MAHBAR SUKMA W.,IMPLEMENTASI ALGORITMA GENETIKA UNTUK OPTIMASI PEMILIHAN HERO BERDASARKAN HERO ROLE DAN PLAY STYLE DALAM DOTA 2,,,"DOTA 2, algoritma genetika, hero role, play style","Faizah, S.Kom., M.Kom",2,3,0,2016,1,"DOTA 2 adalah sebuah permainan berjenis Multiplayer Online Battle Arena (MOBA). Pada DOTA 2, pemain yang terdiri dari 10 orang dibagi menjadi dua tim. Masing-masing pemain harus memilih satu hero, yaitu tokoh yang akan dimainkan. Setiap hero memiliki kemampuan yang unik, oleh karena itu pemain harus memilih hero dengan pertimbangan hero yang sudah atau akan dipilih oleh rekan satu timnya. Komposisi hero dalam satu tim sangat menentukan besarnya peluang untuk kemenangan. Cara bermain tim juga harus disesuaikan dengan komposisi hero dalam tim tersebut untuk memperbesar peluang kemenangan.
Algoritma genetika merupakan algoritma yang mengadopsi perilaku makhluk hidup, misalnya perkawinan dan mutasi, untuk menyelesaikan suatu masalah. Algoritma genetika sangat cocok digunakan untuk menyelesaikan masalah optimasi. Kualitas output yang dihasilkan tergantung dari bagaimana mendesain suatu permasalahan tersebut ke dalam algoritma genetika dengan baik.
Pada penelitian ini, algoritma genetika digunakan untuk mencari komposisi-komposisi hero yang optimal berdasarkan hero role dan play style dari tim. Kondisi optimal yang dimaksud dinilai dari kualitas bagaimana komposisi hero tersebut dapat memenuhi kriteria play style yang diinginkan dan dapat digunakan pada permainan DOTA 2. Hasil pengujian pada Dota Picker menunjukkan bahwa 8 dari 10 komposisi hero yang dihasilkan pada play style balance, 7 dari 10 komposisi hero yang dihasilkan pada play style late game, dan 10 dari 10 komposisi hero yang dihasilkan pada play style fast push memenuhi kriteria pengujian. Pengujian pada DOTA 2 menunjukkan bahwa 12 pertandingan yang diujikan dapat dimenangkan secara keseluruhan menggunakan komposisi hero yang merupakan hasil paling optimal yang dihasilkan sistem.","DOTA 2 is a game with the genre of Multiplayer Online Battle Arena (MOBA). On DOTA 2, players consisting of 10 people divided into two teams. Each player must choose one hero, the character to be played. Each hero has a unique ability, therefore, the player must choose a hero comsider to hero that have been or will be selected by a teammate. Composition of the hero in the team will determine the chances of that team to win. The play style of the team also had to be adjusted to the composition of the hero in the team to increase the chance of victory.
Genetic algorithm is an algorithm that adopts the behavior of living things, such as mating and mutation, to solve a problem. Genetic algorithm are well suited to solve optimization problems. The quality of output produced depends on how the problem designed into a well-designed genetic algorithm.
In this research, a genetic algorithm is used to find the optimal compositions of hero based hero role and play style of the team. Optimal condition is assessed by the quality of how the composition of the hero is able to meet the criteria of the desired play style and can be used in DOTA 2 game. Results of Dota Picker test shows that 8 out of 10 hero compositions that produced in the balance play style, 7 of 10 hero compositions that produced in the late game play style, and 10 of 10 hero compositions that produced in the fast push play style meet the test criteria. Tests on DOTA 2 shows that 12 test matches can be won using the hero composition which is the most optimal result produced by the system.",,,43025,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100321,,MAHARANI HASTUTI,SISTEM PEDUKUNG KEPUTUSAN UNTUK MENENTUKAN TOKO ONLINE TERBAIK DENGAN MENGGUNAKAN  METODE AHP DAN TOPSIS,,,"Sistem  Pendukung  Keputusan,  Toko Online Terbaik,  AnalyticHierarchy Process (AHP),  Technique for Others Reference by Similarity to Ideal Solution (TOPSIS)","Drs. Retantyo Wardoyo, M.Sc. Ph.D",2,3,0,2016,1,"Salah satu pemikiran seorang calon pembeli yang akan membeli secara online adalah pemilihan toko online terbaik. Sehingga calon pembeli sering mempertimbangkan secara detail terhadap beberapa toko online yang paling sesuai. Namun selama ini belum ada sistem yang digunakan untuk menentukan toko online terbaik yang sesuai dengan kebutuhan yang dinginkan. 
Sistem pendukung keputusan untuk menentukan toko online terbaik ini dibangun dengan menggunakan metode gabungan AHP dan TOPSIS. AHP digunakan untuk memperoleh pembobotan dan TOPSIS digunakan untuk menghitung perankingan.Kriteria sistem berdasarkan pada halaman http://gotvantage.com tentang &acirc;€œ9 factors impact ecommerce sales&acirc;€yaitu meliputi product Quality, Word of Mouth Recommendation, Loyality Reward, Easy Return Policy, Easy Navigation, Shipping Time &amp; Cost, Easy of Checkout, Competitive pricing, dan Online Review.
Hasil  dari  proses  pada  sistem  pendukung  keputusan  ini  berupa  daftar peringkat  toko online terbaik  mulai  dari  peringkat  pertama  sampai  peringkat terakhir.
","One thought a prospective buyer who will buy online is the selection of the best online store. So that prospective buyers often consider in detail the several online stores most suitable. But so far there has been no system used to determine the best online stores that fit the needs of a chill.
Decision support system to determine the best online store is built using the combined method of AHP and TOPSIS. AHP is used to derive the weighting and TOPSIS is used to calculate the rank. Criteria system based on pagehttp://gotvantage.com about &quot;9 ecommerce factors impact sales&quot; which include product Quality, Word of Mouth Recommendation, Loyalty Reward, Easy Return Policy, Easy Navigation, Shipping Time &amp; Cost, Easy of Checkout, Competitive pricing, and online reviews.

The results of the process of decision support systems is a list ranking the best online stores ranging from the first rank to the last rank.
",,,50730,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
96226,,BRAMIANTO RESA ADI,IMPLEMENTASI PENCARIAN FULL TEXT PADA BERITA ONLINE MENGGUNAKAN SISTEM INDEXING PADA SPHINX,,,"pencarian  berita,  media  monitoring,  proses  index,  mysql,  sphinx, real time indexing Sphinx","I Gede Mujiyatna, S.Kom., M.Kom",2,3,0,2016,1,"Media  monitoring  merupakan  aktifitas  yang  digunakan  untuk  melakukan monitor  dan  review  berita  online.  Media  monitoring  pada  media  online mengoleksi  berita  online,  selanjutnya  data  berita  akan  disimpan  ke  dalam 
database  dan  akan  diolah  dan  direview  untuk  keperluan  monitoring.  Kegiatan review  berita  pada  media  monitoring  memungkinkan  adanya  transaksi  read  and 
write yang cukup tinggi di dalam database, sehingga akan membebani kinerja dari database tersebut.
Untuk  mengurangi  beban  kinerja  database  media  monitoring,  maka dilakukan  implementasi  pencarian  full  text  pada  berita  online  menggunakan sistem  indexing  pada  Sphinx.  Sistem  ini  dibangun  dengan  menggunakan  bahasa pemrograman  PHP,  MySQL  dan  tool  Sphinx.  Hasil  pengujian  implementasi sistem  indexing  pada  Sphinx  untuk  media  monitoring  diperoleh  kesimpulan bahwa proses  indexing  data dan pencarian berita  pada  media monitoring dengan menggunakan Sphinx  real time index  menghasilkan waktu pencarian berita yang lebih cepat dan stabil.","Media  monitoring  is  activities  used  to  monitoring  and  reviewing  online news. Media monitoring  on online media  collect online news, then news data will be  stored  into  a  database  and  will  be  processed  and  reviewed  for monitoring purposes.  News  reviewing  in  media monitoring  allows  high  transaction  to  read and write in the database, therefor it caused overload performance of the database.
Implementation of full text search on online news using indexing system of Sphinx can  reduce the overload performance of media monitoring  database. This system is built using programming language PHP, MySQL and Sphinx tool. The results  of  the  test  show  that  processes  of  data  indexing  and  news  searching  on media monitoring using real time index Sphinx are faster and stable.",2016-04-19 00:00:00,53,46808,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107234,,THORIQ PRIMA PUTRA D.,RANCANG BANGUN SISTEM SMART ROOM BERBASIS INTERNET OF THINGS DAN INTEL EDISON,,,"Smart Room, Internet of Things, Cylon.JS, Socket.IO, Intel Edison","Ahmad Ashari, Drs., M.I.Kom., Dr.Techn",2,3,0,2016,1,"Otomatisasi pengendalian perangkat elektronik dalam ruangan menjadi salah satu solusi untuk menciptakan kondisi ruangan yang nyaman. Sistem smart room yang berbasis pada Internet of Things dapat memberikan fitur otomatisasi terhadap perangkat elektronik di dalam ruangan dan juga memungkinkan pengguna mengendalikan perangkat elektronik dari antarmuka pengguna tanpa perlu menyentuh perangkat tersebut secara fisik.
Pada penelitian ini dilakukan suatu rancang bangun Sistem Smart Room berbasis Internet of Things. Framework yang digunakan untuk membangun sistemnya adalah Cylon.JS dengan perangkat keras Intel Edison. Selain Cylon.JS, beberapa modul lain juga digunakan seperti Socket.IO untuk komunikasi soket, Express.JS sebagai framework server web, dan LocalTunnel untuk menyediakan metode tunneling sehingga sistem dapat diakses dari jaringan Internet.
Pada penelitian ini Sistem Smart Room berhasil diimplementasikan dan diuji berdasarkan kebutuhan fungsional dan non-fungsionalnya. Selain secara fungsionalitas, sistem juga memberikan kinerja dari segi waktu respons untuk aktuator lampu dan kipas yaitu rata-rata waktu yang dibutuhkan adalah 5,52 detik dan  4,31 detik dengan input sensor, 0,19 detik dan 0,4 detik dengan input pengguna dari LAN, 0,56 detik dan 0,76 detik dengan input pengguna dari Internet. Pemakaian sumber daya CPU membutuhkan rata-rata 35,07% dengan 2 core. Waktu akses menuju sistem dari LAN membutuhkan rata-rata 1,77 detik dan dari Internet membutuhkan waktu rata-rata 4,82 detik.
","Control automation of electronic devices in the room is a solution to create a comfortable room condition. Smart room system based on Internet of Things can provide automation features of the electronic devices in the room and also allows users to control electronic devices from user interface without having to touch the devices physically.
In this research, design and implementation of Smart Room System based on Internet of things has been done. Framework used to build the system is Cylon.JS and Intel Edison as the hardware. In addition, there are another modules like Socket.IO for socket communication, Express.JS as a web server framework, and LocalTunnel which provides tunneling method so that the system can be accessed from the Internet.
In this research, Smart Room System has been successfully implemented and tested based on its functional and non-functional requirement. In addition of functionality, the system also provides the performance in terms of response time for lights and fans actuator which is the average time required was 5.52 seconds and 4.31 seconds with sensor input, 0.19 seconds and 0.4 seconds with the user input from LAN, 0.56 seconds and 0.76 seconds with the user input from Internet. The average usage of CPU resources is 35,07% with 2 cores. Time needed for access the system from LAN is 1,77 seconds on average and access from Internet needed 4,82 seconds on average.
",,,57818,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100067,,AMIN EL KHARIS,PENGEMBANGAN VERSI ANDROID SISTEM KONTAK UGM DENGAN MEMANFAATKAN GOOGLE CLOUD MESSAGING,,,"Aplikasi Mobile, Google Cloud Messaging, GCM, Android","Bambang Nurcahyo P, Drs., M.Sc",2,3,0,2016,1,"Kontak UGM merupakan sebutan untuk sistem Paperless Office (PLO) yang digunakan di lingkungan Universitas Gadjah Mada yang dapat diakses melalui web browser baik itu desktop browser maupun mobile browser. Namun, pengguna yang mengakses sistem Kontak UGM melalui perangkat mobile ini tidak dapat menikmati semua fitur yang ada di dalamnya. Sebagai contoh notifikasi langsung yang pengguna sistem Kontak UGM dapatkan pada desktop browser tidak dapat dinikmati jika pengguna mengakses sistem Kontak UGM melalui mobile browser. Hal tersebut dikarenakan aplikasi sistem Kontak UGM versi mobile berjalan di atas mobile browser yang memiliki kemampuan terbatas dalam menyajikan fitur sistem Kontak UGM.
Pada penelitian ini dikembangkan sebuah aplikasi mobile pada sistem Kontak UGM berbasis Android dengan memanfaatkan Google Cloud Messaging (GCM) yang dapat memberikan layanan fungsi notifikasi secara realtime. Apabila terjadi perubahan data pada server, aplikasi mobile mendapatkan notifikasi server layanan secara langsung. Adapun untuk berkomunikasi dengan server, aplikasi ini akan menggunakan Application Programming Interface (API) sistem Kontak UGM versi website sebagai jembatan untuk bertukar data dengan server.
Dari hasil pengujian, dilakukan perbandingan keberhasilan implementasi fitur notifikasi GCM pada aplikasi mobile antara pengiriman fitur post baru yang menggunakan sistem Kontak UGM versi web dan pengiriman fitur post  baru yang menggunakan aplikasi mobile yang sudah ter-install. Terdapat beberapa fitur notifikasi dalam pengiriman fungsi post baru dalam sistem Kontak UGM yang tidak dapat diaplikasikan secara maksimal pada aplikasi mobile. Hal ini terjadi karena kurang terstrukturnya database pada sistem Kontak UGM.","Kontak UGM is the name of a Paperless Office system (PLO) which is used in Universitas Gadjah Mada. It can be accessed via web browser, either a desktop browser or a mobile browser. Unfortunately, users who access it from mobile browsers cannot access all of its features. For example, in the desktop browser, a user can get a real time notification which will not appear if the user use the mobile browser. This happens because mobile browsers have limited functions which cannot display all of the features.
In this research, an Android based mobile application is developed for the Kontak UGM system using Google Cloud Messaging (GCM) which can provide real time notifications.  If there are any changes with the data in the system, users will get notifications from the server to their phones. The application is using Application Programming Interface (API) website version as the communication line between the application and the server.
The research compares the success rate of new post feature between the mobile apps and the website version of the system. Results show that some of the notification features cannot be implemented in the mobile apps optimally. This problem occurs because the database structure in the system.
",,,50665,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98020,,MUH RAFIF MURAZZA,Perbandingan Basis Data Relasional dengan NoSQL Cassandra untuk Data Warehouse yang Menyimpan Data Twitter Secara Real-Time,,,"Data Warehouse, Twitter, NoSQL, Cassandra, Real-time","Arif Nurwidyantoro, S.Kom., M.Cs.",2,3,0,2016,1,"Pesatnya perkembangan aktivitas jejaring sosial saat ini mengakibatkan munculnya ledakan arus data dengan volume yang besar dan menyebabkan beberapa perusahaan mulai memanfaatkan data tersebut untuk mendukung pengambilan keputusan secara mendekati real-time. Salah satu media sosial yang sangat populer adalah Twitter. Namun, kebanyakan dari tools yang dikembangkan dari hasil penelitian terhadap Twitter masih bersifat spesifik terhadap suatu jenis analisis. Hal tersebut disebabkan proses penelusuran keseluruhan aktivitas dari database yang sangat besar sangatlah rumit terlebih secara mendekati real-time. Oleh karena itu diperlukan teknologi data warehouse yang mampu menyimpan secara mendekati real-time hasil pengolahan dan pengubahan data semi-terstruktur menjadi terstruktur sehingga dapat digunakan dalam berbagai jenis analisis. 

Pengembangan data warehouse dengan basis data relasional mulai menunjukkan keterbatasannya dalam menyimpan dan mengolah data besar sehingga perhatian publik mulai mengarah pada basis data NoSQL (Not Only SQL). Penelitian ini mengembangkan data warehouse menggunakan salah satu basis data NoSQL yang populer, yaitu Cassandra. Penelitian ini berfokus pada eksplorasi kemampuan Cassandra sebagai media data warehouse untuk menyimpan data Twitter yang mendekati real-time dengan membandingkan performa proses penyimpanan dan pemanggilan data dengan basis data relasional MySQL dan PostgreSQL. 

Hasil penelitian ini menunjukkan bahwa diantara kedua basis data relasional, PostgreSQL memiliki performa penyimpanan lebih baik dibanding MySQL. Namun, secara keseluruhan Cassandra memiliki performa penyimpanan yang paling cepat diantara kedua basis data lainnya secara signi&Atilde;&macr;&Acirc;&not;&iuml;&iquest;&frac12;kan. Dalam aspek pemanggilan data, MySQL lebih unggul dibanding PostgreSQL secara keseluruhan dan hanya unggul pada data kecil ketika dibandingkan dengan Cassandra.
","The rapid growth of social network activities these days cause the explosion of data stream with big volume which also cause some corporation to start using it to support their decision making in real-time. Twitter is one of the most popular social media. However, most of the tools developed from research based on Twitter are still specifics in some tasks only. Because, exploring every activities from a big database is problematic let alone in real-time. Therefore, a data warehouse which able to store, process, and transform semi-structured data into structured in real-time is needed. 

The data warehouse development using relational database start to show its limits on storing big data let alone real-time. Hence, public attention start going toward NoSQL (Not Only SQL) technology. In this research, a near real-time data warehouse using one of NoSQL databases, Cassandra, will be developed. This research focuses on exploring Cassandra ability as the solution of near real-time Twitter data warehouse by comparing its storing and querying performance with relational database, MySQL and PostgreSQL. 

Results show that between the two relational database, the storing performance of PostgreSQL exceeds that of MySQL statistically. Although, Cassandra has the best storing performance out of the three databases in general. On the querying performance aspects, MySQL is faster that PostgreSQL. The same goes when comparing it to Cassandra, but only in relatively small data. When a large data is queried, the query performance of Cassandra surpass that of the other two relational database.
",,,48328,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
107241,,EDO SYAHPUTRA,PENDEKATAN DATA MINING UNTUK ANALISIS KEPRIBADIAN REMAJA BERDASARKAN PENGGUNAAN DATA SELULER,,,"MBTI, data mining, Naive Bayes, pohon keputusan C4.5, FP-Growth","Aina Musdholifah, S.Kom., M.Kom., Ph.D.",2,3,0,2016,1,"Psikoinformatika merupakan salah satu sub disiplin ilmu baru yang terlahir dari kombinasi disiplin ilmu psikologi dan informatika. Berbeda dengan informatika sosial yang membahas mengenai masyarakat di era digital secara umum, psikoinformatika mengupas hubungan manusia sebagai individu dengan perkembangan teknologi khususnya teknologi informasi.
	Penelitian psikoinformatika yang dilakukan menggunakan teknik penggalian data untuk menentukan kepribadian seseorang berdasarkan tes Myers-Briggs Type Indicator dan menganalisis pola penggunaan data seluler. Algoritma yang digunakan meliputi algoritma klasifikasi Naive Bayes, pohon keputusan tipe C4.5 dan algoritma asosiasi FP-Growth yang biasa digunakan untuk analisis &quot;keranjang belanja&quot;. Berbagai metode praproses digunakan untuk memperoleh hasil terbaik. Normalisasi data juga dilakukan untuk mengecilkan jarak yang diakibatkan dari persebaran data yang terlalu lebar.
	Pada pengujian dengan Naive Bayes, akurasi tertinggi didapatkan ketika dilakukan eksperimen terhadap sub kepribadian perceiver dan judger hingga 66.15% dengan mempertimbangkan jenis kelamin dan usia seseorang.  Pembangunan pohon keputusan yang terbaik terjadi ketika jenis data yang digunakan diubah ke dalam bentuk persentase, artinya nilai penggunakan data dalam megabyte diubah ke dalam proporsi tertentu bagi tiap orang. Hasil yang didapatkan menunjukkan akurasi sebesar 74.34%.
	Analisis asosiasi dengan algoritma FP-Growth yang digunakan pada penelitian ini menghasilkan beberapa strong rules dan menunjukkan frequent itemset yang dalam hal ini adalah sekumpulan pilihan aplikasi yang paling sering digunakan oleh remaja di Kota Yogyakarta. Hasilnya, aplikasi seperti Instagram, Line, Whatsapp dan Google Chrome menjadi pilihan remaja perempuan sedangkan remaja laki-laki lebih sering menggunakan Facebook dan Youtube.
","Psycho-informatics is a new emerging sub-discipline from the combination of psychology and informatics. In contrast with social informatics, which discuss about human as a society in digital world generally, psychoinformatics is focused on the relationship between human as an individual with technological development especially in technology of information.
	This research is try to do experiment in psychoinformatics area by implementing data mining technique to determine someone's character based on  Myers-Briggs Type Indicator test by analyze their cellular data usage pattern. The algorithm which are used in this research including classification algorithm of Naive Bayes, decision tree type C4.5 and FP-Growth as association algorithm which is very popular in &quot;market basket&quot; analysis. Many pre-processing methods were conducted to ensure the best result. Data normalization also conducted to minimize the gap between data from one instances to others.
	In Naive Bayes experiment, the highest accuracy is shown when the model classify sub-character of &quot;perceiver&quot; and &quot;judger&quot; from dataset with the value up to 66.15%. This result also consider gender and age attribute as addition in dataset which is designed to only analyze people's character by their cellular data usage. 
	The best decision tree from this experiment is built when the value of data in each attribute are converted from megabyte into percentage. This percentage represent the proportion of data usage from each instances. The accuracy of this decision tree is 74.34%.
	Association analysis with FP-Growth algorithm which is used in this research, resulted in showing some strong rules and frequent itemset of mobile applications that chosen by teenagers in Yogyakarta City. Instagram, Line Whatsapp and Google Chrome are the most popular applications for female teenagers while the male teenagers prefer Facebook and Youtube.
",,,57844,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
101867,,AHMAD AZAM HUDA,IMAGE PROCESSING APPLICATION FOR TONGUE DIAGNOSIS,,,"hue filtering, color difference CIE94, tongue diagnosis","Drs. Agus Harjoko, M.Sc, Ph.D",2,3,0,2016,1,"In the traditional Chinese medicine, one of the methods is tongue diagnosis. By diagnosis based on the tongue human, it gives the condition of the human body. Although the condition is not specific but it is still useable. By knowing this, the researcher creating a computerized system to do the tongue diagnosis. This can help people know their condition without having gone to the doctor.
	The system will analyze the image of tongue and determine the diagnosis. System implemented in computer program. Using basic algorithm of image processing such as grayscaling, filtering, resizing, and thresholding. Also classification by using calculating color difference. The filtering based on hue value from HSL color space and the calculation color difference using formula CIE94.
	The result of experiment shows that the filtering using hue value give good result, although not nearly perfect. Classification using the formula CIE94 also give overall normal result, because color of the input image is not corrected for the true color and effect the result of classification.
","Dalam pengobatan tradisional Cina, salah satu metodenya adalah diagnosis lidah. Dengan diagnosis berdasarkan pada lidah manusia akan memberikan kondisi tubuh manusia. Meskipun kondisi ini tidak spesifik tetapi masih dapat digunakan. Dengan mengetahui ini, peneliti menciptakan sebuah sistem komputerisasi untuk melakukan diagnosis lidah. Hal ini dapat membantu orang tahu kondisi mereka tanpa pergi ke dokter.
Sistem ini akan menganalisis citra lidah dan menentukan diagnosis. Sistem diterapkan dalam program komputer. Menggunakan algoritma dasar pengolahan gambar seperti grayscaling, filtering, resizing, dan thresholding. Juga klasifikasi dengan menggunakan menghitung perbedaan warna. Filtering yang berdasarkan nilai hue dari ruang warna HSL dan perhitungan perbedaan warna menggunakan rumus CIE94.
Hasil penelitian menunjukkan bahwa filtering menggunakan nilai hue memberikan hasil yang baik, meskipun tidak sempurna. Klasifikasi menggunakan rumus CIE94 juga memberikan hasil yang normal secara keseluruhan, karena warna gambar input tidak dikoreksi untuk warna yang benar dan mempengaruhi hasil klasifikasi.
",,,52255,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104173,,AKBAR DZUKHA ASYIQIN,CLUSTERING TIME SERIES MENGGUNAKAN DYNAMIC TIME WARPING DAN EUCLIDEAN DISTANCE (STUDI KASUS DATA PENYAKIT DBD DAERAH ISTIMEWA YOGYAKARTA),,," K-Means, K-Medoid, Hierarchical K-Means, Hierarchical K-Medoid, silhouette, Data DBD","Afiahayati, S.Kom, M.Cs, Ph.D",2,3,0,2016,1,"Penyakit Demam Berdarah merupakan salah satu penyakit yang terjadi setiap tahun di Indonesia. Di Daerah Istimewa Yogyakarta kejadian penyakit DBD terjadi di seluruh kecamatan. Puskesmas pada masing-masing daerah banyak melakukan upaya penanggulangan, salah satunya dengan mendata kasus penyakit DBD. Pendataan yang dilakukan puskesmas bertujuan untuk mengumpulkan informasi kejadian penyakit DBD di wilayah kerja puskesmas. Pendataan telah dilakukan pada tahun 2010-2014 di 78 kecamatan DIY. Data tersebut digunakan dalam penelitian Clustering ini. Metode Clustering melakukan pengelompokkan data sesuai dengan kemiripan data. Penelitian ini menggunakan Clustering K-Means, K-Medoid, Hierarchical K-Means, dan Hierarchical K-Medoid. Penerapan keempat algoritma dilakukan pada data DBD DIY. Pengujian yang dilakukan pada masing-masing algoritma adalah uji kasus jumlah k yang bernilai 2,3,4, dan 5 serta evaluasi silhouette yang berguna untuk mengetahui hasil algoritma yang baik untuk clustering. Hasil eksperimen menunjukkan bahwa daerah di kota Yogyakarta memiliki kecenderungan berkategori kasus DBD sedang sampai tinggi, sedangkan di luar kota Yogyakarta memiliki kecenderungan berkategori rendah. Penelitian ini menemukan bahwa semakin besar nilai k maka nilai silhouette semakin kecil.
","Dengue Fever is a disease that occurs every year in Indonesia. In Daerah Istimewa Yogyakarta the dengue fever happened in every subdistrict. Puskesmas in every regions carry out some countermeasures, one of them is collecting data of dengue fever. The aim of data documentation conducted by puskesmas is to collect information of DBD incidence on the puskesmas working area. Datacollectionwas conducted in 2010-2014 in 78 districts DIY. The Data will be used on this Clustering research. Clustering method groups data that have similarity between each other. This research is using Clustering K-Means, K-Medoid, Hierarchical K-Means, and Hierarchical K-Medoid. Application of four algorithms will be conducted for DBD DIYs data. Testing that will be done on every algorithm is the experiment with k 2,3,4, and 5 with silhouette evaluation thats useful to know the best algorithm for clustering. The experimental results in this study indicate the city of Yogyakarta has suffered Dengue Fever from medium to high, in another city of Yogyakarta Dengue Fever has suffered in low categories. This study found that the greater the value of k then the value silhouette getting smaller.",,,54702,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
103918,,SATRIA DINA ASTARI,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN SEKOLAH  MENENGAH ATAS (SMA) DI KOTA YOGYAKARTA,,,"sistem pendukung keputusan, sekolah menengah atas, AHP, Profile Match.","Sigit Priyanta,S.Si., M.Kom",2,3,0,2016,1,"Di  Indonesia  pendidikan  sudah  menjadi  kebutuhan pokok  bagi  semua orang  dari  berbagai  kalangan.  Namun  orang  tua  murid  seringkali  menghadapi masalah  yang  beragam  bahwa  mereka  merasa  kesulitan  untuk  memilih  sebuah 
sekolah  bagi  anaknya.  Mulai  dari  persoalan  nilai kelulusan  yang  menjadi  syarat masuk  sekolah,  besarnya  iuran  bulanan  yang  terkadang  menjadi  sebuah pertimbangan  bagi  orang  tua  calon  siswa  terutama  d ari  kalangan  menengah  ke bawah. Sehingga para orang tua calon siswa dituntut untuk cerdas dalam memilih sekolah bagi anak-anaknya. 
Penelitian  ini  dilakukan  dalam  rangka  mengembangkan  suatu  aplikasi sistem pendukung keputusan untuk membantu orang tua dalam  memilih sekolah yang sesuai bagi anak namun memenuhi kriteria yang diinginkan oleh orang tua. 
Konsep  dasar  penelitian  yang  digunakan  untuk  analisis  masalah  adalah  metode AHP  (Analytical  Hierarchy  Process)  dan  Profile  Matching.  AHP  dan  Profile Matching  adalah  teknik  pengambilan  keputusan  yang  digunakan  dalam  analisis kebijaksanaan. Persoalan  yang akan diselesaikan, diuraikan  menjadi unsur -unsur yang terdiri dari kriteria, sub kriteria dan alternatif. 
Hasil  penelitian  berupa  sebuah  sistem  pendukung keputusan  untuk membantu orang tua calon siswa menentukan sekolah sesuai dengan kriteria dan sub kriteria yang diinginkan berdasarkan nilai bobot sekolah tersebut. Sistem ini dibangun  dengan  berbasis  web  menggunakan  bahasa  pemrograman  PHP framework Codeigniter serta basis data MySQL.
Kata kunci : sistem pendukung keputusan, sekolah menengah atas, AHP, Profile Match.","In  Indonesia  education  has  became  primary  need  for  people,  but  the parents  of students  often meet  the various problems that they feel the difficulties to choose a school for their children. It begins from the problem graduation score which means the requirement to enter the school, the expensive monthly fee as a consideration  for  parents,  especially  medium-end  to  low-end  class  so  that  they must be smart to choose schools for their children.
This research was develop an application decision support system to assist parents in choosing a suitable school for children but meet the criteria desired by parents.  The  basic  concept  used  for  decision-making  is  AHP  (Analytical 
Hierarchy  Process)  and  Profile  Matching  method.  AHP  and  Profile  Matching  is decision-making  techniques  used  in  policy  analysis.  The  issue  will  be  resolved, brake down into elements that consist of criteria, sub-criteria and alternatives.
The results of this research is  a  decision support system to help parents of prospective students  choosing  the school in accordance with the criteria and subcriteria to be desired based on the weight of the school. The system  was  built  in  a web-based  using  framework  CodeIgniter,  PHP  programming  language  and MySQL database.
Keywords : Decision Support System, Senior High School, Analytical Hierarchy Process, Profile Matching",,,54403,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
104951,,NUR ROHMAH HIDAYATIN,ARTS: IMPLEMENTASI TEKNOLOGI AUGMENTED REALITY PADA APLIKASI TUNTUNAN SHALAT BERBASIS ANDROID,,,"Augmented Reality, AR, Natural Feature Marker, NFM, tuntunan shalat, Android","Agus Harjoko, Drs., M.Sc., Ph.D.",2,3,0,2016,1,"Penelitian ini berusaha mengimplementasikan teknologi Augmented Reality (AR) pada aplikasi tuntunan shalat berbasis Android. AR merupakan teknologi yang dapat membantu manusia dalam menyajikan informasi agar lebih mudah untuk diterima dan difahami oleh indera dengan menampilkan konten tertambah ke dunia nyata. Edutainment merupakan salah satu bidang dimana teknologi AR banyak diimplementasikan. Dalam bidang tersebut, AR banyak digunakan sebagai media pembelajaran.Tuntunan shalat dipilih karena shalat merupakan salah satu rukun Islam dan meninggalkan shalat adalah haram hukumnya bagi setiap Muslim yang telah memenuhi syarat wajib shalat dalam keadaan apapun. Bimbingan orang tua dan guru merupakana cara utama dalam mempelajari tuntunan shalat, akan tetapi terdapat beberapa alternatif lain yang umum dipraktikan dalam mempelajari tuntunan shalat termasuk membaca buku ataupun menonton CD tutorial yang lebih interaktif daripada sekedar membaca buku. Teknologi AR memungkinkan kedua metode pembelajaran tersebut disatukan dengan menjadikan buku sebagai penanda alami untuk menampilkan konten tertambah dalam bentuk multimedia. 
Hasil akhir penelitian ini berupa aplikasi tuntunan shalat berbasis Android bernama ARTS. Konten dan materi dalam aplikasi ARTS disarikan dari buku Risalah Tuntunan Shalat karya Drs. Moh. Rifa&acirc;€™i dari penerbit PT. Karya Toha Putra Semarang. Upaya implementasi teknologi AR pada aplikasi berhasil dilakukan sehingga ARTS mampu menampilkan objek tertambah berupa model 3D gerakan shalat, wudhu dan tayammum dengan memakai buku Risalah Tuntunan Shalat sebagai Natural Feature Marker (NFM). Hasil pengujian kuesioner pada 12 responden menunjukkan bahwa tujuan pembangunan ARTS sebagai alat bantu umat muslim dalam mempelajari tata cara shalat dengan lebih interaktif telah tercapai.
	
","This study seeks to implement Augmentes Reality (AR) technology in the Android-based Tuntunan Shalat application. AR is a technology that can help people in presenting information to make it easier to be accepted and understood by the human senses by presenting augmented content to the real world. Edutainment is one area in which the AR technology widely implemented. In these fields, AR is widely used as a medium of learning.Tuntunan shalat have been selected for shalat is one of the pillars of Islam and leave the prayer is unlawful for any Muslim who has qualified obligatory prayers under any circumstances. Guidance from parents and teachers are the main way in studying shalat, but there are some other alternatives commonly practiced in studying shalat includes read a book or watch a tutorial CD that is more interactive than simply reading a book. AR technology allows both the learning methods to collaborate using the books as a natural marker to display the augmented content in a multimedia format.
The final result of this research is a Tuntunan Shalat application based on Android OS called ARTS. The content and materials in the application excerpted from the book Risalah Tuntunan Shalat by Drs. Moh. Rifa'i of PT. Karya Toha Putra Semarang. Implementation efforts of AR technology on the application successfully done so the application have the ability to display augmented objects in the form of a 3D model for shalat, wudu and tayammum movements using the Risalah Tuntunan Shalat book as a Natural Feature Marker (NFM). Results of testing the questionnaire at 12 respondents indicated that the purpose of the construction of ARTS as a tool for Muslims in studying the ordinance of shalat in a more interactive way has been reached.

",,,55427,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92926,,SRI MULIANY BOUTY,PENGEMBANGAN APLIKASI PENGINGAT DENGAN PEMANFAATAN OPTICAL CHARACTER RECOGNITION BERBASIS SISTEM OPERASI ANDROID,,," Pengingat berbasis waktu, character recognition, Android, time-based reminder, character recognition","Agus Harjoko Drs., M.Sc., Ph.D",2,3,0,2016,1,"Pengembangan aplikasi pengingat sudah bukan menjadi hal baru lagi
sekarang. Semakin banyak jenis aplikasi pengingat baik berbasis waktu maupun
berbasis tempat, masing-masing dengan kelebihan dan fiturnya sendiri. Akan tetapi,
kebanyakan aplikasi pengingat yang telah dipublikasikan memiliki satu kesamaan
yaitu menggunakan ketikan manual pengguna sebagai masukan.
Berangkat dari hal ini dirancang suatu sistem pengingat berbasis waktu
yang memiliki fitur gambar sebagai masukan dimana gambar yang dimaksudkan
adalah gambar poster, selebaran maupun publikasi sejenis baik cetak maupun
elektronik yang diambil dari galeri pengguna maupun difoto kamera. Aplikasi ini
mengimplementasikan Optical Character Recognition (OCR) untuk memproses
gambar menjadi suatu event baru lengkap dengan judul, tanggal dan waktu
pelaksanaan. Pengingat ini diimplementasikan pada perangkat mobile sehingga
dapat diakses dimana saja dan kapan saja.
Berdasarkan hasil pengujian, sistem dapat membuat pengingat dengan
masukan manual pengguna maupun dari hasil pemrosesan gambar menjadi
karakter. Aplikasi juga meluncurkan notifikasi beserta alarm pada waktu suatu
event dilaksanakan. Sementara itu, hasil akurasi yang didapatkan untuk pengenalan
karakter dari input gambar adalah sebesar 76.7% untuk format tanggal dan 80%
untuk format waktu pada softfile yang diambil dari galeri dan 56.67% untuk kedua
format pada input yang difoto kamera.","Development of reminder application is not a new topic in this time. The
number of type of reminder are increasing both in location-based or time-based
reminder, each with its own advantages and features. However most published
reminder applications have one common feature which is using user&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;s manual type
as input.
Departing from this issue, a time-based reminder system with image input
feature is developed in which images meant are posters, flyers or similar images
both printed and electronic that existed in user's gallery or taken by camera. This
application implements Optical Character Recognition (OCR) to process picture
into new event along with its title, date, and time. This system is implemented on
mobile in order to ease user's access.
Based on the testing result, system can create a new reminder by user's
manual input or by image-to-character processing results. This application also can
launch notification along with alarm at the time an event is held. Meanwhile,
character recognition of image input produce accuration about 76.7% for date
format, and 80% for time format in softfile attached from gallery and 56.67% for
both format in picture taken by camera.",,,43055,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
100350,,ADRI ANISA P,ANALISIS PENGARUH SERANGAN DDOS TERHADAP KINERJA VIRTUAL DATA CENTER,,,"data center, IDS, XenServer, Snort, performa CPU, performa jaringan.","Drs. Medi, M.Kom",2,3,0,2016,1,"Saat ini keberadaan cloud computing sangat berkembang dan
menjadi tren teknologi informasi. Cloud computing mampu menyediakan
dynamic resource pool, virtualisasi, dan kemampuan untuk melakukan
sharing resource serta dapat menjadi solusi masalah kebutuhan resource
yang berskala besar. Virtualisasi merupakan salah satu infrastruktur dari
cloud, karena virtualisasi menyediakan kemampuan untuk melakukan
sharing resource tanpa mempengaruhi satu sama lain. Dalam melakukan
proteksi terhadap data yang ada pada cloud, faktor keamanan sangat perlu
untuk menjadi perhatian.
Pada penelitian ini yang dilakukan yaitu melakukan pengujian
kinerja static dan dynamic virtual machine pada virtual data center terhadap
kerentanan serangan dengan menggunakan Xerxes dan hping3 yang
dibangun secara virtual terhadap suatu suatu server yang dikonfigurasi
dengan menggunakan tools Snort 2.9.7.5 beserta BASE sebagai web-front
end yang diimplementasikan pada XenServer 6.5.0. Hasil dari penelitian
dengan melakukan serangan Ddos diperoleh hasil bahwa static virtual
machine pada virtual data center lebih optimal dalam menggunakan
resource daripada dynamic virtual machine dalam hal performa CPU dan
network pada saat dilakukan serangan.","Nowadays, the existence of cloud computing is developing rapidly
and become a trend on information technology. Cloud computing provides
dynamic resource pool, virtualization, and have the ability to do resource
sharing and can solve problem on large-scale resource. Virtualization
becaming pasrt of cloud infrastructure, because virtualization provides the
ability to do resource sharing without affecting each other. In conducting
the protection of data in cloud, security factor have to be considered greatly.
In this research, researcher tests the vulnerability of static and
dynamic virtual machine on virtual data center which configured using
Snort 2.9.7.5 tools using BASE web-front end and implemented on
XenServer 6.5.0. Doing the Ddos attack shows that static virtual machine in
the virtual data center have better stability performance than dynamic server
in term of CPU dan network performance.",,,50638,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81923,,RIZQI NURROHMAH,ANALISIS DAN IDENTIFIKASI HUBUNGAN JENIS FILE DENGAN PARAMETER PRESERVASI UNTUK PERENCANAAN PRESERVASI MENGGUNAKAN STANDAR PLATO 4.4,,,"preservation, significant properties, metadata, plato 4.4.","Khabib Mustofa, M.Kom., Dr.Techn",2,3,0,2015,1,"Perkembangan jenis data kian hari kian meningkat seiring meningkatnya tools dan perangkat lunak, untuk itu dibutuhkan preservasi data. Jumlah tools yang tersedia untuk preservasi data yang sesuai standar seperti images atau dokumen elektronik juga semakin meningkat. Sampai sekarang proses perencanaan preservasi menyita banyak waktu dan prosedur yang langkahnya dilakukan secara manual. Sebagai proses preservasi digital maka dibutuhkan sebuah analisis terhadap parameter yang mendukung perencanaan preservasi atau yang disebut Significant Properties. Perencanaan preservasi dapat menyimpan informasi data dan dapat dengan mudah mengetahui informasi metadata mengenai data tersebut. Plato merupakan alat pendukung keputusan yang menerapkan proses dan pelayanan perencanaan preservasi yang berintegritas untuk mendukung analisa tersebut. Untuk itu diperlukan adanya sebuah penelitian terhadap jenis data yang termasuk dalam parameter perencanaan preservasi atau significant properties. 
Penelitian ini melakukan analisis dengan pemetaan hasil ekstraksi metadata pada Plato dan parameter preservasi significant properties. Setiap metadata akan dipetakan terhadap 5 parameter yaitu appearance, content, context, structure, dan behaviour. Untuk melakukan analisa dibuatlah sebuah sistem yang membantu untuk melakukan pemetaan tersebut.
Hasil yang diharapkan pada penelitian ini yaitu sebuah data yang menunjukkan beberapa jenis data yang dapat digunakan sebagai obyek perencanaan preservasi. Semakin banyak metadata dan parameter yang melengkapi maka semakin besar pula peluang sebuah data untuk dapat dipreservasi.","The increase of data types is developing day by day with the tools and software, it is necessary for the preservation of data. The availability of number of tools for data preservation standards such as images or electronic documents are also increase. Until now preservation planning process had the time consuming and the step procedure done manually. As digital preservation process then takes an analysis of the parameters that support preservation planning significant p[roperties. Preservation planning can save the information of data and can easily figure out the metadata information about the data. Plato is a decision support tool that implements the process and service of preservation planning integrity to support the analysis. It required a study of the data type that is included in the preservation planning parameters or significant properties.
This study analysis by mapping the extracted metadata in Plato and preservation of significant Properties parameters. Each metadata will be mapped into five parameters such as appearance, content, context, structure, and behavior. A system was built to perform the analysis that helps the mapping activities. 
The results are expected in this study is a data presented several data type that can be used as an object of preservation planning. The more metadata and parameters that complements the greater the chances of a data is to be preserved.",,,31882,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92678,,BASITH ZAINURROHMAN,Algoritma Pencarian Tetangga Terdekat Menggunakan Pohon K-Dimensi Berbasis MapReduce untuk Memprediksi Tujuan Akhir Perjalanan Taksi,,,"pencarian tetangga terdekat, pohon k-dimensi, MapReduce, Hadoop","Nur Rokhman, S.Si., M.Kom.",2,3,0,2015,1,"Algoritma pencarian tetangga terdekat telah dikenal luas untuk mencari kemiripan suatu objek terhadap objek lainnya sehingga implementasinya biasa digunakan pada bidang pengenalan pola, penggalian data, pengolahan citra, dan lain-lain. Algoritma ini dioptimasi dalam stuktur data pohon k-dimensi. Hal ini dilakukan dengan melewatkan objek pencarian yang tidak mungkin menghasilkan hasil yang optimal sehingga mempercepat waktu proses pencarian.
Implementasi suatu algoritma yang berjalan secara paralel dan terdistribusi telah menjadi objek penelitian yang sangat diminati pada dekade terakhir ini. Hal ini dilakukan untuk menangani data berskala besar. Salah satu model implementasi yang digunakan dalam penanganan data besar adalah MapReduce. Akan tetapi, library yang dapat dimanfaatkan untuk keperluan penggalian data menggunakan model ini belum banyak tersedia. Salah satunya adalah algoritma pencarian tetangga terdekat menggunakan pohon k-dimensi.
Dalam penelitian ini dikembangkan algoritma pencarian tetangga terdekat menggunakan pohon k-dimensi yang berjalan secara paralel dan terdistribusi menggunakan model MapReduce. Program ini dijalankan di atas kerangka kerja Apache Hadoop. Penelitian ini dilakukan dengan menguji 1.7 juta data titik query terhadap 2888, 4138, dan 5439 pusat klaster objek data spasial berupa koordinat GPS trayek taksi sebagai data pelatihan. Hal ini dijalankan pada 1 hingga 5 mesin. Hasilnya algoritma ini mampu berjalan sekitar 400000 ms pada 5 mesin.","The nearest neighbor search algorithm has been widely used to search similarity of object toward the other object, so that the implementation commonly used for subject such as pattern recognition, data mining, image processing, and so on. This algorithm can be optimized by using k-dimensional tree. This is done by skipping searching object which does not produce optimal result so this mechanism can accelerate the running time of searching process.
The implementation of algorithm which running in parallel and distributed has become an interest research object in recent decades. It is used to handle large scale data. One implementation model that used for handling big data is MapReduce. However, there are few libraries available that could be used for data mining cases with MapReduce technique. One of which is the nearest neighbor search algorithm using k-dimensional tree.
In this research, nearest neighbor with k-dimensional tree which running in parallel and distributed technique based on MapReduce has been developed. The program is running on Apache Hadoop framework. This research is done by testing about 1.7 million of query data toward 2888, 4138, and 5439 centroids of cluster data which is spatial data object, GPS coordinate of taxi trajectory as training data. It runs on 1 until 5 machines. The result, this algorithm can run about 400000 ms on 5 machines.",2015-12-31 00:00:00,53,42820,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
90119,,NURUL WIJAYANTI,SISTEM INFORMASI GEOGRAFIS PEMETAAN GEMPA BUMI TEKTONIK DI JAWA TENGAH DAN YOGYAKARTA,,,"Gempa, Great Circle Formula, Twitter, Google Maps, CodeIgniter, Sistem Informasi Geografis","Janoe Hendarto, Drs., M.Kom",2,3,0,2015,1,"	Indonesia merupakan negara yang terletak pada perbenturan tiga lempeng kerak bumi yaitu lempeng Eurasia, lempeng Pasifik, dan lempeng India Australia. Berdasar lokasi tersebut menjadikan Indonesia salah satu negara dengan potensi kegempaan yang tinggi. BMKG merupakan badan kepemerintahan yang menangani mengenai gempa bumi. BMKG mempunyai tugas untuk menyebarkan informasi mengenai gempa yang sedang terjadi, salah satu media yang digunakan BMKG dalam menyebarkan informasi yaitu dengan menggunakan media sosial. Salah satu media sosial yang digunakan BMKG adalah Twitter. Tweet BMKG mengandung informasi mengenai gempa bumi dan informasi detail mengenai gempa. 
	Informasi tweet BMKG merupakan sumber data utama dalam melakukan pemetaan terhadap lokasi pusat gempa bumi. Tokenisasi dilakukan untuk memecah tweet menjadi penggalan kata sesuai dengan kategori. Penentuan lokasi pusat gempa dihitung dengan menggunakan Great Circle Formula karena pada tweet BMKG tidak memuat informasi latitude maupun longitude. Hasil perhitungan dengan menggunakan Great Circle Formula menghasilkan perkiraan nilai latitude dan longitude pusat gempa. Latitude dan longitude hasil perhitungan memiliki nilai yang hampir mendekati nilai latitude dan longitude yang tertera di BMKG. Pemetaan dilakukan dengan mencetak marker di google maps sesuai hasil perhitungan Great Circle Formula. Pembuatan sistem informasi geografis berbasis web dengan menggunakan framework codeigniter.
	Luaran penelitian berupa sistem informasi geografis yang mampu memetakkan lokasi pusat gempa bumi dan menampilkan informasi detail mengenai gempa bumi dan cakupan wilayah yang terkena dampak berdasar dari nilai mmi gempa.
","Indonesia is a country that located on a three earth plates, Eurasian plate, the Pacific plate and India Australia plate. It make Indonesia become a country with high seismic potential. BMKG is the government agent that handles about earthquake.It has a duty to share information about earthquake. It using a social media to share the information, one of social media that used by BMKG is Twitter. The tweet of BMKG has detail information about the earthquake.
 The information of BMKG's tweet is the main source for mapping the earthquake epicenter. Tokenisation used to clean the tweet from unnecessary character, so the data that will be store in database is the clean data. Great circle formula is used to calculate the latitude and longitude. The latitude and longitude have the almost as same as BMKG's. The mapping of eartuhquake epicenter using marker to mark the location on google maps. Beside that, codeigniter is used as the framework.
The result of this research is a geographical information system that can mapping the epicenter of the earthquake and displays detailed information about the earthquake and the scope of the affected area based on the value of earthquake mii.",,,40240,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
85001,,ERICK,KLASIFIKASI JENIS KELAMIN PENULIS MENGGUNAKAN ALGORITMA K-NEAREST NEIGHBOUR DAN HARMONY SEARCH,,,"author profiling, k-NN, HSA","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2015,1,"Author profiling adalah suatu proses penentuan profil pengarang dari suatu data tulisan tertentu. Adapun profil dari suatu pengarang adalah kumpulan karakteristik dari pengarang tersebut yang bisa berupa jenis kelamin, usia, kebangsaan ataupun kepribadian. 
Banyak algoritma yang dapat digunakan dalam mengimplementasikan author profiling. Salah satu algoritma yang dapat digunakan adalah k-Nearest Neighbour (k-NN). Algoritma ini digunakan untuk mencari k buah data yang memiliki kemiripan terbesar dengan data uji yang kemudian data uji tersebut diklasifikasikan ke dalam kelas mayoritas dari k buah data yang memiliki kemiripan tertinggi. Jumlah fitur yang tidak sedikit yang digunakan pada penelitian dapat menyebabkan terjadinya permasalahan curse of dimensionality pada k-NN, sehingga digunakan algoritma Harmony Search untuk mengimplementasikan feature selection dengan harapan jumlah fitur yang digunakan dapat teroptimalkan.
Berdasarkan pengujian sistem yang telah dilakukan, klasifikasi profil penulis menggunakan algoritma k-Nearest Neigbour dan Harmony Search mendapatkan akurasi terbaik sebesar 61% dengan recall dan precision yang diperoleh masing-masing sebesar 54% dan 62.79%.","Author profiling is a task to determine profile of the author of some specific documents. Where as profile of an author may consist of some or all of the author's characteristics such as gender, age, nationality, or personality.
There are a lot of algorithms that can be used in implementing author profiling. One of them is k-Nearest Neighbour (k-NN). In k-NN, the test data is classified to the majority class of the k datas that have the highest similarity with the test data. As the numbers of features that are not in a small amount, it may lead to the curse of dimensionality problem in k-NN, Harmony Search Algorithm is used in order to implement feature selection so that the number of features can be optimalized.
From various testing scheme through the system, authors' profile classification using k-Nearest Neighbour and Harmony Search Algorithm yields accuracy with the maximum value at 61% with recall and precision each at the value of 54% and 62.79%.",,,34998,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83978,,SEPTA BAGAS KARA,RANCANG BANGUN APLIKASI ENKRIPSI DAN DEKRIPSI MENGGUNAKAN ALGORITMA RC4 PADA SISTEM OPERASI ANDROID,,,"Android, Kriptografi, RC4, Enkripsi, Dekripsi, Stream Chipper","Dr. Ing. Mhd. Reza M.I. Pulungan, S.si., M.Sc.",2,3,0,2015,1,"Smartphone dan internet terus mengalami perkembangan. Salah satu teknologi smartphone yang paling pesat perkembangannya adalah Android. Perkembangan yang terjadi pada 2 hal tersebut menjadikan meningkatnya
kebutuhan informasi untuk setiap orang. Salah satu cara untuk memperoleh informasi adalah dengan cara berkirim pesan. Dahulu orang menggunakan layanan SMS untuk berkirim pesan namun dengan perkembangan smartphone dan internet layanan SMS mulai berkurang. Kemudian mulai beralih menggunakan aplikasi messenger. Penggunaan aplikasi messenger meningkatkan orang untuk berkirim pesan namun aplikasi messenger masih memiliki kekurangan dari segi keamanan isi pesan karena aplikasi messenger belum mampu menjamin kerahasiaan isi pesan.
Salah satu cara yang dapat digunakan untuk pengamana pesan adalah dengan menggunakan kriptografi. Pada penelitian ini akan dibangun sebuah aplikasi enkripsi dan dekripsi yang mengimplementasikan algoritma RC4.
Algoritma RC4 adalah merupakan salah satu algoritma kunci simetris dibuat oleh RSA Data Security Inc (RSADSI) yang berbentuk stream chipper. Dari hasil pengujian menunjukan bahwa aplikasi yang dibangun telah dapat mengamankan
isi sebuah pesan dengan melakukan enkripsi pada sebuah teks dalam pesan tersebut.
","Smartphone and internet are continuosly developed. One of the most frequently used technologies is Android. It results in the rise of information that is needed by people. One way to share information is by texting. Previously people
used SMS (Short Message Service) to text. Due to the development of smartphone and internet, people turn to use messenger application that is more attractive and easy than SMS. Therefore, the use of SMS is decreased.
However messenger application still lacks of security for the message content. It cannot guarantee the secret of the message content. To secure the message content, cryptography can be applied on Android. This research
establishes an encryption and decryption which applies RC4 algorithm. The RC4 algorithm is one of the symmetric-key algorithms designed by RSA data security Inc. (RSADSI) in a form of a stream chipper. The result shows that the application has secured the message content by using encryption on the text of a certain message.

",,,33994,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
91658,,IMANUEL B. DREXEL,Pengembangan Aplikasi Edukasi Matematika &quot;Cool Math Worksheet&quot; Berbasis Android,,,"Android, Mathematics, Kumon, Google Analytic","Agus Sihabuddin, S.Si, M.Kom",2,3,0,2015,1,"Pengguna smartphone sudah sangat berkembang pesat terutama dikalangan siswa. Tetapi penggunaan dari perangkat tersebut lebih banyak digunakan untuk kegiatan hiburan, dibandingkan untuk membantu proses pembelajaran mereka. Oleh karena itu, dibuat sebuah aplikasi edukasi matematika untuk membatu proses pembelajaran matematika pada penggunanya. 
Pada penelitian ini dibangun aplikasi &quot;Cool Math Worksheet&quot; yang berbasis Android agar dapat digunakan sebagai sarana pendukung kegiatan belajar pelajaran matematika pada siswa kelas 1-3 sekolah dasar. Aplikasi ini memakai kurikulum matematika kumon sebagai dasar dari pembagian tingkatan kesulitan soal. Aplikasi ini juga mempunyai fitur untuk melihat ulasan soal yang telah dikerjakan. Aplikasi ini juga diimplementasikan Google Analytic, untuk memantau penggunaan aplikasi ini, berdasarkan pengguna yang telah mengunduh aplikasi. 
Dari hasil pengujian, aplikasi edukasi matematika ini telah berjalan sesuai dengan rancangan. Angka yang dihasilkan oleh sistem pembuatan soal sudah sesuai dengan tingkatan pada kurikulum matematika kumon. Hasil dari implementasi Google Analytic juga sudah menunjukkan data mengenai penggunaan aplikasi.","Smartphone users are increasing year after year, especially for student age. That student age use their device a lot more for entertainment purpose, compared to education purpose. Therefore, this mathematic education mobile application that support them for learning mathematics is created.  
In this research, a mobile application named &quot;Cool Math Worksheet&quot; which based on Android is created. This application have aims to support students for 1st up to 3rd grade elementary school learning mathematics. Cool Math Worksheet is based on kumon mathematics curriculum for dividing mathematics questions into several levels. This application also has a feature to review the answered problems. Google Analytic was implemented for monitoring the used of this application.
Based on test results, this mathematics education application has been implemented in accordance with the design itself. The numbers that have been generated are in line with kumon mathematic curriculum, and the Google Analytic's results show the data about the general used of this application.
",,,41751,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81935,,SATYA NUGRAHA,CLASSIFYING TWITTER USERS AS RESIDENTS OR TOURIST BASED ON TWITTER USER HISTORICAL DATA,,,"Classification, MNB, SVM, Decision Tree, Twitter","Edi Winarko, M.Sc., Ph.D",2,3,0,2015,1,"Researches confirms that social media provides good insights on what people think, feel, concern, etc. It is expected that those insight mined from Twitter data has potential to support a better decision-making, especially in public sectors. Public sector wants to know local's insight; therefore they need to make sure they use the conversation from local. However, the ground truth shows that tweets are mixed from the locals and tourist. This study investigated the best automatic fashion model to classify tweets posted by resident and tourist, in NTB.  Indonesia.  To do so, several phases were conducted. Those are pre-processing, data training, classification system, data testing, accuracy comparison, and result visualization.
First of all, a Twitter dataset, which has 700,000 tweets posted by approximately 26,000 users in Nusa Tenggara Barat, Indonesia was prepared. The dataset divided into two sets, tweets from 4,000 users for data training and 22,000 users for data testing. Then, three popular classification algorithms were applied to the datasets. There are Multinomial Naive Bayes, Support Vector Machines and Decision Tree. After that, 7 features are created. There are Bag of Words, Normalizer location, Total Tweet, Total Day, Tweet per Day, Total Location and Location per Day.
Experiment shows that Multinomial Naive Bayes with Bag of Words feature has 86% accuracy, while the rest of features give less than 65% accuracy. This is different with Support Vector Machines and Decision Tree results. These two algorithms produce better accuracy results excluding Bag of Words feature. It implies that Support Vector Machine and Decision Tree are more powerful when processing numerical value. However, among all classification system, Multinomial Naive Bayes still being the most accurate algorithm for the model.
","Researches confirms that social media provides good insights on what people think, feel, concern, etc. It is expected that those insight mined from Twitter data has potential to support a better decision-making, especially in public sectors. Public sector wants to know local's insight; therefore they need to make sure they use the conversation from local. However, the ground truth shows that tweets are mixed from the locals and tourist. This study investigated the best automatic fashion model to classify tweets posted by resident and tourist, in NTB.  Indonesia.  To do so, several phases were conducted. Those are pre-processing, data training, classification system, data testing, accuracy comparison, and result visualization.
First of all, a Twitter dataset, which has 700,000 tweets posted by approximately 26,000 users in Nusa Tenggara Barat, Indonesia was prepared. The dataset divided into two sets, tweets from 4,000 users for data training and 22,000 users for data testing. Then, three popular classification algorithms were applied to the datasets. There are Multinomial Naive Bayes, Support Vector Machines and Decision Tree. After that, 7 features are created. There are Bag of Words, Normalizer location, Total Tweet, Total Day, Tweet per Day, Total Location and Location per Day.
Experiment shows that Multinomial Naive Bayes with Bag of Words feature has 86% accuracy, while the rest of features give less than 65% accuracy. This is different with Support Vector Machines and Decision Tree results. These two algorithms produce better accuracy results excluding Bag of Words feature. It implies that Support Vector Machine and Decision Tree are more powerful when processing numerical value. However, among all classification system, Multinomial Naive Bayes still being the most accurate algorithm for the model.
",,,33453,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83984,,DHIAZ ARGI SETIAWAN,Opinion Mining Terhadap Resensi Video Games Menggunakan Support Vector Machine,,,"opinion mining, data mining, support vector machine, resensi video game","Aina Musdholifah, S.Kom., M.Kom., Ph.D.",2,3,0,2015,1,"Saat ini industri video games adalah salah satu industri yang cukup berkembang pesat dengan puluhan hingga ratusan judul rilis setiap tahun. Akan tetapi tidak semua judul tersebut memiliki tingkat kualitas dan kepuasan yang sama sehingga banyak orang yang mengutarakan opini mereka dalam bentuk resensi yang dapat dibaca oleh orang lain untuk dijadikan bahan pertimbangan dalam menentukan judul yang akan dimainkan. Opinion mining merupakan salah satu bagian dari data mining yang dapat digunakan untuk mengekstrak opini dari resensi-resensi tersebut.
Penelitian ini mencoba membangun suatu sistem yang dapat melakukan opinion mining terhadap resensi-resensi dari video games dengan menggunakan metode klasifikasi Support Vector Machine (SVM). Penelitian ini juga membandingkan jenis-jenis kernel yang biasa digunakan dalam SVM, yaitu kernel linier, polinomial dan RBF. Selain itu jumlah atribut yang digunakan untuk setiap resensi atau baris data dibatasi hanya berjumlah 500, 1000 dan 1500 dengan menggunakan algoritma information gain. Masing-masing jumlah atribut tersebut juga akan dibandingkan hasilnya.
Terdapat dua jenis metode pengujian yang digunakan dalam penelitian ini, yaitu 10-fold cross validation dan juga pengujian dengan menggunakan set data pengujian yang independen. Dari kedua pengujian tersebut menunjukkan bahwa penggunaan kernel dan jumlah atribut yang berbeda tidak membawa pengaruh yang besar, yaitu sekitar 0-2%. Jumlah atribut sebanyak 1000 menghasilkan nilai akurasi tertinggi pada kedua pengujian dengan nilai 89,68% pada 10-fold cross validation dan 81,72% pada pengujian dengan data pengujian. Nilai tersebut dicapai dengan menggunakan kernel polinomial derajat tiga pada pengujian 10-fold cross validation dan dengan menggunakan kernel linier pada pengujian dengan menggunakan data pengujian.","Currently the video games industry is one of the rapidly growing industry with tens to hundreds of titles released each year. However, not every title has the same quality and satisfaction leading many people expressing their thought on a review which others can read and used as a consideration in determining which title to be played. Opinion is a task of data mining that can be used to extract the opinions of those reviews.
This research is trying to build a system that can do opinion mining on video games reviews using Support Vector Machine (SVM) as a classifier. This research also comparing the commonly used kernels, such as linear kernel, polynomial kernel and RBF kernel. In addition, the number of attributes used on each review or instance is limited to 500, 1000 and 1500 using information gain algorithm. The results of each respective attribute number will also be compared.
There are two types of testing method in this research, which are 10-fold cross validation test and testing using an independent testing dataset. Both of the tests show that there are no much differences in using different kernels and using different amounts of attributes, which are only amounted to 0-2%. Using 1000 attributes resulted in highest accuracy on both tests, which are 89.68% on 10-fold cross validation test and 81.72% on test using test set. Those values are achieved using third degree polynomial kernel on 10-fold cross validation test and using linear kernel on test using test set.",,,33944,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
79377,,SHANDRA DEWI K.,ANALISIS PENGARUH SEO TERHADAP JUMLAH KEDATANGAN WISATAWAN MANCANEGARA DI PROPINSI BALI,,,"Kata kunci : SEO (Search Engine Optimization), SEO Analyzer, wisatawan Bali, analisis SEO, parameter SEO","Azhari SN, Dr",2,3,0,2015,1,"Internet dapat sangat mempengaruhi bagi pemasaran produk komoditas pariwisata terutama bagi wisatawan mancanegara. Oleh karena itu dalam tulisan ini, akan dianalisa pengaruh SEO secara tidak langsung terhadap jumlah kedatangan wisatawan mancanegara di Propinsi Bali. Dalam analisa ini akan disesuaikan dengan kondisi dan keadaan di Propinsi Bali.
Tujuan dilaksanakannya penelitian ini adalah menganalisa faktor-faktor SEO yang dapat mempengaruhi jumlah kedatangan wisatawan mancanegara di Propinsi Bali dengan menggunakan SEO Analyzer. Sehingga dapat disimpulkan seberapa besar peranan SEO bagi pariwisata di Propinsi Bali. Sampel diambil dengan menggunakan pengambilan sampel secara berkelompok dari suatu negara berdasarkan kecocokan data yang diteliti.
Hasil yang dicapai pada penelitian ini adalah adanya pengaruh antara pemberitaan kondisi di Pulau Bali di media internet dengan kedatangan wisatawan mancanegara ke Pulau tersebut. Dari analisis yang dilakukan, maka dibuat sistem untuk menganalisa SEO dengan masukan link yang dibandingkan dan membandingkan dengan link lain.
Penelitian ini berhasil mendapatkan parameter-parameter yang berpengaruh terhadap optimisasi suatu website dalam mesin pencari, parameter tersebut adalah page authority, domain authority, page mozrank, external link, total link, SEO score, Google page rank, alexa rank, Google backlinks, social media backlinks, meta description, meta keywords, dan rating.

      ","Internet marketing can greatly affect the tourism, especially for foreign tourists. Therefore, in this research, will be analyzed SEO indirectly influence on the number of foreign tourist arrivals in Bali. In this analysis will be adapted to the conditions and circumstances in Bali. 
The objective of this study was to analyze the SEO factors that can affect the tourist arrivals in Bali by using SEO Analyzer. Therefore we can conclude how big the role of SEO for tourism in Bali. Samples were taken using sampling in groups of a state based on the fit of the data under study.
The results achieved in this study is the influence of the news on condition from the island of Bali in the internet with the arrival of foreign tourists to the island. From the analysis carried out, then made the system SEO analysis compared with the input link and comparing with other link.
This study managed to get the parameters that influence the optimization of a website in search of engines, the parameter is a page authority, domain authority, page mozrank, external links, total links, SEO score, Google page rank, Alexa rank, Google backlinks, social media backlinks, meta description, meta keywords, and rating.



      
",,,29246,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
86033,,FATINA PUTRI MAULIDA,Sistem Pakar Manajemen Tekanan Psikologi Mahasiswa Berbasis Android,,,"manajemen tekanan psikologi, mahasiswa, sistem pakar, berbasis aturan, forward chaining","Faizah, S.Kom, M.Kom",2,3,0,2015,1,"Tekanan psikologi adalah hal yang dialami oleh individu ketika berhadapan dengan suatu kejadian. Mahasiswa, sebagai tokoh akademik, juga tidak lepas dari tekanan psikologi dalam kehidupan sehari-harinya. Konsultasi kepada pakar merupakan langkah yang baik untuk menangani tekanan dalam diri seseorang. Di klinik kampus juga disediakan ruang konsultasi bagi mahasiswa untuk menangani masalah tersebut. Namun, tidak banyak orang menyadari dan merasa perlu untuk mendapatkan bantuan pakar sehingga masalah yang dihadapinya menjadi semakin berat.
Untuk itu, penelitian ini membangun sebuah sistem pakar berbasis aturan (rule based) dengan menggunakan metode forward chaining untuk mendiagnosa tingkat tekanan pada seorang mahasiswa pada device berbasis Android. Sistem pakar ini diharapkan dapat membantu pakar dala menentukan tingkat tekanan psikologi mahasiswa, yang biasa dilakukan dengan cara manual. Hasil yang diperoleh berupa seberapa besar tingkat tekanan yang dialami oleh mahasiswa dari jenis tekanan psikologi seperti depresi, kegelisahan, dan stres beserta deskripsi hasil dan solusi penanganan bagi mahasiswa tersebut.","Psychological pressure is something that people can't avoid, and it goes same for college students. The best thing that students can do, is to express their problem to an expert through counseling but most people don't realize how important it is.

Psychological Pressure Management Expert System for College Student is a rule based expert system with forward chaining method to mimic the role played by a counselor of psychological pressure such as depression, anxiety, and stress to provide a virtual consultancy. Counselor always uses these systems manually to determine level of pressure, and this system is aimed to change the manual system into mobilized computerized system using Android, by extracting data from an original scale to determine best management for particular individual, in this case, for college students.
\",,,36115,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83225,,KURNIA GALIH PAMBAYUN,ANALISIS STRUKTUR DATABASE PADA PROSES PELAPORAN DALAM BUSINESS INTELLIGENCE TOOLS,,,"business intelligence, database, pentaho, saiku, cde, sql developer, Oracle, reporting, ETL, warehouse.","Mardhani Riasetiawan, M.T.",2,3,0,2015,1,"Perkembangan data dari waktu ke waktu menyebabkan terbentuknya berbagai macam variasi bentuk data dari yang terstruktur hingga dalam bentuk tidak terstruktur. Bentuk data yang tidak terstruktur yang dimaksud disini adalah data yang memiliki relasi yang tidak optimal. Bentuk relasi yang tidak optimal tersebut akan berpengaruh pada pemrosesan untuk pelaporan, sehingga diperlukan solusi untuk dapat menyelesaikan permasalahan tersebut yaitu dengan menerapkan konsep business intelligence. Proses pelaporan yang diproses menggunakan konsep BI inilah yang dapat digunakan untuk menentukan keputusan. Dalam hal ini business intelligence tools dapat digunakan sebagai alat bantu untuk melakukan pengolahan atas data-data yang sebelumnya telah tersimpan di dalam data warehouse untuk dijadikan acuan dalam melihat bagaimana grafik aktivitas yang telah dilakukan selama ini sehingga kemudian keputusan terbaik dapat diambil berdasarkan hasil pengolahan data tersebut.
Penelitian ini akan melakukan analisis terhadap struktur database sample lalu kemudian mengolahnya agar menjadi bentuk database yang siap digunakan sebagai source dalam proses pelaporan dengan konsep BI. Parameter yang digunakan adalah kecepatan pengolahan data dengan BI tools serta pemahaman informasi yang ditampilkan pada report BI menurut hasil survey.
Berdasarkan hasil pengujian, telah didapatkan bentuk struktur database baru yang siap digunakan sebagai source pada BI, kemudian juga dihasilkan pola pemrosesan dari bentuk database asli ke dalam bentuk database baru sehingga data bisa ditampilkan dalam reporting berbasis BI. Selain itu hasil penelitian yang didapatkan adalah pengaruh ukuran dan struktur database terhadap waktu proses pengolahan business intelligence tool serta seberapa besar pemahaman user atas report BI yang berhasil ditampilkan pada dashboard. Analisis penelitian mendapatkan hasil bahwa bentuk data source awal dan bentuk relasinya sangat berpengaruh pada informasi akhir report BI.
","The development of data over time lead to the formation of a wide variety forms of data from structured to unstructured's form. Forms of unstructured data in question here are data that haven't optimal relationship. This type of forms will affect the processing for reporting, so the necessary solutions to solve the problem is to apply the concept of business intelligence. The reporting process is processed using BI concept that can be used to determine the decision. In this case the business intelligence tools can be used as a tool to carry out the processing of the data previously stored in the data warehouse to be used as reference in seeing how the chart's activities that have been done so far so then the best decision can be taken based on the results of the data processing.
This study will carry out an analysis of the structure of the sample database and then process them in order to form a database that is ready to be used as source in the reporting process with the concept of BI. The parameters used are the speed of data processing with BI tools and understanding of the information displayed on BI report according to survey results.
Based on test results, this research has obtained form new database structure that is ready to be used as source in the BI, then also generated pattern database processing from the original form into the new database so that the data can be displayed in a BI-based reporting. In addition the results obtained is the effect of the size and structure of the database for processing time business intelligence tools as well as users' understanding of how big a successful BI report is displayed on the dashboard. Analysis of the research to get the result that the initial source of data forms and shapes their relationships are very influential in the final report of business intelligence information.
",2015-05-06 00:00:00,53,33239,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
79643,,REZA YOGASWARA,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN KONSENTRASI STUDI PADA PROGRAM STUDI TEKNIK ELEKTRO UNIVERSITAS GADJAH MADA,,,"Sistem pendukung keputusan, pemilihan konsentrasi studi, sistem berbasis web, decision support system, concentration course selection, web base system","Faizah, S. Kom. , M. Kom.",2,3,0,2015,1,"Pemilihan konsentrasi studi di program studi Teknik Elektro Universitas Gadjah Mada merupakan salah satu contoh permasalahan yang tergolong dalam kategori permasalahan krusial. Penyebab utama munculnya permasalahan ini salah satunya berawal dari ketidaktahuan mahasiswa mengenai seberapa besar pengaruh pemahaman ilmu umum yang mereka miliki pada masing-masing konsentrasi studi yang ada. Salah satu alternatif solusi yang dapat diterapkan untuk permasalahan ini adalah pembuatan suatu sistem yang dapat membantu mahasiswa dalam memilih konsentrasi studi.

Spektrum (Sistem Pendukung Keputusan Teknik Elektro UGM) merupakan suatu sistem interaktif terkomputerisasi berbasis web yang dibuat untuk berusaha menjawab permasalahan di atas. Spektrum dibuat sebagai sistem terkomputerisasi untuk dapat mengakomodasi jumlah mahasiswa (sebagai pengguna sistem) yang cukup banyak, sementara konsep sistem berbasis web ditujukan untuk memberikan fleksibilitas dalam hal waktu dan tempat penggunaan sistem. Spektrum dibuat di atas sistem operasi Windows XP Professional SP 3 32 - bit dengan web browser Internet Explorer 8, menggunakan database server MySQL 5.0.51a, web server Apache 2.0, PHP 5.2.5, dan HTML 4.01. Dengan menggunakan sistem ini, mahasiswa program studi Teknik Elektro Universitas Gadjah Mada sebagai pengambil keputusan akan mendapatkan bantuan pertimbangan dari sistem yang nilai variabel modelnya telah ditentukan sebelumnya oleh dosen pembimbing akademik ataupun bagian akademik program studi Teknik Elektro Universitas Gadjah Mada.

Uji coba yang dilakukan setelah sistem selesai dibuat menunjukkan bahwa sebagian besar sampel mahasiswa program studi Teknik Elektro Universitas Gadjah Mada yang menguji coba Spektrum merasa bahwa Spektrum dapat membantu mereka dalam memilih konsentrasi studi.","Concetration course selection at Universitas Gadjah Mada Electrical Engineering Program of Study is a case that can be categorized as crucial. One of the main cause of this problem emerge from students unknowingness about how much their understanding of general courses influence in each of concetration courses is. One solution that can be applied to the problem is development of a system that can help students to choose concetration course.

Spektrum (Sistem Pendukung Keputusan Teknik Elektro UGM) is an interactive web based computerized system made to overcome the case. Spektrum was build as a computerized system to accommodate considerable number of students (as system users), meanwhile web based concepts is aimed for flexibility in place and time of access. Spektrum was made on Windows XP Professional SP 3 32 - bit operation system with Internet Explorer 8 web browser, MySQL 5.0.51a database server, Apache 2.0 web server, PHP 5.2.5, and HTML 4.01. With this system, students at Universitas Gadjah Mada Electrical Engineering Program of Study as the decision maker will get consideration assistance from the system that model variable values has been defined by academic supervisor or academic departments at Universitas Gadjah Mada Electrical Engineering Program of Study.

Test run that was conducted after the system is finished show that most of student samples at Universitas Gadjah Mada Electrical Engineering Program of Study that get involved in the test feel that Spektrum can help them to choose concetration course.",,,29615,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81947,,YUSVIAR LUKY TANZIRA,Analisis Normalized Compression Distance (NCD) Pada Content-Based Image Retrieval (CBIR),,,"NCD, Normalized Compression Distance,  CBIR, Image Retrieval","Lukman Heryawan, S.T., M.T ",2,3,0,2015,1,"Content-Based Image Retrieval (CBIR) pada umumnya dilakukan dengan melakukan ekstraksi fitur terhadap data citra yang dimasukan. Namun ekstraksi fitur akan suatu bentuk citra dengan bentuk citra yang lain akan berbeda sehingga campur tangan manusia masih dibutuhkan dalam menganalisis model ekstraksi yang dibutuhkan. Dengan pesatnya perkembangan teknologi saat ini, dibutuhkan suatu sistem otomatis yang bersifat universal sehingga sistem dapat bekerja secara independen terhadap semua model bentuk citra yang ada saat ini.
Pada penelitian ini, akan diterapkan CBIR menggunakan pengukuran disimilaritas citra berdasarkan teori Normalized Compression Distance (NCD). Keuntungan utama metode ini adalah tidak terdapat ekstraksi fitur dalam CBIR sehingga dapat diterapkan pada semua jenis citra. Metode NCD yang digunakan pada penelitian ini adalah NCD Sederhana dan NCD Interleaving dengan alat kompresi menggunakan teknik BZIP2. Data yang digunakan pada penelitian kali ini adalah dataset SIVAL dan dataset Corel. Pengujian akan dilakukan terhadap Image segmentation akan diterapkan sebagai data preparation dengan tujuan meningkatkan kemampuan NCD. Sebagai perbandingan akan diuji pula dataset tersebut dengan tool CBIR Octagon sehingga dapat diketahui kemampuan NCD dibandingkan tool CBIR lainya.
Hasil penelitian ini menunjukan bahwa metode NCD bekerja lebih baik pada dataset SIVAL dimana objek berada pada lokasi spasial yang berbeda. Penggunaan Image Segmentation mampu meningkatkan kemampuan hampir 50% kemampuan NCD pada dataset struktur SIVAL. Namun pada percobaan menggunakan dataset gabungan antara SIVAL dengan Corel (struktur campuran), NCD memiliki precision lebih kecil dari yang dihasilkan oleh tool CBIR Octagon. Namun Image Segmentation tidak memberikan peningkatan pada performa precision NCD saat dikenakan pada dataset struktur campuran.
","Generally, Content-Based Image Retrieval (CBIR) is done by performing feature extraction toward data images input. However, feature extraction of an image is different from one to another, thus to analyze the extraction model still needs a help from human hands. Along with rapid growth of technology, we need a universal automatic system that works independently on any variant of current image model.
This research will apply the CBIR technique using images dissimilarity measurements based on the theory of Normalized Compression Distance (NCD). The main advantage of this method is, the absence of feature extraction within CBIR can be applied in any kind of images. The NCD method used is the Simple NCD and NCD Interleaving with compression tools by applying BZIP2 technique. The data being used are SIVAL data set and Corel data set. Trial on Image Segmentation will be applied as data preparation in order to upgrade the NCD competence. As a comparison, the data set will be also tested using CBIR Octagon, so, the NCD competence can be compared with another CBIR tools. 
The results show that NCD method worked better on SIVAL Data set, where the objects are located on different spatial. The utilizing of Image Segmentation increase the NCD competence for almost 50% on SIVAL Data set. However, the trials on mixture data set showed that NCD method had smaller precision rather than CBIR Octagon, but the Image Segmentation also did not show escalation.
",,,31909,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
88353,,ADRIANUS DIYAN PANDUR,SISTEM INFORMASI WISATA TERINTEGRASI DENGAN LAYANAN BERBASIS LOKASI PADA PLATFORM ANDROID,,,"Sistem Informasi Wisata Terintegrasi, Layanan Berbasis Lokasi","Y. Suyanto, Dr.",2,3,0,2015,1,"Rekreasi merupakan hal yang menyenangkan untuk dilakukan, baik seorang diri, dengan teman-teman, maupun bersama keluarga. Permasalahan yang sering dihadapi dalam melakukan rekreasi yaitu kurangnya ketersediaan informasi wisata yang terintegrasi, dengan adanya informasi harga tiket, jam buka, dan deskripsi tempat serta lokasi tempat yang seringkali berada pada beberapa media yang berbeda.
Dengan permasalahan di atas, maka dapat dibuat sebuah sistem informasi wisata terintegrasi dengan layanan berbasis lokasi. Sistem tersebut perlu untuk dibuat pada platform Android agar informasi lokasi tempat wisata dapat dipergunakan secara lebih maksimal.
Berdasarkan hasil pengujian, sistem ini telah mampu menampilkan informasi wisata yang terintegrasi dengan layanan berbasis lokasi yang telah berjalan dengan baik. Fungsi-fungsi utama di dalam sistem, seperti fungsi penampilan peta dan informasi wisata, fungsi pencarian, dan fungsi pengaturan juga telah berjalan sesuai dengan yang diinginkan.
","Recreation is a fun thing to do, either alone, with friends, or with family. Problems that are often encountered in recreation is a lack of availability of integrated travel information, with ticket information, opening hours, and a description and location of the tourism place often in several different media.
With the above problems, it can be made an integrated travel information system with location-based services. The system needs to be made on the Android platform so that location information can be used optimally.
Based on test results, this system has been able to showintegrated tourism information with location-based services that have been running well. Main functions within the system, such as the mapand tourism informationappearancefunction, search functions, andsettings functions have also been running as hoped.
",2015-09-07 00:00:00,53,38481,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81954,,ARAWINDA LAKSMI  PUTRI,PENGEMBANGAN FITUR STEGANOGRAFI DALAM APLIKASI MOBILE PESAN INSTAN PADA SISTEM OPERASI ANDROID,,,"Steganografi, LSB, pesan instan, Android","Lukman Heryawan, S.T., M.T.",2,3,0,2015,1,"Steganografi merupakan ilmu untuk menyembunyikan informasi tanpa diketahui oleh pihak ketiga. Sebagai sarana komunikasi, pesan instan merupakan media yang paling sering digunakan untuk bertukar informasi. Namun, pada pesan instan, pengguna tidak dapat melakukan pertukaran informasi rahasia. Sebagai solusinya, penulis akan mengembangkan sebuah aplikasi pesan instan bernama KiiChat yang menggunakan steganografi dengan tujuan untuk menyembunyikan pesan rahasia dalam group chat pada aplikasi pesan instan berbasis Android.
Penelitian ini menerapkan algoritma LSB (Least Significant Bit), dimana algoritma ini merupakan algoritma yang paling sederhana dalam menempelkan informasi pada gambar. Algoritma ini menggunakan bit yang paling signifikan dari beberapa byte dalam sebuah gambar untuk diubah menjadi pesan rahasia. Penelitian ini juga akan menguji keselamatan pesan pada gambar stego berdasarkan implementasi dari algoritma LSB.
Hasil pengujian yang dilakukan dalam penelitian ini menunjukkan bahwa menunjukkan bahwa algoritma LSB mampu menyembunyikan pesan pada gambar secara visual dan menunjukkan perbedaan pada histogram gambar.","Steganography is a method of hiding information without being noticed by a third party. As a communication media, instant messaging is the most commonly used medium for information exchange. However, user can not perform confidential information exchange in the instant messaging. As a solution, the author will develop a chat application that uses steganography, named KiiChat, in order to hide secret messages in the group chat on Android-based chat application.
This research applies LSB (Least Significant Bit) algorithm, which is the simplest algorithm to embed an information at a picture. This algorithm uses the most significant bits of a few bytes at a picture to be transformed into a secret message. This research will also test the secret message&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s safety on stego image based on LSB algorithm implementation. 
The results show that LSB algorithm is able to hide messages at a picture visually and it showed changes in the histogram of stego image.",2015-04-15 00:00:00,53,31962,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
89123,,NANDA KUMALA,DESAIN DAN IMPLEMENTASI SEBUAH FRONT-END UNTUK ATTACK TREE,,,"keamanan, attack tree, front-end, back-end, dokumen XML","M. Reza Pulungan, S.Si.,M.Sc.,Ph.D.",2,3,0,2015,1,"Salah satu bahasan yang menarik dalam membangun sebuah sistem adalah membahas masalah keamanan dari sistem tersebut. Proses merancang keamanan terkadang membutuhkan usaha yang ekstra agar dapat menutup semua celah yang dapat digunakan pihak luar untuk menghasilkan gangguan pada sistem. Metode formal attack tree merupakan salah satu cara untuk merancang keamanan sistem dengan menggambarkannya ke dalam bentuk tree. Metode penggambaran attack tree merupakan metode yang mudah digunakan dan mudah digunakan untuk memahami suatu masalah serangan. Penelitian ini bertujuan untuk membangun sebuah aplikasi front-end untuk membangun model attack tree. Aplikasi ini dibangun untuk mendukung penelitian yang dilakukan oleh Arnold et. al. (2014). Aplikasi ini bertujuan untuk membantu mempermudah dalam pembuatan model dan menghasilkan ekspresi model sesuai dengan format yang akan digunakan untuk melakukan analisis pada back-end. Dari hasil pengujian, aplikasi ini mampu membangun model secara grafis dan menyimpan model dalam bentuk dokumen XML. Aplikasi ini mampu memudahkan dalam membangun model dan menghasilkan ekspresi dengan cepat.","One of the topics of interest in building a system is discussing the security issues of the system. The process of designing security sometimes requires extra effort in order to close all the loopholes that can be used by outsiders to generate interference in the system. Formal methods of attack tree is one way to design a security system by describing it in a tree form. Attack tree is a method that is easy to use and easy to use to understand a problem attack. This study aims to build a front-end application to build a model of attack tree. This application is built to support research conducted by Arnold et. al. (2014). This application aims to help facilitate in making the model and produce a model of expression in accordance with the format that will be used to perform analysis on the back-end application. From the test results, the application is able to build graphical models and save the model in the form of an XML document. This application is able to facilitate the expression construct models and produce quickly.",,,39251,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
80422,,RAHMAT AJI WIJAYANTO,APLIKASI PENGOLAHAN CITRA UNTUK PENINGKATAN MUTU PADA CITRA RADIOLOGI MENGGUNAKAN METODE PERATAAN HISTOGRAM,,,"Perataan Histogram, Morphologi, Citra Rontgen.","Drs. Medi, M.Kom",2,3,0,2015,1,"Perataan Histogram (Histogram Equaliation) merupakan suatu metode untuk
memperbaiki frekuensi perataan/penyebaran pada setiap nilai piksel citra digital. Dalam
penelitian, metode ini digunakan untuk memperbaiki kualitas citra pada citra rontgen.
Perbaikan citra dilakukan dengan memfilter citra asli yang kurang jelas, di mana terjadi proses
perataan/penyebaran intensitas piksel pada citra yang bersangkutan, sehingga titik tinggi
rendah pada histogram dapat merata dan tersebar.
Pengujian dilakukan dengan 10 sampel citra rontgen dada. Hasil pengujian
menunjukkan latar belakang yang lebih gelap dengan citra rontgen dada yang lebih jelas, juga
memudarnya derau (noise) pada citra. Hal ini dipengaruhi oleh intensitas piksel citra yang telah
diratakan dan disebarkan melalui proses perataan histogram.","Histogram Equalization is a method for improving frequency smoothing/spread on each
digital image pixel values. In the study, this method is used to improve image quality in X-ray
image. Repairs carried out by filtering the image of the original image that is less clear, where
there is a process of smoothing/spread of pixel intensities in the image in question, so that the
high point is low on the histogram can be uneven and scattered.
Tests conducted with samples of 10 X-ray image of a chest. The test results showed a
darker background with a chest X-ray image clearer, also waning noise in the image. This is
influenced by the intensity of image pixels that have been leveled and propagated through the
process of smoothing the histogram.",,,30376,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
85802,,GITA LISTYA ANGGRAINI,Analisis User Experience dan User Interface pada Website Job Portal dengan Pendekatan User-Centered Design dan GOMS Analysis,,,"UX, UI, User-Centered Design, SUS, GOMS, Job Portal","Faizah, S.Kom., M.Kom.",2,3,0,2015,1,"User Experience merupakan pengalaman yang melibatkan persepsi pengguna berkaitan dengan manfaat yang dirasa dan kemudahan yang didapat ketika menggunakan suatu sistem. Dalam pengembangannya, User Interface pada sistem dirancang dengan User Experience Research untuk menciptakan kenyamanan dan kepercayaan pengguna. Sementara itu, seiring meningkatnya jumlah pencari kerja melalui internet membuat beberapa perusahaan swasta menjalin kerja sama dengan job portal. Job portal yang ada bervariasi dengan berbagai User Experience. Sesuai dengan tagline-nya, Temukan karir impianmu di sini, Workaria merupakan sebuah protoype website penyedia lowongan kerja online yang khusus dibangun dalam penelitian ini. Workaria menyediakan media bagi pengguna untuk mencari atau mempublikasikan lowongan pekerjaan dan diharapkan dapat hadir sebagai sebuah solusi.
Pendekatan User-Centered Design digunakan dengan menggabungkan metode kualitatif dan kuantitatif untuk membuat persona atau user model. Persona tersebut digunakan untuk melakukan pengembangan website yang sesuai dengan penggunanya. Pada bagian akhir, peneliti juga melakukan pengukuran untuk mendapatkan nilai kebergunaannya menggunakan System Usability Scale dan GOMS Analysis.
Hasil dari penelitian ini yaitu tampilan antarmuka website Workaria yang memiliki kesesuaian dengan pengguna dan metodologi yang telah dirumuskan. Untuk pengujian, UI pada prototype memiliki kebergunaan dengan nilai SUS yang didapatkan melebihi rata-rata nilai SUS berdasarkan riset Jeff Sauro. Sementara itu, nilai GOMS Workaria menggunakan perhitungan KLM setelah dibuat perbandingan dengan tiga job portal lainnya menghasilkan prediksi waktu eksekusi melebihi nilai rata-rata. Prototype Workaria juga sudah meng-cover saran dan bagian paling menyulitkan pada job portal menurut hasil survei awal oleh responden, baik dari segi UI, fitur, maupun fungsional.","User Experience is an experience that involves user perception refers to perceived benefit and convenience gained while using a system. In its development, User Interface on the system is designed  User Experience Research to create the comfort and confidence of user. Meanwhile, the increasing number of online job seekers makes some private companies cooperate with the job portals. Job portals who exist is varied with different User Experiences. In keeping with its tagline, Find your dream job here, Workaria is a protoype of online job portal website specially built in this research. Workaria provides a media for user to search for or publish job vacancies and expected to be present as a solution.
User-Centered Design is used by combining qualitative and quantitative methods to create a persona or a user model. Persona is used to develop websites according to its users. At the end, the researcher also perform measurements to get the usefulness value using System Usability Scale and GOMS Analysis.
The result of this research is a Workaria's website interface, which has compatibility with users and methodologies which have been formulated. For its testing, UI of the prototype has usability of SUS value which obtained more than its averages based on Jeff Sauro's research. Meanwhile, the GOMS value of Workaria using KLM calculation after a comparison is made between three other job portals and it generates excecution time prediction exceeds the average value. The prototype of Workaria also covering advice and the most difficult part of job portals according to the results of a preliminary survey by the respondents, both in terms of UI, features, and functional.",,,36009,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
86827,,RACHMASARI CAHYANINGDYAH,IMPLEMENTASI SISTEM QUESTION ANSWERING  PADA TWITTER UNTUK MENJAWAB PERTANYAAN SEPUTAR WISATA DI DAERAH ISTIMEWA YOGYAKARTA,,,"question answering, auto reply, Twitter, Twitter API","Drs. Janoe Hendarto, M.Kom",2,3,0,2015,1,"Twitter merupakan salah satu situs microblog yang memberikan fasilitas bagi penggunanya untuk mengirimkan teks pembaruan dengan panjang maksimum 140 karakter. Pemanfaatan Twitter dapat digunakan sebagai media promosi produk atau jasa, salah satunya untuk melakukan promosi wisata. Yogyakarta merupakan salah satu daerah wisata yang memiliki beberapa akun Twitter untuk mempromosikan wisatanya. Namun, jika pengguna Twitter lain menanyakan informasi melalui tweet mention, pihak pengelola Twitter tidak membalas satu persatu mention yang diterima. 
Pertanyaan yang diajukan kemungkinkan memiliki kesamaan antara satu pertanyaan dengan pertanyaan lainnya. Tweet balasan dari setiap pertanyaan merupakan salah satu bentuk pelayanan pengelola akun terhadap setiap orang yang tertarik dengan apa yang dipromosikan. Oleh karena itu, maka dibangun sistem question answering (QA) menggunakan auto reply pada Twitter untuk menjawab pertanyaan seputar informasi wisata Yogyakarta menggunkan web service. 
Sistem QA akan merespon tweet dengan hashtag #askwisata yang mengandung kata tanya apa, dimana, berapa, kapan, bagaimana dan mengapa disertai kata kunci sesuai dengan rancangan pola EAT (Expected Answer Type). Tahapan question answering meliputi proses question analyzer dan answer finder. Semua tahapan sistem dibangun dengan menggunakan framework CodeIgniter dan Twitter API.","Twitter is a microblog site that provides service for its user to send text updates with a maximum length of 140 character. Twitter can be used as media promotion of product or service, for instance tourism promotion. Yogyakarta is one of tourism city destination which have many of Twitter account to share its tourism place. However, if there's Twitter user ask the information using tweet mention, the Twitter admin didn't respond one by one tweet mention.
Sometimes, the question has similarities between one question and another question. Tweet replies of each question is one form of service to everyone who is interested in what is promoted. Based on that problem, question answering is built using auto reply Twitter to answer Yogyakarta tourism question using web service.
This system is able to respond tweet automatically that uses hashtag #askwisata and contain question word like &quot;apa&quot;,&quot;dimana&quot;,&quot;berapa&quot;,&quot;kapan&quot;, &quot;bagaimana&quot; and &quot;mengapa&quot; and contain keyword based on EAT (Expected Answer Type) design. The question answering step includes process question analyzer and answer finder. The system is built using CodeIgniter Framework and Twitter API. ",,,36896,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92975,,LUTFI RESTU S,PENGEMBANGAN VISUALISASI DATA PADA RUMAH SAKIT UMUM DR SOEKARDJO TASIKMALAYA DALAM E-DASHBOARD,,,"Big Data, Visualisasi Data, Rumah Sakit","I Gede Mujiyatna, S.Si., M.Kom.",2,3,0,2015,1,"Big Data didefinisikan sebagai sebuah masalah domain dimana teknologi tradisional seperti relasional database tidak mampu lagi untuk melayani. Permasalahan data, juga terjadi di bidang kesehatan, seperti yang terjadi pada Rumah sakit di Houston, karena banyaknya data yang terdapat di rumah sakit tersebut, sehinnga 2.200 data informasi pasien hilang. Informasi merupakan suatu hasil dari pemrosesan data menjadi sesuatu yang bermakna bagi yang menerimanya.

Rumah Sakit di Indonesia pun semakin lama semakin berkembang. Perkembangan Rumah Sakit tersebut bukan hanya penambahan jumlah staff karyawan dan jumlah Rumah Sakit lainnya tetapi juga peningkatan pelayanan kesehatan yang menjadi lebih lengkap dan memuaskan baik dari segi pelayanan maupun dari segi peralatan rumah sakit. Untuk menjalankan kegiatannya Rumah Sakit memerlukan suatu sistem pengolahan data informasi yang mendukungnya. Kesalahan yang sering terjadi adalah dalam pencatat dan pengolahan data pasien, karena banyaknya data. Informasi yang ada, baiknya divisualisasikan dengan baik. Seperti dibuatkannya grafik, atau tabel dari informasi tersebut. Pada penelitian ini dibuat sebuah program visualisasi data, dari berbagai informasi yang ada di RSUD Dr. Soekardjo Tasikmalaya, dalam bentuk web e-dashboard.
","Big Data defined as a domain problem where traditional technology such  as relational database could no longer serve effectively. Data case also happens in health area of work, for example the case of a hospital in Houston. Due to the high amount of data in said hospital, 2200 datas from the patients were vanished. Information is a result from data processing which the output is something that is useful for the receiver.

Indonesian hospitals are developing day by day. The improvement is not only about the rise amount of staff and quantity of the hospital itself, but also in the increasing health service which become better and satisfying from the perspective of service and hospital tools and equipments. In order to run itself, hospital needs the data information analytic system as a supporting equipment. Frequent mistakes are done in recording and analizing patient's data due to the high amount of it. The collected information should be well visualized. For example,  data is visualized as graphs or data table from the said information. In this research, data visualization program has made from all the information taken in RSUD Dr. Soekardjo Tasikmalaya, in the form of e-dashboard website.
",,,43133,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
80433,,PUJI KUSUMA WARDANI,IMPLEMENTASI ANALYTICAL HIERARCHY PROCESS PADA PENGEMBANGAN SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN ATLET KARATE KOTA YOGYAKARTA,,,"Sistem pendukung keputusan, AHP, Pemilihan atlet karate kota Yogyakarta","MEDI, Drs., M.Kom.",2,3,0,2015,1,"Pada cabang olahraga beladiri karate, para pelatih memiliki kemampuan yang berbeda dalam mengimplementasikan ilmu yang mereka miliki. Kemampuan yang dimiliki oleh pelatih sangat berpengaruh terhadap peningkatan prestasi. Kemampuan tersebut meliputi penguasaan teknik karate, latar belakang pendidikan, sertifikasi kepelatihan, penerapan Ilmu Pengetahuan dan Teknologi (IPTEK) dan penguasaan peraturan penilaian pertandingan. Dalam penerapan IPTEK ini masih banyak kendala yang ditemui, salah satunya pada penyeleksian atlet-atlet yang akan dipertandingkan.
	Belum diterapkannya suatu sistem yang terkomputerisasi yang dapat menyajikan informasi serta menyediakan pilihan bagi para pelatih sebagai sarana pendukung dalam pengambilan suatu keputusan, menjadi salah satu penyebab kurang optimalnya proses penyeleksian atlet yang ideal agar sesuai dengan kriteria yang diharapkan.
Aplikasi Seleksi Atlet Karate ini dikembangkan dengan menggunakan Sublime Text sebagai editor kode, lalu digunakan server lokal yaitu XAMPP Control Panel Version 2.5 sebagai aplikasi untuk menjalankan web. Dengan bantuan Sistem Pendukung Keputusan (SPK) proses penyeleksian atlet dapat ditentukan dengan cepat dan tepat sesuai dengan kriteria yang telah ditentukan. Algoritma SPK yang digunakan adalah Analytical Hierarchy Process (AHP) dengan kriteria Fisik, Teknik, Psikology, Taktik dan Prestasi.
Hasil yang diperoleh dari penelitian sistem pendukung keputusan pemilihan atlet karate kota Yogyakarta berbasis web ini adalah perangkingan dan memberikan informasi berupa rekomendasi atlet-atlet yang telah diseleksi dengan berdasarkan kriteria yang telah ditentukan. Alternatif dengan nilai tertinggi akan lebih dipilih sebagai alternatif pilihan.

","The Karate trainers have different capabilites to implement their knowledge. Those capabilites have a big influence on the performance and the achievement of their athletes. The capabilities include the mastery of karate techniques, educational background, coaching certification, application of science and technology (science and technology) and the ability to control rules valuation of the match. In the application of science and technology, there are still many obstacles encountered, such as on the selection of athletes who will be ready to be contested. 
The  absence of the computerized system that presents informations and provides the choices for the trainers as support for making the decision becomes one of the obstacles that cause the selection process for the ideal athlete based on the expected criterias is not optimal. 
The Karate athlete selection application is developed using Sublime Text as a code editor and the local server XAMPP Control Panel Version 2.5 for running the web application. With the help of Decision Support System (DSS), athlete selection process can be determined quickly and accurately in accordance to the established criteria. DSS Algorithm used is Analyitical Hierarchy Process (AHP) with the criteria of physics, techniques, achievements, tactics, and psychology.
The result of this study is to provide the scoring and the information about the recommended athletes that are being selected based on the predetermined criteria. The alternative with the highest value is preferred to be choosen as the best alternative

",,,30381,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
88626,,ASTIKA PUTRI R,PREDIKSI PENGEMBALIAN ATAU RETUR DALAM BELANJA ONLINE MENGGUNAKAN METODE KLASIFIKASI K-NN (K-NEAREST NEIGHBOR),,,"prediksi, retur, belanja online, algoritma k-Nearest Neighbor, k-NN","Aina Musdholifah, S.Kom., M.Kom., Ph.D",2,3,0,2015,1,"Berbelanja di internet kini menjadi alternatif bagi konsumen, tetapi timbul kendala mengenai produk yang tidak sesuai dengan ekspektasi pelanggan. Untuk mengatasi hal itu banyak toko online yang menyediakan kebijakan retur secara gratis. Oleh karena biaya retur ditanggung oleh pihak toko online maka toko harus memprediksi retur dari pelanggan yang telah melakukan transaksi pembelian. Dengan prediksi ini diharapkan toko online dapat menyiapkan solusi untuk mengurangi retur atau agar nantinya transaksi serupa tidak terjadi retur lagi.
Algoritma k-Nearest Neighbor atau k-NN adalah salah satu algoritma klasifikasi yang menghasilkan data yang kuat dan efektif jika digunakan pada data yang besar. Dengan menggunakan algoritma k-NN yang menerapkan metode klasifikasi terhadap obyek berdasarkan data pembelajaran, prediksi dapat dicari dari similaritas sesuai jumlah tetangga terdekat atau nilai k. Proses algoritma k-NN pada penelitian ini dimulai dengan optimasi parameter untuk memilih nilai k yang optimal, fase training, lalu fase klasifikasi, dan sejumlah k data terdekat diambil untuk dicari label mayoritasnya.
Pengujian sistem pada penelitian ini dapat dilakukan untuk mendapatkan prediksi retur untuk masa lampau dan masa mendatang. Pada pengujian sistem prediksi retur ini menghasilkan nilai k paling optimal yaitu k = 21 dengan nilai F-measure sebesar 0.6427. Akurasi pengujian yang didapatkan untuk data transaksi pembelian sebuah toko online pada tahun 2013 adalah sebesar 58.94%.","Shopping on the internet has become an alternative for customers, but it presents an obstacle with the product which are not in accordance with customer expectations. To overcome this, many online shops provide free return policy. Because the cost of return shipment are borne by the online shop, therefore shop have to predict returns of customers who have made purchases. With this prediction, the online shop is expected to prepare a solution to prevent returns on similiar transaction will not happen again.
K-Nearest Neighbor algortihm or k-NN is a classification algorithm that generates strong data and effective when used on large data. By using k-NN algorithm that applies to the object classification method based on learning data, predictions can be sought from the similarity according to the number of nearest neighbors or the value of k. The process of k-NN algorithm in this study begins with a parameter optimization to select the optimal values of k, the training phase, the phase of classification, and take as many as k data closest to seek a majority label.
Testing the system in this study can be predict the return shipment for the past and the future. In testing the prediction of return shipment system generate the most optimal value of k is k = 21 with F-measure value 0.6427. Accuracy testing of the historical purchase data of online shop in 2013 amounted to 58.94%.",2015-09-15 00:00:00,53,38753,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81978,,NOVIA ARUM SARI,PREDIKSI TINGKAT INFLASI BERDASARKAN PERUBAHAN HARGA BAHAN BAKAR MINYAK (BBM) DENGAN MENGGUNAKAN ALGORITMA GENETIKA,,,"prediksi, inflasi, harga BBM, algoritma genetika","Faizah, S.Kom., M.Kom.",2,3,0,2015,1,"Kebijakan pemerintahan untuk menaikkan harga Bahan Bakar Minyak (BBM) khususnya premium pada beberapa bulan terakhir menyebabkan berbagai dampak. Dampak yang sangat berpengaruh salah satunya adalah tingkat inflasi. Jika harga BBM mengalami kenaikan, tingkat inflasi juga akan mengalami kenaikan. Oleh karena itu, pemerintah harus dapat memprediksi tingkat inflasi berdasarkan kenaikan harga BBM. Dari hasil prediksi yang diperoleh, pemerintah akan dapat mempersiapkan kebijakan-kebijakan fiskal maupun non-fiskal untuk menghadapi segala resiko yang akan terjadi dan menjaga kestabilan perekonomian di Indonesia. 
Algoritma genetika adalah salah satu metode komputasional yang bagus untuk melakukan prediksi. Dengan menggunakan algoritma genetika yang menerapkan teknik generasi alam yaitu evolusi, solusi dapat dicari dengan model komputasi yang menjanjikan. Proses algoritma genetika pada penelitian ini dimulai dari pembangkitan populasi awal, seleksi parent dengan seleksi roda roulette, proses crossover dengan whole arithmetic crossover, proses mutasi acak sampai dengan seleksi survivor berupa steady state update dilakukan dengan baik untuk mendapatkan solusi prediksi.
Pengujian sistem pada penelitian ini dapat dilakukan prediksi tingkat inflasi untuk masa lampau dan masa mendatang. Pada pengujian sistem prediksi tingkat inflasi ditemukan faktor lain selain harga premium. Faktor lain tersebut bernilai sebesar 3,83. Dengan pengurangan faktor lain tersebut, pengujian hasil prediksi pada masa pemerintahan Susilo Bambang Yudhoyono selama 10 tahun diperoleh peningkatan akurasi dari 42,98% menjadi 62,16%. Akurasi pengujian untuk prediksi bulan Januari 2015 sebesar 87%. Sedangkan akurasi pengujian untuk prediksi bulan Februari 2015 sebesar 97 %.","Government policy to raise the price of fuel oil (BBM), especially premium in recent months led to a variety of effects. Impact of a very influential one of which is the rate of inflation. If the fuel price increases, the inflation rate will also increase. Therefore, the government should be able to predict the rate of inflation based on the increase in fuel prices. Of the prediction results obtained, the government will be able to prepare fiscal policies and non-fiscal to face all the risks that will happen and maintain economic stability in Indonesia.
Genetic algorithm is one of the great computational methods to predict. By using a genetic algorithm that applies natural generation technique that is evolution, a solution can be sought by promising computational model. The process of genetic algorithms in this study starts from the generation of the initial population, the selection of the parent with the roulette wheel selection, crossover process with whole arithmetic crossover, random mutation process up to the selection of survivors in the form of steady-state updates are done well to get a prediction solution.
Testing the system in this study can be predicted inflation rate for the past and the future. In testing the predictions of inflation system found other factors besides the price premium. Another factor is worth 3.83. With the reduction of other factors, the test results predicted in the reign of Susilo Bambang Yudhoyono during a 10-year gained increased accuracy from 42.98% to 62.16%. Accuracy testing for prediction of the month January 2015 by 87%. While testing accuracy for the prediction of the month February 2015 amounted to 97%.",,,31941,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
82239,,ADE ARMYATNA Y.,Wireless Positioning System Menggunakan Metode Fingerprint dan Algoritma LVQ,,,"wireless positioning system, reference point, fingerprint, learning vector quantization","Bambang Nurcahyo Prastowo DRS., M.SC",2,3,0,2015,1,"Perkembangan teknologi saat ini sudah sangat pesat, salah satunya adalah GPS yang dapat menentukan posisi penggunanya . Akan tetapi GPS susah bekerja jika dalam ruangan tertutup dikarenakan sinyal satelit tidak langsung diterima oleh penangkap sinyal karena terhalang oleh obyek tertentu. Alternatif penentuan lokasi dalam ruangan sudah ada, yaitu dengan memanfaatkan wifi dan disebut dengan wireless positioning system. Penggunaan wifi sebagai penentu lokasi sudah banyak diteliti, akan tetapi karena faktor-faktor tertentu akurasinya kurang akurat.
	Pada penelitian ini akan digunakan algrotima LVQ(Learning Vector Quantization) untuk penentuan posisi. Algoritma LVQ memerlukan data training dan data testing. Dilakukan survey untuk mendapatkan data training dan data testing, tempat-tempat pengambilan data pada suatu tempat disebut Reference Point (RP). Pada proses pengambilan data, kekuatan sinyal yang didapat belumlah tentu sama walaupun berada pada posisi yang sama. Penelitian ini dicari bagaimana mendapatkan reference point yang memberikan hasil baik untuk menentukan posisi dengan menggunakan sinyal wifi. Data-data yang terkumpul akan diolah menjadi koordinat posisi dengan menggunakan metode fingerprint. Diujikan pada pengambilan data dari 1 RP yang memiliki sedikit pola variasi kekuatan sinyal dengan 5 RP yang memiliki lebih banyak pola variasi kekuatan sinyal.
	Hasil yang diperoleh dari penelitian ini adalah data dari 5 RP hasilnya tidak sebaik data dari 1 RP, karena terlalu banyak variasi kekuatan sinyal yang menghasilkan bobot referensi kurang baik. Pada akhirnya saat dilakukan testing jumlah hasil deteksi tempat yang tepat sasaran pada 5 RP tidak sebanyak 1 RP.
","Now technology development is really fast. GPS is one of them that can determine user position. However GPS difficult to work in indoor location because sattelite signal not directly receive by its user. There is alternative way to determine location in indoor, which way is using wifi and its name is wireless positioning system (WPS). Wireless positioning system's research have been widely studied, but because certain factor it's has less accuration.
In this research using Learning Vector Quantization (LVQ) algoritm for positioning. LVQ need data testing and data training. To get data training and testing survey is needed, place for collecting data called Reference Point (RP). In process of collecting data, signal strength not always same although in same position. In this research will look for how to get reference point that give a good result in wireless positioning system. Collected data  will be process into position coordinat using fingerprint. Tested in data from 1 RP that have less signal strength variation pattern and data from 5 RP which have more signal strenght variation pattern.
	The result obtained from this research is data from 5 RP are not as good as data from 1 RP because it is too much variation in signal strength that resulting not good reference weight. At last in testing appropriate detection with 5 RP not as much as 1 RP.
",,,32171,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
87617,,ARIF AKBAR AMINULLAH,Rancang Bangun Virtual Data Center dengan Menggunakan Mesin Virtual dari Proxmox,,,"Proxmox VE, DRBD, cluster, shared storage","Mardhani Riasetiawan, M.T",2,3,0,2015,1,"Sistem Data Center dituntut untuk memiliki ketersediaan yang tinggi agar dapat memenuhi kebutuhan sistem secara berlanjut. Apabila server atau aplikasi di dalamnya mengalami kegagalan atau memerlukan maintenance, mesin virtual akan dimigrasi kepada server node lain di dalam cluster yang masih tersedia. Mesin virtual hanya bisa melakukan migrasi secara offline, sehingga mesin virtual harus dimatikan terlebih dahulu dan terdapat kemungkinan kehilangan data. Peran shared storage sangat penting disini untuk menjaga ketersediaan mesin virtual. Shared storage bekerja dengan melakukan replikasi dan distribusi data VM pada setiap node agar kemudian bisa melakukan migrasi secara online. Mesin virtual selanjutnya dapat langsung melanjutkan pekerjaan dan tidak kehilangan data di server tujuan.
Penelitian ini berusaha mengimplementasi virtualisasi pada sistem  Data Center menggunakan mesin virtual dari Proxmox VE dan Distributed Replicated Block Device (DRBD) sebagai shared storage. Implementasi dilakukan dengan menggunakan cluster dengan dua buah server node dalam lingkungan virtualisasi nested. Terdapat mesin virtual OpenVZ dan KVM di dalam node. Penggunaan shared storage berpengaruh kepada ketersediaan layanan serta performa jaringan dan disk sistem. Pengujian dilakukan untuk menguji ketersediaan dan kondisi VM pada saat dilakukan migrasi. Pengujian juga dilakukan untuk menguji performa jaringan dan disk pada server.
Hasil pengujian menunjukkan sistem dengan mesin virtual KVM memiliki performa migrasi 75,76% lebih baik dari OpenVZ. Walaupun begitu baik KVM maupun OpenVZ memiliki ketersediaan yang sebanding dan integritas data dapat terjaga. Didapatkan performa disk yang lebih kecil dari performa jaringan sehingga kecepatan replikasi dan distribusi shared storage terbatas pada disk. Performa disk dengan shared storage didapatkan 38,64% hingga 42,39% dari performa disk maksimal.","Data Center systems are required to have high availability in order to meet the continuing needs of the system. If server or application in which failure or require maintenance, virtual machine will be migrated to another node in cluster that are still available. Virtual machine can only migrate offline, so virtual machine must be turned off and there is possibility if data loss. The role of shared storage is very important on this point to maintain the availability of virtual machine. Shared storage works by performing replication and distribution of VM data on each node in order to then be able to migrate online. Virtual machine can then be immediately resume work and not lose data on the destination server. 
This research seeks to implement virtualization in Data Center system using virtual machines from Proxmox VE and Distributed Replicated Block Device (DRBD) as shared storage. Implementation is done by using a server cluster with two nodes in the nested virtualization environment. There are OpenVZ and KVM virtual machines within the node. Shared storage works by performing VM replication and distribution of data on each node in order to then be able to do online migration. The use of shared storage affect the service availability and system's network and disk performance. Tests conducted to test the availability and condition of the VM at the time of migration. Tests were also conducted to test the network and disk performance on server.
The test results show the system has KVM virtual machine migration  performance 75,76% better than OpenVZ. However both KVM and OpenVZ has a comparable avalability and data integrity can be maintained. Obtained disk performance smaller than network performance so that the speed of replication and distribution of shared storage is limited to the disk. Performance with shared storage disk obtained 38.64% to 42.39% of the maximum disk performance.",2015-08-04 00:00:00,53,37768,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
89411,,ROCHMAD HANDOKO AJI,ANALISA KINERJA DAN KARAKTERISTIK IPV6 PADA VIRTUAL DATA CENTER DENGAN XENSERVER,,,"tunneling, virtaul data center, xenserver, ipv6, network performance","Mardhani Riasetiawan, M.T.",2,3,0,2015,1,"Kebutuhan data center meningkat karena perkembangan zaman yang  menghasilkan banyak data. Pembangunan data center mewajibkan menggunakan infrastruktur IP untuk saling terhubung. Sedangkan persediaan IPv4 sudah menipis. Solusinya adalah menerapkan sistem pengalamatan IP yang baru. IPv6 adalah sistem pengalamatan IP terbaru yang menggantikan IPv4. Dengan sistem pengalamatan 128-bit membuat IPv6 dapat memberikan pengalamatan jauh lebih banyak dibanding IPv4. 
Pada penelitian ini akan dilakukan implementasi IPv6 di dalam virtual 
data center dengan Xenserver. Implementasi menggunakan metode tunneling 6to4 yang dibandingkan dengan penggunaan IPv4 murni dan IPv6 murni. Untuk mengetahui kinerjanya, dilakukan pengujian dengan parameter throughput, jitter, packet loss, dan latency. Selain itu pengujian lain dilakukan dengan menganalisa paket yang melewati jaringan. 
Hasil dari penelitian ini menunjukan bahwa implementasi ipv6 pada 
virtual data center setidaknya membutuhkan 4 buah virtual machine sebagai router dan host. Dilihat dari kinerja jaringan, hasil throughput skenario ipv6 murni paling tinggi dibanding skenario lainnya, dengan nilai rata-rata 2791.29 Mbit/s. Nilai j itter pada skenario IPv6 tunneling paling tinggi dibanding dua skenario lainya, dengan nilai rata-rata 0.0336 ms. Berbeda dengan skenario IPv6 tunneling, skenario IPv6 murni dan IPv4 murni sama sekali tidak terjadi packet loss. Namun untuk parameter latency, skenario IPv6 tunneling memiliki latency 
paling besar dengan nilai rata-rata 1.137 ms.","The needs of data centers is increasing because of the ever increasing data. The construction of data centers require use of IP infrastructure to be connected. While the supply of IPv4 is already thinning. The solution is to apply the new IP addressing systems. IPv6 is the newest IP addressing systems that replace IPv4. With a 128-bit addressing system make the IPv6 addressing can give so much more than IPv4. 
In this research will be performed IPv6 implementation in virtual data center with xenserver. Implementation using tunneling 6to4 methods were compared to use of pure IPv4 and pure IPv6. To determine the performance, the test performed with the parameters are throughput, jitter, packet loss, and latency. In addition to other testing done by analyzing packets that pass through the network. 
The result of this research show that the IPv6 implementation on virtual 
data center requires at least 4 pieces of virtual machine as a router and host. Views of network performance, the results of throughput on pure IPv6 skenario is the highest compared to the other skenario, with average value of 2791.29 Mbit/s. Jitter value on Ipv6 tunneling skenario is the highest than two other skenario, with average value of 0.0336 ms. Different to IPv6 tunneling skenario, skenario of pure IPv6 and pure IPv4 there is no packet loss at all. However, for the latency parameters, IPv6 tunneling skenario has greatest latency with an average value is 
1.137 ms. ",,,39596,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
78411,,WERDA BUANA PUTRA,ALGORITMA ALPHA-BETA PRUNING DALAM CHESS ENGINE,,,"Alpha-Beta Pruning, child, Minimax, node, parent","Lukman Heryawan S.T., M.T.",2,3,0,2015,1,"Kapasitas sebuah software catur ditentukan oleh teknik pencarian yang dilakukan chess engine dan didukung oleh hardware yang digunakan. Minimax adalah sebuah algoritma pencarian yang dipakai sebagai konsep permainan zero sum(catur, go). Jika konsep algoritma Minimax diterapkan dalam permainan catur tanpa dimodifikasi, akan memberikan beban yang besar pada hardware komputer canggih sekalipun.
Algoritma Alpha-Beta Pruning, sebagai pengembangan dari algoritma Minimax, merupakan suatu solusi untuk mengurangi beban hardware. Idenya adalah mengurangi jumlah node yang akan dievaluasi oleh algoritma Minimax dengan membuang suatu cabang di dalam sebuah search tree jika node child tidak dapat memberikan nilai yang dicari oleh node parent.
Chess engine yang dibangun kali ini menggunakan algoritma Alpha-Beta Pruning dan diberi nama Harmonia. Dalam masa penelitian, Harmonia akan dipertandingkan dengan beberapa chess engine lain yang memiliki ukuran lebih besar (175 KB) dan menggunakan lebih banyak resource komputer. Hasil yang diperoleh dari penelitian membuktikan Harmonia yang menggunakan algoritma Alpha-Beta Pruning dapat mengimbangi permainan melawan engine catur yang memiliki ukuran dan menggunakan sumber daya yang lebih besar.
","A chess engine's capability is determined by the search technique it used, supported by the hardware. Minimax is an algorithm used as foundation in zero-sum game(chess, go). Should Minimax concept applied in chess, unmodified, they will give a massive burden on hardware, even the sophisticated one.
Algorithm Alpha-Beta Pruning, as the advancement of Minimax algorithm, is a solution to reduce the burden on hardware. The idea is to reduce the number of nodes that will be evaluated by the Minimax algorithm by removing a branch in a tree search should the child node can't provide the value sought by the parent node.
Chess engines we built use the Alpha-Beta Pruning algorithm and named Harmonia. In the study period, Harmonia would be competed with several other chess engines with larger size (175 KB) and uses more computer resources in execution. Results obtained at the end of the study proved that Harmonia which built using Alpha-Beta Pruning algorithm can withstand the might of another chess engines that greater in term of  the size and use of resources.
",,,28322,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
90187,,ARIE AKBAR MULIA,PENGENALAN WAJAH PADA GAMBAR DAN VIDEO DENGAN PERBANDINGAN BEBERAPA METODE PENGUKURAN KEMIRIPAN WAJAH,,,"Face Recognition, L1 distance, L2 distance, Local Binary Pattern, Eigenface, Fisherface","Faizah, S. Kom., M. Kom",2,3,0,2015,1,"Pengenalan wajah dalam lingkungan yang tidak dikondisikan (uncontrolled environment) seperti pencahayaan yang berubah, ekspresi wajah yang berbeda, dan perubahan wajah akibat umur membuat pengenalan wajah menjadi tugas yang sulit bagi komputer. Variasi penampakan wajah dapat menyebabkan rendahnya nilai akurasi suatu sistem face recognition karena variasi dan jumlah sampel dalam sistem terbatas. 
Penelitian ini menyajikan sebuah rancangan sistem yang dapat mempelajari dan mengakusisi wajah setiap subjek dari berbagai kondisi pencahayaan, ekspresi wajah, dan perubahan lainnya melalui file gambar, video, atau live streaming video yang diterima sistem dengan menyimpan berbagai data wajah secara bertambah. Penerapan metode pengenalan wajah ini pada sistem bertujuan untuk meningkatkan kemampuan pengenalan wajah oleh sistem pada berbagai kondisi lingkungan. 
Akurasi dan kecepatan merupakan parameter utama untuk menilai performa suatu metode pengenalan wajah. Setelah dilakukan pengujian beberapa metode pengukuran kemiripan wajah seperti L1 distance, L2 distance, Local Binary Pattern (LBP), Eigenface, Fisherface, dll. menggunakan beberapa dataset, seperti AT&amp;T faces, Yale Facedatabase B, dll. L1 distance yang memberikan kecepatan pencarian yang tercepat dengan akurasi hasil pencarian yang baik. Kecepatan pencarian ini diperoleh dari rumus L1 distance yang paling sederhana untuk mencari distance dari dua buah vektor gambar wajah.
","Face recognition under uncontrolled environmet such as changes of illumination, different facial expression, and aging effect make face recognition becomes a difficult task for computer. Variation of face appearance could reduce the accuracy of a face recognition system because the number and variation of training sample in system is limited.
This research present a design of system that learn and acquire people face incrementally from face data under various condition of illumination, facial expression, face direction and other condition under uncontrolled environment. Application of this method aims to improve system's ability to recognize face in various condition under uncontrolled environment. System able to collect face data from live streaming video, video file, or image files that contain face.
Accuracy and time are main parameter used to measure performance of face recognition method. Based on this research, some face similarity measurement method such as L1 distance, L2 distance, Local Binary Pattern (LBP), Eigenface, Fisherface, etc. that have been tested using some standard dataset for bencmarking such as AT&amp;T faces, Yale Faces B, etc. L1 distance show fastest searching time and good accuracy on result. Fast processing time by L1 distance achieved by its very simple equation so it minimize processing time to calculate distance between two face vector.
",,,40243,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83278,,SEKAR PERMATA SARI,OPTIMALISASI VIRTUAL MACHINE PADA DATA CENTER DENGAN XENSERVER,,,"data center, virtual machine, penggunaan CPU, penggunaan memori, Xenserver","Medi, Drs., M.Kom",2,3,0,2015,1,"Teknologi cloud computing atau komputasi awan telah berkembang dan menjadi tren dari teknologi informasi pada sektor bisnis. Dasar dari cloud computing adalah teknologi virtualisasi yang mampu membuat versi virtual dari sistem operasi, penyimpanan data atau sumber daya jaringan. Pada cloud computing, jika virtual machine dibuat secara sembarangan tanpa memperhatikan sumber daya yang ada, maka dapat menimbulkan virtual data center yang tidak berfungsi dengan baik. Karena itu faktor keamanaan berdasarkan desain konfigurasi virtual machine pada infrastruktur virtual data center menjadi penting untuk diperhatikan.
Pada penelitian ini dilakukan pengujian terhadap beberapa virtual machine dengan konfigurasi alokasi sumber daya, vCPU dan topologi berbeda, yang diimplementasikan pada Xenserver 6.5. Parameter penelitian yang digunakan adalah penggunaan CPU dan penggunaan memori.
Hasil dari penelitian ini menunjukkan bahwa virtual data center memerlukan CPU dan memori tinggi saat pertama kali dijalankan. Berdasarkan jenis alokasi sumber daya yang digunakan virtual machine, alokasi sumber daya secara dynamic menunjukkan penggunaan memori dan CPU yang stabil. Berdasarkan topologi yang digunakan virtual machine diperoleh bahwa virtual machine dengan 4vCPU dan topologi 1 socket dengan 4 core memerlukan penggunaan CPU lebih kecil jika dibandingkan dengan yang lainnya.","Cloud computing has evolved and become the trend of information technology in business sectors. The basic of cloud computing is virtualization technology which is able to create a virtual version of the operating system, data storage or network resources. In cloud computing data center is replaced with a virtual data center. If all virtual machines on the virtual data center made arbitrarily without notice the existing resources, it can cause failure to the virtual data center. Therefore, the safety factor based on the configuration design of virtual machine in the virtual data center infrastructure becomes important to note.
In this research, multiple virtual machine are tested with configurations for resource allocation, vCPU and different topologies, which are implemented in XenServer 6.5. Parameters that are used in this research are CPU and memory usage.
The results of this research indicate that the virtual data center requires CPU and high memory on the first run. Based on the type of resource allocation that is used by virtual machine, resource allocation dynamically shows memory and CPU usage are stable. Based on the topology used by the virtual machine is obtained that virtual machine with 4vCPU and topology 1 socket with 4 cores require smaller CPU usage when compared to the others.",2015-05-15 00:00:00,53,33283,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
86867,,SAMANI,IMPLEMENTASI PERINGKAS TEKS OTOMATIS PADA JURNAL BAHASA INDONESIA DENGAN METODE SENTENCE SCORING DAN RANKING,,,": text summarization, sentence scoring, journal, feature based, tfidf","Edi Winarko, Drs., M.Sc., Ph.D ",2,3,0,2015,1,"Peringkasan teks merupakan proses untuk mengkompres dokumen original kedalam versi yang lebih pendek dengan mengekstrak informasi paling penting dari teks dokumen tersebut. Penelitian ini mengimplementasikan peringkasan teks pada jurnal bahasa Indonesia untuk menghasilkan ringkasan dengan metode sentence scoring dan ranking. Penelitian ini bertujuan untuk mengetahui akurasi dari ringkasan yang dihasilkan sistem yang menggunakan metode sentence scoring dan ranking.
Metode sentence scoring dan ranking termasuk dalam kategori ekstraksi, yaitu dalam menghasilkan ringkasan tidak dilakukan pembuatan kalimat baru, melainkan hanya dilakukan ekstraksi dari kalimat yang  memiliki bobot fitur paling tinggi. Sentence scoring yang dilakukan yaitu menjumlahkan nilai dari Fitur thematic, fitur position, fitur cueword dan fitur background. Dalam penelitian ini dianalisa perbandingan hasil akurasi dari sistem yang menggunakan pembobotan tf dan yang menggunakan pembobotan tfidf sebagai fitur thematic, serta yang menggunakan pembobotan paragraf dan pembobotan bab sebagai fitur position. Sistem ini diimplementasikan menggunakan bahasa pemrograman PHP.
Hasil penelitian menunjukkan bahwa menggunakan pembobotan tfidf sebagai fitur thematic memiliki akurasi yang lebih baik dibanding menggunakan pembobotan tf. Selain itu pembobotan bab sebagai fitur position memiliki akurasi yang lebih baik dibandingkan menggunakan pembobotan paragraf. Dari keempat percobaan, diperoleh kesimpulan bahwa kombinasi dari pembobotan tfidf, pembobotan bab, cueword, dan background memiliki nilai akurasi rata-rata yaitu sebesar 42.19% yang merupakan nilai akurasi yang tertinggi yang diperoleh dalam penelitian ini menggunakan metode sentence scoring dan ranking.
","Text summarization is to compress an original document into a shortened version by extracting the most important information out  of  the  document. This research implements text summarization on indonesian journal to generate an automatic summary using sentence scoring and ranking method. The purpose of this research is to find out the accuracy of the summary generated by system that using sentence scoring and ranking method.
Sentence scoring and ranking method is included in the category of extraction. This method constructs  the  summary  by  taking  sentences that has the highest score  out  of  the  original  document. Sentence scoring summing the value of thematic feature, position feature, cueword feature and background feature. The accuracy ratio of system that using tf scoring as thematic feature and system that using tfidf as thematic value is analyzed in this research, as well as the accuracy of system using section scoring as position feature and system that using paragraph scoring as position value. This sistem is implemented using PHP programming language.
The result shows that using tfidf scoring as thematic feature has better accuracy than using tf scoring. Using section scoring as position feature has better accuracy than using paragraph scoring. The combination of tfidf scoring, section scoring, cueword and background has an avarage of accuracy value 42.19% which is the best results that can be obtained in this research using sentence scoring and ranking method.
",,,36913,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
82520,,ESTI MULYAWATI,PERANCANGAN DAN PENGEMBANGAN AUDIT TOOLS UNTUK INFORMATION SECURITY ASSESSMENT MENGGUNAKAN STANDAR COBIT 5,,,"Audit, Cobit 5, Set of questions, Project.","Mardhani Riasetiawan, S.E., M.TI.,",2,3,0,2015,1,"Proses audit sistem informasi saat ini dilakukan bersifat manual, sehingga menyulitkan auditor dalam melakukan proses audit. Pekerjaan audit sistem informasi salah satunya adalah melakukan pencarian evidence, assessment evidence dan penghitungan skor untuk penentuan resiko audit. Proses ini akan menjadi efektif apabila didukung oleh tools yang berbasis sistem informasi. Hal inilah yang menjadi latar belakang perlunya audit tools untuk memberikan kemudahan bagi para auditor dalam melakukan proses audit.
Penelitian ini bertujuan untuk merancang serta mengembangkan Tools yang dapat melakukan perhitungan skor dari proses audit menggunakan standar Cobit 5. Proses audit yang diakomodasi dalam sistem ini adalah Assesment pada Information Secutrity. Tools yang dibangun dengan menggunakan bahasa pemrograman Web PHP, framework Code Igniter, dan database MySQL. Set of questions yang digunakan adalah set of questions standar pada Cobit 5.
 Penelitian ini menghasilkan sebuah audit tools yang menyediakan layanan untuk mengelola dan melakukan perhitungan serta kesimpulan dari proses audit. Adapun fitur-fitur yang terdapat pada tools tersebut adalah pengelolaan user, company profile, multiple project, set of questions, review dan report. Sehingga di akhir  auditor dapat langsung menarik kesimpulan tentang hasil audit yang dilakukan.

","Processing audit in information system has been running in manual system, so that auditor get difficulty in audit processing. One of part from audit in information system is  searching for evidence, assessment for evidence and calculating score for result in audit risk. These process will be more effective, if supported by tools in Information system based. The reason that has mentioned is being how audit tools to make auditor easier in audit processing.
The purpose of this research for design and develop tools that can calculate Score from audit process using standar of Cobit 5. Processing audit is accommodated in system is Assessment in Information Security. Audit Tools is built by programming language Web PHP, Framework Code Igniter, and database MySQL. Set of questions which is used is set of questions in Cobit 5.
This research produce tools that provide services for manage and calculate Score and also the conclusion from audit process. As for the features contained in the Tools is user management, company profile, multiple project, set of questions, review and report. So that, in the last session of tools, auditor can make a conclusion about the result of audit process.


 
",,,32448,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
86875,,RIAN DWI PAWESTRI ,SISTEM PENDUKUNG KEPUTUSAN REKOMENDASI KELANJUTAN KONTRAK TENAGA HARIAN LEPAS TENAGA BANTU PENYULUH PERTANIAN DENGAN METODE PROFILE MATCHING DAN ANALYTICAL HIERARCHY PROCESS  (STUDI KASUS : DINAS PERTANIAN KABUPATEN KLATEN),,,"Sistem pendukung keputusan, decision support system, THL-TBPP, profile matching, Analitycal Hierarchy Process (AHP)","Sri Mulyana, Drs., M.Kom",2,3,0,2015,1,"Salah satu kegiatan di Dinas Pertanian kabupaten Klaten yang membutuhkan pengambilan keputusan adalah penilaian kinerja untuk menentukan rekomendasi kelanjutan kontrak Tenaga Harian Lepas Tenaga Bantu Penyuluh Pertanian (THL-TBPP). Namun, selama ini penentuan rekomendasi kontrak di Dinas Pertanian kabupaten Klaten masih dilakukan secara manual. Oleh karena itu, dibutuhkan suatu sistem pendukung keputusan untuk membantu menentukan rekomendasi kelanjutan kontrak agar lebih efektif dan efisien.
Sistem ini menerima masukan berupa THL-TBPP yang akan mendapatkan rekomendasi kelanjutan kontrak, dan akan memberikan keluaran berupa daftar nilai akhir dan rangking THL-TBPP mulai dari rangking pertama sampai rangking terakhir. Metode yang digunakan untuk pengambilan keputusan adalah metode profile matching dan AHP. Sehingga diharapkan memperoleh solusi terbaik dalam penentuan rekomendasi kelanjutan kontrak THL-TBPP sesuai dengan aspek psikologis dan berdasarkan pada hasil evaluasi kinerja sesuai dengan Peraturan Menteri Pertanian No.91 Tahun 2013 Tentang Pedoman Evaluasi Kinerja Penyuluh Pertanian.","An activity in the Department of Agriculture in Klaten district that requires a decision making is a performance assessment to determine THL-TBPP&acirc;€™s contract continuity recommendation. However, all this time, the determination of contract recommendation at the Department of Agriculture Klaten district is still done manually. Therefore, it takes a decision support system to help determine the contract continuity recommendation to be more effective and efficient.
The system accepts input in the form of THL-TBPP that will get a contract continuity recommendation and give the output of final scores list and THL-TBPP rankings which start from the first to the last rank. The method used for decision-making is a method of profile matching and AHP. So it is expected to obtain the best solution in determining the THL-TBPP&acirc;€™s contract continuity recommendation in accordance with the psychological aspect and based on the results of performance evaluation that build upon The Regulation of Agriculture Minister No. 91 in 2013 about the Guidelines for Performance Evaluation of Agricultural Extension.",,,36911,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
80220,,FATTA SYAIFUL AMIN,IMPLEMENTASI APPLICATION PROGRAMMING INTERFACE (API) PADA SISTEM KONTAK UGM ,,,"Kontak UGM, CodeIgniter, application programing interface","Bambang Nurcahyo Prastowo, Drs., M.Sc",2,3,0,2015,1,"Kontak adalah sistem komunikasi antar masyarakat dan civitas akademika di lingkungan kampus Universitas Gadjah Mada (UGM). Kontak terdiri dari beberapa modul, yaitu berita, curah gagas, ekspresi, grup, pesan grup, profil pengguna, peristiwa, dan pemberitahuan. Implementasi Kontak menggunakan aplikasi PLO, yang merupakan sistem informasi untuk komunikasi perkantoran di satuan-satuan kerja UGM.  PLO dikembangkan menggunakan arsitektur aplikasi web tahun 90-an yang belum menggunakan arsitektur pemrograman seperti CMS dan framework, sehingga sulit untuk dikembangkan lebih lanjut.
Untuk memudahkan pengembangan sistem yang sesuai dengan perkembangan teknologi, dibutuhkan sebuah penghubung antara pengembang sistem dan sistem Kontak UGM. Penghubung tersebut berupa Application Programing Interface (API) dengan format keluaran JSON dan menggunakan framework CodeIgniter. Selain itu API disesuaikan dengan proses bisnis yang sama dengan sistem Kontak UGM. API yang telah dibangun tersebut kemudian diuji menggunakan API Rest Tools.
Implementasi antarmuka pemrograman aplikasi telah dapat digunakan untuk mengembangkan sistem Kontak yang disesuaikan dengan perkembangan teknologi informasi tanpa harus merubah operasional penggunaan sistem yang ada.","Kontak is a communication system that used for academicians in Universitas Gadjah Mada (UGM). Kontak consist of some module, such as news, curah gagas, expression, group, group message, profile, events, and notifications. Kontak implementation uses PLO application, an information system that used to communicate between work units in UGM. PLO was developed using 90s web application architecture that its not use programming architecture such as CMS and framework, so its difficult to develop further.
For facilitating the system development which is suitable by technological growth, it needed a communicator between developer system and Kontak UGM. The communicator is Application Programming Interface (API) which is use JSON format and Codeigniter framework. Beside that, API adjusted with the business process of Kontak UGM. After API has been built, then it tested using API REST Tools.
API implementation has been used to develop Kontak system that is adapted to the development of information technology without changing  operational use of existing system.",,,30182,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
87391,,DANIEL OSCAR BASKORO,"Big Data Benchmark Pada Hadoop 2, Preso dan Spark Menggunakan Metode Perbandingan Waktu Respon Query",,,"Big Data, Big Data Framewrok, Query, High Performance 	                   Query,Cloud Storage, Cloud Engine, Computing, Twitter",Lukman Heryawan,2,3,0,2015,1,"Seiring dengan peningkatan informasi yang berbentuk digital (data digital), kebutuhan akan analisa big data menjadi hal prioritas bagi masyarakat luas khususnya pada sektor swasta dan pemerintahan. Kebutuhan tersebut menjadi prioritas karena analisa pada Big Data dapat menghasilkan suatu informasi yang dapat mempermudah para pembuat kebijakan dalam menentukan suatu kebijakan. Tingginya permintaan masyarakat dalam analisa Big data, memunculkan banyak penelitian yang menciptakan berbagai macam framework untuk menganalisa suatu Big Data. 
	Pada penelitian dan pengujian ini dilakukan suatu analisa performa beberapa framework yang digunakan dalam menganalisa suatu data. framework yang diteliti adalah Spark, Hadoop 2, dan Presto. Metode yang dipakai dalam pengujian adalah evaluasi waktu yang dibutuhkan framework dalam melakukan  query pada parameter-parameter tertentu. Parameter dalam pengujian adalah waktu yang digunakan dalam query tingkat rendah, query tingkat menengah, query tingkat tinggi, query dengan peningkatan 2 kali jumlah core prosessor, dan terakhir adalah query dengan peningkatan 4 kali jumlah core prosessor. Penelitan ini menggunakan contoh kasus nyata dalam analisa big data, data yang diuji berada di cloud storage Amazon S3, infrastruktur analisa yang dipakai menggunakan cloud engine pada Qubole, data testing yang dianalisa adalah data Twitter yang terdiri dari  100.000 dan 3.000.000 status twitter.
	Hasil akhir merupakan suatu informasi performa Spark, Hadoop, dan Presto dalam melakukan proses query. Dari informasi tersebut diketahui diantara framework tersebut mana yang memiliki performa terbaik sehingga dapat sebagai pertimbangan dalam pemilihan framework untuk menganalisa data pada berbagai kebutuhan.","Along with the improvement of the information in digital form (digital data), the need for analysis of big data becomes a priority for the society, especially the private sector and government. These needs to be a priority for the Big Data analysis can provide information that can facilitate policy-makers in determining a policy. High demand in the analysis of Big Data, raises a lot of research that creates a wide variety of framework to analyze Big Data.
In  this research and testing carried out an analysis of the performance of several framework that are used in analyzing the data. framework that testing were Spark, Hadoop 2, and Presto. The method used in testing is the evaluation of the time required to perform a high perfomance framework queries on certain parameters. The parameters in the test is the time spent in the low-Model query, Query middle Model, high-Model query, Query with an increase of 2 times the number of processors, and the last is a query with an increase of 4 times the number of processors. This research uses real case examples of big data analysis, data were tested in the Amazon S3 cloud storage, infrastructure analysis using cloud engine used in Qubole, testing the data analyzed is data of 1,000,000 and  3,000,000 Twitter twitter status.
The end result is a performance information Spark, Hadoop 2, and Presto in high perfomance perform queries. From the information is known among the framework which has the best performance that can be a consideration in the selection of the framework to analyze the data in a variety of needs.",,,37447,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
85857,,SEPTRI NUR ITHMAM,Analisis Performa Restful Web Service Dan Parser Json Menggunakan Algoritma Stream Parsing Pada Sistem Operasi Android (Studi Kasus : Sistem Informasi Laboratorium D3 Komsi Sekolah Vokasi Ugm),,,"web service, RESTful, JSON, JSON parser, Stream parsing, Android, kecepatan parsing","Mhd. Reza M.I Pulungan, M.Sc., Dr.-Ing",2,3,0,2015,1,"Web Service memungkinkan dua perangkat elektronik untuk berkomunikasi melalui Internet. Web Service digunakan sebagai suatu fasilitas yang disediakan oleh suatu website untuk menyediakan layanan (dalam bentuk informasi) kepada sistem lain, sehingga sistem lain dapat berinteraksi dengan sistem tersebut melalui layanan-layanan (service) yang disediakan oleh suatu sistem yang menyediakan web service. Salah satu arsitektur web service adalah REST (Representational State Transfer). REST dapat menyimpan data informasi dalam tipe teks, baik yang berformat XML maupun JSON sehingga data ini dapat diakses oleh sistem lain walaupun berbeda platform, sistem operasi, maupun bahasa compiler. JSON (JavaScript Object Notation) adalah format pertukaran data yang ringan, mudah dibaca dan ditulis oleh manusia, serta mudah diterjemahkan dan dibuat (generate) oleh komputer. Untuk membaca suatu data dengan format JSON, diperlukan suatu perantara yang disebut parser. Ada begitu banyak parser berbasis bahasa pemrograman Java yang dapat digunakan untuk membaca data JSON. Setiap parser memiliki kelebihan dan kekurangan masing-masing. Salah satu kelebihan tersebut adalah dalam hal kecepatan. 
Pada penelitian ini dilakukan analisis performa RESTful web service dan JSON parser pada Android. Parser yang digunakan adalah SimpleJason (org.json), Jackson, dan GSON. Hal ini bertujuan untuk mengetahui performa RESTful web service dan perbandingan performa kecepatan antara org.json, Jackson, dan GSON. Hasil pengujian pada penelitian ini menunjukkan RESTful web service yang dibuat dengan PHP mampu memberikan respon dari setiap request. Parser Jackson memiliki kecepatan lebih baik daripada org.json dan GSON dalam melakukan proses parsing menggunakan algoritma stream parsing pada JSON array.
","Web Service allows electronic devices to communicate each other over the Internet. Web Service is used as a facility provided by a website to provide services (in the form of information) to other systems, so that other systems can interact with the system through the services that is provided by a web service. One of the web service architecture is REST (Representational State Transfer). REST can store data in the text, either XML or JSON format so that the data can be accessed by other systems, although different platforms, operating systems, compilers and language. JSON (JavaScript Object Notation) is a lightweight data interchange format, easily read and written by humans, and easily translated and generated by the computer. To read the data in JSON format, needed a parser.  There are so many parser-based Java programming language that can be used to read the JSON data. Each parser has advantages and disadvantages of each. One of these advantages is speed.
In this study, analyzed performance of RESTful web services and JSON parser in Android. Parser used is SimpleJason (org.json), Jackson, and GSON. It aims to determine the performance of RESTful web service and performance comparisons between org.json speed, Jackson, and GSON. The test results in this study show the RESTful web service created with PHP is able to give a response of each request. Jackson parser has better speed than org.json and GSON in the process of parsing with stream parsing algorithm using the JSON array.
",,,35865,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
93542,,WAHYUDI WIBOWO,"IMPLEMENTASI SERVICE ORIENTED ARCHITECTURE PADA SINGLE PAGE APPLICATION MENGGUNAKAN ANGULARJS PADA APLIKASI PENCARIAN PENERBANGAN, HOTEL, DAN RUTE PERJALANAN",,,"Traveling, Service Oriented Architecture, Single Page Application, Web Service, Web API, AngularJS, Penerbangan, Hotel, Rute Perjalanan, Flights, Hotels, Traveling Route","Dr-Ing.Mhd.Reza M.I. Pulungan, S.Si, M.Sc",2,3,0,2015,1,"Saat ini, kegiatan traveling menjadi salah satu kegiatan yang digemari oleh
masyarakat. Oleh karena itu, masyarakat membutuhkan sebuah aplikasi yang
dapat memberikan kemudahan untuk mendapatkan informasi tentang perjalanan
yang akan ditempuh, termasuk didalamnya transportasi, akomodasi, keterangan
rute serta harga yang harus dibayarkan. Sayangnya, sebagian besar aplikasi yang
menyediakan informasi terkait perjalanan tidak menyediakan informasi
komponen perjalanan seperti penerbangan dan hotel dalam satu tempat. Jika
ada, proses pencarian penerbangan dan hotel dibuat secara terpisah sehingga
membutuhkan tambahan waktu untuk melakukan pencarian. Oleh karena itu,
aplikasi ini dibuat agar dapat mencari komponen perjalanan (transportasi,
akomodasi, rute perjalanan, dan harga) hanya dalam satu langkah saja.
Aplikasi ini dibangun menggunakan service oriented architecture (SOA) yang
mengambil service dari beberapa sumber seperti Tiket.com, Google dan
Foursquare untuk mendapatkan data yang diperlukan berdasarkan input dari
pengguna. Aplikasi ini juga menerapkan prinsip single page application
menggunakan AngularJS sebagai front end framework.
Hasil akhir dari proses pencarian yang dilakukan oleh aplikasi ini adalah
sebuah paket perjalanan yang meliputi jadwal penerbangan, hotel, keterangan
rute dalam bentuk peta, serta harga yang harus dibayarkan oleh pengguna.","Nowadays, traveling has become one of favorite activity for most of the
people. Because of that, people need an application that can facilitate them to
get information about their trip, including transportation, accommodation,
routes, and price they need to pay, in easier way. However, most of the
application that providing information about traveling component, such as flight
schedule and hotel, did not provide the information on the same places. If they do
provide information in one place, then the searching process for flight schedule
and hotel is done separately which takes more time. Because of that, this
application will be made so the searching process of traveling component will
take only one step to complete.
This application is built using service oriented architecture (SOA) that takes
services from several sources such as Tiket.com, Google and Foursquare to get
the necessary data based on users input. This application also apply single page
application principle using AngularJS as its front end framework.
The results from searching process in this application is a traveling plan
consists of flight schedules, hotels, routes in map, and price that users need to
pay.",,,43676,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92775,,M SHOFWAN AMRULLAH,Sistem Pengolahan Video untuk Pengawasan Benda Statis,,,"background subtraction, deteksi kontur, deteksi objek, pengawasan objek, CCTV, contour detection, object detection, object surveillance,","Faizah, S.Kom., M.Kom.",2,3,0,2015,1,"Tingginya angka pencurian kendaraan bermotor membuat upaya pengamanan selalu ditingkatkan. Salah satu cara yang sering dilakukan adalah dengan memasang kamera pengawas atau kamera CCTV (closed circuit television). Lebih jauh lagi, teknologi IP camera membuat gambar dari kamera CCTV dapat dilihat secara online. Namun IP camera tetap membutuhkan bantuan pengawasan dari manusia. Oleh karena itu dibutuhkan program yang dapat membantu manusia mengawasi benda statis secara otomatis.
Program dibuat dengan menggunakan algoritma background subtraction serta contour detection. Algoritma diterapkan secara simpel, background subtraction digunakan dengan background model ditentukan sendiri oleh user, sedangkan contour detection digunakan tanpa menghiraukan hirarki dari objek yang didapat. Setelah itu dilakukan pengawasan dengan cara membandingkan tiap objek yang terdeteksi pada tiap framenya. Pada program terdapat variabel-variabel yang dapat dikonfigurasi untuk membantu dalam mendeteksi objek dan mendeteksi kehilangan benda.
Setelah program dibuat, dapat disimpulkan bahwa program dapat mengawasi benda dan dapat mengeluarkan peringatan saat benda menghilang. Kondisi tempat yang berbeda membutuhkan konfigurasi variabel yang berbeda, dan tidak ada nilai variabel yang bagus untuk semua kondisi dalam melakukan pengawasan. Selain itu program yang dibuat sangat sensitif terhadap perubahan cahaya yang ada, dikarenakan learning rate yang bernilai 0. Hal ini membuat program tidak cocok digunakan pada tempat yang terkena perubahan pencahayaan, seperti siang dan malam.
","The high number of motor vehicle thefts drives security efforts to always be improved. One of the method that have been done is to install CCTV cameras (closed circuit television) for surveillance. Furthermore, IP camera&acirc;€™s technology makes it possible to see images from CCTV cameras online. However, IP camera still needs humans for manual surveillance. Therefore we need a program that can help us to keep track of some static objects automatically.
Program is built by using background subtraction and contour detection algorithm. The algorithms are applied plainly, background subtraction is used with the background model solely specified by the user, while contour detection is used with hierarchy of the detected object ignored. Then the surveillance were done by comparing each object that have been detected on each frame. There are variables in the program that can be configured manually by users to help program in detecting objects and detect the disappearance of the objects afterward.
After the program is created, we can conclude that this program are able to keep track of some objects and put out a warning when the object disappear. Different environment and place, requires distinct configuration of variables, and no variable values that suits all conditions of surveillance. However, the program that has been made were very sensitive to illumination changes, because the value of learning rate is set to 0. This makes this kind of programs are not suitable for use in places that exposed to illumination changes, such as day and night transitions.
",,,42907,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83561,,DYAH KURNIASIH NURAZIZAH,PENGEMBANGAN APLIKASI UJI SIMILARITAS BERITA DENGAN MENGGUNAKAN ALGORITMA WINNOWING,,,"Press release, Media monitoring, Similaritas, Google Custom Search API,  Algoritma Winnowing","I Gede Mujiyatna, S. Kom., M. Kom.",2,3,0,2015,1,"Perusahaan penyedia barang dan jasa biasanya akan mengeluarkan press release ketika meluncurkan produk nya. Press release ini ketika dimuat oleh media khususnya media online terkadang terjadi perubahan. Oleh karena itu dibutuhkan media monitoring untuk mengetahui seberapa besar press release yang dikeluarkan oleh sebuah perusahaan dijadikan acuan dalam pembuatan berita. Hal ini dilakukan selain untuk memantau penerimaan berita tersebut oleh masyarakat juga untuk memantau kinerja PR pembuat press release tersebut.
Pada penelitian ini dikembangkan aplikasi media monitoring yang akan membandingkan similaritas antara berita-berita pada situs media online di Indonesia dengan press release tertentu yang dimasukkan oleh pengguna ke dalam sistem. Pencarian berita-berita dari beberapa situs media online di Indonesia dilakukan dengan menggunakan Google Custom Search API. Sedangkan perbandingan similaritas dilakukan dengan menggunakan algoritma winnowing. Hasil dari sistem berupa persentase kesamaan keseluruhan antara berita-berita dengan press release yang dimasukkan serta  persentase detail perbandingan tiap-tiap paragraf antara berita dan press release.
Hasil penelitian dan pengujian pada aplikasi menunjukkan bahwa aplikasi berhasil berjalan dengan baik. Selain itu hasil yang dikeluarkan oleh sistem cukup akurat dengan kesalahan kurang dari 10%.
","Company which focused in goods and services usually will release a press release when launching their product. When the press release published by media, especially online media, the press release often changed. Because of this, media monitoring is needed to know how much press release influence the news written by media. This was done to monitors how the news accepted by people and also to monitors the performance of PR who make press release.
In this research, application to monitor news from a few Indonesian news site and compare those news with press release is developed. To search the news, Google Custom Search API is used to do the crawler. While for similarity comparison researcher using Winnowing Algorithm. The output of the system are similarity  percentage of news and press release in a whole and percentage detail similarity for each paragraph of news and press release.
The result of this research and the testing process of the application show that the application working well. The output of the application also quite accurate with error less than 10%.
",,,33558,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
78957,,SHELLY TAMARA,IMPLEMENTASI WEB SERVICE SISTEM AUTO REPLY HARGA TIKET PENERBANGAN PESAWAT BERDASARKAN ARSITEKTUR RESTFUL PADA TWITTER (STUDI KASUS : PT. POJOK CELEBES MANDIRI),,,"Auto Reply, Web Service, RESTful, Twitter, Twitter API, Pointer","I Gede Mujiyatna, S.Kom., M.Kom.",2,3,0,2015,1,"Twitter merupakan salah satu situs microblog yang memberikan fasilitas bagi penggunanya untuk mengirimkan teks pembaruan dengan panjang maksimum 140 karakter. PT. Pojok Celebes Mandiri atau lebih dikenal dengan Pointer sebagai salah satu perusahaan bisnis travel yang memberikan layanan jasa pemesanan dan pembelian tiket penerbangan pesawat, memanfaatkan Twitter sebagai media informasi dan promosi paket perjalanan secara up to date. Tingginya jumlah tweet atau mention dari user Twitter lain yang mempertanyakan harga tiket penerbangan pesawat ke akun Twitter Pointer menjadikan admin Pointer terlalu menghabiskan waktu dalam membalasnya secara manual. Seiring dengan kendala dan kebutuhan user terhadap layanan infomasi harga tiket penerbangan pesawat yang bersifat real time mampu diterapkan melalui media sosial tersebut, maka dibangun sistem auto reply tweet atau mention yang membutuhkan informasi harga tiket penerbangan pesawat dari sistem Pointer.
Sistem auto reply ini mampu merespon secara otomatis tweet atau mention yang mempertanyakan harga tiket penerbangan pesawat pada akun Twitter Pointer secara lebih cepat dan real time. Sistem ini dibangun dengan sebuah web service yang mampu mengintegrasikan Pointer Engine dengan Twitter melalui Twitter API. Selain itu, sistem ini memanfaatkan arsitektur RESTful sebagai model komunikasi web service dan dibangun dengan menggunakan bahasa pemrograman PHP.","Twitter is a microblog site that provides service for its users to send text updates with a maximum length of 140 characters. PT. Pojok Celebes Mandiri or better known as Pointer as one of the travel business company which provides service of reservation or issued flight tickets, using Twitter as a medium of information and promotion of travel packages are up to date. The high number of tweets or mentions of other Twitter users who questioned air fares to Twitter account Pointer made admin spend more time to respond manually. Along with the problem and user needs of service real time air fares information can be applied through social media, auto reply system for tweet or mention that require information about air fares from Pointer Engine has been built.
Auto reply system is able to responds automatically tweet or mention that questioned air fares to a Twitter account Pointer more quickly and in real time. Twitter auto reply system has been built using a web service that is able to integrate Pointer Engine and Twitter using Twitter API. In addition, this system utilizes RESTful architecture as communication models of web service and was built using PHP programming language.",,,28834,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83834,,YODI DILA PADILLA,VISUALISASI INFORMASI UNTUK MEMETAKAN BERITA YANG BERKORELASI DENGAN TWEET MENGGUNAKAN ANALISIS LEKSIKAL DAN NAIVE BAYES,,,"Informasi Visualisasi, Naive Bayes, NER, Twitter, media massa","Mardhani Riasetiawan, M.T.",2,3,0,2015,1,"Internet mengubah setiap bagian yang ada pada media masa juga melahirkan media baru yang bernama media sosial, salah satu diantaranya Twitter, dimana Twitter memiliki sejumlah potensi dan potensi itu mampu dideteksi oleh media massa. Kemudian, potensi tersebut dimanfaatkan oleh media massa dengan cara masuk ke dalam media sosial. Hal ini justru menyebabkan rusaknya alasan media sosial lahir yaitu sebagai media penyeimbang. Media sosial menjadi tempat media massa untuk membuat dan menyebarkan opini publik. Sejauh ini, opini publik yang muncul tentang suatu hal baru sebatas gambaran umum sementara alasan yang menyebabkan opini publik tersebut muncul tidak pernah diketahui. Oleh karena itu, dibutuhkan sebuah sistem yang mampu melihat penyebab opini publik muncul dan pola-pola yang terbentuk yang disebabkan oleh opini publik yang berkorelasi dengan berita yang telah dibuat oleh media massa.

Penelitian ini mencoba untuk membuat sistem yang mampu melihat opini publik dalam ukuran yang lebih kecil berdasarkan asumsi bahwa opini publik disebabkan oleh sejumlah berita. Berita yang  dikumpulkan merupakan berita yang memicu masyarakat untuk membuat opini di media sosial, dalam kasus ini adalah Twitter. Tweet, satuan data di Twitter, akan diklasifikasikan berdasarkan sentimennya. Kemudian, sistem juga akan memprediksi lokasi berita yang kemudian akan dipetakan berdasarkan lokasinya. Kombinasi sentimen dan lokasi digunakan untuk melihat apakah ada pola yang muncul. 

Metode yang digunakan dalam penelitian ini untuk mengklasifikasikan Tweet berdasarkan sentimennya menggunakan Multinomial Naive Bayes (MNB), sedangkan metode untuk memprediksi lokasi berita adalah analisis leksikal. Penelitian ini menghasilkan akurasi klasifikasi sentimen sebsar 69,43%. Kemudian, akurasi untuk memprediksi lokasi adalah sebesar 80,1% untuk relaxed accuracy, 90% untuk regular accuracy, dan 71% untuk hard accuracy. Penelitian ini juga melakukan percobaan menggunakan sejumlah kata kunci untuk menunjukan bahwa sistem dapat bekerja dengan cara menunjukan sejumlah hasil secara statistik dan visualisasi.
","Internet shifted  every part of mass media also triggered the newly born media called social media, such as Twitter, which has a number potential within itself that could be detected by mass media. Then, the potential of Twitter exploited by mass media by infiltrating into social media. This broke one of the reasons why social media born was that people wished that it could become a balancer tool against mass media. Social media turned out to be the place for mass media to create and to spread public opinion. As far as it goes, public opinion that had been emerged was still in general picture while the reasons why it emerged were never been discovered. Therefore, a system is needed to see the causes of the public opinions and patterns that were made based on public opinion which correlated with news that were conducted by mass media. 

The research is trying to make a system to see public opinion in smaller scope based on assumption as public opinion was triggered by news. News that were collected are news that provoked community to make opinion,especially in social media, which in this case is in Twitter. In regards of case Tweet, denumeration data post of Twitter, is going to be classified based on sentiment. Afterwards, the system is going to predict the location of the news and be spatially mapped. The combination of sentiment and location used for  seeing whether any pattern exist.

The method used by this reasearch to classify sentiment of Tweet is Multinomial Naive Bayes (MNB), while the method used to predict location of the news is lexical analyisis. This reasearch produces accuracy of sentiment classification for about 69,43%. Then, the accuracy of location prediction are 80,1% , 90%, and 71% for relaxed, regular, and hard accuracy respectively. This research also tests the general system using several keyword in order to show that system can work well by produce several statistic summary and visualization.
",,,33833,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
80256,,KARMILA,analisis perbandingan pemampatan citra menggunakan algoritma embedded zerotree wavelet dan algoritma Huffman,,," transpormasi wavelet, Embedded Zerotree Wavelet komperesi data,system embedded, ","Agfianto Eko Putro, Dr., M.Si",2,3,0,2015,1," Sebagai bentuk representasi data, kendala yang dihadapi ketika menggunakan citra digital adalah besarnya volume data yang dibutuhkan untuk merepresentasikan citra tersebut. Untuk itu dibutuhkan suatu teknik yang dapat mengecilkan volume data, yaitu teknik kompresi. 
       Pada skripsi ini dipilih metode transformasi wavelet sebagai salah satu metode yang bisa digunakan untuk pemampatan data citra. Wavelet yang digunakan adalah embedded zerotree wavelet (EZW). Selanjutnya algoritma
tersebut diimplementasikan dengan menggunakan perangkat lunak matlab sebagai alat bantu pemrograman untuk melihat pengaruh kualitas pada citra yang dimampatkan. 
       Hasil yang diperoleh dari penelitian ini adalah sebuah aplikasi pemampatan menggunakan EZW. Adapun parameter-parameter yang diteliti yakni ukuran citra sebelum dan sesudah dimampatkan, rasio citra termampatkan serta waktu proses pemampatan. Dari hasil penelitian diketahui bahwa pemampatan citra dengan menggunakan algoritma EZW mampu memampatkan lebih banyak ukuran citra. Adapun rata-rata rasio pemampatan pada algoritma EZW 99%, akan tetapi jika dilihat dari waktu pemampatan makan algoritma EZW lebih banyak membutuhkan waktu untuk proses pemampatan citra.","      As a form of data representation, obstacles encountered when using the digital image is the large volume of data required to represent the image. That requires a technique that can decrease the volume of data, the compression technique.
      In this paper, wavelet transform method selected as one of the methods that can be used for image data compression. Wavelet used is embedded zerotree wavelet (EZW). Furthermore, the algorithm is implemented using matlab software programming as a tool to see the effect on the quality of the compressed image.
      The results obtained from this study is an application using EZW compression. The parameters studied the size of the image before and after compressed, compressed image ratio as well as time compression process. The survey results revealed that the image compression using EZW algorithm is able to compress more the size of the image. The average compression ratio of 99% EZW algorithm, but when seen from the time compression eat more EZW algorithm takes time to process the image compression.
      
",,,30275,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
82835,,ZATUT MARYATI,SISTEM PAKAR PERTOLONGAN PERTAMA PADA PENDERITA GAWAT DARURAT BERBASIS MOBILE,,,"sistem pakar,android,pertolongan pertama,gawat darurat","Faizah, S.Kom, M.Kom",2,3,0,2015,1,"       Pelayanan kesehatan untuk kondisi gawat darurat  adalah hak setiap orang dan kewajiban yang harus dimiliki setiap orang. Sedangkan setiap orang belum tentu punya pendidikan melakukan penanganan kondisi gawat darurat yang harusnya dilakukan setiap orang yang menemui kondisi tersebut. Permasalahan yang  muncul adalah terbatasnya jumlah, waktu dan tenaga dari seorang dokter  sebagai pakar yang memiliki ilmu tentang penanganan kondisi gawat darurat yang tepat. Untuk itu seorang dokter membutuhkan pendamping atau asisten dalam menangani kondisi gawat darurat, sehingga kebutuhan penderita gawat darurat untuk mendapatkan pelayanan medis yang lebih baik dapat segera terpenuhi. 
Tujuan dari penelitian ini adalah membangun sistem pakar untuk membantu masyarakat melakukan penanganan yang tepat pada penderita gawat darurat. Pengguna  akan mendapatkan pengetahuan tentang tahap-tahap yang harus dilakukan dalam melakukan penanganan ketika menjumpai penderita gawat darurat. Sistem pakar ini menggunakan metode penelusuran dalam mesin inferensi yaitu pelacakan mundur (forward chaining), sedangkan untuk metode representasi menggunakan kaidah produksi untuk merepresentasikan pengetahuan tentang jenis-jenis kategori kondisi gawat darurat dan tahap penanganan yang tepat. Sistem pakar ini dibagi menjadi dua sistem yang pertama adalah sistem untuk admin yaitu web admin dan aplikasi untuk pengguna sistem yaitu mobile Android.
Hasil uji dari sistem pakar ini adalah sistem mampu memberikan solusi tahap penanganan yang dapat dilakukan pada kondisi gawat darurat. Terdapat pula penjelasan tambahan yaitu tentang gejala dan tanda serta peringatan dari kategori gawat darurat yang dipilih.

Kata kunci : sistem pakar, android, pertolongan pertama, gawat darurat
","	Medical aid for emergent condition is every one's right and obligation. However, every individual does not necessarily have the education of medical aid to face critical condition. The problem emerges concerning the limited time and number of doctor as an expert who masters the exact medical aid for emergency or critical condition. Therefore, a doctor needs an assistant in face such emergency so that the victim's need of better medical aid can be fulfilled.
  	The purpose of this research is to develop an expert system to help people do the correct handling of emergency treatment. Users will gain knowledge about the stage - stage should be done in handling the emergency room when the patient encounter. This expert system applies a searching method in inferential machine i.e. forward chaining, while representation method uses production method to presents the information on the categories of emergent condition and the exact aid method. This expert system is divided into two system, the first the system for admin i.e. admin web and aplication for Android user. The result of this study is the application of expert system that can help victim of emergency.
	The test results of this expert system is a system capable of providing a solution treatment stage to do in emergency conditions . There is also an additional explanation is about the symptoms and signs and warnings of emergency selected category .

 
Keywords: expert system, android, first aid, emergency.
",,,33006,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81812,,ADITYA MARDIKA NUGRAHA,IMPLEMENTASI DAN ANALISIS KINERJA KLASTER CUBIEBOARD 3 SEBAGAI FILE SERVER,,,"klaster, cubieboard, file server, LVS, load balancing","Bambang Nurcahyo Prastowo, Drs., M.Sc ",2,3,0,2015,1,"Server sebagai penyedia lokasi akses disk bersama menjadi salah satu faktor yang memegang peranan penting dalam proses berbagi data dan tukar menukar informasi. Perubahan data yang berjalan cepat serta peningkatan jumlah akses pada server menuntut tersedianya server yang handal. Salah satu alternatif memiliki server handal adalah dengan membangun suatu server menggunakan metode klaster. Cubieboard merupakan komputer processor ARM cortex dengan ukuran sedikit lebih besar dari raspberryPi yang pada penerapannya dapat diklasterkan. Oleh karena itu dibuat komputer klaster dari cubieboard yang dilihat kinerjanya sebagai file server.
 Arsitektur klaster cubieboard sebagai file server dibangun dengan menggunakan konsep load balancing sehingga proses transfer data dapat terbagi secara merata ke semua cubieboard dengan teknologi linux virtual server serta dibangun juga media penyimpanan paralel pada klaster cubieboard menggunakan glusterFS. Hasil penelitian berupa perbandingan kinerja file server pada proses transfer data dalam bentuk nilai throughput dan rata-rata persentase beban kerja cpu berdasarkan jumlah cubieboard dan tipe media penyimpanan paralel.","A server as a provider of together disk access location is to be one of factor that plays important role in processing data sharing and information exchange. Running data changes rapidly and increasing access amount on server requires availability of a reliable server. One of the alternatif in having reliable server is by building a server with cluster method. Cubieboard is computer with ARM cortex processor whose size is little bigger than raspberryPi which can be clustered in its application. Therefore, it is made clustered computer using cubieboard seen its performance as the file server.
Cubieboard cluster architecture as file server is built using load balancing concept so the process of data transfering can be devided evenly to all cubieboard by server linux virtual technology and also built a parallel storage media on cubieboard cluster using glusterFS. The result of this research is a comparision of the file server performance on data transfering process in the form of throughput value and percentage average of CPU's workload based on the cubieboard amount and parallel storage media type.",,,31797,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81813,,RIZQI BAYU ANGGARA,ANALISIS KLASTER CUBIETRUCK SEBAGAI VIDEO STREAMING SERVER,,,"Klaster, Cubietruck, Video Streaming Server, Darwin Streaming Server","Bambang Nurcahyo Prastowo Drs., M.Sc",2,3,0,2015,1,"Melimpahnya konten multimedia digital dan perkembangan kebutuhan online video memotivasi perkembangan video streaming server. Umumnya model streaming media menggunakan single server sehingga mengakibatkan layanan paralel yang rendah dan kapasitas yang buruk. Peningkatan performa layanan dilakukan dengan menandai task ke node yang berbeda dengan mekanisme load balancing. Cubietruck merupakan minikomputer berdaya rendah serta memiliki kecepatan proses 1GHz dan RAM 2GB, sehingga dapat dibangun menjadi video streaming server dengan arsitektur load balancing cluster sehingga dapat membagi beban kerja ke beberapa node.
Cubietruck dibangun menjadi arsitektur klaster load balancing menggunakan file system LVS, dimana satu buah berperan sebagai director dan tiga buah berperan sebagai real server. Pengujian menggunakan parameter cpu usage, throughput, bitrate, dan  packet loss. Dstat digunakan untuk memonitoring setiap unit real server, mendapatkan nilai cpu usage dan throughput. OpenRTSP digunakan untuk menghasilkan request, mendapatkan nilai packet loss dan bitrate. Hasil penelitian menunjukkan bahwa pada setiap penambahan node dapat meningkatkan performa sebesar 100% terhadap server cluster 2 node.","The abundance of multimedia content and increase of online video needs to motivate development of video streaming server. Generally streaming media model using single server so resulting low parallel service and poor capacity. Improving of performance service is done by assign task to different node with load balancing machanism. Cubietruck is a minicomputer with low power and 1GHz processing speed and 2GB of RAM, so it can to develop to be a streaming video server with load balancing architecture so it can divide workload into some node.
Cubietruck is built to be load balancing cluster using LVS file system, where the one to be director and three other to be real server. The test using cpu usage, throughput, bit rate, and packet loss parameters. Dstat is used to monitor each unit of real server, to get the value of cpu usage and throughput. OpenRTSP is used to generate the request and to get packet loss and bit rate. The results show that on each additional node can improve performance 100% to server cluster 2 node.",,,31779,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
98207,,MUHAMMAD BAGUS RAMAD,ANALISIS PERFORMA RASPBERRY PI THIN CLIENT MENGGUNAKAN METODE SLOW-MOTION BENCHMARKING,,,"thin-client performance, slow-motion benchmarking raspberry pi","Triyoga W. Widodo, S.Kom, M.Kom",2,3,0,2015,1,"Kebutuhan terhadap penggunaan komputer semakin meningkat dengan semakin tingginya perkembangan teknologi. Salah satu alat bantu penggunaan komputer adalah thin-client. Dengan thin-client, kita tidak diharuskan mengeluarkan biaya besar untuk dapat menggunakan komputer. Hanya dengan perangkat yang sederhana dan koneksi jaringan, kita sudah dapat menggunakan
komputer. Salah satu perangkat yang dapat dijadikan sebagai thin-client adalah Raspberry Pi.
Pada penelitian ini digunakan metode Slow-Motion Benchmarking untuk menghitung kinerja dari 2 buah sistem operasi thin-client pada platform Raspberry Pi. Parameter utama dalam pengujian ini adalah memberikan jeda atau delay pada tiap operasi yang dilakukan sehingga semua operasi yang diminta dapat diterima secara utuh oleh thin-client setelah diproses oleh server tanpa adanya proses yang ditutupi oleh proses lainnya. Penelitian ini melibatkan 3 buah pengujian, yaitu pengujian web text page load performace, web image page load performance, dan video playback performance.
Dari hasil pengujian diperoleh bahwa kinerja pada masing-masing thin- client memiliki perbedaan yang cukup signifikan. Hal ini disebabkan oleh perbedaan pada model protokol yang digunakan,  dan model pengolahan informasi grafis yang dilakukan oleh masing-masing thin-client.","The needs of computer usage have increased with the growth computer technology. One thing that help us to use a computer is thin-client. Using thin-client, we don&Atilde;&cent;&iuml;&iquest;&frac12;&iuml;&iquest;&frac12;t have to spend a lot of money to use a computer. With a simple hardware component and a network connection, we can use a computer. One device that can be used as a thin-client computer is Raspberry Pi.
This research use slow-motion benchmarking method to calculate performance of 2 thin-client operating system on Raspberry Pi platform. The main parameter of this method is using delay between operation performed so that every operation requested can be received by the thin-client completely after being processed in the server with no operation that is overwrited by other operation. This research involved 3 test, they are web text page load performance test, web image page performance test, and video playback performance test.
The test shows that there is a huge difference in the performance test result of each thin-client operating system. The difference in performance test result is caused by the difference of the protocol used and the graphic information processing method of each thin-client operating system.",,,48540,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
90785,,RADHWA DHIYA'RAZANI,IMPLEMENTASI XML POOLING DENGAN MENGGUNAKAN ARSITEKTUR RESTful  UNTUK PERTUKARAN DATA,,,"Kata Kunci : XML Pooling, Web Service, XML, RESTful, Metadata, Parameter Big Data.","Mardhani Riasetiawan, M.T.",2,3,0,2015,1,"Pertumbuhan data yang kian meningkat, membuat pelaku bisnis ataupun organisasi mencari strategi yang tepat untuk mengolah data yang dimiliki, agar memberikan manfaat dan menghasilkan informasi yang bisa digunakan untuk keperluan organisasi. Data digital memiliki metadata yang mampu memberikan informasi mengenai data itu sendiri, serta dapat membantu memahami data yang ada. Dengan pengelolaan metadata, dapat dilakukan pengembangan metadata untuk keperluan pemetaan elemen metadata data ke dalam parameter Big Data.
XML Pooling menyediakan resource dalam format XML yang menampung informasi hasil ekstraksi metadata, dan juga hasil pemetaan elemen metadata ke dalam parameter Big Data. Untuk keperluan manajemen data, terdapat bagian yang menangani proses ekstraksi metadata hingga pemetaan ke Big Data. Pemetaan ke dalam parameter Big Data dilakukan melalui uji sample, kemudian mengambil elemen metadata hasil ekstraksi yang sesuai dengan definisi setiap parameter Big Data (match by definition). Sehingga, diperoleh elemen metadata yang sesuai untuk pemetaan ke Big Data.
Implementasi XML Pooling menggunakan teknologi web service dengan arsitektur RESTful, digunakan untuk pertukaran data antar sistem, yang memberikan manfaat untuk penyebarluasan konten ekstraksi metadata dan hasil pemetaan ke dalam parameter Big Data. Kemudian dilakukan pengujian dengan membangun sistem client untuk menguji pertukaran data. Hasil yang diperoleh bahwa sistem client mampu mengirim request dan menerima response dengan media XML. Response yang diperoleh dapat ditampilkan kembali pada web client.
","The increasing growth of the amount of data makes businesses and organizations look for the appropriate strategy to manage the data they possess, so that the data can turn into information that fulfils the organization&acirc;€™s needs. Digital data contain metadata, which can assist users in understanding the details about the data themselves. It is by managing metadata that it is possible to develop metadata in order to map the elements of the metadata into Big Data parameters.
XML Pooling provides resources in the form of XML, which holds the information of metadata extraction, and the result of mapping into Big Data parameters. For the purpose of data management, there is a part that organizes the metadata extraction process up to mapping into Big Data. The mapping process was carried out through data sample testing, which is followed by the process of collecting metadata extracts that match every Big Data parameter&acirc;€™s definition so that the appropriate metadata element for the purpose of mapping into Big Data parameters can be obtained.
The implementation of XML Pooling, which uses web service technology based on RESTful architecture, can be used for data interchange, delivering benefits for the distribution of the contents of both metadata extraction and the result of mapping into Big Data parameters. Afterwards, the client systems were built to test data interchange. The obtained result is that the client systems were able to send requests and receive responses through XML as a medium, and the received responses could be displayed on web client.
",,,40732,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
90786,,PANDU WICAKSONO,Analisis Sistem Rekomendasi Collaborative Filtering Pada Data Ternormalisasi,,,"Sistem rekomendasi, Collaborative Filtering, Metode similarity, Normalisasi, Akurasi","Lukman Heryawan, S.T., M.T",2,3,0,2015,1,"Pertumbuhan yang sangat pesat pada volume dan keragaman informasi yang tersedia di web dewasa ini menimbulkan masalah baru bagi user yaitu semakin susahnya mencari konten yang relevan dengan kebutuhan user. Untuk itu dibutuhkan suatu sistem yang dapat merekomendasikan konten yang sekiranya relevan bagi user. Salah satu algoritme pada sistem rekomendasi yang populer adalah Collaborative Filtering (CF). Algoritme CF sering dipilih sebagai algoritme rekomendasi karena hasil rekomendasinya yang lebih personal sehingga dapat memberikan rekomendasi yang berbeda untuk tiap user sesuai dengan selera masing-masing user.
Berbagai upaya peningkatan akurasi algoritme CF sering dilakukan, salah satunya adalah dengan normalisasi. Dalam penelitian ini akan diterapkan sistem rekomendasi menggunakan dua algoritme CF yaitu user-based dan item-based CF dengan tiga metode perhitungan similarity yang berbeda yaitu cosine similarity, pearson correlation, dan adjusted cosine. Kemudian variasi algoritme ini diuji dan dibandingkan performanya bila diterapkan pada data sebelum dan sesudah dinormalisasi dengan modified standard score. Dataset yang digunakan adalah dataset MovieLens yang berasal dari situs rekomendasi film movielens.org milik tim riset GroupLens.
Hasil penelitian ini menunjukkan bahwa normalisasi data dengan modified standard score meningkatkan akurasi pada seluruh variasi algoritme item-based CF dengan rata-rata peningkatan akurasi terbesar sebesar 3,63% sedangkan pada algoritme user-based CF hanya terjadi peningkatan pada metode adjusted cosine saja. Akurasi tertinggi pada data sebelum dan sesudah dinormalisasi dihasilkan oleh algoritme user-based CF dengan metode adjusted cosine. Terbukti user-based CF mampu menghasilkan akurasi yang lebih tinggi daripada item-based CF.
","The rapid growth in volume and variety of information found on the web has its own downside: finding relevant information become somewhat more difficult. This problem has led to the development of recommender systems which filters information to the likings of a certain user. One of the most popular algorithm used for recommendation systems is Collaborative Filtering (CF). CF is widely used because it delivers a more personalized recommendation thus producing different recommendations for different users based on their preference.
There are various efforts that can be done to increase CF prediction accuracy, one of them is through data normalization. This research will apply two CF algortihms, user-based  and item-based CF with three different similarity measurement method for each algorithm. The similarity measurement methods used are cosine similarity, pearson correlation, and adjusted cosine similarity. These various algorithms performance will then be tested and compared on data both with and without normalization. The normalization method used in this research is modified standard score which will be applied on the MovieLens dataset from the movie recommendation website movielens.org developed by the GroupLens research team. 
Result shows that data normalization with modified standard score improved the accuracy of all variations of the item-based CF algorithm with the highest improvement of 3,63%, but only managed to improve user-based CF just on the adjusted cosine method. The highest accuracy both before and after data normalization was achieved by user-based CF with the adjusted cosine similarity method. This proves that user-based CF can produce better accuracy than item-based CF.",,,40770,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
78755,,ALVISTA RIVIERA,"ANALISIS WEB RESPONSIF UNTUK PENINJAUAN FILE PADA PERANGKAT DESKTOP, TABLET DAN SMARTPHONE",,,"File Preview, Viewer.JS, Google Docs Viewer, HTML Element, Desktop, Tablet, Smartphone, CodeIgniter","Mardhani Riasetiawan, M.T",2,3,0,2015,1,"Web responsif dapat menampilkan konten yang beragam yang tampilannya
dapat menyesuaikan ukuran layar perangkat yang menampilkannya. Salah satu
konten tersebut adalah peninjauan file. File yang ditampilkan pada saat melakukan
peninjauan file dapat ditampilkan secara berbeda di setiap perangkat. Perbedaan
tersebut terjadi karena tools yang digunakan untuk menampilkan file-file tersebut
berbeda.
Pada penelitian ini dilakukan pengumpulan data terlebih dahulu untuk
pengujian yaitu file-file yang akan diuji. File-file tersebut adalah file pdf, jpg, png,
bmp, gif, doc, docx, xls, xlsx, ppt, pptx, odt, ods, odp, txt, mp4, xml, csv, html dan
htm. File tersebut akan diuji responsifitasnya terhadap perangkat yang
mengaksesnya yaitu desktop, tablet dan smartphone. Tingkat responsifitas dilihat
dari adaptasi file terhadap perangkat yang mengaksesnya, yaitu teks yang mudah
dibaca tanpa harus menggeser halaman dan gambar yang secara otomatis
ukurannya berubah mengikuti perangkat yang mengaksesnya. Teori tersebut
digunakan sebagai acuan pengujian yang dilakukan pada sistem yang dapat
menampilkan file secara responsif. Sistem dibangun menggunakan framework
CodeIgniter yang dapat melakukan pengunggahan file, penyajian file, dan
pencarian file.
Dari hasil penelitian dan pengujian yang dilakukan file yang dapat
ditampilkan secara responsif di semua perangkat yaitu desktop, tablet dan
smartphone baik landscape maupun portrait adalah file jpg, bmp, png, gif, mp4, txt,
htm dan html. File yang dapat ditampilkan secara responsif di perangkat desktop,
tablet dan smartphone berorientasi landscape namun tidak pada smartphone
berorientasi portrait adalah pdf, odt, ods, odp, docx, doc, xlsx, xls, pptx, ppt dan
xml. File yang dapat tampil responsif di perangkat desktop dan tablet namun tidak
pada smartphone adalah file csv.","Responsive web displays variety of contents that can adjust the device
screen size. One of the contents is file preview. File may be displayed differently in
each device when doing a file preview. The difference occur because the tool that
is used to display each file was different.
In this research, the data which are pdf, jpg, png, bmp, gif, doc, docx, xls,
xlsx, ppt, pptx, odt, ods, odp, txt, mp4, xml, csv, html and htm files are collected
for testing. That files will be tested and analyzed for the responsive changes that
appear when displayed on desktop, tablet and smartphone. File is proved to be
responsive with the adaptations of the file to the device that open it, which are the
text is readable without scrolling the page and the picture width is adjustable with
the device that open it. The theory is used for proving the responsiveness testing
that is done in a system that can display the file responsively. The system was built
using the CodeIgniter framework that can upload files, preview files, and search
files.
The results of the testing process show that jpg, bmp, png, gif, mp4, txt, htm
and html files can be displayed responsively on desktop, tablet and smartphone.
Files that can be displayed on desktop, tablet and smartphone in landscape mode
but not on smartphone in portrait mode are pdf, odt, ods, odp, docx, doc, xlsx, xls,
pptx, ppt dan xml. And File that can be displayed on desktop and tablet but not on
smartphone is csv.",,,28618,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
86179,,RIEFKA SONCA,Implementasi Algoritma Apriori dan Algoritma CT-Pro pada Market Basket Analisys,,,"Data Mining, frequent pattern, Apriori, CT-Pro, Compressed Tree, FP-Growth, Algoritma Association rule","Agus Sihabuddin, S.Si., M.Kom",2,3,0,2015,1,"Semakin maraknya ritel modern tentu saja menimbulkan persaingan sesama ritel modern tersebut. Para pemilik ritel harus saling bersaing satu sama lain dalam mencari celah untuk memperoleh pelanggan, sehingga pemilik ditekan untuk melakukan improvement dengan berbagai cara. Salah satu metode yang dapat dilakukan adalah dengan mengolah data transaksi untuk menemukan kecendrungan-kecendrungan yang lazim terjadi. Salah satu metode efektif untuk dapat menemukan kecendrungan adalah dengan menggunakan data mining. 
Algoritma Apriori merupakan metode dasar untuk melakukan data mining. Prinsip yang digunakan algoritma Apriori adalah jika sebuah itemset sering muncul, maka seluruh subset dari itemset tersebut juga harus sering muncul (Apriori property). Dengan melakukan pengecekan yang berulang-ulang, maka waktu yang dibutuhkan untuk menyelesaikan metode akan berlipat ganda. Sebuah metode diusulkan untuk dapat memecahkan masalah yang dimiliki oleh metode Apriori, yaitu dengan menggunakan algoritma Tree. Harapannya adalah dengan menggunakan Compressed Tree maka penggunaan waktu dapat ditekan, karena tidak diperlukan pengecekan secara berulang terhadap subset dari setiap itemset yang frequent.
Pada penelitian yang dilakukan, metode Compressed Tree terbukti dapat memecahkan masalah waktu eksekusi dengan sangat baik. Waktu eksekusi yang dibutuhkan relatif jauh lebih cepat dengan hasil aturan asosiasi yang sama yang dihasilkan oleh metode Apriori. Metode Compressed Tree yang juga memiliki sifat anti-monotone property disebut-sebut sebagai sebuah metode yang lebih baik dibandingkan dengan metode Apriori. 
","The increasingly widespread of modern retailing lead to the fierce competition among the modern retail. The owner of retail have to compete each other to gain customers, so the owners are forced to perform improvement in many aspects. One of improvement that can be done is to process the transaction data to find out the trend which prevalent to occur. One effective method which can be used to find the trend is by using data mining method.

Apriori algorithm is the basic method to perform data mining. The principal of Apriori algorithm is if the itemset are often arises, then the whole subset of the itemset must also be frequently appears (Apriori property). By performing repetitive checking, then the time required to complete the method will be doubled. A method is proposed to solve the problem of Apriori method, which is by applying Tree algorithm. It is expect that by applying Compressed Tree the completion time can be reduced. It can be done since it is not necessary to repetitively check the subset of each frequent itemset.

In the research conducted, proven that compressed Tree method were excellent to solve the problem of execution time. The execution time required were relatively much faster for the same result of association rules generated by Apriori method. Compressed Tree method also has anti-monotone property characteristic which is touted as a better method compared to Apriori method.
",,,36225,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
90788,,ALIF ANANTO,ANALISIS KINERJA SUMBER DAYA UNTUK RENDERING PADA XENSERVER,,,"virtual data center, virtual machine, XenServer, CPU usage, memory usage, rendering file  ","Mardhani Riasetiawan, M.T",2,3,0,2015,1,"Peran dari virtual data center telah berkembang seiring dengan perkembangan teknologi informasi dan komunikasi, dimana hal tersebut menjadi awal terbentuknya cloud computing (komputasi awan). Oleh sebab itu dalam pembangunan dan pengoperasian virtual data center perlu memperhatikan server performance, server capacity planning, dan load balancing yang akan mempengaruhi kinerja virtual data center itu sendiri. Maka dibutuhkan pengalokasian kinerja sumber daya berupa processor dan memory yang disesuaikan dengan kebutuhan aplikasi serta aktivitas user, sehingga kinerja virtual data center menjadi optimal. Selanjutnya untuk melihat seberapa tinggi optimalisasi virtual data center perlu dilakukan pengujian beban.
Pada penelitian ini dilakukan pengujian terhadap kemampuan dari sebuah virtual data center yang pembagian alokasi sumber dayanya dedicated dan dynamic dengan menggunakan XenServer versi 6.5. Pengujian yang dilakukan berupa pemberian beban rendering image sebanyak 11 file serta rendering animasi sebanyak 3 file, dengan menggunakan aplikasi Blender versi 2.75. Parameter yang digunakan adalah CPU usage dan memory usage.
Dari hasil pengujian, dilakukan perbandingan rata-rata CPU usage dan memory usage yang terpakai pada masing-masing skenario. Berdasarkan data pengujian menunjukan virtual machine dengan alokasi sumber daya secara dynamic lebih optimal ketika diberikan beban rendering file image, sedangkan virtual machine dengan alokasi sumber daya secara dedicated lebih optimal ketika diberikan beban rendering file animasi.
","The role of the virtual data center has evolved along with the development of information and communication technology, which became the beginning of the formation of cloud computing (cloud computing). Therefore, in the development and operation of virtual data centers need to take notice of server performance, server capacity planning and load balancing that will affect the performance of the virtual data center itself. Then it takes the form of resource performance assignment processor and memory are tailored to the needs of the application as well as the activity of the user, so that the performance of the virtual data center becomes optimized. Next to see how high the efficiency of virtual data center load testing needs to be done.
On the research of the testing was performed against the ability of a virtual data center a division dedicated its resources allocation and dynamic by using XenServer version 6.5. Testing is done in the form of granting a total of 11 image rendering load file and rendering the animation as much as 3 files, using the Blender application version 2.75. The parameters used are CPU usage and memory usage.
From the test results, carried out a comparison of the average CPU usage and memory usage that is used in each scenario. Based on test data shows virtual machine with dynamic allocation of resources more optimally when given the load image file rendering, while the virtual machine with the allocation of dedicated resources be optimized when given the burden of rendering animated files.
",,,40771,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
87205,,ANNI KARIMATUL F,SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN PENERIMA BANTUAN REHABILITASI RUMAH DENGAN METODE FUZZY-SAW,,,"Decision Support System, SAW, Fuzzy Logic, Java, MySQL","Faizah Skom, M.Kom",2,3,0,2015,1,"Proses pemilihan penerima bantuan rehabilitasi rumah merupakan program wajib tahunan yang selalu dilakukan Dinas Sosial DIY Subbidang Bantuan dan Jaminan Sosial. Proses pemilihan masih dilakukan secara manual dan membutuhkan waktu yang lama untuk melakukan proses evaluasi karena kebutuhan data masih manual berupa berkas-berkas yang tidak sistematis.

Oleh karena itu penelitian ini dirancang untuk menyediakan suatu aplikasi dekstop Sistem Pendukung Keputusan Pemilihan Penerima Bantuan Rehabilitasi Rumah. Sistem ini mengaplikasikan penilaian dengan 2 tahap, yaitu tahap 1 dengan metode fuzzy , dan penilaian tahap 2 dengan metode SAW. Metode fuzzy digunakan karena memiliki toleransi terhadap data-data yang tidak tepat serta konsepnya yang sederhana dan fleksibel. Metode SAW digunakan untuk perangkingan data hasil akhir dengan mempertimbangkan bobot kriteria baik kriteria benefit maupun cost. Sistem Pendukung Keputusan Pemilihan Penerima Bantuan Rehabilitasi Rumah ini dibangun menggunakan bahasa perograman Java dan MySQL. 

Hasil dari penelitian ini yaitu hasil penilaian tahap 1 berupa data lolos tidaknya peserta ke penilaian tahap 2 yaitu berupa bobot preferensi beserta rangking yang diperoleh berdasarkan nilai preferensi tertinggi dimana nilai preferensi dihitung dengan menggunakan metode SAW. 

","Selection of home rehabilitation is an annual mandatory program of Dinas Sosial DIY. The election process is still done manually and requires a long time to carry out the evaluation process because of the need for data is still manual in the form of files that are not systematic.

Therefore, this study was designed to provide a desktop application Decision Support System Selection of Recipient Rehabilitation Home. This system applies to the assessment 2 stages: stage 1 with fuzzy methods, and assessment phase 2 with the SAW method. Fuzzy method is used because it can tolerate data that is not appropriate, and the concept is simple and flexible. SAW method is used for ranking the final outcome data by considering the weighting of criteria both benefit and cost criteria. Application was built using the Java language and MySQL.

Results from this study is the result of one form of data assessment phase escapes least be participants to the second stage of assessment in the form of weight and their preference rankings obtained by which the value of highest preference preference value calculated using the SAW.

",,,37275,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
90027,,YULINRI TRI MADYA,Pemetaan Big Data dengan Implementasi Aggregation Pipeline pada MongoDB untuk Pembentukan Datamart,,,"metadata, ektraksi, datamart,aggregation pipeline, basis data nonrelasional","Mardhani Riasetiawan, M.T",2,3,0,2015,1,"Data digital memiliki metadata yang mampu memberikan informasi mengenai data itu sendiri. Berawal dari pengelolaan metadata, bisa diperoleh banyak informasi yang dapat disajikan dan dikembangkan. Metadata sebagai hasil ekstraksi dari sebuah data selanjutnya dianalisis kembali untuk kemudian melalui proses pemetaan dengan 5 karakteristik Big Data (Variety, Volume, Velocity,Veracity, dan Value).

Penelitian ini akan berfokus pada pemetaan parameter big data dengan menggunakan metode aggregation pipeline. Pemetaan elemen big data akan dilakukan berdasarkan kesesuaian definisi objek parameter hasil ekstraski metadata dengan definisi 5 karaketristik yang dimiliki big data. Setiap file dalam system akan diekstraksi dan hasil ekstraksi akan menghasilkan objek-objek parameter.
Kumpulan objek hasil ekstraksi ini kemudian di agregasi menggunakan metode aggregation pipeline yang diimplementasikan di dalam basis data mongoDB, lalu
disimpan ke dalam basis data non-relasional. Informasi dari proses diatas akan disajikan dalam bentuk tampilan datamart. 

Berdasarkan hasil pengujian, telah berhasil dikembangkan sebuah metode , yaitu Aggregation Pipeline yang mampu mendukung pemetaan parameter big data
by definition dan kemudian disajikan dalam bentuk data mart, sesuai kebutuhan sistem yang telah dirancang. Aplikasi ini mampu menyimpan, mengelola dan menampilkan berbagai format file, elemen big data yang terkandung pada file serta metadata preservasinya. Adapun fitur-fitur yang terdapat pada aplikasi ini adalah upload file, download, file, explore file, searching dan pengelolaan user.","A digital data has metadata element that presents information about the data itself. Started from metadata management, it can be obtained a lot of information that can be served and developed. One of them is metadata development as the way of managing data in the era of Big Data nowadays. This research will be focused on mapping big data as element by using agregation pipeline method. Mapping elements big data will be conducted by virtue
of conformity definition object parameter, the result of FITS metadata extraction, with definition from five of Big Data characteristic. 

Every file which processed in system will be extracted and the outcome from the extraction will generate some object parameters. Assambleged of objects extraction will passed to aggregation process which using aggregation pipeline method, and will be stored in a nonrelational database. Information from the process will be presented in the form of datamart display.

Based on the test result, Implementation of aggregation pipeline method has been succesfull developed, wich capable to supporting mapping Big Data parameter based on defifnition and then presented in datama mart concept, according to the needs that system has been designed. This application is able to store, manage and display a variety of file formats and it is contained with big data element and
preservation metadata. Meanwhile features contained in the system is upload file, download file, explore file, searching, and user management.",,,40049,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
77746,,M. ARIZA RAMADITIA RAMADHAN,PEMBANGUNGAN EROTIC VIDEO CLASSIFIER DENGAN MENERAPKAN NDA (NUDITY DETECTION ALGORITHM),,,"Deteksi Video Erotis, NDA, Frame Acak, Erotic Video Detection, NDA, Random Frames","Janoe Hendarto, Drs., M.Kom",2,3,0,2015,1,"Sekarang ini, mengendalikan persebaran video erotis sangatlah sulit. Hal ini dikarenakan mudahnya untuk mendapatkan akses internet. Namun, usaha untuk mengendalikan persebaran video erotis pun mulai banyak bermunculan.
Penelitian ini bertujuan untuk membangun sebuah penggolong video erotis dengan menerapkan algoritma NDA (Nudity Detection Algorithm). Proses penggolongan diawali dengan pengambilan beberapa frame acak dari video yang akan digolongkan, kemudian ditentukan apakah semua frame tersebut adalah frame erotis atau bukan dengan menggunakan NDA. Bila persentase frame erotis yang tedeteksi telah mencapai batas yang ditetapkan, maka video tersebut akan digolongkan sebagai video erotis.
Penelitian ini akan diimplementasikan dalam C++ dan berjalan di Windows OS. Sistem pun akan berjalan tanpa menggunakan basis data. Dari hasil pengujian, metode ini memiliki nilai akurasi sebesar 91.67% dan false positive sebesar 10.33%. Dengan membandingkan hasil pengujian terhadap penelitian sebelumnya, metode yang diajukan dalam penelitian ini terbukti cukup akurat dalam melakukan penggolongan video.","It has been very hard to control the spread of erotic video nowadays, mainly because people can get internet access with ease. But, the number of attempts to control the spread of erotic video also has been started to increase.
This research aimed to build an erotic video classifier by implementing NDA (Nudity Detection Algorithm). The classifying process started by choosing some random frames from the video being classified, then all the frames were classified whether it was considered erotic frame or not with NDA. If the detected erotic frame percentage had reached a predefined threshold, then the video would be classified as erotic video.
This research was implemented in C++ and ran in Windows OS. The system also ran without any database. Experiment result showed that this method had 91.67% accuracy rate and 10.33% false positive rate. By comparing the experiment result with the previous research, the proposed method in this research had been proven quite accurate in classifying video.",,,27872,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92851,,HERO SATRIYA SUPANGAT,ANALISIS IMPLEMENTASI ALGORITMA CLUSTERING SQUEEZER PADA SISTEM REKOMENDASI ITEM-BASED COLLABORATIVE FILTERING,,,"Sistem rekomendasi, collaborative filtering, clustering, squeezer","Lukman Heryawan, S.T., M.T.",2,3,0,2015,1,"Jumlah data dan keragaman informasi data yang sangat besar memunculkan masalah tentang pemilihan data yang relevan bagi user. Untuk itu perlu dibuat satu sistem yang mampu merekomendasikan data. Sistem rekomendasi menggunakan algoritma item-based collaborative filtering adalah salah satu sistem yang mampu memberikan rekomendasi data berdasarkan prediksi dari nilai kemiripan antar item.
Metode clustering untuk mengelompokan data yang mirip antara satu sama lain adalah salah satu cara untuk meningkatkan akurasi dan jumlah rekomendasi. Proses clustering pada penelitian ini diterapkan penggunaan clustering menggunakan algoritma squeezer. Penerapan clustering akan dilakukan pada dataset dari MovieLens yang akan diolah untuk menghasilkan rekomendasi dengan
menggunakan algoritma item-based collaborative filtering, kemudian dihitung menggunakan Mean Absolute Error (MAE) sebagai indikator akurasi dan jumlah rekomendasi yang dihasilkan dan dibandingkan dengan hasil rekomendasi sistem yang tidak menggunakan clustering.

Hasil penelitian ini menunjukan bahwa implementasi clustering dengan algoritma squeezer meningkatkan jumlah prediksi dengan nilai rata-rata kenaikan 3,563 % dengan kecenderungan semakin besar nilai threshold pada proses
pembentukan cluster, jumlah rekomendasi semakin meningkat dan penurunan akurasi dengan nilai rata-rata 0.76% dengan kecenderungan semakin besar nilai threshold pada proses pembentukan cluster, nilai MAE yang dihasilkan semakin meningkat.","The amount of data and the diversity of a very large data information gave rise to new problems for the user, namely the difficulty of retrieving relevant data for the user. Recommendation system using an algorithm of the item-based collaborative filtering is one of the system that are capable of providing recommendations data based on predictions from value similarity between items.

Using clustering method to group similar data between each other is one way to improve the accuracy and number of recommendations. In this research will be applied to the use of clustering algorithm with a Squeezer. Application of clustering will be conducted on MovieLens dataset that will be processed to produce recommendations by using item-based collaborative filtering algorithm which then calculated by Mean Absolute Error (MAE) as an indicator of accuracy
and the number of recommendations that are generated and compared with the results of the recommendation without clustering.

The results of this research show that the implementation of clustering algorithm with squeezer increases the number of predictions with the average increase in value of 3.563% with the greater tendency of value of the threshold on
the process of the formation of the cluster, the number of recommendations has increased and decreased accuracy with an average rating of 0.76% with the tendency of the larger value of the threshold on the process of the formation of clusters, value of MAE generated increases.",,,43027,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
91841,,AGUNG NURDIYANTO,Klasifikasi Jenis Penyakit Kanker Payudara Benign dan Malignant dengan Metode Jaringan Syaraf Tiruan Learning Vector Quantization,,,"Klasifikasi, Kanker Payudara, Jaringan Syaraf Tiruan, Learning Vector Quantization","Anifuddin Azis, S.Si, M.kom",2,3,0,2015,1,"Kanker adalah istilah yang luas untuk kelas penyakit yang ditandai dengan sel-sel tumbuh secara abnormal dan menyerang sel-sel yang sehat dalam tubuh. Kanker payudara dimulai pada sel-sel payudara sebagai sekelompok sel-sel kanker yang kemudian dapat menyerang di sekitarnya atau menyebar ke area lain dari tubuh. Kanker payudara adalah suatu penyakit sel ganas (kanker) terbentuk di jaringan payudara. Sel-sel yang rusak dapat menyerang jaringan disekitarnya, tetapi dengan deteksi dini dan pengobatan, kebanyakan orang bisa hidup secara normal. Ada dua jenis tumor kanker payudara yaitu yang non-kanker atau jinak dan yang bersifat kanker atau ganas.
Learning vector quantization (LVQ) adalah suatu metode untuk melakukan pembelajaran atau pelatihan pada lapisan kompetitif yang terawasi. Suatu lapisan kompetitif secara otomatis belajar untuk mengklasifikasikan vektor-vektor input. Kelas yang didapatkan sebagai hasil dari lapisan kompetitif hanya tergantung pada jarak antara vektor-vektor input. 
Pengujian sistem pada penelitian ini dapat dilakukan klasifikasi jenis kanker payudara antara benign dan malignant. Hasil evaluasi Accuracy Score klasifikasi jenis kanker payudara dataset Winconsin Diagnostic Breast Cancer sebesar 98.6% pada data proses 4 dengan parameter maksimum error sebesar 0.001, learning rate sebesar 0.1 dan decrease rate sebesar 0.3.","Cancer is a broad term for a class of diseases characterized by abnormal cells that grow and invade healthy cells in the body.  Breast cancer starts in the cells of the breast as a group of cancer cells that can then invade surrounding tissues or spread (metastasize) to other areas of the body. Breast cancer is a disease in which malignant (cancer) cells form in the tissues of the breast. The damaged cells can invade surrounding tissue, but with early detection and treatment, most people continue a normal life. There are two types of breast cancer tumors: those that are non-cancerous, or &acirc;€˜benign&acirc;€™, and those that are cancerous, which are &acirc;€˜malignant&acirc;€™.
Learning vector quantization is a method to learning or training in supervised competitive layer. A competitive layer automatically learns to classify input vectors. Class obtained as a result of competitive layer only depends on the distance between vectors input.
Testing the system in this research can be done classification of types breast cancer between benign and malignant. Result of the evaluation Accuracy Score classification of types breast cancer dataset Winconsin Diagnostic Breast Cancer by 98.6% on the data process 4 with parameter error maximum by 0.001, learning rate by 0.1 and decrease rate by 0.3. 
",,,41925,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83395,,ZYGA NUR MUSLIMAH,APLIKASI PENCARIAN BERITA UNTUK MEDIA MONITORING DENGAN MENGGUNAKAN WEB CRAWLER DAN GOOGLE CUSTOM SEARCH API,,,"Media monitoring, Web crawler, Google Custom Search API","I Gede Mujiyatna, S. Kom., M. Kom.",2,3,0,2015,1,"Perusahaan melakukan media monitoring untuk mengetahui seperti apa reaksi publik terhadap produk yang mereka keluarkan. Umumnya, sebuah perusahaan memiliki pemonitor media yang bertugas untuk mencari berita-berita di media online publik dan membuat rangkuman dari berita-berita yang telah dikumpulkan. Diperlukan suatu alat bantu pencarian yang dapat mengecek secara langsung apakah berita memuat konten yang dimaksud atau tidak.

Aplikasi yang dibuat pada penelitian ini adalah aplikasi yang akan mencari berita mengenai produk teknologi informasi pada beberapa situs media online Indonesia dengan menggunakan web crawler Google Non-API dan Google Custom Search API. Berita-berita yang didapat dari hasil pencarian ini akan dibuka dan dibaca menggunakan regular expressions oleh sistem kemudian akan diambil kesimpulan apakah berita tersebut memuat kata kunci pencarian produk atau tidak. Pada penelitian ini juga akan dibandingkan beberapa aspek yang dimiliki oleh web crawler Google Non-API dan Google Custom Search API yaitu kecepatan pencarian berita, ketepatan hasil pencarian berita, dan berita hasil pencarian sistem pencarian berita.

Hasil pengujian pencarian berita dengan menggunakan web crawler Google Non-API dan Google Custom Search API menunjukkan bahwa pencarian berita dengan Google Custom Search API lebih cepat dibandingkan dengan web crawler Google Non-API. Pencarian berita menggunakan web crawler Google Non-API berpeluang diblokir oleh Google, namun tidak pada Google Custom Search API.","In order to identify how public reacts on their products, media monitoring is being applied by companies. Generally, a company executes media monitoring by assigning someone as media monitor who is being tasked to find and summarize news that are found on public online medias. Therefore, a searching tool that can check directly whether the news contains correlated contents or not is needed.

The main purpose of this research is to create an application that retrieve IT products related news from various Indonesian online media by using web crawler Google Non-API and Google Custom Search API. By using regular expressions, this application detect whether the retrieved news contain correlated keywords or not. This research also serves its purpose as a comparator between web crawler Google Non-API and Google Custom Search API which compare few aspects such as running time, accuracy, and the search results.

The test results of news searching using web crawler Google Non-API and Google Custom Search API show that news searching using Google Custom Search API is faster than using web crawler Google Non-API. News searching using web crawler Google Non-API has a chance to be blocked by Google, but not for Google Custom Search API.",,,33401,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83655,,SHINTA NURAISYA ARIZKY,IMPLEMENTASI VIRTUAL DATA CENTER PADA XENSERVER MENGGUNAKAN ALOKASI SUMBER DAYA DEDICATED DAN DYNAMIC,,,"virtualization, data center, XenServer, resource allocation","Mardhani Riasetiawan, M.T.",2,3,0,2015,1,"Keberadaan data center menjadi krusial sebab pada data center semua data
disimpan dan diamankan dalam sebuah sistem. Namun, mengelola data center
tidaklah mudah. Apabila sebuah data center kekurangan sumber daya, maka solusi
yang ada cenderung untuk menambah hardware. Konsep virtualisasi mencoba
mengefisienkan sumber daya yang ada dengan kebutuhan informasi yang semakin
besar. Dalam hal ini, penggunaan sumber daya menjadi sangat penting agar sumber
daya yang ada dapat dimanfaatkan sebaik-baiknya.
Pada penelitian ini dilakukan implementasi XenServer dan perbandingan
kebutuhan sumber daya pada virtual data center menggunakan alokasi dedicated
maupun dynamic untuk mengatur sumber daya. Parameter yang digunakan adalah
CPU usage dan memory usage.
Proses pengujian dilakukan dengan menjalankan 22 virtual machine (VM)
dengan skenario pertama inisiasi 4 VM lalu menjalankan 18 VM-dedicated dan
skenario kedua dengan inisasi 4 VM lalu menjalankan 18 VM-dynamic. Dilakukan
perhitungan rata-rata CPU usage dari masing-masing host dan pengukuran jumlah
memory yang terpakai baik VM maupun Xen secara keseluruhan selama 370 menit.
Dari hasil pengujian, dilakukan perbandingan rata-rata CPU usage dan
jumlah memory yang terpakai pada masing-masing skenario pengujian. Dari sisi
kinerja CPU diperoleh hasil bahwa rata-rata CPU usage untuk alokasi sumber daya
dedicated menghasilkan nilai lebih rendah dibandingkan dengan alokasi sumber
daya dynamic. Penggunaan memory yang digunakan pada kedua host untuk semua
skenario sebesar 3804 MB dikarenakan masing-masing host memiliki jumlah VM
yang sama sehingga penggunaan memory yang digunakan kedua host memiliki nilai
yang sama.","The existence of data center becomes crucial because all of the data are
stored and secured in a system. However, managing the data center is not easy. In
the data center, when lack of resources, the existing solutions tend to add hardware.
Though the concept of virtualization is trying to streamline the existing resources
with the growing needs of information. In this case, resources utilization becomes
very important that the existing resources can be used well.
In this research, the author makes implementation of virtual data center on
XenServer using dynamic and dedicated allocation to manage resources. The
parameters used are the CPU usage and memory usage.
The testing process is done by running 22 virtual machines (VM) gradually
from initiation 4 VM, run 18 dedicated VM and continue to initiation 4 VM, run 18
dynamic VM and calculating the average CPU usage by using the average of data
from each host and memory usage as a whole for 370 minutes.
From the test results, the author makes comparison of CPU and memory
usage on each test scenario. In terms of CPU performance, the average of CPU
usage for dedicated resource allocation produce a lower value than the dynamic
resource allocation. The memory used in both of the hosts for all the scenarios are
3804 MB because each VM of the host has the same amount.",,,33643,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81868,,BONDAN BHASKARA,Aplikasi Repositori Data Ilmiah Menggunakan Information Retrieval dengan Algoritma Latent Semantic Indexing,,,"Information Retrieval, Latent Semantic Indexing, Singular Value Decomposition, Scientific Data,  Data Repository"," Mardhani Riasetiawan, M.T.",2,3,0,2015,1,"Sumber daya informasi digital ilmiah (scientific data) meningkat secara signifikan dengan banyaknya penelitian yang menghasilkan data ilmiah dalam jumlah besar. Data ilmiah biasanya berhubungan satu sama lain dalam hal bidang yang sama, maupun obyek ilmiah yang sama. Karena banyaknya data yang tersedia terkadang membuat proses pencarian data yang berhubungan membutuhkan waktu yang lama dan sulit. Penelitian ini mengimplementasikan metode information retrieval yang efektif dan memberikan hasil pencarian yang sesuai dengan keinginan user aplikasi pengumpulan data ilmiah
Pada penelitian ini diimplementasikan metode Latent Semantic Indexing pada repositori data ilmiah. Pencarian dokumen dengan metode ini dapat mencari hasil berdasarkan basis konseptual dari dokumen. Proses dekomposisi matriks yang terjadi di metode ini dapat mencari hubungan kata dengan topik. Hasil dari dekomposisi matriks adalah nilai vektor dari semua dokumen dan query dari pengguna yang digunakan untuk mencari nilai kesamaannya.
Berdasarkan hasil pengujian, kinerja sistem repositori data ilmiah yang dinilai dengan precision dan recall mampu memberikan hasil yang relevan,  dan jumlah dokumen berpengaruh terhadap kecepatan sistem pencarian.
","Scientific based digital information is increasing significantly equal to increasingly research with digital data. Scientific data is usually related one to another in terms of same field or scientific object. Because a lot of data is available, search process can take long and difficult. In this research, it is implement an effective  method in information retrieval, providing search results based on user query in scientific repository.
In this research, Latent Semantic Algorithm is implemented in scientific data repository. This searching method can find results from conceptual basis of basis. Matrix decomposition process from this method can be used to extract topic from document collection. Result from matrix decomposition is vector value from all documents and search query to calculate the similarity score.
Based on test result, scientific data repository performance which is assessed with precision and recall, can give relevant result. Number of documents in database affect the performance of system.
",,,31843,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
88016,,WIDI SETYAWAN,RANCANG BANGUN APLIKASI PELAPORAN KEJADIAN KECELAKAAN LALU LINTAS BERBASIS CROWDSOURCING,,,"Android, web, GPS, kecelakaan, crowdsourcing / Android, web, GPS, traffic accidents, crowdsourcing","Triyogatama Wahyu Widodo, S.Kom., M.Kom",2,3,0,2015,1,"Sarana dan prasarana tranportasi jalan yang kurang memadai akan menimbulkan banyak, salah satu masalahnya yaitu rendahnya keselamatan lalu lintas sehingga sering terjadi kecelakaan lalu lintas. Tahun 2011 kejadian kecelakaan lalu lintas di DIY mencapai 2.302 kejadian. Dengan kemajuan teknologi informasi, kebutuhan informasi yang akurat sangat dibutuhkan untuk perkembangan masyarakat diwaktu mendatang. Salah satu perkembangan teknologi yang pesat yaitu pada perangkat bergerak (mobile device). Penggunaan teknologi tersebut tidak hanya sekedar alat untuk berkomunikasi saja tetapi sering  digunakan untuk mengirimkan informasi pada media sosial tidak terkecuali tentang kejadian kecelakaan lalu lintas. Informasi tersebut sangat dibutuhkan untuk mengetahui daerah rawan kecelakaan lalu lintas.  
Pada penelitian ini akan dibangun sebuah aplikasi yang menerapkan konsep klien server yang memanfaatkan fitur internet dan GPS. Pada sisi klien, aplikasi yang dibangun dapat mengirimkan informasi kejadian kecelakaan lalu lintas berbasis crowdsourcing. Berdasarkan data tersebut, sisi server akan melakukan perhitungan angka kecelakaan dengan metode EAN (Equivalent Accident Number)  dan dianalisa untuk mendapatkan daerah rawan kecelakaan dengan menggunakan metode BKA (Batas Kontrol Atas) dan UCL (Upper Control Limit) 
Sistem ini mampu mengirimkan data lokasi kejadian kecelakaan lalu lintas ke web server via internet. Berdasarkan data yang dikirimkan oleh klien, server mampu mengolah data tersebut dan menghasilkan daerah rawan kecelakaan lalu lintas pada satu kilometer segmen jalan disetiap ruas jalan tertentu.","Decreasing quality infrastructure of public transportation today is causing any traffic problems, one of the problems that increasing number of traffic accident. In 2011, traffic accidents in Yogyakarta reached up 2,302 incidences. By the recently advancement in information technology, necessity of accurate information for everyone daily life, so that the information becomes an essential element of to days society and future. One of the successful technological developments is the existence Smart Phone. The use of such technology is not just for communication but often used for send information on social media including the traffic accidents. Traffic accident information is needed to determine the accident-prone areas.
This research will develop an application by applying clients server concept that utilizing the internet and GPS features. On the client side, an application was created to transmitting information about traffic accidents based on crowdsourcing. Based on this data, the server will calculate the number of accidents by using EAN (Accident Equivalent Number) method and analyzed to obtain accident-prone areas using BKA (Batas Kontrol Atas) and UCL (Upper Control Limit)
This system is capable to transmit data traffic accident scene to the web server via internet. Based on data submitted by the client, the server is able to process the data and generate traffic accident-prone areas on one kilometer road segment in each specific roads.",,,38206,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
85972,,ANGGA FEBRIAN SAHID,MAPPING PROCESS ON UNSTRUCTURED DATA FOR DATA MANAGEMENT PROTOTYPING,,,"big data, unstructured data, database, Rapid Application Development","Mardhani Riasetiawan, M.T",2,3,0,2015,1,"Growth of information that has dramatically increase nowadays, make the data that is stored in the database will increasing. Big Data is the tracking and aggregation of a large volume of data (including personal information) from search engine histories, emails, sales transaction histories, reward/loyalty programs, app downloads and others. The Big scale of data that stored in the database can be defined into two kind of type, one is a structured data, and the other is unstructured data. Unstructured data (or unstructured information) refers to information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Research approach to establish System Database Management Application that can handle unstructured data and displays the database informatively to the user that accessed the system. 
Focus of this research is extracting information from the unstructured database and implemented in a Rapid Application Development (RAD). This research is try to analyze the unstructured data by through the following process, which are Data Gathering, Data Extraction, Data Selection, Data analysis, v. Implementing for prototyping so that the unstructured data can transform into structured data then displayed informatively using prototyping Rapid Application Development.
Experiment shows the research is success through the data gathering process, data extraction process, data selection Process, data analysis process, and process of implementing the result of data analysis in RAD (Rapid Application Development), which is the method of transforming unstructured data to structured data. ","Growth of information that has dramatically increase nowadays, make the data that is stored in the database will increasing. Big Data is the tracking and aggregation of a large volume of data (including personal information) from search engine histories, emails, sales transaction histories, reward/loyalty programs, app downloads and others. The Big scale of data that stored in the database can be defined into two kind of type, one is a structured data, and the other is unstructured data. Unstructured data (or unstructured information) refers to information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Research approach to establish System Database Management Application that can handle unstructured data and displays the database informatively to the user that accessed the system. 
Focus of this research is extracting information from the unstructured database and implemented in a Rapid Application Development (RAD). This research is try to analyze the unstructured data by through the following process, which are Data Gathering, Data Extraction, Data Selection, Data analysis, v. Implementing for prototyping so that the unstructured data can transform into structured data then displayed informatively using prototyping Rapid Application Development.
Experiment shows the research is success through the data gathering process, data extraction process, data selection Process, data analysis process, and process of implementing the result of data analysis in RAD (Rapid Application Development), which is the method of transforming unstructured data to structured data. ",,,36111,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
82394,,BETI TUNTARI,Metode untuk Menghasilkan Counterexample Berukuran Minimal pada SPIN,,,"verifikasi sistem, model checking, SPIN, counterexample, acceptance cycle, depth first search, breadth first search","Mhd. Reza M.I Pulungan, M.Sc., Dr.-Ing",2,3,0,2015,1,"Kesalahan yang terdapat pada suatu sistem, terutama sistem yang kritikal, dapat mengakibatkan sesuatu yang fatal. Oleh karena itu, kesalahan pada sebuah sistem harus diminimalisasi. Salah satu cara meminimalisasinya adalah dengan menggunakan model checking.
Model checking digunakan untuk membuktikan bahwa suatu sistem telah memenuhi properti-properti yang ditentukan. Ketika sistem tidak memenuhi properti-properti tersebut, counterexample akan dihasilkan. Lalu counterexample itu lah yang akan dianalisis dan digunakan sebagai dasar untuk memperbaiki sistem tersebut.
Ukuran counterexample akan mempengaruhi proses analisis dan perbaikan sistem. Counterexample yang berukuran panjang akan mempersulit proses analisis dan perbaikan sistem. Sementara counterexample yang pendek akan mempermudah proses tersebut. Dengan demikian, proses model checking harus menghasilkan counterexample yang berukuran pendek.
SPIN adalah suatu perangkat lunak yang bisa digunakan untuk melakukan model checking. SPIN akan memverifikasi model sistem dan sebuah properti. Jika model sistem tidak memenuhi properti, counterexample akan dihasilkan. SPIN menggunakan algoritma pencarian depth-first search (DFS) untuk menghasilkan counterexample. Penelitian ini menggunakan metode lain yang berdasarkan penelitian Gastin dan Moro (2007) untuk menghasilkan counterexample berukuran minimal pada SPIN. Metode ini menggunakan algoritma breadth-first search (BFS) yang telah ada di kode program SPIN untuk menghasilkan counterexample berukuran minimal. Counterexample yang dimaksud adalah counterexample yang berbentuk acceptance cycle. 
Hasil pengujian menunjukkan bahwa SPIN yang telah dimodifikasi menghasilkan counterexample yang lebih pendek atau sama dengan SPIN yang tidak dimodifikasi. ","Errors in systems, especially safety-critical systems, can cause something fatal. Therefore, errors in a system should be minimalized. There are some techniques to do that. One of them is called model checking. 
Model checking is used to prove that a system satisfies some certain properties. If the system doesn't satisfy the properties, counterexample will be produced. Then, the counterexample will be analyzed and used as the basis for correcting the system.
Counterexample size will affect analysis process and process to correct the system. Large counterexamples will make those processes more complicated. On the other hand, small counterexamples will make them easier to do. So, model checking process should compute small counterexample.
SPIN is a software that can be used to perform model checking. SPIN will verify a system model and a certain property. If the property is violated by the system model, SPIN will produce a counterexample. SPIN uses depth-first search algorithm to compute counterexamples. In this research, we use another method, based on Gastin and Moro's (2007) research, to generate minimal counterexample for SPIN. We use breadth-first search algorithm that has already existed in SPIN's source code to generate minimal counterexample. Counterexamples addressed in this research are acceptance cycle. 
The result of this research shows that the modified SPIN can generate equal or smaller counterexamples than the unmodified one.",,,32330,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92639,,ISTHOFANY IRFANA A.,Pengenalan Entitas Bernama Pada Artikel Berita Bahasa Indonesia Menggunakan Metode Berbasis Aturan,,,"ekstraksi informasi, pengenalan entitas bernama, rule-based, kelas kata, morfologi kata, information extraction, named entity recognition, rule-based, part-of-speech, NER ","Sigit Priyanta, S.Si., M.Kom",2,3,0,2015,1,"Pengenalan entitas bernama merupakan subpekerjaan dari ekstraksi infomasi. Pengenalan entitas bernama bertujuan untuk mengidentifikasi entitas di dalam teks. Pengenalan entitas bernama sering kali digunakan untuk mengidentifikasi hubungan dalam ekstraksi informasi.
Salah satu pendekatan pengenalan entitas bernama adalah menggunakan metode rule-based, yaitu sekumpulan aturan untuk mengidentifikasi entitas dalam suatu dokumen teks. Pada penelitian ini dikembangkan sistem dengan metode rule-based dimana fitur yang diperhatikan adalah morfologi kata, kontekstual, dan kelas kata. Kelas kata tidak ditentukan secara manual melainkan menggunakan POStagger. Pada penelitian ini pengenalan entitas bernama lebih difokuskan untuk artikel berita berbahasa Indonesia.
Berdasarkan pengujian sistem yang telah dilakukan, pengenalan entitas bernama menggunakan metode rule-based untuk artikel berita berbahasa Indonesia ini mendapatkan f-measure 75,98% dengan nilai recall 77,36% dan precission 74,64%.
","Named entity recognition is a subtask of information extraction. Named entity recognition objective to identify entities in the text. Named entity recognition are often used to identify relations between information extraction.
One of approaches that can be used in named entity recognition is rule-based method, which is a set of rules to identify entities in a text document. In this study, a system developed with rule-based method which is concerned on the morphological features, contextual and word class. Word class is not specified manually, but using POStagger. News articles in Indonesian language is used in this study.
Based on system testing that has been done, named entity recognition using rule-based method for the Indonesian language news articles get 75,98% for f-measure with 77,36% of recall value and 74,64% of precission value.

",,,42756,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92895,,BINTANG ALAM S.,"Analisis Kinerja Jaringan VSAT, FO, dan PLC di PT. Bank Rakyat Indonesia (Persero), Tbk. Cabang Ponorogo Ditinjau Dari Delay Time, Throughput, dan Packet Loss",,,"network performance, very-small-aperture terminal, fiber-optic, power-line communication, delay time, throughput, packet loss","Lukman Heryawan, S.T., M.T.",2,3,0,2015,1,"PT. Bank Rakyat Indonesia (Persero), Tbk. sebagai korporasi besar yang memiliki cabang yang tersebar di berbagai tempat memerlukan infrastruktur jaringan yang handal untuk menjalankan operasionalnya. Dengan berbagai macam media komunikasi data dengan kekurangan dan kelebihannya masing-masing. Tentunya, diperlukan kecermatan dalam pemilihan dan pengoperasian media komunikasi data. Oleh sebab itu, dalam pemilihannya harus memperhatikan delay time, throughput, dan packet loss yang akan mempengaruhi kinerja dari jaringan itu sendiri.
Pada penelitian ini dilakukan pengujian terhadap kinerja jaringan dengan media komunikasi data VSAT, FO, dan PLC. Pengujian dilakukan dengan menggunakan parameter delay time, throughput, dan packet loss.
Dari hasil pengujian, dilakukan perbandingan nilai rata-rata delay time, throughput, dan packet loss pada masing-masing media komunikasi data. Berdasarkan data pengujian, menunjukkan media komunikasi data FO dari vendor B menunjukkan kinerja terbaik pada dua dari tiga parameter kinerja yang digunakan, yaitu delay time dan packet loss. Sedangkan media komunikasi data VSAT dari vendor A menunjukkan kinerja terbaik pada satu dari tiga parameter kinerja yang digunakan, yaitu throughput.","PT. Bank Rakyat Indonesia (Persero), Tbk. as large company that have branches located in various places require a reliable network infrastructure to run its operations. With a wide range of data communication media, with its advantages and disadvantages. Certainly, carefullness is a must in the selection and operation of data communication media. Therefore, the selection must pay attention to the delay time, throughput, and packet loss that will affect the performance of the network itself.
In this research, the testing of network performance was performed with VSAT, FO, and PLC data communication media. The tests carried out using various parameters, such as delay time, throughput, and packet loss.
From the test results, carried out comparisons of the average delay time, throughput, and packet loss for each data communication media. Based on test data, FO data communication media from vendor B showed the best performance in two out of three performance parameters used, which is the delay time and packet loss. While the VSAT data communication media from vendor A showed the best performance in one of the three performance parameters used, which is the throughput.",,,43026,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
89568,,ARDHIAN HERU NUGROHO,Perbandingan Empat Algoritma Identifikasi Gambar Porno,,,"Deteksi Gambar Porno, Morphological Features, NDA, SPQ, RSOR","Nur Rokhman, S.Si, M.Kom",2,3,0,2015,1,"Sebagai salah satu dampak negatif yang timbul akibat perkembangan teknologi informasi saat ini, pornografi nampaknya menjadi permasalahan utama. Salah satu jenis pornografi yang berkembang adalah pornografi dalam media gambar. Oleh sebab itu dirasa perlu adanya suatu penelitian yang dapat membantu dalam menghadapi permasalahan tersebut.
Salah satu parameter yang dapat digunakan dalam menentukan apakah sebuah gambar porno atau tidak adalah dengan melihat apakah gambar tersebut mengandung objek manusia telanjang (Nudity Image). Jika suatu gambar mengandung objek manusia telanjang maka gambar tersebut masuk ke dalam kategori porno (Lin dkk., 2003).
Algoritma Morphological Features, Nudity Detection Algorithm (NDA), algoritma Skin Pixel Quantifier (SPQ), dan algoritma RSOR menggunakan model warna HSV adalah beberapa algoritma yang bisa digunakan dalam mendeteksi apakah suatu gambar termasuk dalam gambar porno atau tidak. Adanya perbandingan beberapa algoritma tersebut dapat membantu dalam menentukan algoritma yang tepat dalam klasifikasi gambar porno.
Dari implementasi algoritma deteksi gambar porno, algoritma Morphological Features (dengan data gambar berukuran seragam) menghasilkan nilai akurasi paling tinggi bila dibandingkan algoritma lain. Hasil terbaik pada algoritma Morphological Features mempunyai nilai keakuratan sebesar 83.22%, false negatif sebesar 16.00% dan false positif sebesar 17.55% dengan menggunakan transformasi model warna YCbCr pada algoritma Morphological Features dengan ukuran gambar 60x40. Selain itu didapatkan bahwa perubahan ukuran gambar, baik diperkecil ataupun diperbesar, tidak terlalu mempengaruhi nilai akurasi.","As one of the negative impacts arising from the development of infornation technology today, pornography seems to be the main problem. One type of pornography that develops is pornography in the media image. Therefore, it's necessary for a study that may help in dealing with these problems.
One of the parameters that can be used in determining wether an image is pornographic or not is to see wheter the image contains a naked human objects. If an image contains a naked human objects that image should be categorized as porn image (Lin et al, 2003).
Morphological Features algorithm, Nudity Detection Algorithm (NDA), Skin Pixel Quantifier (SPQ) algorithm and RSOR algorithm using HSV color model are several algorithms that can be used to detect wheter an image is included in pornographic images or not. The existence of a comparison of several algorithms can assist in determining the good algorithm to classify pornographic image.
From the implementation of porngraphic image detection algorithm, Morphological Features Algorithm (with uniform sized image data) produces the highest accuracy values compared to other algorithms. The best results on Morphological Features Algorithm have accuracy of 83.22%, false negatif of 16.00% and false positif of 17.55% by using the YCbCr color model transformation with a resolution of 60x40. In addition it also found that changes in image size, either reduced or enlarged, not overly affect the value of accuracy.",2015-10-09 00:00:00,53,39861,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
86498,,M IQBAL Z N,IMPLEMENTASI HTTP PROXY DENGAN PEMECAHAN KONEKSI,,,"Proxy, TCP, HTTP","Triyogatama Wahyu Widodo S.Kom, M.Kom",2,3,0,2015,1,"TCP merupakan protokol yang memiliki koneksi reliabel, namun algoritma kontrol kemacetan yang dimilikinya, bisa membuat bandwidth yang digunakan tidak termanfaatkan sepenuhnya. Hal itu disebabkan karena adanya suatu paket data yang rusak atau tidak datang, bukan karena kemacetan yang diartikan sebagai tanda kemacetan.

Penelitian ini akan membahas mengenai cara untuk menangani masalah tersebut dengan cara membuka koneksi tambahan untuk suatu \textit{request} pada protokol HTTP. Penggunaan koneksi paralel memiliki efek peningkatan kecepatan. Pemecahan tersebut diimplementasikan dengan proxy HTTP. Sistem akan membuat koneksi tambahan apabila ada pengunduhan file yang memiliki ukuran sesuai kriteria.

Pengujian dilakukan dengan mengunduh file menggunakan sistem pemecah koneksi yang dibuat dan dibandingkan dengan pengunduhan tanpa sistem pemecahan koneksi. Hasil pengujian menunjukkan adanya kenaikan kecepatan setelah sistem yang dibuat.","TCP is a reliable connection, but the congestion control algorithm, makes it not fully use available bandwidth. Thing that causes that is corrupted packet or didn't arrive, not because of congestion.

This research will focusing in solve the problem about that with connection splitting for HTTP.  Parallel connection has effect increasing speed. The system implemented with HTTP proxy. System will create additional connection if the requested object has size met criteria defined. 

The experiment will downloading file using connection splitting method and comparing the result without connection splitting. The experiment have results that increasing speed happen after using connection splitting. 
",,,36620,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
85222,,SHOFIYYUL FUAD,IMPLEMENTASI ANALYTICAL HIERARCHY PROCESS (AHP) DAN SIMPLE ADDITIVE WEIGHTING (SAW) PADA PENGEMBANGAN SISTEM PENDUKUNG KEPUTUSAN PEMILIHAN GURU BERPRESTASI KABUPATEN BOYOLALI,,,"Sistem pendukung keputusan, AHP, SAW","Drs. Medi, M.Kom.",2,3,0,2015,1,"Pada pemilihan guru berprestasi, pemerintah sudah menetapkan bobot untuk setiap kriteria penilaian. Akan tetapi, pada penilaian subkriteria dibawahnya pengambil keputusan diberi kebebasan untuk menentukan bobot dan pemerintah tidak memberikan ketetapan. Hal ini mengakibatkan pembuat keputusan kesulitan menentukan bobot yang lebih tepat bila tidak menggunakan suatu metode pencarian bobot yang benar. Belum diterapkannya suatu sistem memiliki metode pembobotan yang terkomputerisasi secara efektif yang dapat menyajikan informasi serta menyediakan pilihan bagi para penilai sebagai sarana pendukung dalam pengambilan suatu keputusan, menjadi salah satu penyebab kurang optimalnya proses pemilihan guru berprestasi yang ideal agar sesuai dengan  hasil yang diharapkan. Penggunaan Metode gabungan antara Analytical Hierarchy Process (AHP) dan Simple Additive Weighting (SAW)  dan mengaplikasikanya ke dalah suatu sistem pendukung keputusan adalah solusi yang ditawarkan untuk mengatasi permasalah tersebut.
Aplikasi pemilihan guru berprestasi ini dikembangkan dengan menggunakan Sublime Text sebagai editor kode, lalu digunakan server lokal yaitu XAMPP Control Panel Version 3.1.0 sebagai aplikasi untuk menjalankan web. Dengan bantuan sistem pendukung keputusan, proses penyeleksian pemilihan guru berprestasi dapat ditentukan dengan tepat sesuai dengan kriteria dan subkriteria yang telah ditentukan. Algoritma sistem pendukung keputusan yang digunakan adalah AHP dan SAW dengan kriteria Portofolio, tes tertulis dan wawancara, Psikotes, Karya tulis Ilmiah.
Hasil yang diperoleh dari penelitian sistem pendukung keputusan pemilihan pemilihan guru berprestasi kabupaten Boyolali berbasis web ini adalah perangkingan dan memberikan informasi berupa rekomendasi guru berprestasi yang telah diseleksi dengan berdasarkan kriteria yang telah ditentukan. Alternatif dengan nilai tertinggi akan lebih dipilih sebagai alternatif pilihan.
","In the selection of outstanding teacher, the government has set a weighting for each assessment criteria. However, on the assessment of the underlying sub-criteria, decision-maker is given the freedom to determine the weight and the government does not provide provisions. This resulted in difficulties for decision maker to determine the proper weight without  a correct method. The absence of a computerized weighting system that can effectively present information and provide options for the assessors as a means of support in making a decision, be the one of the causes that make selection process of outstanding teachers less optimal to fit the expected result. The implementation of Analytical Hierarchy Process (AHP) and Simple Additive Weighting (SAW) in a decision support system is a proposed solution to overcome this problem. 
This outstanding teacher application is developed using Sublime Text as a code  editor with XAMPP Control Panel Version 3.1.0 as aplication for running the web application. The help of Decision Support System (DSS) athlete selection process can be determined accurately in accordance with established criteria and subcriteria. DSS Algorithm used is Analytical Hierarchy Process (AHP) and Simple Additive Weighting (SAW) with the criteria of portfolio, written test and interview, psychotest, and scientific paper.
The result of this study is to provide the scoring and the information about the recommended outstanding teachers that are being selected based on the predetermined criteria. The alternative with the highest value is preferred to be choosen as the best alternative.
",,,35337,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81896,,DEWI SHINTIA,RANCANG BANGUN APLIKASI PEMESANAN PAKET PERJALANAN WISATA PADA SISTEM OPERASI ANDROID,,,"Android, paket wisata, e-commerce , Google Map","Anny Kartika Sari, M.sc.,Dr",2,3,0,2015,1,"Penggunaan  Teknologi  Informasi  dan  Komunikasi  dalam  dunia  bisnis dewasa  ini  berkembang  pesat.  Dalam  bisnis  pariwisata  salah  satu  pihak  yang memanfaatkannya  adalah penyedia perjalanan wisata atau yang disebut dengan biro wisata.  Biro  Wisata  telah  memanfaatkan  teknologi  informasi  khususnya  internet untuk media promosi hingga jual beli  atau  transaksi dengan pelanggan  yang dikenal sebagai  E-commerce.  Namun penggunaan  teknologi  informasi  tersebut belumlah optimal dan  masih hanya berbasis  website. Penggunaan  teknologi pada perangkat bergerak  (mobile  device)  oleh  para  biro perjalanan  wisata  masih  minim  padahal mobile  device  saat  ini  tidak  lagi  hanya  sekedar  alat  berkomunikasi  tetapi  juga menjadi salah satu sumber informasi yang sangat dibutuhkan oleh para wisatawan. Salah satu sistem operasi yang paling populer pada  mobile device  saat ini adalah Android. Pada  penelitian  ini  akan  dibangun  sebuah  aplikasi  pada  sistem  operasi Android yang  memungkinkan pengguna untuk melakukan pemesanan produk paket perjalanan  wisata  melalui  perangkat  mobile.  Aplikasi  yang  dibangun  dapat memberikan informasi paket perjalanan wisata yang ditawarkan oleh biro  wisata berupa  informasi  lokasi  wisata,  rencana perjalanan  dan  peta  lokasi  wisata menggunakan Google Maps, serta terdapat sisi administrator yang berfungsi untuk mengolah data pemesanan yang masuk, data paket wisata,  dan anggota  serta data biro wisata. Berdasarkan hasil pengujian, seluruh fungsionalitas yang disebutkan di atas telah  dapat dipenuhi oleh aplikasi yang dibuat. Selain itu aplikasi juga telah berhasil diujicobakan pada beberapa perangkat Android yang berbeda. ","The use of Information and Communication Technology in today's business world is growing fast. In the tourism business, one of those who use them is  travel provider  or  called  by  travel  agency.  Travel  Agency  have  used  technology information  especially  the  internet  for product  promotion  or  doing  transaction process with  customer that is known as e-commerce. However, the utilization of 
information technology is not yet optimal and still only based on the website. The use of mobile device technology by the travel agency is  still not enough when the mobile  device  is  no  longer  just  a  communication  tool  now  but  also  one  of  the information  resources  that  are  needed  by  the  tourists.  One  of  the  most  popular operating system in mobile devices today is Android.
This research will build an application  on Android operating system that allows users to book  tour package  products through mobile devices. The application will be  able to provide information about the tour packages that offered by travel agents  such  as  location  information,  itineraries  and  maps  locations  that  using Google  Maps,  and  there  is  also  administrator  side  that  serves  to  process  the 
incoming reservation data, tour packages data, and members data as well as travel agent data.
Based on test results, the entire functionality mentioned above can already be  fulfilled  by  the  application  made.  In  addition,  the  application  also  has  been successfully tested on several different Android devices.",,,31856,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92650,,AKMAL NOVIZAR,INVESTIGATE PART OF SPEECH TAGGING PERFORMANCE USING COMBINATION OF CLINICAL TEXT CORPORA,,,"Natural Language Processing, POS tagging, N-Gram Tagger, clinical-text","Lukman Heryawan, S.T., M.T",2,3,0,2015,1,"Part of Speech (POS) tagging berperan penting dalam beberapa ruang lingkup Natural Language Processing. Beberapa pendekatan dan metode telah di kembangkan untuk automatisasi pemberian label kata dalam teks dengan part-of-speech tag untuk Bahasa Inggris  dan beberapa Bahasa yang berada di kawasan barat. Dari hal tersebut, penelitian ini dilakukan untuk meng-investigasi performa POS tagging melalui pendekatan metode stochastic, seperti N-Gram tagger (Unigram, Bigram, dan Trigram tagger) yang di applikasikan terhadap teks yang berbasis pelayanan klinik yang berasal dari Electronic Health Record (EHR). Disisi lain, beberapa pendapat menyatakan sebuah supervised POS tagging membutuhkan pelatihan corpus dengan jumlah besar untuk dapat memberikan label atau tag dengan baik dan benar. Untuk itu dengan mencoba meng-investigasi dengan penelitian ini diharapkan dapat mengerti bagaimana performa POS tagging dipengaruhi oleh sumber yang terbatas pada corpora dan di implementasikan dalam beberapa scenario optimisasi data: Intra-institution, Inter-institution, dan Mix-institution.

Pada penelitian ini menggunakan pelatihan corpus yang ter-anotasi berjumlah 10 catatan klinik dari Beth Medical Center dan jumlah yang sama dari Partners HealthCare yang mana pelatihan corpus di anotasi secara manual tanpa seorang ahli apakah dari ahli Bahasa Inggris maupun ahli medis. Akan tetapi, hasil dari penelitian menunjukan semua N-gram tagger yang di applikasikan dalam scenario yang berbeda menunjukkan Mix-institution dengan 10 intra-institution cross-validation dan combined tagger yang dapat menggabungkan semua N-gram tagger secara bersamaan memberikan hasil tagging yang lebih baik saat di uji pada kedua sumber corpora dari catatan klinik dengan estimasi akurasi lebih dari 84 %. dan memberikan manfaat saat proses pelatihan pada POS tagger.
","Part of Speech (POS) tagging is important in various areas of Natural Language Processing. There are different approaches or methods that have been developed to automate the problem of assigning each words or tokens of a text with part-of-speech tag for general English and Western languages. Therefore, this research conducted to investigate POS tagging performance based on stochastic approach, such as N-Gram tagger (Unigram, Bigram and Trigram tagger) applied in clinical-text domain. In other hand, it was believed a supervised POS tagging requires a large amount of annotated training corpus to tag properly. To investigate it, this research performed experiments to understand how POS tagging performance could also affected with limited resources of available corpora which was implemented in different scenario of data optimization following with: Intra-institution, Inter-institution, and Mix-Institution. 

This research utilize annotated training corpus consisting of ten clinical notes from Beth Medical Center and equal size of Partners HealthCare that were manually annotated the training corpus without knowledgeable expert from English linguist and Medical expert. Although, the results from all N-gram taggers applied in different scenario shows Mix-institution with 10 intra-institution cross-validation and also combined tagger that allows chain all N-gram taggers together does best tagging in both clinical-text corpora by estimated an accuracy of 84 % tested on Beth and Partners corpus that could benefitted the training process of POS taggers.
",,,42785,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81899,,BRAMANTYO ADRIAN,IMPLEMENTASI ALGORITMA CLUSTERING K-MEANS UNTUK ALOKASI MESIN VIRTUAL DALAM KOMPUTASI AWAN MENGGUNAKAN CLOUDSIM,,,"Cloud Computing, Virtual Machine Allocation, K-means, CloudSim","Lukman Heryawan, S.T., M.T.",2,3,0,2015,1,"Komputasi awan dikenal dengan penyedia layanan secara dinamis menggunakan resource secara fisik maupun secara virtual di internet. Layanan tersebut terdiri dari Software as a Service (SaaS), Platform as a Service (PaaS), dan Infrastructure as a Service (IaaS). Mesin virtual merupakan teknologi yang digunakan oleh pengguna komputasi awan yang tidak memerlukan sumber daya pada dedicated server. Masalah yang penting dalam komputasi awan adalah manajemen sumber daya untuk meningkatkan utilisasi. Alokasi mesin virtual adalah salah satu cara untuk melakukan peningkatan utilisasi sumber daya dalam komputasi awan. Spesifikasi mesin virtual berupa atribut numerik dan jumlah cluster ditentukan sesuai jumlah host atau datacenter sehingga algoritma K-means cocok digunakan untuk melakukan clustering pada permintaan mesin virtual.
Pada penelitian ini digunakan framework cloud simulator CloudSim versi 3.0 serta algoritma clustering K-means digunakan untuk metode alokasi mesin virtual. Alokasi mesin virtual tersebut dilakukan dengan cara membuat cluster pada permintaan mesin virtual sesuai dengan tingkat kesamaan spesifikasi, kemudian setiap mesin virtual dialokasikan pada host dan datacenter yang sama. Jumlah cluster ditentukan secara dinamis sesuai dengan jumlah host atau jumlah datacenter. Metode alokasi mesin virtual dengan algoritma clustering K-means dibandingkan dengan algoritma FIFO yang ada pada CloudSim. Pengujian terdiri dari dua skenario, pada skenario pertama setiap datacenter hanya memiliki sebuah host dan pada skenario kedua setiap datacenter memiliki dua host.
Hasil analisis dari kedua skenario pengujian adalah metode alokasi mesin virtual dengan K-means lebih baik daripada metode FIFO dalam utilisasi CPU mesin virtual dengan meminimalisir idle time dan melakukan load balancing mesin virtual pada setiap datacenter.","Cloud computing is known as dynamic service providers using physical resource or virtualized on the internet. These service consist of Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). Virtual machine technology is used by cloud computing client who do not require dedicated server. Important challenge in cloud computing is resource management to improve utilization. Virtual machine allocation method is one of the way to improve resource utilization in cloud computing. Virtual machine specification is form of numerical attributes and the number of clusters is determined by the amount of host or datacenter so that K-means algorithm suitable to perform clustering on virtual machine demand.
This research used a framework cloud simulator CloudSim version 3.0 and K-means clustering algorithm is used for virtual machine allocation method. Allocation of virtual machine is done by making virtual machine cluster based on level of specification similarity, then each virtual machine in cluster allocated on same host and datacenter. The number of cluster is determined dynamically according to the number of hosts and number of datacenter. Virtual machine allocation method using K-means clustering algorithm compared with existing FIFO algorithm on CloudSim. The test consists of two scenarios, first scenario each datacenter only has a host and the second scenario each datacenter has two host. In both of scenarios have same amount of work.
The analysis result obtained from both of scenario is virtual machine allocation method using K-means is better than FIFO in virtual machine CPU utilization by reducing idle time and performing load balancing virtual machine in each datacenter.",,,31880,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81900,,SHINTA NURDWITA KARTIKASARI,APLIKASI MANAJEMEN DATA UNTUK METADATA RETRIEVAL MENGGUNAKAN AUTOMATIC METADATA GENERATION,,,"metadata, extraction, preservasi, non-relational, scientific data","Mardhani Riasetiawan, M.T. ",2,3,0,2015,1,"Pengelolaan sumber daya informasi digital ilmiah menjadi topik penting saat ini dengan banyaknya penelitian yang menghasilkan data dalam jumlah besar dan memiliki berbagai format. Data digital sangat rentan terhadap kerusakan yang menyebabkan data tidak dapat digunakan kembali. Karena itu informasi mengenai metadata suatu sumber daya digital secara rinci sangat diperlukan untuk menjamin data agar dapat bertahan dan dapat diakses dikemudian hari.
Penelitian ini akan membuat sebuah aplikasi manajemen data menggunakan automatic metadata generation bernama FITS untuk mengekstraksi metadata preservasi dari data digital. Metadata hasil ekstraksi lalu disimpan dalam basis data non-relasional, MongoDB. 
Berdasarkan hasil pengujian, telah berhasil dibuat sebuah aplikasi manajemen data sesuai kebutuhan sistem yang telah dirancang. Aplikasi ini mampu menyimpan, mengelola dan menampilkan berbagai format file dan metadata preservasinya. Adapun fitur-fitur yang terdapat pada aplikasi ini adalah upload file, explore file, searching dan pengelolaan user. 
","	Management of scientific based digital information become an important topic at this time with the number of research that produces large amounts of data and have a variety of formats. Data digital is vulnerable to damage that causes the data cannot be used again. Therefore, detailed information about the metadata of a digital resources are needed to ensure data in order to survive and accessible in the future.
This research will develop a data management application using automatic metadata generation, named FITS, for extraxtion preservation metadata of digital data. The result fi metadata extraction is stored in non-relational database, MongoDB
Based on test result, data management application have been created according to the needs that system has been designed. This aplication is able to store, manage and display a variety of file formats and its preservation metadata. As for the feartures contained in the application is upload file, explore file, searching, and user management. ",,,31863,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81645,,HABIB MALIK,"Penerapan Bottom-Up Parser, Stemmer, dan Pendeteksi Ambiguitas dalam Sistem Penganalisis Tata Bahasa dalam Kalimat Bahasa Indonesia",,,"NLP, Indonesian grammar, parsing, stemming, ambiguity detection","Sigit Priyanta S.Si., M.Kom.",2,3,0,2015,1,"Bahasa  Indonesia  merupakan  bahasa  resmi  negara  Indonesia  yang
penggunaannya  telah  diatur  dalam  Undang-Undang  Nomor  24  tahun  2009.
Bahasa Indonesia wajib dipakai untuk, antara lain, penulisan karya ilmiah serta
publikasi informasi melalui media. Namun, kesalahan penggunaan tata bahasa
Indonesia masih banyak ditemukan dalam berbagai penulisan resmi.
Penelitian  ini  akan  menggunakan  tiga  teknik  NLP  untuk  membangun
sebuah  sistem  penganalisis  tata  bahasa.  Teknik  tersebut  adalah  stemming,
pendeteksi ambiguitas kelas kata (sintaksis), dan  parsing. Teknik  stemming  dan
pendeteksi ambiguitas akan digunakan untuk membangun sistem  POS-Tagging
(penentuan  kelas  kata)  berbasis  aturan  dan  kamus,  sedangkan  teknik  parsing
digunakan untuk menguji keakuratan sintaksis dari suatu kalimat setelah kelaskelas  katanya  diketahui.  Setelah  itu,  keakuratan  sistem  akan  diuji  untuk
menganalisis ketepatan data teks dari media berita daring dan abstrak penelitian
ilmiah.
Dari hasil pengujian, akurasi sistem secara keseluruhan mencapai 96,17%,
dengan  ketepatan  pendeteksian  kesalahan  sebesar  98,38%  dan  ketepatan
pendeteksian kalimat tepat sebesar 93,61%","Indonesian is the official language of Indonesia, whose utilization has been
regulated by the Law No.24 of 2009. The Indonesian Language must be utilized
for, among others, scientific papers and media publication. However, grammatical
mistakes are still often seen in various formal writings.
This research will employ three NLP techniques to develop a grammar
analyzer  system.  The  techniques  are  stemming,  part  of  speech  ambiguity
detection, and parsing. Stemming and ambiguity detection will be employed to
develop  a  dictionary-  and  rule-based  POS-Tagging  system,  while  parsing
technique will be employed to determine syntactic accuracy of the sentences after
the parts of speeches have been known. Subsequently, the system's accuracy will
be tested by analyzing the correctness of text data from online news media and
scientific paper abstracts.
Based on the test result, the system, which was developed by combining
parsing, stemming, and ambiguity detection techniques, could achieve an overall
accuracy  of  96.17%;  specifically,  it  could  detect  grammatical  mistakes  in
sentences  with  an  accuracy  of  98.38%  and  parse  correct  sentences  with  an
accuracy of 93.61%. ",,,31754,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
87797,,DARNANTO,PREDIKSI CURAH HUJAN HARIAN DI KABUPATEN GUNUNGKIDUL PROVINSI DAERAH ISTIMEWA YOGYAKARTA DENGAN JARINGAN SYARAF TIRUAN METODE BACKPROPAGATION,,,"Perubahan iklim, curah hujan, Kabupaten Gunungkidul, jaringan syaraf tiruan, backpropagation","Anifuddin Azis, S.Si., M.Kom.",2,3,0,2015,1,"      Kabupaten Gunungkidul adalah salah satu daerah di Indonesia yang rentan terhadap dampak negatif perubahan iklim terutama bencana kekeringan dan anomali curah hujan. Faktor relief alam di Kabupaten Gunungkidul yang didominasi perbukitan kapur semakin memperparah kekeringan di Gunungkidul. Dampak kekeringan berpengaruh pada kondisi sosial ekonomi masyarakat Kabupaten Gunungkidul yang mayoritas berprofesi sebagai petani.
      Prediksi curah hujan harian adalah salah satu upaya yang dapat dilakukan untuk menanggulangi dampak kekeringan di Kabupaten Gunungkidul. Jaringan syaraf tiruan(JST) metode backpropagation adalah satu metode yang dapat dimanfaatkan untuk tujuan prediksi. Sifat JST cocok untuk tujuan prediksi karena JST mampu mempelajari pola data di masa lalu sehingga menghasilkan formula prediksi berdasarkan selisih target dengan keluaran jaringan.
      Pada penelitian ini dilakukan implementasi JST metode backpropagation untuk mencari model prediksi curah hujan harian di Kabupaten Gunungkidul. Pelatihan jaringan menggunakan 205 dataset dan pengujian jaringan menggunakan 88 dataset. Akurasi maksimal pelatihan jaringan adalah  78% dan akurasi maksimal pengujian jaringan adalah 68%. Parameter latih yang menghasilkan akurasi terbaik adalah jumlah node hidden sebanyak 400-500 unit, faktor momentum sebesar 0,1, dan learning rate sebesar 0,05.
","      Gunungkidul Regency is region in Indonesia are vulnerable to the negative impacts  of climate change, especially drought and rainfall anomalies. Natural relief factor in Gunungkidul predominantly limestone hill aggravate drought in Gunung Kidul. The impact of drought effect on the socio-economic conditions of people in Gunung Kidul Regency who work as farmers.
      Daily rainfall prediction is one efforts that can be done to mitigate the impact of drought in Gunungkidul. Artificial neural networks (ANN) method of backpropagation is a method that can be used for predictive purposes. ANN  is suitable for the purpose of prediction because ANN able to study the data patterns in the past so as to produce a prediction formula based on the difference between the target with a network output.
      In this research the implementation of backpropagation neural network(BPNN) method to look for daily rainfall prediction model in Gunungkidul. Network training using 205 datasets and network testing using 88 datasets. Maximum accuracy is 78% of network training and network testing maximal accuracy is 68%. Parameters trainers who produce the best accuracy are the number of hidden nodes 400-500 units, the momentum factor of 0,1, and learning rate of 0,05.
",,,37959,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
92917,,DIAN ADI SULISTYONO,RANCANG BANGUN APLIKASI LOCATION BASED SERVICE PADA PENCARIAN LOKASI WISATA PERANGKAT ANDROID,,,"Android, LBS, Google Map, Pariwisata, Aplikasi Jogja Guide, Tourism, Jogja Guide Application","Agus Sihabuddin, S.Si., M.Kom",2,3,0,2015,1,"Smartphone sebagai sebuah terobosan dalam teknologi informasi banyak diminati oleh berbagai pihak. Salah satunya pada bidang pariwisata. Adanya lokasi yang belum diketahui, wisatawan akan kesulitan dalam menemukan lokasi wisata tersebut. Oleh karena itu, dibuat sebuah aplikasi berbasis LBS untuk membantu wisatawan mendapatkan lokasi wisata dan informasi wisata.
Pada penelitian ini akan dibangun sebuah aplikasi LBS pada sistem operasi Android yang memungkinkan pengguna untuk mengetahui lokasi wisata. Teknologi LBS dimanfaatkan untuk mengetahui lokasi pengguna dengan posisi lokasi yang akan dituju. Aplikasi ini dapat menghitung jarak dan mengurutkan daftar lokasi wisata berdasarkan jarak terdekat. Aplikasi juga dapat menampilkan rute dan navigasi menggunakan layanan Google Map API. Aplikasi ini memiliki fitur yaitu fitur call, fitur share dan fitur membuka alamat web. Aplikasi ini juga diimplementasikan Google Anaytic untuk memantau penggunaan aplikasi.
Berdasarkan hasil pengujian, seluruh fungsionalitas yang disebutkan di atas telah dapat dipenuhi oleh aplikasi yang dibuat. Hasil dari implementasi Google Analytic juga sudah menunjukkan data mengenai penggunaan aplikasi. Selain itu aplikasi juga telah berhasil diujicobakan pada beberapa perangkat Android yang berbeda.","Smartphone as an innovation information technology much demanded by various parties. One of them is tourism. Any place that does not know, tourist will difficulty in finding location. Because of that, an application that provides the tourist to get location and tourism information completely. 
This research will build an application on Android operating system that allows user to location through mobile devices. Technology location based service used to know the location of the user with position locations intended. This application can calculate the distance and sorting the location by the nearest distance. This application also can showing route and navigation using Google Map API. This application has some feature such as call feature, share feature and web feature for open the website. Google Analytic was implemented for monitoring the used of this application. 
Based on test results, the entire functionality mentioned above can already be fulfilled by the application made. Google Analytic&Atilde;ƒ&Acirc;&cent;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;&Atilde;&macr;&Acirc;&iquest;&Acirc;&frac12;s results show the data about the general used of this application. In addition, the application also has benn successfully tested on several different Android devices.",,,43048,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
83446,,ISNAN SYAMHUDI,ANALISIS PERBANDINGAN EJABBERD DAN OPENFIRE SEBAGAI SERVER LAYANAN PESAN INSTAN BERBASIS PROTOKOL XMPP ,,,"server XMPP, Ejabberd, Openfire, CPU usage, memory usage","Bambang Nurcahyo Prastowo, Drs., M.Sc",2,3,0,2015,1,"Pesatnya perkembangan teknologi informasi dan komunikasi saat ini telah membuat fenomena baru yang disebut pesan instan. Dibalik pesatnya perkembangan pesan instan selama ini tidak lepas dari peranan protokol yang berfungsi mengatur lalu lintas di dalamnya. Salah satunya adalah protokol XMPP. Protokol XMPP merupakan sebuah standar komunikasi real-time yang berbasis text, suara maupun video dengan teknologi open XML. Protokol XMPP memiliki banyak kelebihan yang ditawarkan didalam mengembangkan layanan pesan instan terutama dari segi server. Banyak server yang terdaftar di dalam protokol XMPP, antara lain Ejabberd dan Openfire.
Pada penelitian ini dilakukan analisis perbandingan kinerja pada server Ejabberd dan Openfire. Perbandingan kinerja meliputi pengujian aktivitas yang berupa registration, online user, online chat dan multi-user chat. Hasil penelitian menampilkan perbandingan kinerja server pada waktu menangani aktivitas pengguna dalam bentuk rata-rata nilai CPU usage dan memory usage berdasarkan jumlah pengguna yang bervariasi, dengan tujuan dapat mengetahui pola penggunaan infrastruktur selama user melakukan aktivitas di server. Hasil penelitian menunjukan bahwa memory usage yang dilakukan Ejabberd lebih rendah dari Openfire, sedangkan untuk CPU usage beban kerja yang dilakukan Ejabberd lebih tinggi dibandingkan Openfire.","The rapid development in information and communication technology has produced a new phenomenon called instant messaging. An important element behind this is the protocol which controls the traffic in it. One is the XMPP protocol. The XMPP protocol is the real-time standard communication based text, sound and video used open XML technology. The XMPP protocol has many advantages to offer in developing instant messaging services primarily in terms of the server. Many servers are scattered in the Protocol XMPP, such as Ejabberd and Openfire.
This research was conducted on the comparative analysis of performance on Ejabberd and Openfire server. The comparison includes the examination of activities in the form of registration, online user, online chat and multi-user chat. The result of this study shows the comparison of the server performance when handling user activities in the form of the average value of the CPU usage and memory usage based on the user numbers variation, with the aim to identify patterns of use of the infrastructure for user activity on the server. The results showed that memory usage is done ejabberd lower than Openfire, while for CPU usage workloads do ejabberd higher than Openfire.",,,33459,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81911,,KHAFID AULIA RAHMAN,Analisis Korelasi Pada Data Warehouse Domain Akademik Di Universitas Gadjah Mada,,,"Analysis, Pentaho, Data Warehouse, Data Mining, Correlation, Spearman Rank Correlation, Academic, Students","Arif Nurwidyantoro, M.Cs.",2,3,0,2015,1,"UGM membutuhkan informasi yang terpercaya untuk mendukung pengambilan keputusan. Domain yang menjadi perhatian universitas adalah bidang akademik. Saat ini level eksekutif UGM dalam beberapa hal masih kesulitan dalam memperoleh informasi akademik karena data yang ada masih dalam bentuk terpisah-pisah dan transaksional. Data transaksional organisasi harus diolah terlebih dahulu untuk dapat digunakan sebagai bahan analisis. Salah satu solusi penyajian informasi sebagai bahan analisis adalah dengan data warehouse.
Pada penelitian ini dilakukan proses perancangan dan pembuatan data warehouse UGM untuk domain akademik. Data warehouse yang dihasilkan digunakan sebagai sumber proses analisis. Ada pun analisis yang dilakukan adalah analisis korelasi dengan empat parameter yaitu IPK lulus, lama studi, umur masuk dan umur lulus. Analisis korelasi dilakukan untuk melihat keterkaitan parameter mahasiswa dan melihat arah hubungannya. Data warehouse dan hasil analisis kemudian divisualisasikan agar memudahkan level eksekutif UGM dalam membuat keputusan atau membuat suatu kebijakan.
Dari hasil penelitian yang dilakukan, secara umum lama studi dengan umur lulus dan umur masuk dengan umur lulus memiliki hubungan positif. Hubungan tersebut menunjukkan bahwa di UGM semakin cepat lama studinya semakin cepat umur lulusnya dan semakin cepat umur masuk kuliahnya semakin cepat umur lulusnya. Kemudian untuk IPK lulus dengan lama studi dan IPK lulus dengan umur lulus memiliki hubungan negatif. Hubungan tersebut menunjukkan bahwa di UGM semakin tinggi IPK lulus semakin cepat lama studinya dan umur lulusnya. Sedangkan yang lainnya yaitu IPK lulus dengan umur masuk dan lama studi dengan umur masuk tidak memiliki hubungan yang signifikan. Namun untuk program studi kehutanan, lama studi dengan umur masuk dan lama studi dengan umur lulus memiliki hubungan negatif dikarenakan hanya memiliki lulusan mahasiswa dari angkatan 2010 sehingga tidak merepresentasikan lulusan mahasiswa secara umum.","UGM require reliable information to support decision making. The university is particularly concerned in academic domain. Currently, UGM's executives are still having problems in obtaining academic information, as the data are not integrated with each other and are still in transactional form. Organizations' transactional data needs to be processed before it can be used as an analysis subject. One way to display information as an analysis subject is by using data warehouse. 
This research works on designing and developing UGM's data warehouse for academic domain. The developed data warehouse shall be used as a data source for correlation analysis with four parameters, i.e. final GPA, duration of study, age of enrollment, and age of graduation. Correlation analysis were conducted to see how the parameters of the students and see the direction of relationship. The data warehouse and the analysis result will then be visualized to facilitate UGM's executives to make decisions or policies.
The result shows that, generally, duration of study has a positive correlation with age of graduation and age of enrollment, i.e. a student with a longer duration of study usually has a younger age of graduation and age of enrollment. Final GPA has a negative correlation with duration of study and age of graduation, i.e. a student with higher final GPA usually has a longer duration of study and older age of graduation. However, for the Faculty of Forestry, the duration of study has a negative correlation with the age of enrollment and age of graduation, as currently, its only graduates were enrolled in 2010; therefore, it does not accurately represent the data population.",,,31879,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
81919,,HIMAWAN ADI PRAKOSO,PENERAPAN ALGORITMA GENETIKA UNTUK PROYEKSI POIN PEMAIN PADA FANTASY PREMIER LEAGUE,,,"Fantasy Premier League, Algoritma Genetika, Regresi Linear, Proyeksi Poin","Faizah, S.Kom., M.Kom.",2,3,0,2015,1,"Fantasy Premier League (FPL) adalah sebuah permainan resmi yang diselenggarakan oleh The Football Association, asosiasi sepakbola Inggris, sebagai ajang promosi Liga Primer Inggris. FPL dimainkan dengan cara memilih 15 pemain sepakbola yang bermain di Liga Primer Inggris setiap pekannya dengan aturan tertentu, untuk memaksimalkan poin sesuai dengan kontribusinya pada pertandingan sebenarnya di Liga Primer Inggris. Pada permainan ini, para pemain FPL, yang selanjutnya disebut sebagai manajer FPL, diuji kemampuan analisisnya dalam menentukan pemain-pemain yang diharapkan meraih poin yang tinggi di setiap pertandingan.
	Algoritma genetika merupakan salah satu metode soft computing yang sering digunakan dalam melakukan proyeksi. Pada penelitian ini, digunakan algoritma genetika untuk memproyeksikan poin pemain FPL, dan kemudian dibandingkan akurasinya dengan metode-metode yang sudah ada. Fungsi fitness yang digunakan adalah mean squared error dari pemodelan regresi linear berganda. Terdapat 3 faktor yang dimodelkan sebagai regresi linear berganda, yaitu tingkat kesulitan pertandingan, rata-rata poin pemain FPL dalam 30 hari terakhir, dan rata-rata rating pemain FPL dalam 30 hari terakhir.
	Hasil proyeksi poin FPL dengan implementasi algoritma genetika, jika dibandingkan dengan hasil proyeksi poin FPL dari tools Fix Points Projections, Rate My Team, dan Player Point Forecast, berada di urutan ketiga dengan MSE = 12,25957.","Fantasy Premier League (FPL) is an official game organized by The Football Association, the England football associations, as promotion of the Premier League. FPL is played by selecting the 15 football players who play in the Premier League on a weekly basis with certain rules, to maximize the points according to their contribution to the actual game in the Premier League. In this game, the manager of FPL tested their ability of analysis in determining the players who are expected to reach the high points in every game.
	Genetic algorithm is one method of soft computing are often used in making projections. In this research, the genetic algorithm is used to project the FPL player points, and then compared the accuracy with methods that already exist. Fitness function used is the mean squared error of multiple linear regression modeling. There are three factors that are modeled as linear regression: the level of difficulty of the game, the average points of FPL player in the last 30 days, and the average rating of FPL player in the last 30 days.
	The FPL projection points with the implementation of a genetic algorithm, when compared with the results of the FPL projection points of Fix Points Projections, Rate My Team, and Player Point Forecast tools, is third with MSE = 12,25957.",2015-04-14 00:00:00,53,31874,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
84735,,BINSAR PRAGIWAKSANA,PENDETEKSIAN KERENTANAN SITUS WEB TERHADAP WEB DEFACEMENT DENGAN VULNERABILITY SCANNER DAN INTRUSION DETECTION SYSTEM  (STUDI KASUS SITUS WEB HIMAKOM UGM),,,"Peretasan, web defacement, vulnerability scan, intrusion detection system","Lukman Heryawan, S.T., M.T.",2,3,0,2015,1,"      Peretasan merupakan permasalahan keamanan pada internet berupa serangan yang berpotensi menerobos sistem keamanan suatu situs web. Serangan Web defacement merupakan suatu permasalahan keamanan Internet yang melibatkan perubahan yang tidak diizinkan terhadap situs web. Antisipasi serangan web dilakukan dengan vulnerability scan dan pemasangan sistem keamanan yang diantaranya dapat berupa intrusion detection system, yang bertujuan melindungi sistem dari ancaman yang datang. 
      Penelitian ini melibatkan dua tahap, yaitu vulnerability scan dan intrusion detection. vulnerability scan dilakukan untuk mendeteksi komponen situs web yang rentan untuk diserang dan membuka kemungkinan adanya defacement. Intrusion detection dilakukan untuk mendeteksi penerobosan sistem. 
      Dari hasil percobaan diperoleh bahwa percobaan yang dilakukan pada rentang waktu yang berbeda menghasilkan hasil yang relatif berbeda meskipun setiap percobaan menunjukkan adanya kerentanan situs web Himakom UGM terhadap serangan defacement pada periode percobaan ini dilakukan. Waktu yang digunakan untuk dilakukannya pemeriksaan kerentanan situs web Himakom UGM pada virtual private server terhadap serangan web defacement berada pada kisaran minimum 313,215 detik dan kisaran maksimum 9.996,636 detik.","      Hacking is an Internet security issue in form of attacks that tends to intrude a website's security system. Web defacement is an Internet security problem which involves unauthorized change towards a website. Anticipating web attacks can be done by vulnerability scaning and instalation of security systems which can be in form intrusion detection system, whose purpose is to detect incoming threats headed towards a system. 
      This research involved two steps, which consisted of vulnerability scan, and intrusion detection. Vulnerability scan was done to detect website components vulnerable to attacks and defacement possibility. Intrusion detection was done to detect system intrusions. 
      Test result shows that testing on different time produced relatively different results although every test indicates the presence of Himakom UGM website  vulnerability to web defacement attacks at the time of experiment. Time elapsed for vulnerability check of virtual private server-based Himakom UGM website for web defacement is at minimum around 313,215 seconds and maximum around 9.996,636 seconds.",,,34716,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
78112,,DZIKRI RAHADIAN FUDHOLI,EKSTRAKSI KATA KUNCI BERDASAR FREKUENSI DAN LOKASI DENGAN PERLUASAN N-GRAM PADA DOKUMEN TUNGGAL,,,"ekstraksi, kata kunci, frekuensi, lokasi, perluasan n-gram","Drs. Sri Mulyana, M.Kom",2,3,0,2014,1,"Memahami suatu isi teks diperlukan secara cepat dan tepat dimana keberadaan kata kunci berperan sebagai perwakilan makna dari teks yang panjang. Mendapatkan kata kunci cukup membutuhkan waktu dan bersifat subyektif jika dilakukan manusia, namun jika dilakukan dengan komputer maka akan cepat dan obyektif. Untuk melakukan ekstraksi kata kunci, sebuah komputer memerlukan metode yang bisa diaplikasikan secara umum ke seluruh macam teks.
Metode frekuensi dan lokasi adalah satu dari sekian banyak metode yang memiliki pengaruh besar untuk ekstraksi kata kunci. Kata kunci itu sendiri seharusnya tidak hanya terdiri dari 1 kata dimana metode yang sering digunakan hanya mendapatkan kata kunci dengan 1 kata. Perluasan kata kunci dengan n- gram memberikan kesempatan untuk kata kunci dengan 2 kata atau lebih bisa terpilih menjadi kata kunci. Dengan menggunakan metode frekuensi dan lokasi serta perluasan n-gram yang diaplikasikan ke dokumen tunggal ini bisa mendapatkan kata kunci yang mewakili isi teks.
Dari pengujian diperoleh hasil bahwa metode frekuensi dan lokasi mendapatkan kata kunci dari teks tunggal dengan akurasi 5.5%. Ketika ditambahkan perluasan n-gram akurasi meningkat 10% menjadi 15.5%.","Understanding a text need to be fast and precise where keywords helps to represent the meaning of a long text. Obtaining keywords needs a lot of time and subjective if done by human, however if it done by computer it could be fast and objective. Computer needs methods to do keyword extraction so it could apply to many kind of document.
The frequency and location method is one from many that giving the most influence in extracting keywords. The keywords itself should not be only 1 word but also contain 2 words or more. Therefore n-gram expansion gives the 2 words or more the chance to be choosen as a keyword. Moreover, a method using frequency and location with n-gram expansion will obtains keywords that represent the text from a single document.
The test result shows that frequency and location method successfully extracts keywords from single document with 5.5% accuracy. In addition of n- gram expansion the accuracy rises by 10% to 15.5%.",,,28053,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
78402,,FAUZAN SANDY PRADANA,PENGEMBANGAN APLIKASI WEB CRAWLER MENGGUNAKAN ALGORITMA BREADTH-FIRST SEARCH UNTUK KEPERLUAN PEMETAAN DAN PERANGKINGAN WEB BERBASIS KONTEN,,,"Penjelajah web otomatis, BREADTH-FIRST SEARCH, Pemetaan dan perangkingan web, web crawler, Mapping and web ranking","Mardhani Riasetiawan, S.E., A.k., M.T.",2,3,0,2014,1,"Perkembangan halaman web saat ini sangat pesat dengan bertambah mudahnya akses yang memungkinkan hampir setiap orang dapat membuat halaman web. Dengan banyaknya halaman web, diikuti semakin banyak pula dokumen yang diunggah atau melekat pada halaman web tersebut sehingga dapat dilakukan penghitungan jumlah dokumen yang dimiliki yang hasilnya dapat digunakan sebagai pemetaan halaman web dan perangkingan dengan berbasis konten. Perlu dibuat suatu sistem yang dapat melakukan pencarian secara otomatis yang dapat menjelajah halaman halaman web tersebut dan kemudian mencatat serta menampilkan ke pengguna. Salah satu algoritma pencarian yang dapat digunakan untuk membantu melakukan penjelajahan halaman web ini ialah algoritma BREADTH-FIRST SEARCH.
Penelitian ini berusaha membangun sistem penjelajah halaman web secara otomatis yang menggunakan algoritma BREADTH-FIRST SEARCH. Sistem dikembangan berbasis web yang memudahkan untuk menampilkan hasil dari pencarian. Dalam pengembangannya diperlukan juga filter halaman web yang berguna untuk menyaring halaman web yang dilakukan kunjungan berikutnya, hal ini berguna untuk membuat sistem lebih tepat melakukan kunjungan halaman web.
Hasil keluaran yang diperoleh merupakan jumlah web yang telah dikunjungi secara otomatis oleh sistem dan hasil pencatatan dokumen yang melekat pada halaman web. Hasil kunjungan halaman-halaman web tersebut ditampilkan dalam bentuk diagram / chart. Keluaran ini dapat membantu pengguna untuk melakukan pemetaan dan perangkingan terhadap beberapa halaman web sekaligus tanpa harus mengunjungi halaman web satu persatu dan mendapatkan informasi tautan dokumen yang dimiliki. Halaman web yang dikunjungi merupakan halaman web yang relevan terhadap kata kunci yang dimasukkan yang berguna menghindari halaman yang tidak sesuai dengan halaman yang diinginkan.
","The development of the current web page very rapidly with increasing ease of access that allows almost anyone can create web pages. With so many web pages, followed by the more documents are uploaded or attached to the web page so that it can be done counting the number of documents held that the results can be used as mapping and ranking the web page with content-based. It needs to make a system that can automatically search pages that can crawl the web page and then record and display to the user. One search algorithms that can be used to help carry out the exploration of this web page is breadth-first search.
This study seeks to build the system automatically explorer web page that uses breadth-first search algorithm. To development of web-based system that allows for the display of search results. In developing the necessary also filter web pages that are useful for filtering web pages do next visit, it is useful to make the system more appropriate to visit the web page.
The output obtained is the number of visited web automatically by the system and attached documents recording the results on a webpage. Results visits web pages are displayed in the form of a diagram / chart. This output can help the user to perform mapping and perangkingan against multiple web pages at once without having to visit web pages one by one and get a document link information possessed. Visited web pages are web pages that are relevant to the keywords entered useful to avoid pages that do not conform to the desired page.",,,28342,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
82154,,CAESAR NATANIEL,OPTIMALISASI PENGATURAN IRIGASI WADUK SESUAI KEBUTUHAN AIR DAN KETERSEDIAAN AIR WADUK,,,"Optimalisasi, Debit Input, Debit Output, Debit Curah Hujan, Debit Penguapan, Release, Rasio, Prioritas","Drs. Edi Winarko, M.Sc., Ph.D.",2,3,0,2014,1,"Waduk Sermo merupakan sumber air di wilayah kabupaten Kulon Progo dan sekitarnya, yang dimanfaatkan untuk kebutuhan irigasi air baku, kebutuhan rumah tangga, kebutuhan sentra industri, dan kebutuhan lainnya. Kendala yang dihadapi adalah daya tampung waduk yang semakin menurun, serta kebutuhan air yang semakin meningkat. Oleh karena itu, diperlukan penelitian mengenai optimalisasi pemanfaatan air dari Waduk Sermo, sesuai kebutuhan air dan ketersediaan air waduk, serta sesuai dengan prioritas pengairan, agar dapat mencapai nilai yang optimal.

Sistem optimalisasi pengaturan irigasi air waduk diimplementasikan ke dalam sistem berbasis web. Data yang diolah yaitu debit input, data debit curah hujan, data debit penguapan, dan data debit output berupa data kebutuhan air. Data output yang dihasilkan berupa nilai distribusi release air per periode.

Nilai optimal diperoleh dari nilai kebutuhan debit air per periode, nilai rasio release debit air per periode, nilai tampungan waduk, dan nilai optimalisasi debit release per periode, sesuai dengan nilai debit kebutuhan air dan prioritas pintu pengairan. Nilai optimal diperoleh ketika nilai distribusi release berbanding lurus dengan nilai debit output, dan kebutuhan debit output terpenuhi. Pada saat kebutuhan tinggi, nilai distribusi release dapat mendekati nilai debit output. Sedangkan pada saat kebutuhan rendah, nilai distribusi release harus seminimal mungkin, sehingga sesuai dengan nilai debit output.","Sermo Reservoir is a water source in Kulon Progo District and its surrounding, which is used for raw irrigation needs, household needs, industrial center needs, and other needs. The problem is, the capacity of reservoir is decreased, and water demands are increased. Therefore, research is needed for the optimal use of water from Sermo Reservoir, according to water needs and reservoir water availability, as well as priority of irrigation, in order to achieve the optimal value.

Optimalization of reservoir irrigation management system implemented into a web based system. The processed data are water input flow data, rainfall input flow data, evaporation discharge flow data, and water needs data per period. The resulting data is water discharge release value per period.

The optimal value is obtained from the value of water needs data per period, ratio of water needs per period, dam reservoir value, and optimalization of water discharge release per period, according to the water discharge output and prioritized sluice. The optimal value is obtained when the proportional value of distribution of water release and water discharge output value are met. At the high demand, the value of distribution of water release can approximate meet the value of water discharge output. While at the low demand, the value of distribution of water release should be kept to minimum, according to the water discharge output.",,,32308,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
77302,,FITRIA PRADANA PUTRI,APLIKASI PEMBELAJARAN HURUF KANJI DASAR,,,"aplikasi pembelajaran, kanji, Delphi, MySQL","Drs. Janoe Hendarto, M.Kom.",2,3,0,2014,1,"Mempelajari urutan penulisan merupakan aspek penting dalam menulis
kanji. Namun, aplikasi pembelajaran bahasa pada umumnya tidak memberikan
penjelasan mengenai urutan penulisan kanji. Oleh sebab itu dibutuhkan sebuah
aplikasi pembelajaran yang dapat membantu pengguna dalam menghafal kanji
dengan mempelajari urutan penulisan yang disajikan dalam bentuk animasi.
Aplikasi ini selain dapat melakukan pencarian kanji berdasarkan kata yang
dimasukkan pengguna, juga memiliki fasilitas tambah data dan latihan membaca
dan menulis kanji. Aplikasi dibangun menggunakan Delphi sebagai front-end dan
MySQL sebagai server basis data.
Pengujian dilakukan untuk memastikan aplikasi berfungsi sesuai dengan
spesifikasi kebutuhan, di antaranya pengujian kesesuaian hasil pencarian, konversi
file image ke dalam atribut blob, dan proses validasi isian.
","Learning stroke order is an important aspect on kanji writing. However,
there is no explanation about kanji stroke order on common language learning
application. Therefore a learning application is needed to help user memorizing
kanji by learning stroke order in animated form.
Besides searching kanji feature based on user input, this application has
data update feature and kanji reading and writing exercises as well. This
application is built using Delphi as front-end and MySQL as database server.
Several tests are given to this application to know whether its performance
is compatible with its requirements, including the compatibility of searching
result, the conversion of image file into blob field, and the authorizing process of
form filling.
",,,27226,,,,,,,,,5236,1,,,,,,,,,,1,Universitas Gadjah Mada,Yogyakarta,3,Skripsi,5236,11,S1 ILMU KOMPUTER
